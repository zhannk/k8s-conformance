Conformance test: not doing test setup.
I1214 07:59:54.527124    4635 e2e.go:116] Starting e2e run "3532263e-47c3-4cea-870e-3d3ec39ba8c0" on Ginkgo node 1
Dec 14 07:59:54.539: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /go/src/k8s.io/kubernetes/platforms/linux/amd64
=====================================================================================
Random Seed: 1671004794 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Dec 14 07:59:54.632: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 07:59:54.633: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 14 07:59:54.654: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Dec 14 07:59:54.697: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 14 07:59:54.697: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Dec 14 07:59:54.697: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.25.4' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Dec 14 07:59:54.709: INFO: e2e test version: v1.25.4
Dec 14 07:59:54.711: INFO: kube-apiserver version: v1.25.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Dec 14 07:59:54.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 07:59:54.716: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.084 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Dec 14 07:59:54.632: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 07:59:54.633: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Dec 14 07:59:54.654: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
    Dec 14 07:59:54.697: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Dec 14 07:59:54.697: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
    Dec 14 07:59:54.697: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.25.4' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Dec 14 07:59:54.709: INFO: e2e test version: v1.25.4
    Dec 14 07:59:54.711: INFO: kube-apiserver version: v1.25.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Dec 14 07:59:54.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 07:59:54.716: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 07:59:54.741
Dec 14 07:59:54.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 07:59:54.742
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 07:59:54.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 07:59:54.76
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Dec 14 07:59:54.780: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 12/14/22 07:59:54.784
Dec 14 07:59:54.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 07:59:54.787: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 12/14/22 07:59:54.787
Dec 14 07:59:54.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 07:59:54.807: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 07:59:55.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 07:59:55.812: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 07:59:56.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 07:59:56.813: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 07:59:57.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 07:59:57.813: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 07:59:58.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 07:59:58.814: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 07:59:59.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 07:59:59.812: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 12/14/22 07:59:59.815
Dec 14 07:59:59.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 07:59:59.833: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Dec 14 08:00:00.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:00.838: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/14/22 08:00:00.838
Dec 14 08:00:00.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:00.847: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 08:00:01.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:01.855: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 08:00:02.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:02.852: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 08:00:03.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:00:03.851: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:00:03.858
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5437, will wait for the garbage collector to delete the pods 12/14/22 08:00:03.858
Dec 14 08:00:03.917: INFO: Deleting DaemonSet.extensions daemon-set took: 5.225176ms
Dec 14 08:00:04.018: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.736864ms
Dec 14 08:00:06.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:06.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 08:00:06.130: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4524"},"items":null}

Dec 14 08:00:06.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4524"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:00:06.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5437" for this suite. 12/14/22 08:00:06.163
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":1,"skipped":8,"failed":0}
------------------------------
• [11.427 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 07:59:54.741
    Dec 14 07:59:54.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 07:59:54.742
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 07:59:54.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 07:59:54.76
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Dec 14 07:59:54.780: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 12/14/22 07:59:54.784
    Dec 14 07:59:54.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 07:59:54.787: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 12/14/22 07:59:54.787
    Dec 14 07:59:54.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 07:59:54.807: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 07:59:55.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 07:59:55.812: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 07:59:56.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 07:59:56.813: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 07:59:57.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 07:59:57.813: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 07:59:58.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 07:59:58.814: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 07:59:59.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 07:59:59.812: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 12/14/22 07:59:59.815
    Dec 14 07:59:59.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 07:59:59.833: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Dec 14 08:00:00.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:00.838: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/14/22 08:00:00.838
    Dec 14 08:00:00.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:00.847: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 08:00:01.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:01.855: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 08:00:02.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:02.852: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 08:00:03.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:00:03.851: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:00:03.858
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5437, will wait for the garbage collector to delete the pods 12/14/22 08:00:03.858
    Dec 14 08:00:03.917: INFO: Deleting DaemonSet.extensions daemon-set took: 5.225176ms
    Dec 14 08:00:04.018: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.736864ms
    Dec 14 08:00:06.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:06.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 08:00:06.130: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4524"},"items":null}

    Dec 14 08:00:06.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4524"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:00:06.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5437" for this suite. 12/14/22 08:00:06.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:06.17
Dec 14 08:00:06.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:00:06.171
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:06.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:06.187
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7207 12/14/22 08:00:06.193
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 08:00:06.208
STEP: creating service externalsvc in namespace services-7207 12/14/22 08:00:06.209
STEP: creating replication controller externalsvc in namespace services-7207 12/14/22 08:00:06.232
I1214 08:00:06.239466    4635 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7207, replica count: 2
I1214 08:00:09.291537    4635 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 08:00:12.291731    4635 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 12/14/22 08:00:12.295
Dec 14 08:00:12.317: INFO: Creating new exec pod
Dec 14 08:00:12.344: INFO: Waiting up to 5m0s for pod "execpodnh9mw" in namespace "services-7207" to be "running"
Dec 14 08:00:12.348: INFO: Pod "execpodnh9mw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.366648ms
Dec 14 08:00:14.354: INFO: Pod "execpodnh9mw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009886145s
Dec 14 08:00:14.354: INFO: Pod "execpodnh9mw" satisfied condition "running"
Dec 14 08:00:14.354: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7207 exec execpodnh9mw -- /bin/sh -x -c nslookup nodeport-service.services-7207.svc.cluster.local'
Dec 14 08:00:14.767: INFO: stderr: "+ nslookup nodeport-service.services-7207.svc.cluster.local\n"
Dec 14 08:00:14.767: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-7207.svc.cluster.local\tcanonical name = externalsvc.services-7207.svc.cluster.local.\nName:\texternalsvc.services-7207.svc.cluster.local\nAddress: 100.108.201.205\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7207, will wait for the garbage collector to delete the pods 12/14/22 08:00:14.767
Dec 14 08:00:14.827: INFO: Deleting ReplicationController externalsvc took: 6.020325ms
Dec 14 08:00:14.928: INFO: Terminating ReplicationController externalsvc pods took: 100.962889ms
Dec 14 08:00:17.140: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 08:00:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7207" for this suite. 12/14/22 08:00:17.152
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":2,"skipped":35,"failed":0}
------------------------------
• [10.987 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:06.17
    Dec 14 08:00:06.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:00:06.171
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:06.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:06.187
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7207 12/14/22 08:00:06.193
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 08:00:06.208
    STEP: creating service externalsvc in namespace services-7207 12/14/22 08:00:06.209
    STEP: creating replication controller externalsvc in namespace services-7207 12/14/22 08:00:06.232
    I1214 08:00:06.239466    4635 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7207, replica count: 2
    I1214 08:00:09.291537    4635 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 08:00:12.291731    4635 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 12/14/22 08:00:12.295
    Dec 14 08:00:12.317: INFO: Creating new exec pod
    Dec 14 08:00:12.344: INFO: Waiting up to 5m0s for pod "execpodnh9mw" in namespace "services-7207" to be "running"
    Dec 14 08:00:12.348: INFO: Pod "execpodnh9mw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.366648ms
    Dec 14 08:00:14.354: INFO: Pod "execpodnh9mw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009886145s
    Dec 14 08:00:14.354: INFO: Pod "execpodnh9mw" satisfied condition "running"
    Dec 14 08:00:14.354: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7207 exec execpodnh9mw -- /bin/sh -x -c nslookup nodeport-service.services-7207.svc.cluster.local'
    Dec 14 08:00:14.767: INFO: stderr: "+ nslookup nodeport-service.services-7207.svc.cluster.local\n"
    Dec 14 08:00:14.767: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-7207.svc.cluster.local\tcanonical name = externalsvc.services-7207.svc.cluster.local.\nName:\texternalsvc.services-7207.svc.cluster.local\nAddress: 100.108.201.205\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7207, will wait for the garbage collector to delete the pods 12/14/22 08:00:14.767
    Dec 14 08:00:14.827: INFO: Deleting ReplicationController externalsvc took: 6.020325ms
    Dec 14 08:00:14.928: INFO: Terminating ReplicationController externalsvc pods took: 100.962889ms
    Dec 14 08:00:17.140: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 08:00:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7207" for this suite. 12/14/22 08:00:17.152
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:17.159
Dec 14 08:00:17.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:00:17.159
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:17.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:17.173
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:00:17.177
Dec 14 08:00:17.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3" in namespace "projected-8164" to be "Succeeded or Failed"
Dec 14 08:00:17.191: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049534ms
Dec 14 08:00:19.197: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00886537s
Dec 14 08:00:21.196: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00772055s
STEP: Saw pod success 12/14/22 08:00:21.196
Dec 14 08:00:21.196: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3" satisfied condition "Succeeded or Failed"
Dec 14 08:00:21.199: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3 container client-container: <nil>
STEP: delete the pod 12/14/22 08:00:21.213
Dec 14 08:00:21.220: INFO: Waiting for pod downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3 to disappear
Dec 14 08:00:21.223: INFO: Pod downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 08:00:21.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8164" for this suite. 12/14/22 08:00:21.228
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":3,"skipped":91,"failed":0}
------------------------------
• [4.073 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:17.159
    Dec 14 08:00:17.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:00:17.159
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:17.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:17.173
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:00:17.177
    Dec 14 08:00:17.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3" in namespace "projected-8164" to be "Succeeded or Failed"
    Dec 14 08:00:17.191: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049534ms
    Dec 14 08:00:19.197: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00886537s
    Dec 14 08:00:21.196: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00772055s
    STEP: Saw pod success 12/14/22 08:00:21.196
    Dec 14 08:00:21.196: INFO: Pod "downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3" satisfied condition "Succeeded or Failed"
    Dec 14 08:00:21.199: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3 container client-container: <nil>
    STEP: delete the pod 12/14/22 08:00:21.213
    Dec 14 08:00:21.220: INFO: Waiting for pod downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3 to disappear
    Dec 14 08:00:21.223: INFO: Pod downwardapi-volume-df668f2b-93de-4cd9-ba66-9e71d72a3ec3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 08:00:21.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8164" for this suite. 12/14/22 08:00:21.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:21.233
Dec 14 08:00:21.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 12/14/22 08:00:21.234
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:21.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:21.248
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 12/14/22 08:00:21.253
STEP: get a list of Events with a label in the current namespace 12/14/22 08:00:21.267
STEP: delete a list of events 12/14/22 08:00:21.271
Dec 14 08:00:21.271: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/14/22 08:00:21.282
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Dec 14 08:00:21.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6963" for this suite. 12/14/22 08:00:21.289
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":4,"skipped":102,"failed":0}
------------------------------
• [0.062 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:21.233
    Dec 14 08:00:21.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 12/14/22 08:00:21.234
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:21.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:21.248
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 12/14/22 08:00:21.253
    STEP: get a list of Events with a label in the current namespace 12/14/22 08:00:21.267
    STEP: delete a list of events 12/14/22 08:00:21.271
    Dec 14 08:00:21.271: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/14/22 08:00:21.282
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Dec 14 08:00:21.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6963" for this suite. 12/14/22 08:00:21.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:21.296
Dec 14 08:00:21.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:00:21.297
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:21.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:21.314
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 08:00:21.319
Dec 14 08:00:21.332: INFO: Waiting up to 5m0s for pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62" in namespace "emptydir-1663" to be "Succeeded or Failed"
Dec 14 08:00:21.338: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62": Phase="Pending", Reason="", readiness=false. Elapsed: 5.303214ms
Dec 14 08:00:23.343: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01092941s
Dec 14 08:00:25.343: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010909425s
STEP: Saw pod success 12/14/22 08:00:25.343
Dec 14 08:00:25.343: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62" satisfied condition "Succeeded or Failed"
Dec 14 08:00:25.347: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62 container test-container: <nil>
STEP: delete the pod 12/14/22 08:00:25.354
Dec 14 08:00:25.386: INFO: Waiting for pod pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62 to disappear
Dec 14 08:00:25.427: INFO: Pod pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:00:25.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1663" for this suite. 12/14/22 08:00:25.433
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":124,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:21.296
    Dec 14 08:00:21.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:00:21.297
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:21.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:21.314
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 08:00:21.319
    Dec 14 08:00:21.332: INFO: Waiting up to 5m0s for pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62" in namespace "emptydir-1663" to be "Succeeded or Failed"
    Dec 14 08:00:21.338: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62": Phase="Pending", Reason="", readiness=false. Elapsed: 5.303214ms
    Dec 14 08:00:23.343: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01092941s
    Dec 14 08:00:25.343: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010909425s
    STEP: Saw pod success 12/14/22 08:00:25.343
    Dec 14 08:00:25.343: INFO: Pod "pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62" satisfied condition "Succeeded or Failed"
    Dec 14 08:00:25.347: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:00:25.354
    Dec 14 08:00:25.386: INFO: Waiting for pod pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62 to disappear
    Dec 14 08:00:25.427: INFO: Pod pod-6253a8a8-e9fe-4b9b-bb60-6c40ed688b62 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:00:25.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1663" for this suite. 12/14/22 08:00:25.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:25.439
Dec 14 08:00:25.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 08:00:25.44
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:25.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:25.455
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 12/14/22 08:00:25.459
Dec 14 08:00:25.469: INFO: Waiting up to 5m0s for pod "pod-26tvw" in namespace "pods-8438" to be "running"
Dec 14 08:00:25.472: INFO: Pod "pod-26tvw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025732ms
Dec 14 08:00:27.477: INFO: Pod "pod-26tvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008508338s
Dec 14 08:00:27.477: INFO: Pod "pod-26tvw" satisfied condition "running"
STEP: patching /status 12/14/22 08:00:27.477
Dec 14 08:00:27.484: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 08:00:27.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8438" for this suite. 12/14/22 08:00:27.489
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":6,"skipped":133,"failed":0}
------------------------------
• [2.055 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:25.439
    Dec 14 08:00:25.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 08:00:25.44
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:25.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:25.455
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 12/14/22 08:00:25.459
    Dec 14 08:00:25.469: INFO: Waiting up to 5m0s for pod "pod-26tvw" in namespace "pods-8438" to be "running"
    Dec 14 08:00:25.472: INFO: Pod "pod-26tvw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025732ms
    Dec 14 08:00:27.477: INFO: Pod "pod-26tvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008508338s
    Dec 14 08:00:27.477: INFO: Pod "pod-26tvw" satisfied condition "running"
    STEP: patching /status 12/14/22 08:00:27.477
    Dec 14 08:00:27.484: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 08:00:27.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8438" for this suite. 12/14/22 08:00:27.489
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:27.494
Dec 14 08:00:27.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 12/14/22 08:00:27.495
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:27.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:27.534
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Dec 14 08:00:27.548: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179" in namespace "security-context-test-3237" to be "Succeeded or Failed"
Dec 14 08:00:27.552: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976139ms
Dec 14 08:00:29.557: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008984672s
Dec 14 08:00:31.703: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154845508s
Dec 14 08:00:33.572: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024259976s
Dec 14 08:00:33.572: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 14 08:00:33.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3237" for this suite. 12/14/22 08:00:33.587
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":7,"skipped":133,"failed":0}
------------------------------
• [6.098 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:27.494
    Dec 14 08:00:27.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 12/14/22 08:00:27.495
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:27.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:27.534
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Dec 14 08:00:27.548: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179" in namespace "security-context-test-3237" to be "Succeeded or Failed"
    Dec 14 08:00:27.552: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976139ms
    Dec 14 08:00:29.557: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008984672s
    Dec 14 08:00:31.703: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154845508s
    Dec 14 08:00:33.572: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024259976s
    Dec 14 08:00:33.572: INFO: Pod "alpine-nnp-false-97a59f46-ba48-4b8a-a4a4-a05430dde179" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 14 08:00:33.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3237" for this suite. 12/14/22 08:00:33.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:33.593
Dec 14 08:00:33.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:00:33.594
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:33.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:33.613
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:00:33.618
Dec 14 08:00:33.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb" in namespace "projected-2371" to be "Succeeded or Failed"
Dec 14 08:00:33.632: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121797ms
Dec 14 08:00:35.638: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010114789s
Dec 14 08:00:37.638: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010083721s
STEP: Saw pod success 12/14/22 08:00:37.638
Dec 14 08:00:37.638: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb" satisfied condition "Succeeded or Failed"
Dec 14 08:00:37.642: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb container client-container: <nil>
STEP: delete the pod 12/14/22 08:00:37.652
Dec 14 08:00:37.659: INFO: Waiting for pod downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb to disappear
Dec 14 08:00:37.663: INFO: Pod downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 08:00:37.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2371" for this suite. 12/14/22 08:00:37.667
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":8,"skipped":144,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:33.593
    Dec 14 08:00:33.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:00:33.594
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:33.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:33.613
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:00:33.618
    Dec 14 08:00:33.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb" in namespace "projected-2371" to be "Succeeded or Failed"
    Dec 14 08:00:33.632: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121797ms
    Dec 14 08:00:35.638: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010114789s
    Dec 14 08:00:37.638: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010083721s
    STEP: Saw pod success 12/14/22 08:00:37.638
    Dec 14 08:00:37.638: INFO: Pod "downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb" satisfied condition "Succeeded or Failed"
    Dec 14 08:00:37.642: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb container client-container: <nil>
    STEP: delete the pod 12/14/22 08:00:37.652
    Dec 14 08:00:37.659: INFO: Waiting for pod downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb to disappear
    Dec 14 08:00:37.663: INFO: Pod downwardapi-volume-f33571f3-180b-400e-a204-e231df3bd1eb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 08:00:37.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2371" for this suite. 12/14/22 08:00:37.667
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:37.671
Dec 14 08:00:37.672: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:00:37.672
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:37.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:37.687
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:00:37.697
Dec 14 08:00:37.705: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9562" to be "running and ready"
Dec 14 08:00:37.709: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91168ms
Dec 14 08:00:37.709: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:00:39.716: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010864343s
Dec 14 08:00:39.716: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 08:00:39.716: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 12/14/22 08:00:39.72
Dec 14 08:00:39.728: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9562" to be "running and ready"
Dec 14 08:00:39.731: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830566ms
Dec 14 08:00:39.731: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:00:41.737: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008535249s
Dec 14 08:00:41.737: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Dec 14 08:00:41.737: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/14/22 08:00:41.74
Dec 14 08:00:41.746: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 08:00:41.749: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 08:00:43.749: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 08:00:43.754: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 08:00:45.749: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 08:00:45.754: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 12/14/22 08:00:45.754
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 14 08:00:45.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9562" for this suite. 12/14/22 08:00:45.768
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":9,"skipped":144,"failed":0}
------------------------------
• [8.101 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:37.671
    Dec 14 08:00:37.672: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:00:37.672
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:37.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:37.687
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:00:37.697
    Dec 14 08:00:37.705: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9562" to be "running and ready"
    Dec 14 08:00:37.709: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91168ms
    Dec 14 08:00:37.709: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:00:39.716: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010864343s
    Dec 14 08:00:39.716: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 08:00:39.716: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 12/14/22 08:00:39.72
    Dec 14 08:00:39.728: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9562" to be "running and ready"
    Dec 14 08:00:39.731: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830566ms
    Dec 14 08:00:39.731: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:00:41.737: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008535249s
    Dec 14 08:00:41.737: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Dec 14 08:00:41.737: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/14/22 08:00:41.74
    Dec 14 08:00:41.746: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 14 08:00:41.749: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 14 08:00:43.749: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 14 08:00:43.754: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 14 08:00:45.749: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 14 08:00:45.754: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 12/14/22 08:00:45.754
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 14 08:00:45.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9562" for this suite. 12/14/22 08:00:45.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:00:45.774
Dec 14 08:00:45.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 08:00:45.775
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:45.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:45.79
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Dec 14 08:00:45.814: INFO: Create a RollingUpdate DaemonSet
Dec 14 08:00:45.818: INFO: Check that daemon pods launch on every node of the cluster
Dec 14 08:00:45.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:45.826: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:00:46.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:00:46.836: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:00:47.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:00:47.836: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:00:48.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:00:48.837: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:00:49.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 08:00:49.836: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Dec 14 08:00:49.836: INFO: Update the DaemonSet to trigger a rollout
Dec 14 08:00:49.845: INFO: Updating DaemonSet daemon-set
Dec 14 08:00:52.884: INFO: Roll back the DaemonSet before rollout is complete
Dec 14 08:00:52.894: INFO: Updating DaemonSet daemon-set
Dec 14 08:00:52.894: INFO: Make sure DaemonSet rollback is complete
Dec 14 08:00:52.905: INFO: Wrong image for pod: daemon-set-hm6ff. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Dec 14 08:00:52.905: INFO: Pod daemon-set-hm6ff is not available
Dec 14 08:00:55.915: INFO: Pod daemon-set-8mctp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:00:55.927
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4477, will wait for the garbage collector to delete the pods 12/14/22 08:00:55.927
Dec 14 08:00:55.986: INFO: Deleting DaemonSet.extensions daemon-set took: 5.467028ms
Dec 14 08:01:01.087: INFO: Terminating DaemonSet.extensions daemon-set pods took: 5.100957471s
Dec 14 08:01:02.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:01:02.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 08:01:02.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5085"},"items":null}

Dec 14 08:01:02.299: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5086"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:01:02.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4477" for this suite. 12/14/22 08:01:02.314
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":10,"skipped":174,"failed":0}
------------------------------
• [16.544 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:00:45.774
    Dec 14 08:00:45.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 08:00:45.775
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:00:45.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:00:45.79
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Dec 14 08:00:45.814: INFO: Create a RollingUpdate DaemonSet
    Dec 14 08:00:45.818: INFO: Check that daemon pods launch on every node of the cluster
    Dec 14 08:00:45.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:45.826: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:00:46.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:00:46.836: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:00:47.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:00:47.836: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:00:48.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:00:48.837: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:00:49.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 08:00:49.836: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Dec 14 08:00:49.836: INFO: Update the DaemonSet to trigger a rollout
    Dec 14 08:00:49.845: INFO: Updating DaemonSet daemon-set
    Dec 14 08:00:52.884: INFO: Roll back the DaemonSet before rollout is complete
    Dec 14 08:00:52.894: INFO: Updating DaemonSet daemon-set
    Dec 14 08:00:52.894: INFO: Make sure DaemonSet rollback is complete
    Dec 14 08:00:52.905: INFO: Wrong image for pod: daemon-set-hm6ff. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Dec 14 08:00:52.905: INFO: Pod daemon-set-hm6ff is not available
    Dec 14 08:00:55.915: INFO: Pod daemon-set-8mctp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:00:55.927
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4477, will wait for the garbage collector to delete the pods 12/14/22 08:00:55.927
    Dec 14 08:00:55.986: INFO: Deleting DaemonSet.extensions daemon-set took: 5.467028ms
    Dec 14 08:01:01.087: INFO: Terminating DaemonSet.extensions daemon-set pods took: 5.100957471s
    Dec 14 08:01:02.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:01:02.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 08:01:02.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5085"},"items":null}

    Dec 14 08:01:02.299: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5086"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:01:02.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4477" for this suite. 12/14/22 08:01:02.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:01:02.319
Dec 14 08:01:02.319: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:01:02.32
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:01:02.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:01:02.335
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-27dc40d7-e3ea-4153-893b-ee2ee58281df 12/14/22 08:01:02.343
STEP: Creating secret with name s-test-opt-upd-23ecea3d-1858-42f8-8c45-f25d91f11da1 12/14/22 08:01:02.347
STEP: Creating the pod 12/14/22 08:01:02.351
Dec 14 08:01:02.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06" in namespace "projected-1981" to be "running and ready"
Dec 14 08:01:02.366: INFO: Pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008273ms
Dec 14 08:01:02.366: INFO: The phase of Pod pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:01:04.372: INFO: Pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06": Phase="Running", Reason="", readiness=true. Elapsed: 2.009132557s
Dec 14 08:01:04.372: INFO: The phase of Pod pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06 is Running (Ready = true)
Dec 14 08:01:04.372: INFO: Pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-27dc40d7-e3ea-4153-893b-ee2ee58281df 12/14/22 08:01:29.036
STEP: Updating secret s-test-opt-upd-23ecea3d-1858-42f8-8c45-f25d91f11da1 12/14/22 08:01:29.042
STEP: Creating secret with name s-test-opt-create-55a43bed-44ba-48a5-bea1-605e9f109382 12/14/22 08:01:29.046
STEP: waiting to observe update in volume 12/14/22 08:01:29.05
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 08:02:19.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1981" for this suite. 12/14/22 08:02:19.537
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":11,"skipped":185,"failed":0}
------------------------------
• [77.223 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:01:02.319
    Dec 14 08:01:02.319: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:01:02.32
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:01:02.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:01:02.335
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-27dc40d7-e3ea-4153-893b-ee2ee58281df 12/14/22 08:01:02.343
    STEP: Creating secret with name s-test-opt-upd-23ecea3d-1858-42f8-8c45-f25d91f11da1 12/14/22 08:01:02.347
    STEP: Creating the pod 12/14/22 08:01:02.351
    Dec 14 08:01:02.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06" in namespace "projected-1981" to be "running and ready"
    Dec 14 08:01:02.366: INFO: Pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008273ms
    Dec 14 08:01:02.366: INFO: The phase of Pod pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:01:04.372: INFO: Pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06": Phase="Running", Reason="", readiness=true. Elapsed: 2.009132557s
    Dec 14 08:01:04.372: INFO: The phase of Pod pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06 is Running (Ready = true)
    Dec 14 08:01:04.372: INFO: Pod "pod-projected-secrets-3c287160-0aa3-411f-922d-ac85e9207c06" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-27dc40d7-e3ea-4153-893b-ee2ee58281df 12/14/22 08:01:29.036
    STEP: Updating secret s-test-opt-upd-23ecea3d-1858-42f8-8c45-f25d91f11da1 12/14/22 08:01:29.042
    STEP: Creating secret with name s-test-opt-create-55a43bed-44ba-48a5-bea1-605e9f109382 12/14/22 08:01:29.046
    STEP: waiting to observe update in volume 12/14/22 08:01:29.05
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 08:02:19.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1981" for this suite. 12/14/22 08:02:19.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:02:19.542
Dec 14 08:02:19.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:02:19.543
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:19.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:19.569
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Dec 14 08:02:19.574: INFO: Creating simple deployment test-new-deployment
Dec 14 08:02:19.585: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 12/14/22 08:02:21.601
STEP: updating a scale subresource 12/14/22 08:02:21.604
STEP: verifying the deployment Spec.Replicas was modified 12/14/22 08:02:21.61
STEP: Patch a scale subresource 12/14/22 08:02:21.614
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:02:21.628: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9635  9eef3b22-5cfe-4856-b95c-3cb17cb87308 5572 3 2022-12-14 08:02:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-14 08:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003727218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 08:02:20 +0000 UTC,LastTransitionTime:2022-12-14 08:02:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-12-14 08:02:20 +0000 UTC,LastTransitionTime:2022-12-14 08:02:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 08:02:21.632: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9635  7a3bf5c0-44eb-4415-b20b-f42bc5c928b8 5576 2 2022-12-14 08:02:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9eef3b22-5cfe-4856-b95c-3cb17cb87308 0xc002b75ea0 0xc002b75ea1}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eef3b22-5cfe-4856-b95c-3cb17cb87308\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:02:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b75f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:02:21.637: INFO: Pod "test-new-deployment-845c8977d9-qpvj7" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-qpvj7 test-new-deployment-845c8977d9- deployment-9635  70ff6164-8671-47e1-bd3b-023100ea7e56 5557 0 2022-12-14 08:02:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a6603bf6601acda3694b27b1f35fffcd2c1159d9e473b740ebe991e49088ea27 cni.projectcalico.org/podIP:100.64.1.22/32 cni.projectcalico.org/podIPs:100.64.1.22/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7a3bf5c0-44eb-4415-b20b-f42bc5c928b8 0xc003727680 0xc003727681}] [] [{kube-controller-manager Update v1 2022-12-14 08:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a3bf5c0-44eb-4415-b20b-f42bc5c928b8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:02:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zt6wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zt6wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.22,StartTime:2022-12-14 08:02:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:02:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f2b8527311bc883b726028d73a885a65ff82d128af3575cce3003b8c9dd1c3c5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:02:21.637: INFO: Pod "test-new-deployment-845c8977d9-tntbh" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-tntbh test-new-deployment-845c8977d9- deployment-9635  e696b051-d01a-48f1-a712-548e3708e383 5575 0 2022-12-14 08:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7a3bf5c0-44eb-4415-b20b-f42bc5c928b8 0xc003727890 0xc003727891}] [] [{kube-controller-manager Update v1 2022-12-14 08:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a3bf5c0-44eb-4415-b20b-f42bc5c928b8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp4hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp4hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:02:21.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9635" for this suite. 12/14/22 08:02:21.642
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":12,"skipped":196,"failed":0}
------------------------------
• [2.113 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:02:19.542
    Dec 14 08:02:19.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:02:19.543
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:19.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:19.569
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Dec 14 08:02:19.574: INFO: Creating simple deployment test-new-deployment
    Dec 14 08:02:19.585: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 12/14/22 08:02:21.601
    STEP: updating a scale subresource 12/14/22 08:02:21.604
    STEP: verifying the deployment Spec.Replicas was modified 12/14/22 08:02:21.61
    STEP: Patch a scale subresource 12/14/22 08:02:21.614
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:02:21.628: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9635  9eef3b22-5cfe-4856-b95c-3cb17cb87308 5572 3 2022-12-14 08:02:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-14 08:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003727218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 08:02:20 +0000 UTC,LastTransitionTime:2022-12-14 08:02:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-12-14 08:02:20 +0000 UTC,LastTransitionTime:2022-12-14 08:02:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 08:02:21.632: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9635  7a3bf5c0-44eb-4415-b20b-f42bc5c928b8 5576 2 2022-12-14 08:02:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9eef3b22-5cfe-4856-b95c-3cb17cb87308 0xc002b75ea0 0xc002b75ea1}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eef3b22-5cfe-4856-b95c-3cb17cb87308\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:02:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b75f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:02:21.637: INFO: Pod "test-new-deployment-845c8977d9-qpvj7" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-qpvj7 test-new-deployment-845c8977d9- deployment-9635  70ff6164-8671-47e1-bd3b-023100ea7e56 5557 0 2022-12-14 08:02:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a6603bf6601acda3694b27b1f35fffcd2c1159d9e473b740ebe991e49088ea27 cni.projectcalico.org/podIP:100.64.1.22/32 cni.projectcalico.org/podIPs:100.64.1.22/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7a3bf5c0-44eb-4415-b20b-f42bc5c928b8 0xc003727680 0xc003727681}] [] [{kube-controller-manager Update v1 2022-12-14 08:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a3bf5c0-44eb-4415-b20b-f42bc5c928b8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:02:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zt6wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zt6wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.22,StartTime:2022-12-14 08:02:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:02:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f2b8527311bc883b726028d73a885a65ff82d128af3575cce3003b8c9dd1c3c5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:02:21.637: INFO: Pod "test-new-deployment-845c8977d9-tntbh" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-tntbh test-new-deployment-845c8977d9- deployment-9635  e696b051-d01a-48f1-a712-548e3708e383 5575 0 2022-12-14 08:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7a3bf5c0-44eb-4415-b20b-f42bc5c928b8 0xc003727890 0xc003727891}] [] [{kube-controller-manager Update v1 2022-12-14 08:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a3bf5c0-44eb-4415-b20b-f42bc5c928b8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp4hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp4hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:02:21.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9635" for this suite. 12/14/22 08:02:21.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:02:21.656
Dec 14 08:02:21.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 08:02:21.657
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:21.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:21.675
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Dec 14 08:02:21.691: INFO: Waiting up to 2m0s for pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" in namespace "var-expansion-5529" to be "container 0 failed with reason CreateContainerConfigError"
Dec 14 08:02:21.695: INFO: Pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968401ms
Dec 14 08:02:23.700: INFO: Pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009781531s
Dec 14 08:02:23.700: INFO: Pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 14 08:02:23.700: INFO: Deleting pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" in namespace "var-expansion-5529"
Dec 14 08:02:23.706: INFO: Wait up to 5m0s for pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 08:02:25.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5529" for this suite. 12/14/22 08:02:25.72
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":13,"skipped":217,"failed":0}
------------------------------
• [4.069 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:02:21.656
    Dec 14 08:02:21.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 08:02:21.657
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:21.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:21.675
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Dec 14 08:02:21.691: INFO: Waiting up to 2m0s for pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" in namespace "var-expansion-5529" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 14 08:02:21.695: INFO: Pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968401ms
    Dec 14 08:02:23.700: INFO: Pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009781531s
    Dec 14 08:02:23.700: INFO: Pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 14 08:02:23.700: INFO: Deleting pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" in namespace "var-expansion-5529"
    Dec 14 08:02:23.706: INFO: Wait up to 5m0s for pod "var-expansion-b9d11854-a1b3-4acf-aa0a-e78853e0a337" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 08:02:25.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5529" for this suite. 12/14/22 08:02:25.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:02:25.726
Dec 14 08:02:25.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:02:25.727
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:25.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:25.746
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:02:25.751
Dec 14 08:02:25.763: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1" in namespace "projected-7237" to be "Succeeded or Failed"
Dec 14 08:02:25.766: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64727ms
Dec 14 08:02:27.771: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1": Phase="Running", Reason="", readiness=false. Elapsed: 2.00824908s
Dec 14 08:02:29.771: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008855264s
STEP: Saw pod success 12/14/22 08:02:29.772
Dec 14 08:02:29.772: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1" satisfied condition "Succeeded or Failed"
Dec 14 08:02:29.775: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1 container client-container: <nil>
STEP: delete the pod 12/14/22 08:02:29.824
Dec 14 08:02:29.832: INFO: Waiting for pod downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1 to disappear
Dec 14 08:02:29.835: INFO: Pod downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 08:02:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7237" for this suite. 12/14/22 08:02:29.84
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":14,"skipped":241,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:02:25.726
    Dec 14 08:02:25.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:02:25.727
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:25.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:25.746
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:02:25.751
    Dec 14 08:02:25.763: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1" in namespace "projected-7237" to be "Succeeded or Failed"
    Dec 14 08:02:25.766: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64727ms
    Dec 14 08:02:27.771: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1": Phase="Running", Reason="", readiness=false. Elapsed: 2.00824908s
    Dec 14 08:02:29.771: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008855264s
    STEP: Saw pod success 12/14/22 08:02:29.772
    Dec 14 08:02:29.772: INFO: Pod "downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1" satisfied condition "Succeeded or Failed"
    Dec 14 08:02:29.775: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1 container client-container: <nil>
    STEP: delete the pod 12/14/22 08:02:29.824
    Dec 14 08:02:29.832: INFO: Waiting for pod downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1 to disappear
    Dec 14 08:02:29.835: INFO: Pod downwardapi-volume-99a6e3af-db73-4bd2-bd4e-a3fb88450ec1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 08:02:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7237" for this suite. 12/14/22 08:02:29.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:02:29.845
Dec 14 08:02:29.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods 12/14/22 08:02:29.846
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:29.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:29.874
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Dec 14 08:02:29.879: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 08:03:29.917: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Dec 14 08:03:29.921: INFO: Starting informer...
STEP: Starting pods... 12/14/22 08:03:29.921
Dec 14 08:03:30.144: INFO: Pod1 is running on shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb. Tainting Node
Dec 14 08:03:30.358: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9317" to be "running"
Dec 14 08:03:30.362: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968593ms
Dec 14 08:03:32.369: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010408189s
Dec 14 08:03:32.369: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Dec 14 08:03:32.369: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9317" to be "running"
Dec 14 08:03:32.373: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.57419ms
Dec 14 08:03:32.373: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Dec 14 08:03:32.373: INFO: Pod2 is running on shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb. Tainting Node
STEP: Trying to apply a taint on the Node 12/14/22 08:03:32.373
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:03:32.389
STEP: Waiting for Pod1 and Pod2 to be deleted 12/14/22 08:03:32.393
Dec 14 08:03:38.541: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 14 08:03:58.584: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:03:58.6
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:03:58.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9317" for this suite. 12/14/22 08:03:58.608
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":15,"skipped":248,"failed":0}
------------------------------
• [88.769 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:02:29.845
    Dec 14 08:02:29.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-multiple-pods 12/14/22 08:02:29.846
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:02:29.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:02:29.874
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Dec 14 08:02:29.879: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 08:03:29.917: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Dec 14 08:03:29.921: INFO: Starting informer...
    STEP: Starting pods... 12/14/22 08:03:29.921
    Dec 14 08:03:30.144: INFO: Pod1 is running on shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb. Tainting Node
    Dec 14 08:03:30.358: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9317" to be "running"
    Dec 14 08:03:30.362: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968593ms
    Dec 14 08:03:32.369: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010408189s
    Dec 14 08:03:32.369: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Dec 14 08:03:32.369: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9317" to be "running"
    Dec 14 08:03:32.373: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.57419ms
    Dec 14 08:03:32.373: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Dec 14 08:03:32.373: INFO: Pod2 is running on shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb. Tainting Node
    STEP: Trying to apply a taint on the Node 12/14/22 08:03:32.373
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:03:32.389
    STEP: Waiting for Pod1 and Pod2 to be deleted 12/14/22 08:03:32.393
    Dec 14 08:03:38.541: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Dec 14 08:03:58.584: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:03:58.6
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:03:58.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-9317" for this suite. 12/14/22 08:03:58.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:03:58.615
Dec 14 08:03:58.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslicemirroring 12/14/22 08:03:58.616
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:03:58.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:03:58.634
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 12/14/22 08:03:58.647
Dec 14 08:03:58.656: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 12/14/22 08:04:00.662
STEP: mirroring deletion of a custom Endpoint 12/14/22 08:04:00.673
Dec 14 08:04:00.683: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Dec 14 08:04:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4718" for this suite. 12/14/22 08:04:02.696
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":16,"skipped":280,"failed":0}
------------------------------
• [4.086 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:03:58.615
    Dec 14 08:03:58.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslicemirroring 12/14/22 08:03:58.616
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:03:58.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:03:58.634
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 12/14/22 08:03:58.647
    Dec 14 08:03:58.656: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 12/14/22 08:04:00.662
    STEP: mirroring deletion of a custom Endpoint 12/14/22 08:04:00.673
    Dec 14 08:04:00.683: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Dec 14 08:04:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-4718" for this suite. 12/14/22 08:04:02.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:04:02.702
Dec 14 08:04:02.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:04:02.703
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:02.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:02.718
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 08:04:02.723
Dec 14 08:04:02.733: INFO: Waiting up to 5m0s for pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8" in namespace "emptydir-769" to be "Succeeded or Failed"
Dec 14 08:04:02.738: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229443ms
Dec 14 08:04:04.743: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010351316s
Dec 14 08:04:06.744: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011008892s
STEP: Saw pod success 12/14/22 08:04:06.744
Dec 14 08:04:06.744: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8" satisfied condition "Succeeded or Failed"
Dec 14 08:04:06.748: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-373b992a-f7cd-4340-860c-61be7668d9a8 container test-container: <nil>
STEP: delete the pod 12/14/22 08:04:06.758
Dec 14 08:04:06.773: INFO: Waiting for pod pod-373b992a-f7cd-4340-860c-61be7668d9a8 to disappear
Dec 14 08:04:06.776: INFO: Pod pod-373b992a-f7cd-4340-860c-61be7668d9a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:04:06.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-769" for this suite. 12/14/22 08:04:06.781
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":304,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:04:02.702
    Dec 14 08:04:02.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:04:02.703
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:02.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:02.718
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 08:04:02.723
    Dec 14 08:04:02.733: INFO: Waiting up to 5m0s for pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8" in namespace "emptydir-769" to be "Succeeded or Failed"
    Dec 14 08:04:02.738: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229443ms
    Dec 14 08:04:04.743: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010351316s
    Dec 14 08:04:06.744: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011008892s
    STEP: Saw pod success 12/14/22 08:04:06.744
    Dec 14 08:04:06.744: INFO: Pod "pod-373b992a-f7cd-4340-860c-61be7668d9a8" satisfied condition "Succeeded or Failed"
    Dec 14 08:04:06.748: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-373b992a-f7cd-4340-860c-61be7668d9a8 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:04:06.758
    Dec 14 08:04:06.773: INFO: Waiting for pod pod-373b992a-f7cd-4340-860c-61be7668d9a8 to disappear
    Dec 14 08:04:06.776: INFO: Pod pod-373b992a-f7cd-4340-860c-61be7668d9a8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:04:06.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-769" for this suite. 12/14/22 08:04:06.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:04:06.787
Dec 14 08:04:06.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 12/14/22 08:04:06.788
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:06.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:06.802
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 12/14/22 08:04:06.81
STEP: Updating PodDisruptionBudget status 12/14/22 08:04:06.817
STEP: Waiting for all pods to be running 12/14/22 08:04:06.827
Dec 14 08:04:06.866: INFO: running pods: 0 < 1
STEP: locating a running pod 12/14/22 08:04:08.87
STEP: Waiting for the pdb to be processed 12/14/22 08:04:08.882
STEP: Patching PodDisruptionBudget status 12/14/22 08:04:08.891
STEP: Waiting for the pdb to be processed 12/14/22 08:04:08.9
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 14 08:04:08.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2382" for this suite. 12/14/22 08:04:08.908
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":18,"skipped":320,"failed":0}
------------------------------
• [2.126 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:04:06.787
    Dec 14 08:04:06.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 12/14/22 08:04:06.788
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:06.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:06.802
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 12/14/22 08:04:06.81
    STEP: Updating PodDisruptionBudget status 12/14/22 08:04:06.817
    STEP: Waiting for all pods to be running 12/14/22 08:04:06.827
    Dec 14 08:04:06.866: INFO: running pods: 0 < 1
    STEP: locating a running pod 12/14/22 08:04:08.87
    STEP: Waiting for the pdb to be processed 12/14/22 08:04:08.882
    STEP: Patching PodDisruptionBudget status 12/14/22 08:04:08.891
    STEP: Waiting for the pdb to be processed 12/14/22 08:04:08.9
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 14 08:04:08.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2382" for this suite. 12/14/22 08:04:08.908
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:04:08.913
Dec 14 08:04:08.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:04:08.914
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:08.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:08.928
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-5630 12/14/22 08:04:08.933
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[] 12/14/22 08:04:08.949
Dec 14 08:04:08.993: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5630 12/14/22 08:04:08.993
Dec 14 08:04:09.003: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5630" to be "running and ready"
Dec 14 08:04:09.008: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785782ms
Dec 14 08:04:09.008: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:04:11.015: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011268048s
Dec 14 08:04:11.015: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 14 08:04:11.015: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[pod1:[80]] 12/14/22 08:04:11.019
Dec 14 08:04:11.031: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 12/14/22 08:04:11.031
Dec 14 08:04:11.031: INFO: Creating new exec pod
Dec 14 08:04:11.040: INFO: Waiting up to 5m0s for pod "execpoddgjds" in namespace "services-5630" to be "running"
Dec 14 08:04:11.043: INFO: Pod "execpoddgjds": Phase="Pending", Reason="", readiness=false. Elapsed: 3.114248ms
Dec 14 08:04:13.049: INFO: Pod "execpoddgjds": Phase="Running", Reason="", readiness=true. Elapsed: 2.008870552s
Dec 14 08:04:13.049: INFO: Pod "execpoddgjds" satisfied condition "running"
Dec 14 08:04:14.050: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 14 08:04:14.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 08:04:14.377: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:04:14.377: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.9 80'
Dec 14 08:04:14.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.46.9 80\nConnection to 100.104.46.9 80 port [tcp/http] succeeded!\n"
Dec 14 08:04:14.844: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5630 12/14/22 08:04:14.844
Dec 14 08:04:14.861: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5630" to be "running and ready"
Dec 14 08:04:14.866: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13197ms
Dec 14 08:04:14.866: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:04:16.871: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009900901s
Dec 14 08:04:16.871: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 14 08:04:16.871: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[pod1:[80] pod2:[80]] 12/14/22 08:04:16.875
Dec 14 08:04:16.891: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 12/14/22 08:04:16.891
Dec 14 08:04:17.892: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 14 08:04:18.215: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 08:04:18.216: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:04:18.216: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.9 80'
Dec 14 08:04:18.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.46.9 80\nConnection to 100.104.46.9 80 port [tcp/http] succeeded!\n"
Dec 14 08:04:18.682: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5630 12/14/22 08:04:18.682
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[pod2:[80]] 12/14/22 08:04:18.691
Dec 14 08:04:18.704: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 12/14/22 08:04:18.704
Dec 14 08:04:19.704: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 14 08:04:20.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 08:04:20.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:04:20.159: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.9 80'
Dec 14 08:04:20.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.46.9 80\nConnection to 100.104.46.9 80 port [tcp/http] succeeded!\n"
Dec 14 08:04:20.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5630 12/14/22 08:04:20.603
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[] 12/14/22 08:04:20.611
Dec 14 08:04:20.619: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 08:04:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5630" for this suite. 12/14/22 08:04:20.635
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":19,"skipped":322,"failed":0}
------------------------------
• [11.726 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:04:08.913
    Dec 14 08:04:08.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:04:08.914
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:08.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:08.928
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-5630 12/14/22 08:04:08.933
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[] 12/14/22 08:04:08.949
    Dec 14 08:04:08.993: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5630 12/14/22 08:04:08.993
    Dec 14 08:04:09.003: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5630" to be "running and ready"
    Dec 14 08:04:09.008: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785782ms
    Dec 14 08:04:09.008: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:04:11.015: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011268048s
    Dec 14 08:04:11.015: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 14 08:04:11.015: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[pod1:[80]] 12/14/22 08:04:11.019
    Dec 14 08:04:11.031: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 12/14/22 08:04:11.031
    Dec 14 08:04:11.031: INFO: Creating new exec pod
    Dec 14 08:04:11.040: INFO: Waiting up to 5m0s for pod "execpoddgjds" in namespace "services-5630" to be "running"
    Dec 14 08:04:11.043: INFO: Pod "execpoddgjds": Phase="Pending", Reason="", readiness=false. Elapsed: 3.114248ms
    Dec 14 08:04:13.049: INFO: Pod "execpoddgjds": Phase="Running", Reason="", readiness=true. Elapsed: 2.008870552s
    Dec 14 08:04:13.049: INFO: Pod "execpoddgjds" satisfied condition "running"
    Dec 14 08:04:14.050: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Dec 14 08:04:14.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 14 08:04:14.377: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:04:14.377: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.9 80'
    Dec 14 08:04:14.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.46.9 80\nConnection to 100.104.46.9 80 port [tcp/http] succeeded!\n"
    Dec 14 08:04:14.844: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-5630 12/14/22 08:04:14.844
    Dec 14 08:04:14.861: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5630" to be "running and ready"
    Dec 14 08:04:14.866: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13197ms
    Dec 14 08:04:14.866: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:04:16.871: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009900901s
    Dec 14 08:04:16.871: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 14 08:04:16.871: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[pod1:[80] pod2:[80]] 12/14/22 08:04:16.875
    Dec 14 08:04:16.891: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 12/14/22 08:04:16.891
    Dec 14 08:04:17.892: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Dec 14 08:04:18.215: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 14 08:04:18.216: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:04:18.216: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.9 80'
    Dec 14 08:04:18.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.46.9 80\nConnection to 100.104.46.9 80 port [tcp/http] succeeded!\n"
    Dec 14 08:04:18.682: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5630 12/14/22 08:04:18.682
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[pod2:[80]] 12/14/22 08:04:18.691
    Dec 14 08:04:18.704: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 12/14/22 08:04:18.704
    Dec 14 08:04:19.704: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Dec 14 08:04:20.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 14 08:04:20.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:04:20.159: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5630 exec execpoddgjds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.9 80'
    Dec 14 08:04:20.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.46.9 80\nConnection to 100.104.46.9 80 port [tcp/http] succeeded!\n"
    Dec 14 08:04:20.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-5630 12/14/22 08:04:20.603
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5630 to expose endpoints map[] 12/14/22 08:04:20.611
    Dec 14 08:04:20.619: INFO: successfully validated that service endpoint-test2 in namespace services-5630 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 08:04:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5630" for this suite. 12/14/22 08:04:20.635
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:04:20.641
Dec 14 08:04:20.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 12/14/22 08:04:20.641
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:20.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:20.657
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 12/14/22 08:04:20.662
STEP: When the matched label of one of its pods change 12/14/22 08:04:20.668
Dec 14 08:04:20.671: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 14 08:04:25.675: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 12/14/22 08:04:25.685
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 14 08:04:25.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4069" for this suite. 12/14/22 08:04:25.708
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":20,"skipped":390,"failed":0}
------------------------------
• [5.075 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:04:20.641
    Dec 14 08:04:20.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 12/14/22 08:04:20.641
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:20.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:20.657
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 12/14/22 08:04:20.662
    STEP: When the matched label of one of its pods change 12/14/22 08:04:20.668
    Dec 14 08:04:20.671: INFO: Pod name pod-release: Found 0 pods out of 1
    Dec 14 08:04:25.675: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/14/22 08:04:25.685
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 14 08:04:25.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4069" for this suite. 12/14/22 08:04:25.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:04:25.717
Dec 14 08:04:25.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 12/14/22 08:04:25.717
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:25.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:25.735
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 12/14/22 08:04:25.739
Dec 14 08:04:25.743: INFO: created test-event-1
Dec 14 08:04:25.747: INFO: created test-event-2
Dec 14 08:04:25.750: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 12/14/22 08:04:25.75
STEP: delete collection of events 12/14/22 08:04:25.753
Dec 14 08:04:25.753: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/14/22 08:04:25.763
Dec 14 08:04:25.763: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Dec 14 08:04:25.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-641" for this suite. 12/14/22 08:04:25.771
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":21,"skipped":410,"failed":0}
------------------------------
• [0.059 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:04:25.717
    Dec 14 08:04:25.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 12/14/22 08:04:25.717
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:25.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:25.735
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 12/14/22 08:04:25.739
    Dec 14 08:04:25.743: INFO: created test-event-1
    Dec 14 08:04:25.747: INFO: created test-event-2
    Dec 14 08:04:25.750: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 12/14/22 08:04:25.75
    STEP: delete collection of events 12/14/22 08:04:25.753
    Dec 14 08:04:25.753: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/14/22 08:04:25.763
    Dec 14 08:04:25.763: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Dec 14 08:04:25.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-641" for this suite. 12/14/22 08:04:25.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:04:25.777
Dec 14 08:04:25.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 08:04:25.778
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:25.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:25.793
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 in namespace container-probe-6377 12/14/22 08:04:25.798
Dec 14 08:04:25.810: INFO: Waiting up to 5m0s for pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77" in namespace "container-probe-6377" to be "not pending"
Dec 14 08:04:25.814: INFO: Pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77": Phase="Pending", Reason="", readiness=false. Elapsed: 3.780328ms
Dec 14 08:04:27.820: INFO: Pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77": Phase="Running", Reason="", readiness=true. Elapsed: 2.010372696s
Dec 14 08:04:27.820: INFO: Pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77" satisfied condition "not pending"
Dec 14 08:04:27.820: INFO: Started pod liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 in namespace container-probe-6377
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:04:27.82
Dec 14 08:04:27.824: INFO: Initial restart count of pod liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is 0
Dec 14 08:04:47.887: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 1 (20.062635442s elapsed)
Dec 14 08:05:07.941: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 2 (40.116872928s elapsed)
Dec 14 08:05:28.004: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 3 (1m0.17946384s elapsed)
Dec 14 08:05:48.132: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 4 (1m20.307318363s elapsed)
Dec 14 08:07:00.326: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 5 (2m32.501987435s elapsed)
STEP: deleting the pod 12/14/22 08:07:00.326
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 08:07:00.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6377" for this suite. 12/14/22 08:07:00.341
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":22,"skipped":447,"failed":0}
------------------------------
• [154.570 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:04:25.777
    Dec 14 08:04:25.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 08:04:25.778
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:04:25.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:04:25.793
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 in namespace container-probe-6377 12/14/22 08:04:25.798
    Dec 14 08:04:25.810: INFO: Waiting up to 5m0s for pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77" in namespace "container-probe-6377" to be "not pending"
    Dec 14 08:04:25.814: INFO: Pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77": Phase="Pending", Reason="", readiness=false. Elapsed: 3.780328ms
    Dec 14 08:04:27.820: INFO: Pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77": Phase="Running", Reason="", readiness=true. Elapsed: 2.010372696s
    Dec 14 08:04:27.820: INFO: Pod "liveness-b1773b68-5419-45b8-a724-e6e7f650ed77" satisfied condition "not pending"
    Dec 14 08:04:27.820: INFO: Started pod liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 in namespace container-probe-6377
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:04:27.82
    Dec 14 08:04:27.824: INFO: Initial restart count of pod liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is 0
    Dec 14 08:04:47.887: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 1 (20.062635442s elapsed)
    Dec 14 08:05:07.941: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 2 (40.116872928s elapsed)
    Dec 14 08:05:28.004: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 3 (1m0.17946384s elapsed)
    Dec 14 08:05:48.132: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 4 (1m20.307318363s elapsed)
    Dec 14 08:07:00.326: INFO: Restart count of pod container-probe-6377/liveness-b1773b68-5419-45b8-a724-e6e7f650ed77 is now 5 (2m32.501987435s elapsed)
    STEP: deleting the pod 12/14/22 08:07:00.326
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 08:07:00.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6377" for this suite. 12/14/22 08:07:00.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:00.348
Dec 14 08:07:00.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:07:00.348
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:00.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:00.366
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 12/14/22 08:07:00.389
Dec 14 08:07:00.390: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b" in namespace "kubelet-test-5657" to be "completed"
Dec 14 08:07:00.394: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389213ms
Dec 14 08:07:02.399: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00965848s
Dec 14 08:07:04.399: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009678143s
Dec 14 08:07:04.399: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 14 08:07:04.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5657" for this suite. 12/14/22 08:07:04.419
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":23,"skipped":489,"failed":0}
------------------------------
• [4.077 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:00.348
    Dec 14 08:07:00.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:07:00.348
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:00.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:00.366
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 12/14/22 08:07:00.389
    Dec 14 08:07:00.390: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b" in namespace "kubelet-test-5657" to be "completed"
    Dec 14 08:07:00.394: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389213ms
    Dec 14 08:07:02.399: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00965848s
    Dec 14 08:07:04.399: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009678143s
    Dec 14 08:07:04.399: INFO: Pod "agnhost-host-aliases76f91e66-3d64-4359-8db8-1f920717ee8b" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 14 08:07:04.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5657" for this suite. 12/14/22 08:07:04.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:04.425
Dec 14 08:07:04.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 08:07:04.425
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:04.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:04.44
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Dec 14 08:07:04.460: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:07:04.464
Dec 14 08:07:04.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:07:04.472: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:07:05.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:07:05.482: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 08:07:06.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 08:07:06.484: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 12/14/22 08:07:06.498
STEP: Check that daemon pods images are updated. 12/14/22 08:07:06.512
Dec 14 08:07:06.516: INFO: Wrong image for pod: daemon-set-d4k75. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 08:07:07.524: INFO: Wrong image for pod: daemon-set-d4k75. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 08:07:08.526: INFO: Pod daemon-set-68mbp is not available
Dec 14 08:07:08.526: INFO: Wrong image for pod: daemon-set-d4k75. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 08:07:10.525: INFO: Pod daemon-set-7wbnf is not available
STEP: Check that daemon pods are still running on every node of the cluster. 12/14/22 08:07:10.532
Dec 14 08:07:10.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:07:10.540: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 08:07:11.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 08:07:11.552: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:07:11.574
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3806, will wait for the garbage collector to delete the pods 12/14/22 08:07:11.574
Dec 14 08:07:11.634: INFO: Deleting DaemonSet.extensions daemon-set took: 6.047396ms
Dec 14 08:07:11.734: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.177585ms
Dec 14 08:07:14.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:07:14.039: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 08:07:14.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7525"},"items":null}

Dec 14 08:07:14.046: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7525"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:07:14.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3806" for this suite. 12/14/22 08:07:14.064
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":24,"skipped":495,"failed":0}
------------------------------
• [9.644 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:04.425
    Dec 14 08:07:04.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 08:07:04.425
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:04.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:04.44
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Dec 14 08:07:04.460: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:07:04.464
    Dec 14 08:07:04.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:07:04.472: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:07:05.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:07:05.482: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 08:07:06.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 08:07:06.484: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 12/14/22 08:07:06.498
    STEP: Check that daemon pods images are updated. 12/14/22 08:07:06.512
    Dec 14 08:07:06.516: INFO: Wrong image for pod: daemon-set-d4k75. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 14 08:07:07.524: INFO: Wrong image for pod: daemon-set-d4k75. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 14 08:07:08.526: INFO: Pod daemon-set-68mbp is not available
    Dec 14 08:07:08.526: INFO: Wrong image for pod: daemon-set-d4k75. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 14 08:07:10.525: INFO: Pod daemon-set-7wbnf is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 12/14/22 08:07:10.532
    Dec 14 08:07:10.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:07:10.540: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 08:07:11.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 08:07:11.552: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:07:11.574
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3806, will wait for the garbage collector to delete the pods 12/14/22 08:07:11.574
    Dec 14 08:07:11.634: INFO: Deleting DaemonSet.extensions daemon-set took: 6.047396ms
    Dec 14 08:07:11.734: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.177585ms
    Dec 14 08:07:14.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:07:14.039: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 08:07:14.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7525"},"items":null}

    Dec 14 08:07:14.046: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7525"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:07:14.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3806" for this suite. 12/14/22 08:07:14.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:14.069
Dec 14 08:07:14.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 08:07:14.07
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:14.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:14.085
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-c0cc0c67-00d8-4b53-ac33-49aba02f1055 12/14/22 08:07:14.09
STEP: Creating a pod to test consume secrets 12/14/22 08:07:14.094
Dec 14 08:07:14.103: INFO: Waiting up to 5m0s for pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692" in namespace "secrets-1365" to be "Succeeded or Failed"
Dec 14 08:07:14.105: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.630262ms
Dec 14 08:07:16.110: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007358974s
Dec 14 08:07:18.110: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007667387s
STEP: Saw pod success 12/14/22 08:07:18.11
Dec 14 08:07:18.110: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692" satisfied condition "Succeeded or Failed"
Dec 14 08:07:18.114: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 08:07:18.124
Dec 14 08:07:18.131: INFO: Waiting for pod pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692 to disappear
Dec 14 08:07:18.134: INFO: Pod pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 08:07:18.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1365" for this suite. 12/14/22 08:07:18.14
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":25,"skipped":500,"failed":0}
------------------------------
• [4.075 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:14.069
    Dec 14 08:07:14.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 08:07:14.07
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:14.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:14.085
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-c0cc0c67-00d8-4b53-ac33-49aba02f1055 12/14/22 08:07:14.09
    STEP: Creating a pod to test consume secrets 12/14/22 08:07:14.094
    Dec 14 08:07:14.103: INFO: Waiting up to 5m0s for pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692" in namespace "secrets-1365" to be "Succeeded or Failed"
    Dec 14 08:07:14.105: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.630262ms
    Dec 14 08:07:16.110: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007358974s
    Dec 14 08:07:18.110: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007667387s
    STEP: Saw pod success 12/14/22 08:07:18.11
    Dec 14 08:07:18.110: INFO: Pod "pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692" satisfied condition "Succeeded or Failed"
    Dec 14 08:07:18.114: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 08:07:18.124
    Dec 14 08:07:18.131: INFO: Waiting for pod pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692 to disappear
    Dec 14 08:07:18.134: INFO: Pod pod-secrets-77b67f08-312d-4e4c-b9cf-875af94a9692 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 08:07:18.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1365" for this suite. 12/14/22 08:07:18.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:18.145
Dec 14 08:07:18.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:07:18.146
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:18.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:18.162
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:07:18.176
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:07:18.386
STEP: Deploying the webhook pod 12/14/22 08:07:18.393
STEP: Wait for the deployment to be ready 12/14/22 08:07:18.402
Dec 14 08:07:18.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:07:20.424
STEP: Verifying the service has paired with the endpoint 12/14/22 08:07:20.432
Dec 14 08:07:21.433: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Dec 14 08:07:21.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7662-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 08:07:21.952
STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 08:07:22.073
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:07:24.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9569" for this suite. 12/14/22 08:07:24.843
STEP: Destroying namespace "webhook-9569-markers" for this suite. 12/14/22 08:07:24.848
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":26,"skipped":505,"failed":0}
------------------------------
• [6.734 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:18.145
    Dec 14 08:07:18.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:07:18.146
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:18.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:18.162
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:07:18.176
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:07:18.386
    STEP: Deploying the webhook pod 12/14/22 08:07:18.393
    STEP: Wait for the deployment to be ready 12/14/22 08:07:18.402
    Dec 14 08:07:18.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:07:20.424
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:07:20.432
    Dec 14 08:07:21.433: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Dec 14 08:07:21.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7662-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 08:07:21.952
    STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 08:07:22.073
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:07:24.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9569" for this suite. 12/14/22 08:07:24.843
    STEP: Destroying namespace "webhook-9569-markers" for this suite. 12/14/22 08:07:24.848
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:24.879
Dec 14 08:07:24.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:07:24.88
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:24.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:24.894
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-4527 12/14/22 08:07:24.898
Dec 14 08:07:24.906: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4527" to be "running and ready"
Dec 14 08:07:24.909: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081873ms
Dec 14 08:07:24.909: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:07:26.915: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008472709s
Dec 14 08:07:26.915: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 14 08:07:26.915: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Dec 14 08:07:26.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 08:07:27.354: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 14 08:07:27.354: INFO: stdout: "iptables"
Dec 14 08:07:27.354: INFO: proxyMode: iptables
Dec 14 08:07:27.361: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 08:07:27.364: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4527 12/14/22 08:07:27.364
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4527 12/14/22 08:07:27.373
I1214 08:07:27.377877    4635 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4527, replica count: 3
I1214 08:07:30.429985    4635 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 08:07:30.446: INFO: Creating new exec pod
Dec 14 08:07:30.453: INFO: Waiting up to 5m0s for pod "execpod-affinity7gg9z" in namespace "services-4527" to be "running"
Dec 14 08:07:30.458: INFO: Pod "execpod-affinity7gg9z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697403ms
Dec 14 08:07:32.463: INFO: Pod "execpod-affinity7gg9z": Phase="Running", Reason="", readiness=true. Elapsed: 2.010252246s
Dec 14 08:07:32.463: INFO: Pod "execpod-affinity7gg9z" satisfied condition "running"
Dec 14 08:07:33.469: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Dec 14 08:07:33.963: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 08:07:33.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:07:33.963: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.210 80'
Dec 14 08:07:34.419: INFO: stderr: "+ nc -v -t -w 2 100.104.46.210 80\n+ echo hostName\nConnection to 100.104.46.210 80 port [tcp/http] succeeded!\n"
Dec 14 08:07:34.419: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:07:34.419: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30474'
Dec 14 08:07:34.873: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30474\nConnection to 10.250.3.58 30474 port [tcp/*] succeeded!\n"
Dec 14 08:07:34.874: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:07:34.874: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30474'
Dec 14 08:07:35.325: INFO: stderr: "+ nc -v -t -w 2 10.250.3.210 30474\n+ echo hostName\nConnection to 10.250.3.210 30474 port [tcp/*] succeeded!\n"
Dec 14 08:07:35.325: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:07:35.325: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30474/ ; done'
Dec 14 08:07:35.833: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n"
Dec 14 08:07:35.833: INFO: stdout: "\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685"
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
Dec 14 08:07:35.833: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.3.58:30474/'
Dec 14 08:07:36.207: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n"
Dec 14 08:07:36.207: INFO: stdout: "affinity-nodeport-timeout-8k685"
Dec 14 08:07:56.208: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.3.58:30474/'
Dec 14 08:07:56.587: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n"
Dec 14 08:07:56.587: INFO: stdout: "affinity-nodeport-timeout-cwhpm"
Dec 14 08:07:56.587: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4527, will wait for the garbage collector to delete the pods 12/14/22 08:07:56.595
Dec 14 08:07:56.654: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.396535ms
Dec 14 08:07:56.755: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.724285ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 08:07:58.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4527" for this suite. 12/14/22 08:07:58.581
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":27,"skipped":505,"failed":0}
------------------------------
• [33.707 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:24.879
    Dec 14 08:07:24.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:07:24.88
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:24.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:24.894
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-4527 12/14/22 08:07:24.898
    Dec 14 08:07:24.906: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4527" to be "running and ready"
    Dec 14 08:07:24.909: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081873ms
    Dec 14 08:07:24.909: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:07:26.915: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008472709s
    Dec 14 08:07:26.915: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Dec 14 08:07:26.915: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Dec 14 08:07:26.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Dec 14 08:07:27.354: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Dec 14 08:07:27.354: INFO: stdout: "iptables"
    Dec 14 08:07:27.354: INFO: proxyMode: iptables
    Dec 14 08:07:27.361: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Dec 14 08:07:27.364: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-4527 12/14/22 08:07:27.364
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-4527 12/14/22 08:07:27.373
    I1214 08:07:27.377877    4635 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4527, replica count: 3
    I1214 08:07:30.429985    4635 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 08:07:30.446: INFO: Creating new exec pod
    Dec 14 08:07:30.453: INFO: Waiting up to 5m0s for pod "execpod-affinity7gg9z" in namespace "services-4527" to be "running"
    Dec 14 08:07:30.458: INFO: Pod "execpod-affinity7gg9z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697403ms
    Dec 14 08:07:32.463: INFO: Pod "execpod-affinity7gg9z": Phase="Running", Reason="", readiness=true. Elapsed: 2.010252246s
    Dec 14 08:07:32.463: INFO: Pod "execpod-affinity7gg9z" satisfied condition "running"
    Dec 14 08:07:33.469: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Dec 14 08:07:33.963: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Dec 14 08:07:33.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:07:33.963: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.46.210 80'
    Dec 14 08:07:34.419: INFO: stderr: "+ nc -v -t -w 2 100.104.46.210 80\n+ echo hostName\nConnection to 100.104.46.210 80 port [tcp/http] succeeded!\n"
    Dec 14 08:07:34.419: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:07:34.419: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30474'
    Dec 14 08:07:34.873: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30474\nConnection to 10.250.3.58 30474 port [tcp/*] succeeded!\n"
    Dec 14 08:07:34.874: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:07:34.874: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30474'
    Dec 14 08:07:35.325: INFO: stderr: "+ nc -v -t -w 2 10.250.3.210 30474\n+ echo hostName\nConnection to 10.250.3.210 30474 port [tcp/*] succeeded!\n"
    Dec 14 08:07:35.325: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:07:35.325: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30474/ ; done'
    Dec 14 08:07:35.833: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n"
    Dec 14 08:07:35.833: INFO: stdout: "\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685\naffinity-nodeport-timeout-8k685"
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Received response from host: affinity-nodeport-timeout-8k685
    Dec 14 08:07:35.833: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.3.58:30474/'
    Dec 14 08:07:36.207: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n"
    Dec 14 08:07:36.207: INFO: stdout: "affinity-nodeport-timeout-8k685"
    Dec 14 08:07:56.208: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4527 exec execpod-affinity7gg9z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.3.58:30474/'
    Dec 14 08:07:56.587: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.3.58:30474/\n"
    Dec 14 08:07:56.587: INFO: stdout: "affinity-nodeport-timeout-cwhpm"
    Dec 14 08:07:56.587: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4527, will wait for the garbage collector to delete the pods 12/14/22 08:07:56.595
    Dec 14 08:07:56.654: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.396535ms
    Dec 14 08:07:56.755: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.724285ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 08:07:58.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4527" for this suite. 12/14/22 08:07:58.581
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:58.587
Dec 14 08:07:58.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:07:58.588
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:58.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:58.604
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 08:07:58.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2186" for this suite. 12/14/22 08:07:58.616
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":28,"skipped":551,"failed":0}
------------------------------
• [0.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:58.587
    Dec 14 08:07:58.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:07:58.588
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:58.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:58.604
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 08:07:58.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2186" for this suite. 12/14/22 08:07:58.616
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:07:58.621
Dec 14 08:07:58.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:07:58.622
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:58.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:58.639
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:07:58.648
Dec 14 08:07:58.657: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2249" to be "running and ready"
Dec 14 08:07:58.661: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244398ms
Dec 14 08:07:58.661: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:08:00.666: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00854704s
Dec 14 08:08:00.666: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 08:08:00.666: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 12/14/22 08:08:00.67
Dec 14 08:08:00.677: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2249" to be "running and ready"
Dec 14 08:08:00.682: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799265ms
Dec 14 08:08:00.682: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:08:02.687: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009546213s
Dec 14 08:08:02.687: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Dec 14 08:08:02.687: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/14/22 08:08:02.69
STEP: delete the pod with lifecycle hook 12/14/22 08:08:02.7
Dec 14 08:08:02.705: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:08:02.709: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:08:04.709: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:08:04.714: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 08:08:06.711: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 08:08:06.715: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 14 08:08:06.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2249" for this suite. 12/14/22 08:08:06.721
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":29,"skipped":566,"failed":0}
------------------------------
• [8.104 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:07:58.621
    Dec 14 08:07:58.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:07:58.622
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:07:58.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:07:58.639
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:07:58.648
    Dec 14 08:07:58.657: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2249" to be "running and ready"
    Dec 14 08:07:58.661: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244398ms
    Dec 14 08:07:58.661: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:08:00.666: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00854704s
    Dec 14 08:08:00.666: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 08:08:00.666: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 12/14/22 08:08:00.67
    Dec 14 08:08:00.677: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2249" to be "running and ready"
    Dec 14 08:08:00.682: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799265ms
    Dec 14 08:08:00.682: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:08:02.687: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009546213s
    Dec 14 08:08:02.687: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Dec 14 08:08:02.687: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/14/22 08:08:02.69
    STEP: delete the pod with lifecycle hook 12/14/22 08:08:02.7
    Dec 14 08:08:02.705: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 14 08:08:02.709: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec 14 08:08:04.709: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 14 08:08:04.714: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec 14 08:08:06.711: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 14 08:08:06.715: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 14 08:08:06.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-2249" for this suite. 12/14/22 08:08:06.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:06.728
Dec 14 08:08:06.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 12/14/22 08:08:06.728
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:06.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:06.746
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 12/14/22 08:08:06.753
STEP: creating a new configmap 12/14/22 08:08:06.766
STEP: modifying the configmap once 12/14/22 08:08:06.77
STEP: changing the label value of the configmap 12/14/22 08:08:06.777
STEP: Expecting to observe a delete notification for the watched object 12/14/22 08:08:06.784
Dec 14 08:08:06.784: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 7995 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 08:08:06.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 7996 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 08:08:06.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 7997 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 12/14/22 08:08:06.785
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/14/22 08:08:06.794
STEP: changing the label value of the configmap back 12/14/22 08:08:16.794
STEP: modifying the configmap a third time 12/14/22 08:08:16.802
STEP: deleting the configmap 12/14/22 08:08:16.81
STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/14/22 08:08:16.814
Dec 14 08:08:16.814: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 8080 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 08:08:16.814: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 8081 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 08:08:16.815: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 8082 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 14 08:08:16.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4843" for this suite. 12/14/22 08:08:16.821
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":30,"skipped":618,"failed":0}
------------------------------
• [10.098 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:06.728
    Dec 14 08:08:06.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 12/14/22 08:08:06.728
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:06.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:06.746
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 12/14/22 08:08:06.753
    STEP: creating a new configmap 12/14/22 08:08:06.766
    STEP: modifying the configmap once 12/14/22 08:08:06.77
    STEP: changing the label value of the configmap 12/14/22 08:08:06.777
    STEP: Expecting to observe a delete notification for the watched object 12/14/22 08:08:06.784
    Dec 14 08:08:06.784: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 7995 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 08:08:06.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 7996 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 08:08:06.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 7997 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 12/14/22 08:08:06.785
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/14/22 08:08:06.794
    STEP: changing the label value of the configmap back 12/14/22 08:08:16.794
    STEP: modifying the configmap a third time 12/14/22 08:08:16.802
    STEP: deleting the configmap 12/14/22 08:08:16.81
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/14/22 08:08:16.814
    Dec 14 08:08:16.814: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 8080 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 08:08:16.814: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 8081 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 08:08:16.815: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4843  8ca7fecd-b423-40d7-9b3e-ef85f264482e 8082 0 2022-12-14 08:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 08:08:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 14 08:08:16.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4843" for this suite. 12/14/22 08:08:16.821
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:16.826
Dec 14 08:08:16.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 08:08:16.826
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:16.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:16.841
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 12/14/22 08:08:16.846
STEP: Creating a ResourceQuota 12/14/22 08:08:21.85
STEP: Ensuring resource quota status is calculated 12/14/22 08:08:21.854
STEP: Creating a Service 12/14/22 08:08:23.859
STEP: Creating a NodePort Service 12/14/22 08:08:23.872
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/14/22 08:08:23.884
STEP: Ensuring resource quota status captures service creation 12/14/22 08:08:23.898
STEP: Deleting Services 12/14/22 08:08:25.903
STEP: Ensuring resource quota status released usage 12/14/22 08:08:25.929
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 08:08:27.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5799" for this suite. 12/14/22 08:08:27.94
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":31,"skipped":620,"failed":0}
------------------------------
• [11.118 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:16.826
    Dec 14 08:08:16.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 08:08:16.826
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:16.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:16.841
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 12/14/22 08:08:16.846
    STEP: Creating a ResourceQuota 12/14/22 08:08:21.85
    STEP: Ensuring resource quota status is calculated 12/14/22 08:08:21.854
    STEP: Creating a Service 12/14/22 08:08:23.859
    STEP: Creating a NodePort Service 12/14/22 08:08:23.872
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/14/22 08:08:23.884
    STEP: Ensuring resource quota status captures service creation 12/14/22 08:08:23.898
    STEP: Deleting Services 12/14/22 08:08:25.903
    STEP: Ensuring resource quota status released usage 12/14/22 08:08:25.929
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 08:08:27.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5799" for this suite. 12/14/22 08:08:27.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:27.945
Dec 14 08:08:27.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 08:08:27.945
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:27.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:27.96
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 12/14/22 08:08:27.965
Dec 14 08:08:27.974: INFO: Waiting up to 5m0s for pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf" in namespace "pods-5362" to be "running and ready"
Dec 14 08:08:27.978: INFO: Pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077306ms
Dec 14 08:08:27.978: INFO: The phase of Pod pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:08:29.984: INFO: Pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009472371s
Dec 14 08:08:29.984: INFO: The phase of Pod pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf is Running (Ready = true)
Dec 14 08:08:29.984: INFO: Pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf" satisfied condition "running and ready"
Dec 14 08:08:29.990: INFO: Pod pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf has hostIP: 10.250.3.210
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 08:08:29.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5362" for this suite. 12/14/22 08:08:29.996
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":32,"skipped":631,"failed":0}
------------------------------
• [2.056 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:27.945
    Dec 14 08:08:27.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 08:08:27.945
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:27.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:27.96
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 12/14/22 08:08:27.965
    Dec 14 08:08:27.974: INFO: Waiting up to 5m0s for pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf" in namespace "pods-5362" to be "running and ready"
    Dec 14 08:08:27.978: INFO: Pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077306ms
    Dec 14 08:08:27.978: INFO: The phase of Pod pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:08:29.984: INFO: Pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009472371s
    Dec 14 08:08:29.984: INFO: The phase of Pod pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf is Running (Ready = true)
    Dec 14 08:08:29.984: INFO: Pod "pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf" satisfied condition "running and ready"
    Dec 14 08:08:29.990: INFO: Pod pod-hostip-9f687516-09c7-460c-951d-82ad5954a4cf has hostIP: 10.250.3.210
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 08:08:29.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5362" for this suite. 12/14/22 08:08:29.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:30.001
Dec 14 08:08:30.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:08:30.002
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:30.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:30.019
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 08:08:30.024
Dec 14 08:08:30.034: INFO: Waiting up to 5m0s for pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204" in namespace "emptydir-7005" to be "Succeeded or Failed"
Dec 14 08:08:30.037: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603642ms
Dec 14 08:08:32.042: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159069s
Dec 14 08:08:34.042: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00798059s
STEP: Saw pod success 12/14/22 08:08:34.042
Dec 14 08:08:34.042: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204" satisfied condition "Succeeded or Failed"
Dec 14 08:08:34.045: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204 container test-container: <nil>
STEP: delete the pod 12/14/22 08:08:34.056
Dec 14 08:08:34.063: INFO: Waiting for pod pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204 to disappear
Dec 14 08:08:34.066: INFO: Pod pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:08:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7005" for this suite. 12/14/22 08:08:34.071
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":33,"skipped":664,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:30.001
    Dec 14 08:08:30.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:08:30.002
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:30.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:30.019
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 08:08:30.024
    Dec 14 08:08:30.034: INFO: Waiting up to 5m0s for pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204" in namespace "emptydir-7005" to be "Succeeded or Failed"
    Dec 14 08:08:30.037: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603642ms
    Dec 14 08:08:32.042: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159069s
    Dec 14 08:08:34.042: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00798059s
    STEP: Saw pod success 12/14/22 08:08:34.042
    Dec 14 08:08:34.042: INFO: Pod "pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204" satisfied condition "Succeeded or Failed"
    Dec 14 08:08:34.045: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:08:34.056
    Dec 14 08:08:34.063: INFO: Waiting for pod pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204 to disappear
    Dec 14 08:08:34.066: INFO: Pod pod-fd9fbec5-3a62-4cfa-ad9f-452a80c98204 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:08:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7005" for this suite. 12/14/22 08:08:34.071
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:34.076
Dec 14 08:08:34.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:08:34.077
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:34.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:34.091
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:08:34.097
Dec 14 08:08:34.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284" in namespace "downward-api-291" to be "Succeeded or Failed"
Dec 14 08:08:34.111: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.395535ms
Dec 14 08:08:36.117: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01049866s
Dec 14 08:08:38.116: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009410832s
STEP: Saw pod success 12/14/22 08:08:38.117
Dec 14 08:08:38.117: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284" satisfied condition "Succeeded or Failed"
Dec 14 08:08:38.121: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284 container client-container: <nil>
STEP: delete the pod 12/14/22 08:08:38.13
Dec 14 08:08:38.137: INFO: Waiting for pod downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284 to disappear
Dec 14 08:08:38.140: INFO: Pod downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 08:08:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-291" for this suite. 12/14/22 08:08:38.145
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":34,"skipped":664,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:34.076
    Dec 14 08:08:34.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:08:34.077
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:34.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:34.091
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:08:34.097
    Dec 14 08:08:34.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284" in namespace "downward-api-291" to be "Succeeded or Failed"
    Dec 14 08:08:34.111: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.395535ms
    Dec 14 08:08:36.117: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01049866s
    Dec 14 08:08:38.116: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009410832s
    STEP: Saw pod success 12/14/22 08:08:38.117
    Dec 14 08:08:38.117: INFO: Pod "downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284" satisfied condition "Succeeded or Failed"
    Dec 14 08:08:38.121: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284 container client-container: <nil>
    STEP: delete the pod 12/14/22 08:08:38.13
    Dec 14 08:08:38.137: INFO: Waiting for pod downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284 to disappear
    Dec 14 08:08:38.140: INFO: Pod downwardapi-volume-2d161f6c-c048-40f9-9d60-41029fe8e284 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 08:08:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-291" for this suite. 12/14/22 08:08:38.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:38.151
Dec 14 08:08:38.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:08:38.152
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:38.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:38.166
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:08:38.181
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:08:38.383
STEP: Deploying the webhook pod 12/14/22 08:08:38.39
STEP: Wait for the deployment to be ready 12/14/22 08:08:38.398
Dec 14 08:08:38.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:08:40.419
STEP: Verifying the service has paired with the endpoint 12/14/22 08:08:40.427
Dec 14 08:08:41.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/14/22 08:08:41.432
STEP: create a configmap that should be updated by the webhook 12/14/22 08:08:41.549
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:08:41.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4309" for this suite. 12/14/22 08:08:41.774
STEP: Destroying namespace "webhook-4309-markers" for this suite. 12/14/22 08:08:41.78
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":35,"skipped":708,"failed":0}
------------------------------
• [3.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:38.151
    Dec 14 08:08:38.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:08:38.152
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:38.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:38.166
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:08:38.181
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:08:38.383
    STEP: Deploying the webhook pod 12/14/22 08:08:38.39
    STEP: Wait for the deployment to be ready 12/14/22 08:08:38.398
    Dec 14 08:08:38.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:08:40.419
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:08:40.427
    Dec 14 08:08:41.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/14/22 08:08:41.432
    STEP: create a configmap that should be updated by the webhook 12/14/22 08:08:41.549
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:08:41.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4309" for this suite. 12/14/22 08:08:41.774
    STEP: Destroying namespace "webhook-4309-markers" for this suite. 12/14/22 08:08:41.78
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:41.811
Dec 14 08:08:41.812: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 12/14/22 08:08:41.812
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:41.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:41.827
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 12/14/22 08:08:41.833
STEP: Waiting for the pdb to be processed 12/14/22 08:08:41.837
STEP: First trying to evict a pod which shouldn't be evictable 12/14/22 08:08:41.845
STEP: Waiting for all pods to be running 12/14/22 08:08:41.845
Dec 14 08:08:41.848: INFO: pods: 0 < 3
STEP: locating a running pod 12/14/22 08:08:43.864
STEP: Updating the pdb to allow a pod to be evicted 12/14/22 08:08:43.872
STEP: Waiting for the pdb to be processed 12/14/22 08:08:43.88
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 08:08:43.883
STEP: Waiting for all pods to be running 12/14/22 08:08:43.883
STEP: Waiting for the pdb to observed all healthy pods 12/14/22 08:08:43.886
STEP: Patching the pdb to disallow a pod to be evicted 12/14/22 08:08:43.905
STEP: Waiting for the pdb to be processed 12/14/22 08:08:43.916
STEP: Waiting for all pods to be running 12/14/22 08:08:43.922
Dec 14 08:08:43.926: INFO: running pods: 2 < 3
STEP: locating a running pod 12/14/22 08:08:45.934
STEP: Deleting the pdb to allow a pod to be evicted 12/14/22 08:08:45.943
STEP: Waiting for the pdb to be deleted 12/14/22 08:08:45.947
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 08:08:45.95
STEP: Waiting for all pods to be running 12/14/22 08:08:45.95
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 14 08:08:45.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3323" for this suite. 12/14/22 08:08:45.963
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":36,"skipped":711,"failed":0}
------------------------------
• [4.157 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:41.811
    Dec 14 08:08:41.812: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 12/14/22 08:08:41.812
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:41.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:41.827
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 12/14/22 08:08:41.833
    STEP: Waiting for the pdb to be processed 12/14/22 08:08:41.837
    STEP: First trying to evict a pod which shouldn't be evictable 12/14/22 08:08:41.845
    STEP: Waiting for all pods to be running 12/14/22 08:08:41.845
    Dec 14 08:08:41.848: INFO: pods: 0 < 3
    STEP: locating a running pod 12/14/22 08:08:43.864
    STEP: Updating the pdb to allow a pod to be evicted 12/14/22 08:08:43.872
    STEP: Waiting for the pdb to be processed 12/14/22 08:08:43.88
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 08:08:43.883
    STEP: Waiting for all pods to be running 12/14/22 08:08:43.883
    STEP: Waiting for the pdb to observed all healthy pods 12/14/22 08:08:43.886
    STEP: Patching the pdb to disallow a pod to be evicted 12/14/22 08:08:43.905
    STEP: Waiting for the pdb to be processed 12/14/22 08:08:43.916
    STEP: Waiting for all pods to be running 12/14/22 08:08:43.922
    Dec 14 08:08:43.926: INFO: running pods: 2 < 3
    STEP: locating a running pod 12/14/22 08:08:45.934
    STEP: Deleting the pdb to allow a pod to be evicted 12/14/22 08:08:45.943
    STEP: Waiting for the pdb to be deleted 12/14/22 08:08:45.947
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 08:08:45.95
    STEP: Waiting for all pods to be running 12/14/22 08:08:45.95
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 14 08:08:45.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3323" for this suite. 12/14/22 08:08:45.963
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:45.968
Dec 14 08:08:45.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:08:45.969
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:45.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:45.984
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-9bf0ddf1-a730-4934-958f-d7586be51c5e 12/14/22 08:08:45.989
STEP: Creating a pod to test consume configMaps 12/14/22 08:08:45.992
Dec 14 08:08:46.001: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0" in namespace "projected-4866" to be "Succeeded or Failed"
Dec 14 08:08:46.005: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699187ms
Dec 14 08:08:48.011: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009815867s
Dec 14 08:08:50.012: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011260797s
STEP: Saw pod success 12/14/22 08:08:50.012
Dec 14 08:08:50.012: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0" satisfied condition "Succeeded or Failed"
Dec 14 08:08:50.016: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0 container projected-configmap-volume-test: <nil>
STEP: delete the pod 12/14/22 08:08:50.025
Dec 14 08:08:50.033: INFO: Waiting for pod pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0 to disappear
Dec 14 08:08:50.036: INFO: Pod pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 08:08:50.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4866" for this suite. 12/14/22 08:08:50.04
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":37,"skipped":714,"failed":0}
------------------------------
• [4.076 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:45.968
    Dec 14 08:08:45.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:08:45.969
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:45.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:45.984
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-9bf0ddf1-a730-4934-958f-d7586be51c5e 12/14/22 08:08:45.989
    STEP: Creating a pod to test consume configMaps 12/14/22 08:08:45.992
    Dec 14 08:08:46.001: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0" in namespace "projected-4866" to be "Succeeded or Failed"
    Dec 14 08:08:46.005: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699187ms
    Dec 14 08:08:48.011: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009815867s
    Dec 14 08:08:50.012: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011260797s
    STEP: Saw pod success 12/14/22 08:08:50.012
    Dec 14 08:08:50.012: INFO: Pod "pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0" satisfied condition "Succeeded or Failed"
    Dec 14 08:08:50.016: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 12/14/22 08:08:50.025
    Dec 14 08:08:50.033: INFO: Waiting for pod pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0 to disappear
    Dec 14 08:08:50.036: INFO: Pod pod-projected-configmaps-76fb66b0-8a6b-46ab-8640-86209bc7b3c0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 08:08:50.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4866" for this suite. 12/14/22 08:08:50.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:50.046
Dec 14 08:08:50.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 08:08:50.047
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:50.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:50.063
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 12/14/22 08:08:50.068
STEP: fetching the ConfigMap 12/14/22 08:08:50.072
STEP: patching the ConfigMap 12/14/22 08:08:50.075
STEP: listing all ConfigMaps in all namespaces with a label selector 12/14/22 08:08:50.08
STEP: deleting the ConfigMap by collection with a label selector 12/14/22 08:08:50.084
STEP: listing all ConfigMaps in test namespace 12/14/22 08:08:50.09
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 08:08:50.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3998" for this suite. 12/14/22 08:08:50.097
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":38,"skipped":780,"failed":0}
------------------------------
• [0.056 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:50.046
    Dec 14 08:08:50.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 08:08:50.047
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:50.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:50.063
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 12/14/22 08:08:50.068
    STEP: fetching the ConfigMap 12/14/22 08:08:50.072
    STEP: patching the ConfigMap 12/14/22 08:08:50.075
    STEP: listing all ConfigMaps in all namespaces with a label selector 12/14/22 08:08:50.08
    STEP: deleting the ConfigMap by collection with a label selector 12/14/22 08:08:50.084
    STEP: listing all ConfigMaps in test namespace 12/14/22 08:08:50.09
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 08:08:50.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3998" for this suite. 12/14/22 08:08:50.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:08:50.103
Dec 14 08:08:50.103: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 08:08:50.104
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:50.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:50.12
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3352 12/14/22 08:08:50.125
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 12/14/22 08:08:50.13
STEP: Creating stateful set ss in namespace statefulset-3352 12/14/22 08:08:50.138
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3352 12/14/22 08:08:50.143
Dec 14 08:08:50.147: INFO: Found 0 stateful pods, waiting for 1
Dec 14 08:09:00.152: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/14/22 08:09:00.152
Dec 14 08:09:00.156: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:09:00.546: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:09:00.546: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:09:00.546: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:09:00.551: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 08:09:10.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:09:10.555: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:09:10.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999762s
Dec 14 08:09:11.575: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996712209s
Dec 14 08:09:12.580: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992266043s
Dec 14 08:09:13.585: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986905725s
Dec 14 08:09:14.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981108049s
Dec 14 08:09:15.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976338647s
Dec 14 08:09:16.600: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971706074s
Dec 14 08:09:17.606: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965571404s
Dec 14 08:09:18.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960569711s
Dec 14 08:09:19.615: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.255873ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3352 12/14/22 08:09:20.615
Dec 14 08:09:20.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:09:20.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:09:20.896: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:09:20.896: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:09:20.900: INFO: Found 1 stateful pods, waiting for 3
Dec 14 08:09:30.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:09:30.906: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:09:30.906: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 12/14/22 08:09:30.906
STEP: Scale down will halt with unhealthy stateful pod 12/14/22 08:09:30.906
Dec 14 08:09:30.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:09:31.245: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:09:31.245: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:09:31.245: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:09:31.245: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:09:31.473: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:09:31.473: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:09:31.473: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:09:31.473: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:09:31.921: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:09:31.921: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:09:31.921: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:09:31.921: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:09:31.924: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 14 08:09:41.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:09:41.934: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:09:41.934: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:09:41.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999735s
Dec 14 08:09:42.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995799369s
Dec 14 08:09:43.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991371652s
Dec 14 08:09:44.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986130931s
Dec 14 08:09:45.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982008943s
Dec 14 08:09:46.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976944341s
Dec 14 08:09:47.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972440631s
Dec 14 08:09:48.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966914273s
Dec 14 08:09:49.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961713955s
Dec 14 08:09:50.990: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.907164ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3352 12/14/22 08:09:51.99
Dec 14 08:09:51.995: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:09:52.415: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:09:52.415: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:09:52.415: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:09:52.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:09:52.821: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:09:52.821: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:09:52.821: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:09:52.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:09:52.994: INFO: rc: 1
Dec 14 08:09:52.994: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 14 08:10:02.995: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:10:03.096: INFO: rc: 1
Dec 14 08:10:03.096: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:10:13.097: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:10:13.172: INFO: rc: 1
Dec 14 08:10:13.172: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:10:23.174: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:10:23.249: INFO: rc: 1
Dec 14 08:10:23.249: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:10:33.249: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:10:33.334: INFO: rc: 1
Dec 14 08:10:33.334: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:10:43.338: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:10:43.431: INFO: rc: 1
Dec 14 08:10:43.431: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:10:53.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:10:53.535: INFO: rc: 1
Dec 14 08:10:53.535: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:11:03.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:11:03.632: INFO: rc: 1
Dec 14 08:11:03.632: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:11:13.634: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:11:13.728: INFO: rc: 1
Dec 14 08:11:13.728: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:11:23.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:11:23.826: INFO: rc: 1
Dec 14 08:11:23.826: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:11:33.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:11:33.908: INFO: rc: 1
Dec 14 08:11:33.908: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:11:43.909: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:11:44.044: INFO: rc: 1
Dec 14 08:11:44.044: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:11:54.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:11:54.131: INFO: rc: 1
Dec 14 08:11:54.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:12:04.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:12:04.203: INFO: rc: 1
Dec 14 08:12:04.203: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:12:14.203: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:12:14.274: INFO: rc: 1
Dec 14 08:12:14.274: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:12:24.274: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:12:24.347: INFO: rc: 1
Dec 14 08:12:24.347: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:12:34.347: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:12:34.417: INFO: rc: 1
Dec 14 08:12:34.417: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:12:44.417: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:12:44.487: INFO: rc: 1
Dec 14 08:12:44.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:12:54.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:12:54.566: INFO: rc: 1
Dec 14 08:12:54.566: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:13:04.567: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:13:04.657: INFO: rc: 1
Dec 14 08:13:04.657: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:13:14.658: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:13:14.730: INFO: rc: 1
Dec 14 08:13:14.730: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:13:24.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:13:24.802: INFO: rc: 1
Dec 14 08:13:24.802: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:13:34.802: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:13:34.867: INFO: rc: 1
Dec 14 08:13:34.867: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:13:44.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:13:44.937: INFO: rc: 1
Dec 14 08:13:44.937: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:13:54.937: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:13:55.007: INFO: rc: 1
Dec 14 08:13:55.007: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:14:05.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:14:05.084: INFO: rc: 1
Dec 14 08:14:05.084: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:14:15.084: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:14:15.149: INFO: rc: 1
Dec 14 08:14:15.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:14:25.151: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:14:25.218: INFO: rc: 1
Dec 14 08:14:25.218: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:14:35.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:14:35.281: INFO: rc: 1
Dec 14 08:14:35.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:14:45.281: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:14:45.357: INFO: rc: 1
Dec 14 08:14:45.357: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 14 08:14:55.357: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:14:55.424: INFO: rc: 1
Dec 14 08:14:55.424: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Dec 14 08:14:55.424: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 12/14/22 08:14:55.436
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 08:14:55.436: INFO: Deleting all statefulset in ns statefulset-3352
Dec 14 08:14:55.439: INFO: Scaling statefulset ss to 0
Dec 14 08:14:55.450: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:14:55.453: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 08:14:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3352" for this suite. 12/14/22 08:14:55.469
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":39,"skipped":800,"failed":0}
------------------------------
• [SLOW TEST] [365.370 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:08:50.103
    Dec 14 08:08:50.103: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 08:08:50.104
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:08:50.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:08:50.12
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3352 12/14/22 08:08:50.125
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 12/14/22 08:08:50.13
    STEP: Creating stateful set ss in namespace statefulset-3352 12/14/22 08:08:50.138
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3352 12/14/22 08:08:50.143
    Dec 14 08:08:50.147: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 08:09:00.152: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/14/22 08:09:00.152
    Dec 14 08:09:00.156: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:09:00.546: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:09:00.546: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:09:00.546: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:09:00.551: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 14 08:09:10.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:09:10.555: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:09:10.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999762s
    Dec 14 08:09:11.575: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996712209s
    Dec 14 08:09:12.580: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992266043s
    Dec 14 08:09:13.585: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986905725s
    Dec 14 08:09:14.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981108049s
    Dec 14 08:09:15.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976338647s
    Dec 14 08:09:16.600: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971706074s
    Dec 14 08:09:17.606: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965571404s
    Dec 14 08:09:18.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960569711s
    Dec 14 08:09:19.615: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.255873ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3352 12/14/22 08:09:20.615
    Dec 14 08:09:20.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:09:20.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 08:09:20.896: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:09:20.896: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:09:20.900: INFO: Found 1 stateful pods, waiting for 3
    Dec 14 08:09:30.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:09:30.906: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:09:30.906: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 12/14/22 08:09:30.906
    STEP: Scale down will halt with unhealthy stateful pod 12/14/22 08:09:30.906
    Dec 14 08:09:30.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:09:31.245: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:09:31.245: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:09:31.245: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:09:31.245: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:09:31.473: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:09:31.473: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:09:31.473: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:09:31.473: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:09:31.921: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:09:31.921: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:09:31.921: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:09:31.921: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:09:31.924: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Dec 14 08:09:41.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:09:41.934: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:09:41.934: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:09:41.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999735s
    Dec 14 08:09:42.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995799369s
    Dec 14 08:09:43.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991371652s
    Dec 14 08:09:44.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986130931s
    Dec 14 08:09:45.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982008943s
    Dec 14 08:09:46.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976944341s
    Dec 14 08:09:47.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972440631s
    Dec 14 08:09:48.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966914273s
    Dec 14 08:09:49.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961713955s
    Dec 14 08:09:50.990: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.907164ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3352 12/14/22 08:09:51.99
    Dec 14 08:09:51.995: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:09:52.415: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 08:09:52.415: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:09:52.415: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:09:52.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:09:52.821: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 08:09:52.821: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:09:52.821: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:09:52.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:09:52.994: INFO: rc: 1
    Dec 14 08:09:52.994: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: unable to upgrade connection: container not found ("webserver")

    error:
    exit status 1
    Dec 14 08:10:02.995: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:10:03.096: INFO: rc: 1
    Dec 14 08:10:03.096: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:10:13.097: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:10:13.172: INFO: rc: 1
    Dec 14 08:10:13.172: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:10:23.174: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:10:23.249: INFO: rc: 1
    Dec 14 08:10:23.249: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:10:33.249: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:10:33.334: INFO: rc: 1
    Dec 14 08:10:33.334: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:10:43.338: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:10:43.431: INFO: rc: 1
    Dec 14 08:10:43.431: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:10:53.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:10:53.535: INFO: rc: 1
    Dec 14 08:10:53.535: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:11:03.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:11:03.632: INFO: rc: 1
    Dec 14 08:11:03.632: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:11:13.634: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:11:13.728: INFO: rc: 1
    Dec 14 08:11:13.728: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:11:23.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:11:23.826: INFO: rc: 1
    Dec 14 08:11:23.826: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:11:33.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:11:33.908: INFO: rc: 1
    Dec 14 08:11:33.908: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:11:43.909: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:11:44.044: INFO: rc: 1
    Dec 14 08:11:44.044: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:11:54.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:11:54.131: INFO: rc: 1
    Dec 14 08:11:54.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:12:04.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:12:04.203: INFO: rc: 1
    Dec 14 08:12:04.203: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:12:14.203: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:12:14.274: INFO: rc: 1
    Dec 14 08:12:14.274: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:12:24.274: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:12:24.347: INFO: rc: 1
    Dec 14 08:12:24.347: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:12:34.347: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:12:34.417: INFO: rc: 1
    Dec 14 08:12:34.417: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:12:44.417: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:12:44.487: INFO: rc: 1
    Dec 14 08:12:44.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:12:54.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:12:54.566: INFO: rc: 1
    Dec 14 08:12:54.566: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:13:04.567: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:13:04.657: INFO: rc: 1
    Dec 14 08:13:04.657: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:13:14.658: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:13:14.730: INFO: rc: 1
    Dec 14 08:13:14.730: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:13:24.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:13:24.802: INFO: rc: 1
    Dec 14 08:13:24.802: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:13:34.802: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:13:34.867: INFO: rc: 1
    Dec 14 08:13:34.867: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:13:44.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:13:44.937: INFO: rc: 1
    Dec 14 08:13:44.937: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:13:54.937: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:13:55.007: INFO: rc: 1
    Dec 14 08:13:55.007: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:14:05.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:14:05.084: INFO: rc: 1
    Dec 14 08:14:05.084: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:14:15.084: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:14:15.149: INFO: rc: 1
    Dec 14 08:14:15.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:14:25.151: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:14:25.218: INFO: rc: 1
    Dec 14 08:14:25.218: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:14:35.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:14:35.281: INFO: rc: 1
    Dec 14 08:14:35.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:14:45.281: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:14:45.357: INFO: rc: 1
    Dec 14 08:14:45.357: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Dec 14 08:14:55.357: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:14:55.424: INFO: rc: 1
    Dec 14 08:14:55.424: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Dec 14 08:14:55.424: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 12/14/22 08:14:55.436
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 08:14:55.436: INFO: Deleting all statefulset in ns statefulset-3352
    Dec 14 08:14:55.439: INFO: Scaling statefulset ss to 0
    Dec 14 08:14:55.450: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:14:55.453: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 08:14:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3352" for this suite. 12/14/22 08:14:55.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:14:55.474
Dec 14 08:14:55.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 08:14:55.475
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:14:55.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:14:55.489
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 12/14/22 08:14:55.494
STEP: waiting for pod running 12/14/22 08:14:55.532
Dec 14 08:14:55.532: INFO: Waiting up to 2m0s for pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" in namespace "var-expansion-6690" to be "running"
Dec 14 08:14:55.537: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392675ms
Dec 14 08:14:57.542: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297": Phase="Running", Reason="", readiness=true. Elapsed: 2.009613254s
Dec 14 08:14:57.542: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" satisfied condition "running"
STEP: creating a file in subpath 12/14/22 08:14:57.542
Dec 14 08:14:57.546: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6690 PodName:var-expansion-eb655291-3575-406e-9efa-3c48b086e297 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:14:57.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:14:57.546: INFO: ExecWithOptions: Clientset creation
Dec 14 08:14:57.546: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-6690/pods/var-expansion-eb655291-3575-406e-9efa-3c48b086e297/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 12/14/22 08:14:57.815
Dec 14 08:14:57.820: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6690 PodName:var-expansion-eb655291-3575-406e-9efa-3c48b086e297 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:14:57.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:14:57.821: INFO: ExecWithOptions: Clientset creation
Dec 14 08:14:57.821: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-6690/pods/var-expansion-eb655291-3575-406e-9efa-3c48b086e297/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 12/14/22 08:14:58.161
Dec 14 08:14:58.672: INFO: Successfully updated pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297"
STEP: waiting for annotated pod running 12/14/22 08:14:58.672
Dec 14 08:14:58.672: INFO: Waiting up to 2m0s for pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" in namespace "var-expansion-6690" to be "running"
Dec 14 08:14:58.690: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297": Phase="Running", Reason="", readiness=true. Elapsed: 17.882686ms
Dec 14 08:14:58.690: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" satisfied condition "running"
STEP: deleting the pod gracefully 12/14/22 08:14:58.69
Dec 14 08:14:58.690: INFO: Deleting pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" in namespace "var-expansion-6690"
Dec 14 08:14:58.695: INFO: Wait up to 5m0s for pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 08:15:32.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6690" for this suite. 12/14/22 08:15:32.72
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":40,"skipped":829,"failed":0}
------------------------------
• [37.252 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:14:55.474
    Dec 14 08:14:55.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 08:14:55.475
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:14:55.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:14:55.489
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 12/14/22 08:14:55.494
    STEP: waiting for pod running 12/14/22 08:14:55.532
    Dec 14 08:14:55.532: INFO: Waiting up to 2m0s for pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" in namespace "var-expansion-6690" to be "running"
    Dec 14 08:14:55.537: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392675ms
    Dec 14 08:14:57.542: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297": Phase="Running", Reason="", readiness=true. Elapsed: 2.009613254s
    Dec 14 08:14:57.542: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" satisfied condition "running"
    STEP: creating a file in subpath 12/14/22 08:14:57.542
    Dec 14 08:14:57.546: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6690 PodName:var-expansion-eb655291-3575-406e-9efa-3c48b086e297 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:14:57.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:14:57.546: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:14:57.546: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-6690/pods/var-expansion-eb655291-3575-406e-9efa-3c48b086e297/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 12/14/22 08:14:57.815
    Dec 14 08:14:57.820: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6690 PodName:var-expansion-eb655291-3575-406e-9efa-3c48b086e297 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:14:57.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:14:57.821: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:14:57.821: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-6690/pods/var-expansion-eb655291-3575-406e-9efa-3c48b086e297/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 12/14/22 08:14:58.161
    Dec 14 08:14:58.672: INFO: Successfully updated pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297"
    STEP: waiting for annotated pod running 12/14/22 08:14:58.672
    Dec 14 08:14:58.672: INFO: Waiting up to 2m0s for pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" in namespace "var-expansion-6690" to be "running"
    Dec 14 08:14:58.690: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297": Phase="Running", Reason="", readiness=true. Elapsed: 17.882686ms
    Dec 14 08:14:58.690: INFO: Pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" satisfied condition "running"
    STEP: deleting the pod gracefully 12/14/22 08:14:58.69
    Dec 14 08:14:58.690: INFO: Deleting pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" in namespace "var-expansion-6690"
    Dec 14 08:14:58.695: INFO: Wait up to 5m0s for pod "var-expansion-eb655291-3575-406e-9efa-3c48b086e297" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 08:15:32.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6690" for this suite. 12/14/22 08:15:32.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:32.727
Dec 14 08:15:32.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:15:32.728
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:32.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:32.748
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Dec 14 08:15:32.756: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 14 08:15:32.766: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 08:15:37.772: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 08:15:37.772
Dec 14 08:15:37.772: INFO: Creating deployment "test-rolling-update-deployment"
Dec 14 08:15:37.777: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 14 08:15:37.786: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 14 08:15:39.797: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 14 08:15:39.800: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:15:39.810: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1977  84029db6-0df4-4456-944b-734807cfba85 10891 1 2022-12-14 08:15:37 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-14 08:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00060c198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 08:15:37 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-12-14 08:15:39 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 08:15:39.814: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1977  b3af0e72-e753-4854-9399-fb3eefafd3a7 10884 1 2022-12-14 08:15:37 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 84029db6-0df4-4456-944b-734807cfba85 0xc00060dae7 0xc00060dae8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84029db6-0df4-4456-944b-734807cfba85\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000958338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:15:39.814: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 14 08:15:39.814: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1977  8bb167ca-9749-48fa-bd10-ee18b6e0bf60 10890 2 2022-12-14 08:15:32 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 84029db6-0df4-4456-944b-734807cfba85 0xc00060d767 0xc00060d768}] [] [{e2e.test Update apps/v1 2022-12-14 08:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84029db6-0df4-4456-944b-734807cfba85\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00060da18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:15:39.818: INFO: Pod "test-rolling-update-deployment-78f575d8ff-m9x4b" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-m9x4b test-rolling-update-deployment-78f575d8ff- deployment-1977  535dd0e6-963f-4c63-84f1-c612ec3b865e 10883 0 2022-12-14 08:15:37 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:d5f23886501ec810c74bca1d756d7ab06dc113cd5afb8c7164153904a670063c cni.projectcalico.org/podIP:100.64.1.56/32 cni.projectcalico.org/podIPs:100.64.1.56/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff b3af0e72-e753-4854-9399-fb3eefafd3a7 0xc000959cb7 0xc000959cb8}] [] [{kube-controller-manager Update v1 2022-12-14 08:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3af0e72-e753-4854-9399-fb3eefafd3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4xjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4xjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.56,StartTime:2022-12-14 08:15:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:15:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1c64f83ccdb7f4fa9447b902a9f315b58ce01f0a672ed2b9e4acb7b455e0f25a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:15:39.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1977" for this suite. 12/14/22 08:15:39.824
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":41,"skipped":859,"failed":0}
------------------------------
• [7.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:32.727
    Dec 14 08:15:32.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:15:32.728
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:32.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:32.748
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Dec 14 08:15:32.756: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Dec 14 08:15:32.766: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 08:15:37.772: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 08:15:37.772
    Dec 14 08:15:37.772: INFO: Creating deployment "test-rolling-update-deployment"
    Dec 14 08:15:37.777: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Dec 14 08:15:37.786: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Dec 14 08:15:39.797: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Dec 14 08:15:39.800: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:15:39.810: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1977  84029db6-0df4-4456-944b-734807cfba85 10891 1 2022-12-14 08:15:37 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-14 08:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00060c198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 08:15:37 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-12-14 08:15:39 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 08:15:39.814: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1977  b3af0e72-e753-4854-9399-fb3eefafd3a7 10884 1 2022-12-14 08:15:37 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 84029db6-0df4-4456-944b-734807cfba85 0xc00060dae7 0xc00060dae8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84029db6-0df4-4456-944b-734807cfba85\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000958338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:15:39.814: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Dec 14 08:15:39.814: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1977  8bb167ca-9749-48fa-bd10-ee18b6e0bf60 10890 2 2022-12-14 08:15:32 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 84029db6-0df4-4456-944b-734807cfba85 0xc00060d767 0xc00060d768}] [] [{e2e.test Update apps/v1 2022-12-14 08:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84029db6-0df4-4456-944b-734807cfba85\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00060da18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:15:39.818: INFO: Pod "test-rolling-update-deployment-78f575d8ff-m9x4b" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-m9x4b test-rolling-update-deployment-78f575d8ff- deployment-1977  535dd0e6-963f-4c63-84f1-c612ec3b865e 10883 0 2022-12-14 08:15:37 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:d5f23886501ec810c74bca1d756d7ab06dc113cd5afb8c7164153904a670063c cni.projectcalico.org/podIP:100.64.1.56/32 cni.projectcalico.org/podIPs:100.64.1.56/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff b3af0e72-e753-4854-9399-fb3eefafd3a7 0xc000959cb7 0xc000959cb8}] [] [{kube-controller-manager Update v1 2022-12-14 08:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3af0e72-e753-4854-9399-fb3eefafd3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:15:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4xjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4xjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.56,StartTime:2022-12-14 08:15:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:15:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1c64f83ccdb7f4fa9447b902a9f315b58ce01f0a672ed2b9e4acb7b455e0f25a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:15:39.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1977" for this suite. 12/14/22 08:15:39.824
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:39.829
Dec 14 08:15:39.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 08:15:39.829
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:39.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:39.846
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Dec 14 08:15:39.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:15:40.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6911" for this suite. 12/14/22 08:15:40.394
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":42,"skipped":859,"failed":0}
------------------------------
• [0.571 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:39.829
    Dec 14 08:15:39.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 08:15:39.829
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:39.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:39.846
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Dec 14 08:15:39.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:15:40.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6911" for this suite. 12/14/22 08:15:40.394
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:40.4
Dec 14 08:15:40.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 12/14/22 08:15:40.4
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:40.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:40.415
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Dec 14 08:15:40.429: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92" in namespace "security-context-test-5801" to be "Succeeded or Failed"
Dec 14 08:15:40.434: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72434ms
Dec 14 08:15:42.440: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92": Phase="Running", Reason="", readiness=false. Elapsed: 2.010896922s
Dec 14 08:15:44.439: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009990325s
Dec 14 08:15:44.440: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 14 08:15:44.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5801" for this suite. 12/14/22 08:15:44.446
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":43,"skipped":861,"failed":0}
------------------------------
• [4.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:40.4
    Dec 14 08:15:40.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 12/14/22 08:15:40.4
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:40.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:40.415
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Dec 14 08:15:40.429: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92" in namespace "security-context-test-5801" to be "Succeeded or Failed"
    Dec 14 08:15:40.434: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72434ms
    Dec 14 08:15:42.440: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92": Phase="Running", Reason="", readiness=false. Elapsed: 2.010896922s
    Dec 14 08:15:44.439: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009990325s
    Dec 14 08:15:44.440: INFO: Pod "busybox-readonly-false-0018b19e-b4a3-4a37-8fc4-c364b753fa92" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 14 08:15:44.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5801" for this suite. 12/14/22 08:15:44.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:44.451
Dec 14 08:15:44.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 12/14/22 08:15:44.452
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:44.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:44.47
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Dec 14 08:15:44.489: INFO: Waiting up to 5m0s for pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860" in namespace "svcaccounts-3727" to be "running"
Dec 14 08:15:44.494: INFO: Pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300491ms
Dec 14 08:15:46.501: INFO: Pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860": Phase="Running", Reason="", readiness=true. Elapsed: 2.0112854s
Dec 14 08:15:46.501: INFO: Pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860" satisfied condition "running"
STEP: reading a file in the container 12/14/22 08:15:46.501
Dec 14 08:15:46.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3727 pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 12/14/22 08:15:46.808
Dec 14 08:15:46.808: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3727 pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 12/14/22 08:15:47.268
Dec 14 08:15:47.268: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3727 pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Dec 14 08:15:47.591: INFO: Got root ca configmap in namespace "svcaccounts-3727"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 14 08:15:47.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3727" for this suite. 12/14/22 08:15:47.602
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":44,"skipped":883,"failed":0}
------------------------------
• [3.156 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:44.451
    Dec 14 08:15:44.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 08:15:44.452
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:44.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:44.47
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Dec 14 08:15:44.489: INFO: Waiting up to 5m0s for pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860" in namespace "svcaccounts-3727" to be "running"
    Dec 14 08:15:44.494: INFO: Pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300491ms
    Dec 14 08:15:46.501: INFO: Pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860": Phase="Running", Reason="", readiness=true. Elapsed: 2.0112854s
    Dec 14 08:15:46.501: INFO: Pod "pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860" satisfied condition "running"
    STEP: reading a file in the container 12/14/22 08:15:46.501
    Dec 14 08:15:46.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3727 pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 12/14/22 08:15:46.808
    Dec 14 08:15:46.808: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3727 pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 12/14/22 08:15:47.268
    Dec 14 08:15:47.268: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-3727 pod-service-account-d85a08cc-16b7-4b6c-8cd9-723aa82be860 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Dec 14 08:15:47.591: INFO: Got root ca configmap in namespace "svcaccounts-3727"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 14 08:15:47.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3727" for this suite. 12/14/22 08:15:47.602
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:47.607
Dec 14 08:15:47.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:15:47.608
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:47.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:47.626
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Dec 14 08:15:47.638: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 14 08:15:52.642: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 08:15:52.642
Dec 14 08:15:52.643: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/14/22 08:15:52.654
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:15:54.675: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6494  b4731e37-959a-4e87-b1e1-a07ff1cb00a5 11060 1 2022-12-14 08:15:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-14 08:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002605068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 08:15:52 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2022-12-14 08:15:54 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 08:15:54.680: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6494  da560930-43db-4cf7-9f89-5b78e5c7c8e9 11053 1 2022-12-14 08:15:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b4731e37-959a-4e87-b1e1-a07ff1cb00a5 0xc0026059e7 0xc0026059e8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4731e37-959a-4e87-b1e1-a07ff1cb00a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002605a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:15:54.684: INFO: Pod "test-cleanup-deployment-69cb9c5497-6fzwq" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-6fzwq test-cleanup-deployment-69cb9c5497- deployment-6494  45d9f3b6-40f3-4a50-9ebb-c311fdcef187 11052 0 2022-12-14 08:15:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:cea6f2452699626139ead3655e2996d8b44ed2f3cb393e45abbca4cadcb3bf06 cni.projectcalico.org/podIP:100.64.1.60/32 cni.projectcalico.org/podIPs:100.64.1.60/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 da560930-43db-4cf7-9f89-5b78e5c7c8e9 0xc002605e97 0xc002605e98}] [] [{kube-controller-manager Update v1 2022-12-14 08:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da560930-43db-4cf7-9f89-5b78e5c7c8e9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:15:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfz25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfz25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.60,StartTime:2022-12-14 08:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://0c437334c01cadebb9f5a397193739305d0f6ed1d82491c0b6140c2c30d53d49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:15:54.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6494" for this suite. 12/14/22 08:15:54.691
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":45,"skipped":886,"failed":0}
------------------------------
• [7.090 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:47.607
    Dec 14 08:15:47.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:15:47.608
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:47.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:47.626
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Dec 14 08:15:47.638: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Dec 14 08:15:52.642: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 08:15:52.642
    Dec 14 08:15:52.643: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/14/22 08:15:52.654
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:15:54.675: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6494  b4731e37-959a-4e87-b1e1-a07ff1cb00a5 11060 1 2022-12-14 08:15:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-14 08:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002605068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 08:15:52 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2022-12-14 08:15:54 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 08:15:54.680: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6494  da560930-43db-4cf7-9f89-5b78e5c7c8e9 11053 1 2022-12-14 08:15:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b4731e37-959a-4e87-b1e1-a07ff1cb00a5 0xc0026059e7 0xc0026059e8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4731e37-959a-4e87-b1e1-a07ff1cb00a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:15:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002605a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:15:54.684: INFO: Pod "test-cleanup-deployment-69cb9c5497-6fzwq" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-6fzwq test-cleanup-deployment-69cb9c5497- deployment-6494  45d9f3b6-40f3-4a50-9ebb-c311fdcef187 11052 0 2022-12-14 08:15:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:cea6f2452699626139ead3655e2996d8b44ed2f3cb393e45abbca4cadcb3bf06 cni.projectcalico.org/podIP:100.64.1.60/32 cni.projectcalico.org/podIPs:100.64.1.60/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 da560930-43db-4cf7-9f89-5b78e5c7c8e9 0xc002605e97 0xc002605e98}] [] [{kube-controller-manager Update v1 2022-12-14 08:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da560930-43db-4cf7-9f89-5b78e5c7c8e9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:15:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfz25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfz25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.60,StartTime:2022-12-14 08:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://0c437334c01cadebb9f5a397193739305d0f6ed1d82491c0b6140c2c30d53d49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:15:54.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6494" for this suite. 12/14/22 08:15:54.691
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:54.698
Dec 14 08:15:54.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:15:54.698
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:54.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:54.715
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-3e7b3dc1-1f32-4177-8e9f-27699b2a6e39 12/14/22 08:15:54.72
STEP: Creating a pod to test consume secrets 12/14/22 08:15:54.725
Dec 14 08:15:54.736: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8" in namespace "projected-4196" to be "Succeeded or Failed"
Dec 14 08:15:54.740: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300499ms
Dec 14 08:15:56.746: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010033849s
Dec 14 08:15:58.746: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010737894s
STEP: Saw pod success 12/14/22 08:15:58.746
Dec 14 08:15:58.747: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8" satisfied condition "Succeeded or Failed"
Dec 14 08:15:58.751: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 08:15:58.766
Dec 14 08:15:58.774: INFO: Waiting for pod pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8 to disappear
Dec 14 08:15:58.777: INFO: Pod pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 08:15:58.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4196" for this suite. 12/14/22 08:15:58.783
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":46,"skipped":890,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:54.698
    Dec 14 08:15:54.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:15:54.698
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:54.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:54.715
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-3e7b3dc1-1f32-4177-8e9f-27699b2a6e39 12/14/22 08:15:54.72
    STEP: Creating a pod to test consume secrets 12/14/22 08:15:54.725
    Dec 14 08:15:54.736: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8" in namespace "projected-4196" to be "Succeeded or Failed"
    Dec 14 08:15:54.740: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300499ms
    Dec 14 08:15:56.746: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010033849s
    Dec 14 08:15:58.746: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010737894s
    STEP: Saw pod success 12/14/22 08:15:58.746
    Dec 14 08:15:58.747: INFO: Pod "pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8" satisfied condition "Succeeded or Failed"
    Dec 14 08:15:58.751: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 08:15:58.766
    Dec 14 08:15:58.774: INFO: Waiting for pod pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8 to disappear
    Dec 14 08:15:58.777: INFO: Pod pod-projected-secrets-fd6e6608-37d3-4953-81d7-9b27a73dbbe8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 08:15:58.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4196" for this suite. 12/14/22 08:15:58.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:15:58.789
Dec 14 08:15:58.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:15:58.79
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:58.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:58.808
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 08:15:58.813
Dec 14 08:15:58.823: INFO: Waiting up to 5m0s for pod "pod-6a924133-257c-4e7e-8e20-12354295af02" in namespace "emptydir-6657" to be "Succeeded or Failed"
Dec 14 08:15:58.827: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535963ms
Dec 14 08:16:00.832: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008513331s
Dec 14 08:16:02.833: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009500841s
STEP: Saw pod success 12/14/22 08:16:02.833
Dec 14 08:16:02.833: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02" satisfied condition "Succeeded or Failed"
Dec 14 08:16:02.836: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-6a924133-257c-4e7e-8e20-12354295af02 container test-container: <nil>
STEP: delete the pod 12/14/22 08:16:02.846
Dec 14 08:16:02.854: INFO: Waiting for pod pod-6a924133-257c-4e7e-8e20-12354295af02 to disappear
Dec 14 08:16:02.858: INFO: Pod pod-6a924133-257c-4e7e-8e20-12354295af02 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:16:02.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6657" for this suite. 12/14/22 08:16:02.863
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":47,"skipped":908,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:15:58.789
    Dec 14 08:15:58.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:15:58.79
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:15:58.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:15:58.808
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 08:15:58.813
    Dec 14 08:15:58.823: INFO: Waiting up to 5m0s for pod "pod-6a924133-257c-4e7e-8e20-12354295af02" in namespace "emptydir-6657" to be "Succeeded or Failed"
    Dec 14 08:15:58.827: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535963ms
    Dec 14 08:16:00.832: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008513331s
    Dec 14 08:16:02.833: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009500841s
    STEP: Saw pod success 12/14/22 08:16:02.833
    Dec 14 08:16:02.833: INFO: Pod "pod-6a924133-257c-4e7e-8e20-12354295af02" satisfied condition "Succeeded or Failed"
    Dec 14 08:16:02.836: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-6a924133-257c-4e7e-8e20-12354295af02 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:16:02.846
    Dec 14 08:16:02.854: INFO: Waiting for pod pod-6a924133-257c-4e7e-8e20-12354295af02 to disappear
    Dec 14 08:16:02.858: INFO: Pod pod-6a924133-257c-4e7e-8e20-12354295af02 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:16:02.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6657" for this suite. 12/14/22 08:16:02.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:02.869
Dec 14 08:16:02.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:16:02.87
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:02.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:02.888
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:16:02.893
Dec 14 08:16:02.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393" in namespace "downward-api-9313" to be "Succeeded or Failed"
Dec 14 08:16:02.908: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27038ms
Dec 14 08:16:04.913: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008637626s
Dec 14 08:16:06.912: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007288792s
STEP: Saw pod success 12/14/22 08:16:06.912
Dec 14 08:16:06.912: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393" satisfied condition "Succeeded or Failed"
Dec 14 08:16:06.916: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393 container client-container: <nil>
STEP: delete the pod 12/14/22 08:16:06.928
Dec 14 08:16:06.935: INFO: Waiting for pod downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393 to disappear
Dec 14 08:16:06.937: INFO: Pod downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 08:16:06.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9313" for this suite. 12/14/22 08:16:06.943
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":48,"skipped":918,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:02.869
    Dec 14 08:16:02.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:16:02.87
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:02.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:02.888
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:16:02.893
    Dec 14 08:16:02.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393" in namespace "downward-api-9313" to be "Succeeded or Failed"
    Dec 14 08:16:02.908: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27038ms
    Dec 14 08:16:04.913: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008637626s
    Dec 14 08:16:06.912: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007288792s
    STEP: Saw pod success 12/14/22 08:16:06.912
    Dec 14 08:16:06.912: INFO: Pod "downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393" satisfied condition "Succeeded or Failed"
    Dec 14 08:16:06.916: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393 container client-container: <nil>
    STEP: delete the pod 12/14/22 08:16:06.928
    Dec 14 08:16:06.935: INFO: Waiting for pod downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393 to disappear
    Dec 14 08:16:06.937: INFO: Pod downwardapi-volume-d4c48fa2-0a33-48c0-9019-d5c691b6e393 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 08:16:06.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9313" for this suite. 12/14/22 08:16:06.943
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:06.947
Dec 14 08:16:06.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 08:16:06.948
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:06.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:06.964
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Dec 14 08:16:06.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:16:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7082" for this suite. 12/14/22 08:16:10.107
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":49,"skipped":921,"failed":0}
------------------------------
• [3.165 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:06.947
    Dec 14 08:16:06.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 08:16:06.948
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:06.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:06.964
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Dec 14 08:16:06.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:16:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7082" for this suite. 12/14/22 08:16:10.107
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:10.112
Dec 14 08:16:10.112: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates 12/14/22 08:16:10.113
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:10.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:10.128
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 12/14/22 08:16:10.603
STEP: getting /apis/certificates.k8s.io 12/14/22 08:16:10.609
STEP: getting /apis/certificates.k8s.io/v1 12/14/22 08:16:10.611
STEP: creating 12/14/22 08:16:10.614
STEP: getting 12/14/22 08:16:10.627
STEP: listing 12/14/22 08:16:10.631
STEP: watching 12/14/22 08:16:10.635
Dec 14 08:16:10.635: INFO: starting watch
STEP: patching 12/14/22 08:16:10.637
STEP: updating 12/14/22 08:16:10.642
Dec 14 08:16:10.647: INFO: waiting for watch events with expected annotations
Dec 14 08:16:10.647: INFO: saw patched and updated annotations
STEP: getting /approval 12/14/22 08:16:10.647
STEP: patching /approval 12/14/22 08:16:10.651
STEP: updating /approval 12/14/22 08:16:10.656
STEP: getting /status 12/14/22 08:16:10.662
STEP: patching /status 12/14/22 08:16:10.665
STEP: updating /status 12/14/22 08:16:10.672
STEP: deleting 12/14/22 08:16:10.678
STEP: deleting a collection 12/14/22 08:16:10.69
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:16:10.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3321" for this suite. 12/14/22 08:16:10.709
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":50,"skipped":923,"failed":0}
------------------------------
• [0.602 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:10.112
    Dec 14 08:16:10.112: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename certificates 12/14/22 08:16:10.113
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:10.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:10.128
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 12/14/22 08:16:10.603
    STEP: getting /apis/certificates.k8s.io 12/14/22 08:16:10.609
    STEP: getting /apis/certificates.k8s.io/v1 12/14/22 08:16:10.611
    STEP: creating 12/14/22 08:16:10.614
    STEP: getting 12/14/22 08:16:10.627
    STEP: listing 12/14/22 08:16:10.631
    STEP: watching 12/14/22 08:16:10.635
    Dec 14 08:16:10.635: INFO: starting watch
    STEP: patching 12/14/22 08:16:10.637
    STEP: updating 12/14/22 08:16:10.642
    Dec 14 08:16:10.647: INFO: waiting for watch events with expected annotations
    Dec 14 08:16:10.647: INFO: saw patched and updated annotations
    STEP: getting /approval 12/14/22 08:16:10.647
    STEP: patching /approval 12/14/22 08:16:10.651
    STEP: updating /approval 12/14/22 08:16:10.656
    STEP: getting /status 12/14/22 08:16:10.662
    STEP: patching /status 12/14/22 08:16:10.665
    STEP: updating /status 12/14/22 08:16:10.672
    STEP: deleting 12/14/22 08:16:10.678
    STEP: deleting a collection 12/14/22 08:16:10.69
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:16:10.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-3321" for this suite. 12/14/22 08:16:10.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:10.715
Dec 14 08:16:10.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:16:10.717
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:10.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:10.737
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 12/14/22 08:16:10.741
Dec 14 08:16:10.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 14 08:16:10.741: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
Dec 14 08:16:11.696: INFO: stderr: ""
Dec 14 08:16:11.696: INFO: stdout: "service/agnhost-replica created\n"
Dec 14 08:16:11.696: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 14 08:16:11.696: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
Dec 14 08:16:12.585: INFO: stderr: ""
Dec 14 08:16:12.585: INFO: stdout: "service/agnhost-primary created\n"
Dec 14 08:16:12.585: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 14 08:16:12.585: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
Dec 14 08:16:12.767: INFO: stderr: ""
Dec 14 08:16:12.767: INFO: stdout: "service/frontend created\n"
Dec 14 08:16:12.767: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 14 08:16:12.767: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
Dec 14 08:16:12.943: INFO: stderr: ""
Dec 14 08:16:12.943: INFO: stdout: "deployment.apps/frontend created\n"
Dec 14 08:16:12.943: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 08:16:12.943: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
Dec 14 08:16:13.129: INFO: stderr: ""
Dec 14 08:16:13.129: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 14 08:16:13.129: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 08:16:13.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
Dec 14 08:16:13.305: INFO: stderr: ""
Dec 14 08:16:13.305: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 12/14/22 08:16:13.305
Dec 14 08:16:13.305: INFO: Waiting for all frontend pods to be Running.
Dec 14 08:16:18.356: INFO: Waiting for frontend to serve content.
Dec 14 08:16:18.461: INFO: Trying to add a new entry to the guestbook.
Dec 14 08:16:18.609: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 12/14/22 08:16:18.721
Dec 14 08:16:18.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
Dec 14 08:16:18.810: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:16:18.810: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 08:16:18.81
Dec 14 08:16:18.810: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
Dec 14 08:16:18.878: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:16:18.878: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 08:16:18.878
Dec 14 08:16:18.878: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
Dec 14 08:16:18.948: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:16:18.948: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 08:16:18.948
Dec 14 08:16:18.948: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
Dec 14 08:16:19.009: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:16:19.009: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 08:16:19.009
Dec 14 08:16:19.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
Dec 14 08:16:19.069: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:16:19.069: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 08:16:19.069
Dec 14 08:16:19.069: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
Dec 14 08:16:19.129: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:16:19.129: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:16:19.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7602" for this suite. 12/14/22 08:16:19.135
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":51,"skipped":933,"failed":0}
------------------------------
• [8.428 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:10.715
    Dec 14 08:16:10.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:16:10.717
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:10.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:10.737
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 12/14/22 08:16:10.741
    Dec 14 08:16:10.741: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Dec 14 08:16:10.741: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
    Dec 14 08:16:11.696: INFO: stderr: ""
    Dec 14 08:16:11.696: INFO: stdout: "service/agnhost-replica created\n"
    Dec 14 08:16:11.696: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Dec 14 08:16:11.696: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
    Dec 14 08:16:12.585: INFO: stderr: ""
    Dec 14 08:16:12.585: INFO: stdout: "service/agnhost-primary created\n"
    Dec 14 08:16:12.585: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Dec 14 08:16:12.585: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
    Dec 14 08:16:12.767: INFO: stderr: ""
    Dec 14 08:16:12.767: INFO: stdout: "service/frontend created\n"
    Dec 14 08:16:12.767: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Dec 14 08:16:12.767: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
    Dec 14 08:16:12.943: INFO: stderr: ""
    Dec 14 08:16:12.943: INFO: stdout: "deployment.apps/frontend created\n"
    Dec 14 08:16:12.943: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 14 08:16:12.943: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
    Dec 14 08:16:13.129: INFO: stderr: ""
    Dec 14 08:16:13.129: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Dec 14 08:16:13.129: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 14 08:16:13.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 create -f -'
    Dec 14 08:16:13.305: INFO: stderr: ""
    Dec 14 08:16:13.305: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 12/14/22 08:16:13.305
    Dec 14 08:16:13.305: INFO: Waiting for all frontend pods to be Running.
    Dec 14 08:16:18.356: INFO: Waiting for frontend to serve content.
    Dec 14 08:16:18.461: INFO: Trying to add a new entry to the guestbook.
    Dec 14 08:16:18.609: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 12/14/22 08:16:18.721
    Dec 14 08:16:18.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
    Dec 14 08:16:18.810: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:16:18.810: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 08:16:18.81
    Dec 14 08:16:18.810: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
    Dec 14 08:16:18.878: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:16:18.878: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 08:16:18.878
    Dec 14 08:16:18.878: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
    Dec 14 08:16:18.948: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:16:18.948: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 08:16:18.948
    Dec 14 08:16:18.948: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
    Dec 14 08:16:19.009: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:16:19.009: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 08:16:19.009
    Dec 14 08:16:19.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
    Dec 14 08:16:19.069: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:16:19.069: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 08:16:19.069
    Dec 14 08:16:19.069: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7602 delete --grace-period=0 --force -f -'
    Dec 14 08:16:19.129: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:16:19.129: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:16:19.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7602" for this suite. 12/14/22 08:16:19.135
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:19.144
Dec 14 08:16:19.144: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 08:16:19.145
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:19.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:19.161
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-e077bc40-33cd-489b-9a44-8e88fb93734a 12/14/22 08:16:19.166
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 14 08:16:19.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9230" for this suite. 12/14/22 08:16:19.173
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":52,"skipped":937,"failed":0}
------------------------------
• [0.033 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:19.144
    Dec 14 08:16:19.144: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 08:16:19.145
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:19.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:19.161
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-e077bc40-33cd-489b-9a44-8e88fb93734a 12/14/22 08:16:19.166
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 08:16:19.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9230" for this suite. 12/14/22 08:16:19.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:19.179
Dec 14 08:16:19.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostport 12/14/22 08:16:19.179
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:19.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:19.196
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/14/22 08:16:19.205
Dec 14 08:16:19.215: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6119" to be "running and ready"
Dec 14 08:16:19.219: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.184615ms
Dec 14 08:16:19.219: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:16:21.225: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009478025s
Dec 14 08:16:21.225: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 14 08:16:21.225: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.3.58 on the node which pod1 resides and expect scheduled 12/14/22 08:16:21.225
Dec 14 08:16:21.234: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6119" to be "running and ready"
Dec 14 08:16:21.237: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534752ms
Dec 14 08:16:21.237: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:16:23.243: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008566353s
Dec 14 08:16:23.243: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 14 08:16:23.243: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.3.58 but use UDP protocol on the node which pod2 resides 12/14/22 08:16:23.243
Dec 14 08:16:23.253: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6119" to be "running and ready"
Dec 14 08:16:23.256: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513542ms
Dec 14 08:16:23.256: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:16:25.262: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009411873s
Dec 14 08:16:25.262: INFO: The phase of Pod pod3 is Running (Ready = false)
Dec 14 08:16:27.262: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009258976s
Dec 14 08:16:27.262: INFO: The phase of Pod pod3 is Running (Ready = true)
Dec 14 08:16:27.262: INFO: Pod "pod3" satisfied condition "running and ready"
Dec 14 08:16:27.271: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6119" to be "running and ready"
Dec 14 08:16:27.275: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363693ms
Dec 14 08:16:27.275: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:16:29.287: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015700989s
Dec 14 08:16:29.287: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Dec 14 08:16:29.287: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/14/22 08:16:29.291
Dec 14 08:16:29.291: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.58 http://127.0.0.1:54323/hostname] Namespace:hostport-6119 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:16:29.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:16:29.291: INFO: ExecWithOptions: Clientset creation
Dec 14 08:16:29.291: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6119/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.3.58+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.58, port: 54323 12/14/22 08:16:29.635
Dec 14 08:16:29.635: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.3.58:54323/hostname] Namespace:hostport-6119 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:16:29.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:16:29.636: INFO: ExecWithOptions: Clientset creation
Dec 14 08:16:29.636: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6119/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.3.58%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.58, port: 54323 UDP 12/14/22 08:16:30.06
Dec 14 08:16:30.060: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.3.58 54323] Namespace:hostport-6119 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:16:30.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:16:30.061: INFO: ExecWithOptions: Clientset creation
Dec 14 08:16:30.061: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6119/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.3.58+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Dec 14 08:16:35.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6119" for this suite. 12/14/22 08:16:35.586
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":53,"skipped":1017,"failed":0}
------------------------------
• [16.412 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:19.179
    Dec 14 08:16:19.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename hostport 12/14/22 08:16:19.179
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:19.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:19.196
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/14/22 08:16:19.205
    Dec 14 08:16:19.215: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6119" to be "running and ready"
    Dec 14 08:16:19.219: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.184615ms
    Dec 14 08:16:19.219: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:16:21.225: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009478025s
    Dec 14 08:16:21.225: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 14 08:16:21.225: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.3.58 on the node which pod1 resides and expect scheduled 12/14/22 08:16:21.225
    Dec 14 08:16:21.234: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6119" to be "running and ready"
    Dec 14 08:16:21.237: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534752ms
    Dec 14 08:16:21.237: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:16:23.243: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008566353s
    Dec 14 08:16:23.243: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 14 08:16:23.243: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.3.58 but use UDP protocol on the node which pod2 resides 12/14/22 08:16:23.243
    Dec 14 08:16:23.253: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6119" to be "running and ready"
    Dec 14 08:16:23.256: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513542ms
    Dec 14 08:16:23.256: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:16:25.262: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009411873s
    Dec 14 08:16:25.262: INFO: The phase of Pod pod3 is Running (Ready = false)
    Dec 14 08:16:27.262: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009258976s
    Dec 14 08:16:27.262: INFO: The phase of Pod pod3 is Running (Ready = true)
    Dec 14 08:16:27.262: INFO: Pod "pod3" satisfied condition "running and ready"
    Dec 14 08:16:27.271: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6119" to be "running and ready"
    Dec 14 08:16:27.275: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363693ms
    Dec 14 08:16:27.275: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:16:29.287: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015700989s
    Dec 14 08:16:29.287: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Dec 14 08:16:29.287: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/14/22 08:16:29.291
    Dec 14 08:16:29.291: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.58 http://127.0.0.1:54323/hostname] Namespace:hostport-6119 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:16:29.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:16:29.291: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:16:29.291: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6119/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.3.58+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.58, port: 54323 12/14/22 08:16:29.635
    Dec 14 08:16:29.635: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.3.58:54323/hostname] Namespace:hostport-6119 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:16:29.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:16:29.636: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:16:29.636: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6119/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.3.58%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.58, port: 54323 UDP 12/14/22 08:16:30.06
    Dec 14 08:16:30.060: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.3.58 54323] Namespace:hostport-6119 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:16:30.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:16:30.061: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:16:30.061: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6119/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.3.58+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Dec 14 08:16:35.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-6119" for this suite. 12/14/22 08:16:35.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:16:35.592
Dec 14 08:16:35.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 12/14/22 08:16:35.593
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:35.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:35.607
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 12/14/22 08:16:35.611
STEP: Ensuring no jobs are scheduled 12/14/22 08:16:35.616
STEP: Ensuring no job exists by listing jobs explicitly 12/14/22 08:21:35.624
STEP: Removing cronjob 12/14/22 08:21:35.627
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 14 08:21:35.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6046" for this suite. 12/14/22 08:21:35.639
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":54,"skipped":1052,"failed":0}
------------------------------
• [SLOW TEST] [300.051 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:16:35.592
    Dec 14 08:16:35.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 12/14/22 08:16:35.593
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:16:35.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:16:35.607
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 12/14/22 08:16:35.611
    STEP: Ensuring no jobs are scheduled 12/14/22 08:16:35.616
    STEP: Ensuring no job exists by listing jobs explicitly 12/14/22 08:21:35.624
    STEP: Removing cronjob 12/14/22 08:21:35.627
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 14 08:21:35.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6046" for this suite. 12/14/22 08:21:35.639
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:21:35.644
Dec 14 08:21:35.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 08:21:35.645
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:21:35.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:21:35.661
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 08:22:35.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7298" for this suite. 12/14/22 08:22:35.696
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":55,"skipped":1056,"failed":0}
------------------------------
• [60.057 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:21:35.644
    Dec 14 08:21:35.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 08:21:35.645
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:21:35.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:21:35.661
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 08:22:35.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7298" for this suite. 12/14/22 08:22:35.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:22:35.701
Dec 14 08:22:35.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 08:22:35.702
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:22:35.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:22:35.719
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 12/14/22 08:22:35.729
STEP: delete the rc 12/14/22 08:22:40.739
STEP: wait for the rc to be deleted 12/14/22 08:22:40.744
Dec 14 08:22:41.764: INFO: 90 pods remaining
Dec 14 08:22:41.764: INFO: 90 pods has nil DeletionTimestamp
Dec 14 08:22:41.764: INFO: 
Dec 14 08:22:42.761: INFO: 70 pods remaining
Dec 14 08:22:42.761: INFO: 70 pods has nil DeletionTimestamp
Dec 14 08:22:42.761: INFO: 
Dec 14 08:22:43.762: INFO: 69 pods remaining
Dec 14 08:22:43.762: INFO: 68 pods has nil DeletionTimestamp
Dec 14 08:22:43.762: INFO: 
Dec 14 08:22:44.759: INFO: 40 pods remaining
Dec 14 08:22:44.759: INFO: 40 pods has nil DeletionTimestamp
Dec 14 08:22:44.759: INFO: 
Dec 14 08:22:45.760: INFO: 40 pods remaining
Dec 14 08:22:45.760: INFO: 40 pods has nil DeletionTimestamp
Dec 14 08:22:45.760: INFO: 
Dec 14 08:22:46.754: INFO: 12 pods remaining
Dec 14 08:22:46.754: INFO: 12 pods has nil DeletionTimestamp
Dec 14 08:22:46.754: INFO: 
STEP: Gathering metrics 12/14/22 08:22:47.752
W1214 08:22:47.768673    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 08:22:47.768: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 08:22:47.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1446" for this suite. 12/14/22 08:22:47.773
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":56,"skipped":1066,"failed":0}
------------------------------
• [12.076 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:22:35.701
    Dec 14 08:22:35.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 08:22:35.702
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:22:35.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:22:35.719
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 12/14/22 08:22:35.729
    STEP: delete the rc 12/14/22 08:22:40.739
    STEP: wait for the rc to be deleted 12/14/22 08:22:40.744
    Dec 14 08:22:41.764: INFO: 90 pods remaining
    Dec 14 08:22:41.764: INFO: 90 pods has nil DeletionTimestamp
    Dec 14 08:22:41.764: INFO: 
    Dec 14 08:22:42.761: INFO: 70 pods remaining
    Dec 14 08:22:42.761: INFO: 70 pods has nil DeletionTimestamp
    Dec 14 08:22:42.761: INFO: 
    Dec 14 08:22:43.762: INFO: 69 pods remaining
    Dec 14 08:22:43.762: INFO: 68 pods has nil DeletionTimestamp
    Dec 14 08:22:43.762: INFO: 
    Dec 14 08:22:44.759: INFO: 40 pods remaining
    Dec 14 08:22:44.759: INFO: 40 pods has nil DeletionTimestamp
    Dec 14 08:22:44.759: INFO: 
    Dec 14 08:22:45.760: INFO: 40 pods remaining
    Dec 14 08:22:45.760: INFO: 40 pods has nil DeletionTimestamp
    Dec 14 08:22:45.760: INFO: 
    Dec 14 08:22:46.754: INFO: 12 pods remaining
    Dec 14 08:22:46.754: INFO: 12 pods has nil DeletionTimestamp
    Dec 14 08:22:46.754: INFO: 
    STEP: Gathering metrics 12/14/22 08:22:47.752
    W1214 08:22:47.768673    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 14 08:22:47.768: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 08:22:47.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1446" for this suite. 12/14/22 08:22:47.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:22:47.778
Dec 14 08:22:47.778: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 12/14/22 08:22:47.779
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:22:47.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:22:47.799
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1166 12/14/22 08:22:47.804
STEP: creating a selector 12/14/22 08:22:47.804
STEP: Creating the service pods in kubernetes 12/14/22 08:22:47.804
Dec 14 08:22:47.804: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 08:22:47.829: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1166" to be "running and ready"
Dec 14 08:22:47.833: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891215ms
Dec 14 08:22:47.833: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:22:49.838: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009472311s
Dec 14 08:22:49.838: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:22:51.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008204348s
Dec 14 08:22:51.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:22:53.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008575941s
Dec 14 08:22:53.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:22:55.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008897009s
Dec 14 08:22:55.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:22:57.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009287511s
Dec 14 08:22:57.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:22:59.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008284632s
Dec 14 08:22:59.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:23:01.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008905637s
Dec 14 08:23:01.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:23:03.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008491993s
Dec 14 08:23:03.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:23:05.839: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010470936s
Dec 14 08:23:05.839: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:23:07.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00849682s
Dec 14 08:23:07.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 08:23:09.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009679381s
Dec 14 08:23:09.838: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 08:23:09.838: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 08:23:09.842: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1166" to be "running and ready"
Dec 14 08:23:09.846: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.232325ms
Dec 14 08:23:09.846: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 08:23:09.846: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 08:23:09.849
Dec 14 08:23:09.857: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1166" to be "running"
Dec 14 08:23:09.861: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78794ms
Dec 14 08:23:11.865: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008378162s
Dec 14 08:23:11.865: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 08:23:11.870: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 14 08:23:11.870: INFO: Breadth first check of 100.64.0.77 on host 10.250.3.58...
Dec 14 08:23:11.873: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.124:9080/dial?request=hostname&protocol=udp&host=100.64.0.77&port=8081&tries=1'] Namespace:pod-network-test-1166 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:23:11.874: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:23:11.874: INFO: ExecWithOptions: Clientset creation
Dec 14 08:23:11.874: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1166/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.0.77%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 08:23:12.264: INFO: Waiting for responses: map[]
Dec 14 08:23:12.264: INFO: reached 100.64.0.77 after 0/1 tries
Dec 14 08:23:12.264: INFO: Breadth first check of 100.64.1.123 on host 10.250.3.210...
Dec 14 08:23:12.268: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.124:9080/dial?request=hostname&protocol=udp&host=100.64.1.123&port=8081&tries=1'] Namespace:pod-network-test-1166 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:23:12.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:23:12.269: INFO: ExecWithOptions: Clientset creation
Dec 14 08:23:12.269: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1166/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.1.123%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 08:23:12.550: INFO: Waiting for responses: map[]
Dec 14 08:23:12.550: INFO: reached 100.64.1.123 after 0/1 tries
Dec 14 08:23:12.550: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 14 08:23:12.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1166" for this suite. 12/14/22 08:23:12.556
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":57,"skipped":1076,"failed":0}
------------------------------
• [24.782 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:22:47.778
    Dec 14 08:22:47.778: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 08:22:47.779
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:22:47.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:22:47.799
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1166 12/14/22 08:22:47.804
    STEP: creating a selector 12/14/22 08:22:47.804
    STEP: Creating the service pods in kubernetes 12/14/22 08:22:47.804
    Dec 14 08:22:47.804: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 08:22:47.829: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1166" to be "running and ready"
    Dec 14 08:22:47.833: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891215ms
    Dec 14 08:22:47.833: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:22:49.838: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009472311s
    Dec 14 08:22:49.838: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:22:51.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008204348s
    Dec 14 08:22:51.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:22:53.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008575941s
    Dec 14 08:22:53.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:22:55.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008897009s
    Dec 14 08:22:55.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:22:57.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009287511s
    Dec 14 08:22:57.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:22:59.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008284632s
    Dec 14 08:22:59.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:23:01.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008905637s
    Dec 14 08:23:01.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:23:03.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008491993s
    Dec 14 08:23:03.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:23:05.839: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010470936s
    Dec 14 08:23:05.839: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:23:07.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00849682s
    Dec 14 08:23:07.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 08:23:09.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009679381s
    Dec 14 08:23:09.838: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 08:23:09.838: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 08:23:09.842: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1166" to be "running and ready"
    Dec 14 08:23:09.846: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.232325ms
    Dec 14 08:23:09.846: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 08:23:09.846: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 08:23:09.849
    Dec 14 08:23:09.857: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1166" to be "running"
    Dec 14 08:23:09.861: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78794ms
    Dec 14 08:23:11.865: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008378162s
    Dec 14 08:23:11.865: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 08:23:11.870: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec 14 08:23:11.870: INFO: Breadth first check of 100.64.0.77 on host 10.250.3.58...
    Dec 14 08:23:11.873: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.124:9080/dial?request=hostname&protocol=udp&host=100.64.0.77&port=8081&tries=1'] Namespace:pod-network-test-1166 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:23:11.874: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:23:11.874: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:23:11.874: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1166/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.0.77%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 08:23:12.264: INFO: Waiting for responses: map[]
    Dec 14 08:23:12.264: INFO: reached 100.64.0.77 after 0/1 tries
    Dec 14 08:23:12.264: INFO: Breadth first check of 100.64.1.123 on host 10.250.3.210...
    Dec 14 08:23:12.268: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.124:9080/dial?request=hostname&protocol=udp&host=100.64.1.123&port=8081&tries=1'] Namespace:pod-network-test-1166 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:23:12.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:23:12.269: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:23:12.269: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1166/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.1.123%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 08:23:12.550: INFO: Waiting for responses: map[]
    Dec 14 08:23:12.550: INFO: reached 100.64.1.123 after 0/1 tries
    Dec 14 08:23:12.550: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 14 08:23:12.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1166" for this suite. 12/14/22 08:23:12.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:12.561
Dec 14 08:23:12.561: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 08:23:12.562
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:12.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:12.589
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 12/14/22 08:23:12.607
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:23:12.611
Dec 14 08:23:12.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:23:12.618: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:23:13.628: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:23:13.628: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:23:14.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 08:23:14.629: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 12/14/22 08:23:14.633
STEP: DeleteCollection of the DaemonSets 12/14/22 08:23:14.637
STEP: Verify that ReplicaSets have been deleted 12/14/22 08:23:14.643
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Dec 14 08:23:14.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14615"},"items":null}

Dec 14 08:23:14.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14615"},"items":[{"metadata":{"name":"daemon-set-8wpjr","generateName":"daemon-set-","namespace":"daemonsets-2537","uid":"ed347eac-dfec-4b21-b945-aa7ed799568b","resourceVersion":"14613","creationTimestamp":"2022-12-14T08:23:12Z","deletionTimestamp":"2022-12-14T08:23:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"714a0d60c0042d6b331eb032d4e97881ef2e4f610fd2002bc90d712bc3682b25","cni.projectcalico.org/podIP":"100.64.1.125/32","cni.projectcalico.org/podIPs":"100.64.1.125/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2e316a75-9b44-4c7c-a918-1bdb713669e2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e316a75-9b44-4c7c-a918-1bdb713669e2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gkq7w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tm5on-jne.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gkq7w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"}],"hostIP":"10.250.3.210","podIP":"100.64.1.125","podIPs":[{"ip":"100.64.1.125"}],"startTime":"2022-12-14T08:23:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T08:23:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6b88e6477be7a4a33ec8889768f6fa30d8844a51132835e925b5798011a0b160","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pwqvl","generateName":"daemon-set-","namespace":"daemonsets-2537","uid":"4248e201-e89a-49b8-9f70-89ba7d2a4d91","resourceVersion":"14614","creationTimestamp":"2022-12-14T08:23:12Z","deletionTimestamp":"2022-12-14T08:23:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"20d62744e6cac5427e994cecbd0b6bad41d016cc97762427072c3e410eb6abb4","cni.projectcalico.org/podIP":"100.64.0.78/32","cni.projectcalico.org/podIPs":"100.64.0.78/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2e316a75-9b44-4c7c-a918-1bdb713669e2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e316a75-9b44-4c7c-a918-1bdb713669e2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l6rk7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tm5on-jne.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l6rk7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"}],"hostIP":"10.250.3.58","podIP":"100.64.0.78","podIPs":[{"ip":"100.64.0.78"}],"startTime":"2022-12-14T08:23:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T08:23:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6d3bf74a8b74d3b0eb3efadcb72e3411ab71ec274aecae64fc3d8ad385457e86","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:23:14.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2537" for this suite. 12/14/22 08:23:14.671
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":58,"skipped":1093,"failed":0}
------------------------------
• [2.114 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:12.561
    Dec 14 08:23:12.561: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 08:23:12.562
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:12.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:12.589
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 12/14/22 08:23:12.607
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:23:12.611
    Dec 14 08:23:12.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:23:12.618: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:23:13.628: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:23:13.628: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:23:14.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 08:23:14.629: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 12/14/22 08:23:14.633
    STEP: DeleteCollection of the DaemonSets 12/14/22 08:23:14.637
    STEP: Verify that ReplicaSets have been deleted 12/14/22 08:23:14.643
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Dec 14 08:23:14.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14615"},"items":null}

    Dec 14 08:23:14.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14615"},"items":[{"metadata":{"name":"daemon-set-8wpjr","generateName":"daemon-set-","namespace":"daemonsets-2537","uid":"ed347eac-dfec-4b21-b945-aa7ed799568b","resourceVersion":"14613","creationTimestamp":"2022-12-14T08:23:12Z","deletionTimestamp":"2022-12-14T08:23:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"714a0d60c0042d6b331eb032d4e97881ef2e4f610fd2002bc90d712bc3682b25","cni.projectcalico.org/podIP":"100.64.1.125/32","cni.projectcalico.org/podIPs":"100.64.1.125/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2e316a75-9b44-4c7c-a918-1bdb713669e2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e316a75-9b44-4c7c-a918-1bdb713669e2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gkq7w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tm5on-jne.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gkq7w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"}],"hostIP":"10.250.3.210","podIP":"100.64.1.125","podIPs":[{"ip":"100.64.1.125"}],"startTime":"2022-12-14T08:23:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T08:23:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6b88e6477be7a4a33ec8889768f6fa30d8844a51132835e925b5798011a0b160","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pwqvl","generateName":"daemon-set-","namespace":"daemonsets-2537","uid":"4248e201-e89a-49b8-9f70-89ba7d2a4d91","resourceVersion":"14614","creationTimestamp":"2022-12-14T08:23:12Z","deletionTimestamp":"2022-12-14T08:23:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"20d62744e6cac5427e994cecbd0b6bad41d016cc97762427072c3e410eb6abb4","cni.projectcalico.org/podIP":"100.64.0.78/32","cni.projectcalico.org/podIPs":"100.64.0.78/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2e316a75-9b44-4c7c-a918-1bdb713669e2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e316a75-9b44-4c7c-a918-1bdb713669e2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T08:23:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l6rk7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tm5on-jne.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l6rk7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T08:23:12Z"}],"hostIP":"10.250.3.58","podIP":"100.64.0.78","podIPs":[{"ip":"100.64.0.78"}],"startTime":"2022-12-14T08:23:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T08:23:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6d3bf74a8b74d3b0eb3efadcb72e3411ab71ec274aecae64fc3d8ad385457e86","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:23:14.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2537" for this suite. 12/14/22 08:23:14.671
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:14.676
Dec 14 08:23:14.676: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename controllerrevisions 12/14/22 08:23:14.677
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:14.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:14.691
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-kz22g-daemon-set" 12/14/22 08:23:14.711
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:23:14.716
Dec 14 08:23:14.724: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 0
Dec 14 08:23:14.724: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:23:15.733: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 0
Dec 14 08:23:15.733: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:23:16.733: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 2
Dec 14 08:23:16.733: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-kz22g-daemon-set
STEP: Confirm DaemonSet "e2e-kz22g-daemon-set" successfully created with "daemonset-name=e2e-kz22g-daemon-set" label 12/14/22 08:23:16.736
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kz22g-daemon-set" 12/14/22 08:23:16.744
Dec 14 08:23:16.748: INFO: Located ControllerRevision: "e2e-kz22g-daemon-set-ff558bb7c"
STEP: Patching ControllerRevision "e2e-kz22g-daemon-set-ff558bb7c" 12/14/22 08:23:16.751
Dec 14 08:23:16.756: INFO: e2e-kz22g-daemon-set-ff558bb7c has been patched
STEP: Create a new ControllerRevision 12/14/22 08:23:16.756
Dec 14 08:23:16.760: INFO: Created ControllerRevision: e2e-kz22g-daemon-set-5bdd98cd57
STEP: Confirm that there are two ControllerRevisions 12/14/22 08:23:16.76
Dec 14 08:23:16.760: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 08:23:16.764: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-kz22g-daemon-set-ff558bb7c" 12/14/22 08:23:16.764
STEP: Confirm that there is only one ControllerRevision 12/14/22 08:23:16.767
Dec 14 08:23:16.767: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 08:23:16.771: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-kz22g-daemon-set-5bdd98cd57" 12/14/22 08:23:16.774
Dec 14 08:23:16.781: INFO: e2e-kz22g-daemon-set-5bdd98cd57 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 12/14/22 08:23:16.781
W1214 08:23:16.786438    4635 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 12/14/22 08:23:16.786
Dec 14 08:23:16.786: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 08:23:17.791: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 08:23:17.797: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kz22g-daemon-set-5bdd98cd57=updated" 12/14/22 08:23:17.797
STEP: Confirm that there is only one ControllerRevision 12/14/22 08:23:17.803
Dec 14 08:23:17.803: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 08:23:17.806: INFO: Found 1 ControllerRevisions
Dec 14 08:23:17.809: INFO: ControllerRevision "e2e-kz22g-daemon-set-7bdff86d" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-kz22g-daemon-set" 12/14/22 08:23:17.812
STEP: deleting DaemonSet.extensions e2e-kz22g-daemon-set in namespace controllerrevisions-3323, will wait for the garbage collector to delete the pods 12/14/22 08:23:17.812
Dec 14 08:23:17.871: INFO: Deleting DaemonSet.extensions e2e-kz22g-daemon-set took: 4.824767ms
Dec 14 08:23:17.972: INFO: Terminating DaemonSet.extensions e2e-kz22g-daemon-set pods took: 100.752723ms
Dec 14 08:23:20.276: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 0
Dec 14 08:23:20.276: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kz22g-daemon-set
Dec 14 08:23:20.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14713"},"items":null}

Dec 14 08:23:20.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14713"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:23:20.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-3323" for this suite. 12/14/22 08:23:20.298
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":59,"skipped":1096,"failed":0}
------------------------------
• [5.627 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:14.676
    Dec 14 08:23:14.676: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename controllerrevisions 12/14/22 08:23:14.677
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:14.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:14.691
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-kz22g-daemon-set" 12/14/22 08:23:14.711
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:23:14.716
    Dec 14 08:23:14.724: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 0
    Dec 14 08:23:14.724: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:23:15.733: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 0
    Dec 14 08:23:15.733: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:23:16.733: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 2
    Dec 14 08:23:16.733: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-kz22g-daemon-set
    STEP: Confirm DaemonSet "e2e-kz22g-daemon-set" successfully created with "daemonset-name=e2e-kz22g-daemon-set" label 12/14/22 08:23:16.736
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kz22g-daemon-set" 12/14/22 08:23:16.744
    Dec 14 08:23:16.748: INFO: Located ControllerRevision: "e2e-kz22g-daemon-set-ff558bb7c"
    STEP: Patching ControllerRevision "e2e-kz22g-daemon-set-ff558bb7c" 12/14/22 08:23:16.751
    Dec 14 08:23:16.756: INFO: e2e-kz22g-daemon-set-ff558bb7c has been patched
    STEP: Create a new ControllerRevision 12/14/22 08:23:16.756
    Dec 14 08:23:16.760: INFO: Created ControllerRevision: e2e-kz22g-daemon-set-5bdd98cd57
    STEP: Confirm that there are two ControllerRevisions 12/14/22 08:23:16.76
    Dec 14 08:23:16.760: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 08:23:16.764: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-kz22g-daemon-set-ff558bb7c" 12/14/22 08:23:16.764
    STEP: Confirm that there is only one ControllerRevision 12/14/22 08:23:16.767
    Dec 14 08:23:16.767: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 08:23:16.771: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-kz22g-daemon-set-5bdd98cd57" 12/14/22 08:23:16.774
    Dec 14 08:23:16.781: INFO: e2e-kz22g-daemon-set-5bdd98cd57 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 12/14/22 08:23:16.781
    W1214 08:23:16.786438    4635 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 12/14/22 08:23:16.786
    Dec 14 08:23:16.786: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 08:23:17.791: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 08:23:17.797: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kz22g-daemon-set-5bdd98cd57=updated" 12/14/22 08:23:17.797
    STEP: Confirm that there is only one ControllerRevision 12/14/22 08:23:17.803
    Dec 14 08:23:17.803: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 08:23:17.806: INFO: Found 1 ControllerRevisions
    Dec 14 08:23:17.809: INFO: ControllerRevision "e2e-kz22g-daemon-set-7bdff86d" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-kz22g-daemon-set" 12/14/22 08:23:17.812
    STEP: deleting DaemonSet.extensions e2e-kz22g-daemon-set in namespace controllerrevisions-3323, will wait for the garbage collector to delete the pods 12/14/22 08:23:17.812
    Dec 14 08:23:17.871: INFO: Deleting DaemonSet.extensions e2e-kz22g-daemon-set took: 4.824767ms
    Dec 14 08:23:17.972: INFO: Terminating DaemonSet.extensions e2e-kz22g-daemon-set pods took: 100.752723ms
    Dec 14 08:23:20.276: INFO: Number of nodes with available pods controlled by daemonset e2e-kz22g-daemon-set: 0
    Dec 14 08:23:20.276: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kz22g-daemon-set
    Dec 14 08:23:20.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14713"},"items":null}

    Dec 14 08:23:20.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14713"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:23:20.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-3323" for this suite. 12/14/22 08:23:20.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:20.303
Dec 14 08:23:20.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 12/14/22 08:23:20.305
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:20.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:20.321
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 12/14/22 08:23:20.325
STEP: wait for the container to reach Failed 12/14/22 08:23:20.335
STEP: get the container status 12/14/22 08:23:23.356
STEP: the container should be terminated 12/14/22 08:23:23.36
STEP: the termination message should be set 12/14/22 08:23:23.36
Dec 14 08:23:23.360: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/14/22 08:23:23.36
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 14 08:23:23.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2955" for this suite. 12/14/22 08:23:23.375
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":60,"skipped":1107,"failed":0}
------------------------------
• [3.075 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:20.303
    Dec 14 08:23:20.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 12/14/22 08:23:20.305
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:20.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:20.321
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 12/14/22 08:23:20.325
    STEP: wait for the container to reach Failed 12/14/22 08:23:20.335
    STEP: get the container status 12/14/22 08:23:23.356
    STEP: the container should be terminated 12/14/22 08:23:23.36
    STEP: the termination message should be set 12/14/22 08:23:23.36
    Dec 14 08:23:23.360: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/14/22 08:23:23.36
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 14 08:23:23.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2955" for this suite. 12/14/22 08:23:23.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:23.379
Dec 14 08:23:23.379: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:23:23.38
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:23.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:23.397
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 12/14/22 08:23:23.403
Dec 14 08:23:23.403: INFO: namespace kubectl-3627
Dec 14 08:23:23.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 create -f -'
Dec 14 08:23:23.586: INFO: stderr: ""
Dec 14 08:23:23.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/14/22 08:23:23.586
Dec 14 08:23:24.590: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 08:23:24.590: INFO: Found 1 / 1
Dec 14 08:23:24.590: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 08:23:24.594: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 08:23:24.594: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 08:23:24.594: INFO: wait on agnhost-primary startup in kubectl-3627 
Dec 14 08:23:24.594: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 logs agnhost-primary-b9f7z agnhost-primary'
Dec 14 08:23:24.710: INFO: stderr: ""
Dec 14 08:23:24.710: INFO: stdout: "Paused\n"
STEP: exposing RC 12/14/22 08:23:24.71
Dec 14 08:23:24.710: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 14 08:23:24.781: INFO: stderr: ""
Dec 14 08:23:24.781: INFO: stdout: "service/rm2 exposed\n"
Dec 14 08:23:24.784: INFO: Service rm2 in namespace kubectl-3627 found.
STEP: exposing service 12/14/22 08:23:26.793
Dec 14 08:23:26.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 14 08:23:26.862: INFO: stderr: ""
Dec 14 08:23:26.862: INFO: stdout: "service/rm3 exposed\n"
Dec 14 08:23:26.865: INFO: Service rm3 in namespace kubectl-3627 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:23:28.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3627" for this suite. 12/14/22 08:23:28.879
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":61,"skipped":1119,"failed":0}
------------------------------
• [5.505 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:23.379
    Dec 14 08:23:23.379: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:23:23.38
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:23.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:23.397
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 12/14/22 08:23:23.403
    Dec 14 08:23:23.403: INFO: namespace kubectl-3627
    Dec 14 08:23:23.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 create -f -'
    Dec 14 08:23:23.586: INFO: stderr: ""
    Dec 14 08:23:23.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/14/22 08:23:23.586
    Dec 14 08:23:24.590: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 08:23:24.590: INFO: Found 1 / 1
    Dec 14 08:23:24.590: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 14 08:23:24.594: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 08:23:24.594: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 14 08:23:24.594: INFO: wait on agnhost-primary startup in kubectl-3627 
    Dec 14 08:23:24.594: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 logs agnhost-primary-b9f7z agnhost-primary'
    Dec 14 08:23:24.710: INFO: stderr: ""
    Dec 14 08:23:24.710: INFO: stdout: "Paused\n"
    STEP: exposing RC 12/14/22 08:23:24.71
    Dec 14 08:23:24.710: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Dec 14 08:23:24.781: INFO: stderr: ""
    Dec 14 08:23:24.781: INFO: stdout: "service/rm2 exposed\n"
    Dec 14 08:23:24.784: INFO: Service rm2 in namespace kubectl-3627 found.
    STEP: exposing service 12/14/22 08:23:26.793
    Dec 14 08:23:26.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3627 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Dec 14 08:23:26.862: INFO: stderr: ""
    Dec 14 08:23:26.862: INFO: stdout: "service/rm3 exposed\n"
    Dec 14 08:23:26.865: INFO: Service rm3 in namespace kubectl-3627 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:23:28.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3627" for this suite. 12/14/22 08:23:28.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:28.885
Dec 14 08:23:28.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 08:23:28.886
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:28.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:28.902
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-645906f9-b40b-463d-83ec-abef0aa5a8fe 12/14/22 08:23:28.907
STEP: Creating a pod to test consume configMaps 12/14/22 08:23:28.911
Dec 14 08:23:28.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7" in namespace "configmap-2144" to be "Succeeded or Failed"
Dec 14 08:23:28.924: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.992644ms
Dec 14 08:23:30.930: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009914253s
Dec 14 08:23:32.931: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010448652s
STEP: Saw pod success 12/14/22 08:23:32.931
Dec 14 08:23:32.931: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7" satisfied condition "Succeeded or Failed"
Dec 14 08:23:32.934: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:23:32.943
Dec 14 08:23:32.949: INFO: Waiting for pod pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7 to disappear
Dec 14 08:23:32.952: INFO: Pod pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 08:23:32.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2144" for this suite. 12/14/22 08:23:32.957
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":62,"skipped":1142,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:28.885
    Dec 14 08:23:28.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 08:23:28.886
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:28.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:28.902
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-645906f9-b40b-463d-83ec-abef0aa5a8fe 12/14/22 08:23:28.907
    STEP: Creating a pod to test consume configMaps 12/14/22 08:23:28.911
    Dec 14 08:23:28.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7" in namespace "configmap-2144" to be "Succeeded or Failed"
    Dec 14 08:23:28.924: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.992644ms
    Dec 14 08:23:30.930: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009914253s
    Dec 14 08:23:32.931: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010448652s
    STEP: Saw pod success 12/14/22 08:23:32.931
    Dec 14 08:23:32.931: INFO: Pod "pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7" satisfied condition "Succeeded or Failed"
    Dec 14 08:23:32.934: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:23:32.943
    Dec 14 08:23:32.949: INFO: Waiting for pod pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7 to disappear
    Dec 14 08:23:32.952: INFO: Pod pod-configmaps-14a1eaae-784c-4ebb-a398-ef34990797a7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 08:23:32.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2144" for this suite. 12/14/22 08:23:32.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:32.963
Dec 14 08:23:32.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 12/14/22 08:23:32.963
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:32.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:32.98
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:32.985
Dec 14 08:23:32.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption-2 12/14/22 08:23:32.986
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:32.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:32.999
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 12/14/22 08:23:33.007
STEP: Waiting for the pdb to be processed 12/14/22 08:23:35.019
STEP: Waiting for the pdb to be processed 12/14/22 08:23:35.028
STEP: listing a collection of PDBs across all namespaces 12/14/22 08:23:35.031
STEP: listing a collection of PDBs in namespace disruption-4872 12/14/22 08:23:35.034
STEP: deleting a collection of PDBs 12/14/22 08:23:35.037
STEP: Waiting for the PDB collection to be deleted 12/14/22 08:23:35.043
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Dec 14 08:23:35.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9249" for this suite. 12/14/22 08:23:35.05
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 14 08:23:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4872" for this suite. 12/14/22 08:23:35.059
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":63,"skipped":1166,"failed":0}
------------------------------
• [2.101 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:32.963
    Dec 14 08:23:32.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 12/14/22 08:23:32.963
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:32.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:32.98
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:32.985
    Dec 14 08:23:32.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption-2 12/14/22 08:23:32.986
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:32.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:32.999
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 12/14/22 08:23:33.007
    STEP: Waiting for the pdb to be processed 12/14/22 08:23:35.019
    STEP: Waiting for the pdb to be processed 12/14/22 08:23:35.028
    STEP: listing a collection of PDBs across all namespaces 12/14/22 08:23:35.031
    STEP: listing a collection of PDBs in namespace disruption-4872 12/14/22 08:23:35.034
    STEP: deleting a collection of PDBs 12/14/22 08:23:35.037
    STEP: Waiting for the PDB collection to be deleted 12/14/22 08:23:35.043
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Dec 14 08:23:35.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-9249" for this suite. 12/14/22 08:23:35.05
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 14 08:23:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4872" for this suite. 12/14/22 08:23:35.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:23:35.064
Dec 14 08:23:35.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 12/14/22 08:23:35.065
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:35.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:35.08
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 12/14/22 08:23:40.133
STEP: referencing matching pods with named port 12/14/22 08:23:45.141
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/14/22 08:23:50.15
STEP: recreating EndpointSlices after they've been deleted 12/14/22 08:23:55.158
Dec 14 08:23:55.180: INFO: EndpointSlice for Service endpointslice-6274/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 14 08:24:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6274" for this suite. 12/14/22 08:24:05.195
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":64,"skipped":1175,"failed":0}
------------------------------
• [30.135 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:23:35.064
    Dec 14 08:23:35.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 12/14/22 08:23:35.065
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:23:35.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:23:35.08
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 12/14/22 08:23:40.133
    STEP: referencing matching pods with named port 12/14/22 08:23:45.141
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/14/22 08:23:50.15
    STEP: recreating EndpointSlices after they've been deleted 12/14/22 08:23:55.158
    Dec 14 08:23:55.180: INFO: EndpointSlice for Service endpointslice-6274/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 14 08:24:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6274" for this suite. 12/14/22 08:24:05.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:05.2
Dec 14 08:24:05.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:24:05.2
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:05.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:05.216
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:24:05.23
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:24:06.024
STEP: Deploying the webhook pod 12/14/22 08:24:06.03
STEP: Wait for the deployment to be ready 12/14/22 08:24:06.038
Dec 14 08:24:06.044: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:24:08.057
STEP: Verifying the service has paired with the endpoint 12/14/22 08:24:08.066
Dec 14 08:24:09.066: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 08:24:09.07
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 08:24:09.193
STEP: Creating a dummy validating-webhook-configuration object 12/14/22 08:24:09.31
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/14/22 08:24:09.369
STEP: Creating a dummy mutating-webhook-configuration object 12/14/22 08:24:09.375
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/14/22 08:24:09.478
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:24:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3224" for this suite. 12/14/22 08:24:09.498
STEP: Destroying namespace "webhook-3224-markers" for this suite. 12/14/22 08:24:09.503
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":65,"skipped":1180,"failed":0}
------------------------------
• [4.336 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:05.2
    Dec 14 08:24:05.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:24:05.2
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:05.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:05.216
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:24:05.23
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:24:06.024
    STEP: Deploying the webhook pod 12/14/22 08:24:06.03
    STEP: Wait for the deployment to be ready 12/14/22 08:24:06.038
    Dec 14 08:24:06.044: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:24:08.057
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:24:08.066
    Dec 14 08:24:09.066: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 08:24:09.07
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 08:24:09.193
    STEP: Creating a dummy validating-webhook-configuration object 12/14/22 08:24:09.31
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/14/22 08:24:09.369
    STEP: Creating a dummy mutating-webhook-configuration object 12/14/22 08:24:09.375
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/14/22 08:24:09.478
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:24:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3224" for this suite. 12/14/22 08:24:09.498
    STEP: Destroying namespace "webhook-3224-markers" for this suite. 12/14/22 08:24:09.503
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:09.537
Dec 14 08:24:09.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 08:24:09.537
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:09.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:09.554
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 12/14/22 08:24:09.561
Dec 14 08:24:09.570: INFO: Waiting up to 5m0s for pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857" in namespace "var-expansion-5109" to be "Succeeded or Failed"
Dec 14 08:24:09.574: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857": Phase="Pending", Reason="", readiness=false. Elapsed: 3.876429ms
Dec 14 08:24:11.580: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050859s
Dec 14 08:24:13.579: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008727192s
STEP: Saw pod success 12/14/22 08:24:13.579
Dec 14 08:24:13.579: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857" satisfied condition "Succeeded or Failed"
Dec 14 08:24:13.583: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857 container dapi-container: <nil>
STEP: delete the pod 12/14/22 08:24:13.593
Dec 14 08:24:13.599: INFO: Waiting for pod var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857 to disappear
Dec 14 08:24:13.602: INFO: Pod var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 08:24:13.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5109" for this suite. 12/14/22 08:24:13.607
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":66,"skipped":1195,"failed":0}
------------------------------
• [4.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:09.537
    Dec 14 08:24:09.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 08:24:09.537
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:09.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:09.554
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 12/14/22 08:24:09.561
    Dec 14 08:24:09.570: INFO: Waiting up to 5m0s for pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857" in namespace "var-expansion-5109" to be "Succeeded or Failed"
    Dec 14 08:24:09.574: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857": Phase="Pending", Reason="", readiness=false. Elapsed: 3.876429ms
    Dec 14 08:24:11.580: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050859s
    Dec 14 08:24:13.579: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008727192s
    STEP: Saw pod success 12/14/22 08:24:13.579
    Dec 14 08:24:13.579: INFO: Pod "var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857" satisfied condition "Succeeded or Failed"
    Dec 14 08:24:13.583: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 08:24:13.593
    Dec 14 08:24:13.599: INFO: Waiting for pod var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857 to disappear
    Dec 14 08:24:13.602: INFO: Pod var-expansion-76b7bc8d-83e2-4245-97b5-320cb1449857 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 08:24:13.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5109" for this suite. 12/14/22 08:24:13.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:13.612
Dec 14 08:24:13.612: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl 12/14/22 08:24:13.613
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:13.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:13.627
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 12/14/22 08:24:13.632
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 08:24:13.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2499" for this suite. 12/14/22 08:24:13.643
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":67,"skipped":1205,"failed":0}
------------------------------
• [0.035 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:13.612
    Dec 14 08:24:13.612: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sysctl 12/14/22 08:24:13.613
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:13.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:13.627
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 12/14/22 08:24:13.632
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 08:24:13.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2499" for this suite. 12/14/22 08:24:13.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:13.648
Dec 14 08:24:13.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:24:13.648
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:13.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:13.663
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 12/14/22 08:24:13.668
Dec 14 08:24:13.677: INFO: Waiting up to 5m0s for pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8" in namespace "downward-api-6292" to be "running and ready"
Dec 14 08:24:13.680: INFO: Pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.923176ms
Dec 14 08:24:13.680: INFO: The phase of Pod annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:24:15.685: INFO: Pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008596372s
Dec 14 08:24:15.685: INFO: The phase of Pod annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8 is Running (Ready = true)
Dec 14 08:24:15.685: INFO: Pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8" satisfied condition "running and ready"
Dec 14 08:24:16.210: INFO: Successfully updated pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 08:24:20.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6292" for this suite. 12/14/22 08:24:20.251
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":68,"skipped":1218,"failed":0}
------------------------------
• [6.608 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:13.648
    Dec 14 08:24:13.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:24:13.648
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:13.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:13.663
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 12/14/22 08:24:13.668
    Dec 14 08:24:13.677: INFO: Waiting up to 5m0s for pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8" in namespace "downward-api-6292" to be "running and ready"
    Dec 14 08:24:13.680: INFO: Pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.923176ms
    Dec 14 08:24:13.680: INFO: The phase of Pod annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:24:15.685: INFO: Pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008596372s
    Dec 14 08:24:15.685: INFO: The phase of Pod annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8 is Running (Ready = true)
    Dec 14 08:24:15.685: INFO: Pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8" satisfied condition "running and ready"
    Dec 14 08:24:16.210: INFO: Successfully updated pod "annotationupdatec3df5be7-6cb9-4e72-b2ac-10c6b555aed8"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 08:24:20.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6292" for this suite. 12/14/22 08:24:20.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:20.258
Dec 14 08:24:20.258: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:24:20.259
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:20.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:20.277
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Dec 14 08:24:20.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 08:24:22.793
Dec 14 08:24:22.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 create -f -'
Dec 14 08:24:23.495: INFO: stderr: ""
Dec 14 08:24:23.495: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 08:24:23.495: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 delete e2e-test-crd-publish-openapi-8388-crds test-cr'
Dec 14 08:24:23.561: INFO: stderr: ""
Dec 14 08:24:23.561: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 14 08:24:23.561: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 apply -f -'
Dec 14 08:24:23.755: INFO: stderr: ""
Dec 14 08:24:23.756: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 08:24:23.756: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 delete e2e-test-crd-publish-openapi-8388-crds test-cr'
Dec 14 08:24:23.823: INFO: stderr: ""
Dec 14 08:24:23.823: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/14/22 08:24:23.823
Dec 14 08:24:23.823: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 explain e2e-test-crd-publish-openapi-8388-crds'
Dec 14 08:24:24.025: INFO: stderr: ""
Dec 14 08:24:24.025: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8388-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:24:26.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2901" for this suite. 12/14/22 08:24:26.596
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":69,"skipped":1256,"failed":0}
------------------------------
• [6.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:20.258
    Dec 14 08:24:20.258: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:24:20.259
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:20.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:20.277
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Dec 14 08:24:20.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 08:24:22.793
    Dec 14 08:24:22.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 create -f -'
    Dec 14 08:24:23.495: INFO: stderr: ""
    Dec 14 08:24:23.495: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 14 08:24:23.495: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 delete e2e-test-crd-publish-openapi-8388-crds test-cr'
    Dec 14 08:24:23.561: INFO: stderr: ""
    Dec 14 08:24:23.561: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Dec 14 08:24:23.561: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 apply -f -'
    Dec 14 08:24:23.755: INFO: stderr: ""
    Dec 14 08:24:23.756: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 14 08:24:23.756: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 --namespace=crd-publish-openapi-2901 delete e2e-test-crd-publish-openapi-8388-crds test-cr'
    Dec 14 08:24:23.823: INFO: stderr: ""
    Dec 14 08:24:23.823: INFO: stdout: "e2e-test-crd-publish-openapi-8388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/14/22 08:24:23.823
    Dec 14 08:24:23.823: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2901 explain e2e-test-crd-publish-openapi-8388-crds'
    Dec 14 08:24:24.025: INFO: stderr: ""
    Dec 14 08:24:24.025: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8388-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:24:26.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2901" for this suite. 12/14/22 08:24:26.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:26.604
Dec 14 08:24:26.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 08:24:26.604
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:26.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:26.617
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 12/14/22 08:24:26.633
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:24:26.637
Dec 14 08:24:26.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:24:26.644: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:24:27.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:24:27.655: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 08:24:28.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 08:24:28.655: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/14/22 08:24:28.658
Dec 14 08:24:28.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 08:24:28.681: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 08:24:29.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 08:24:29.692: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 12/14/22 08:24:29.692
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:24:29.699
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4695, will wait for the garbage collector to delete the pods 12/14/22 08:24:29.699
Dec 14 08:24:29.759: INFO: Deleting DaemonSet.extensions daemon-set took: 6.443065ms
Dec 14 08:24:29.860: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.165765ms
Dec 14 08:24:32.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 08:24:32.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 08:24:32.467: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15439"},"items":null}

Dec 14 08:24:32.470: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15439"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:24:32.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4695" for this suite. 12/14/22 08:24:32.485
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":70,"skipped":1261,"failed":0}
------------------------------
• [5.886 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:26.604
    Dec 14 08:24:26.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 08:24:26.604
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:26.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:26.617
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 12/14/22 08:24:26.633
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 08:24:26.637
    Dec 14 08:24:26.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:24:26.644: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:24:27.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:24:27.655: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 08:24:28.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 08:24:28.655: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/14/22 08:24:28.658
    Dec 14 08:24:28.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 08:24:28.681: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 08:24:29.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 08:24:29.692: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 12/14/22 08:24:29.692
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 08:24:29.699
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4695, will wait for the garbage collector to delete the pods 12/14/22 08:24:29.699
    Dec 14 08:24:29.759: INFO: Deleting DaemonSet.extensions daemon-set took: 6.443065ms
    Dec 14 08:24:29.860: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.165765ms
    Dec 14 08:24:32.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 08:24:32.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 08:24:32.467: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15439"},"items":null}

    Dec 14 08:24:32.470: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15439"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:24:32.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4695" for this suite. 12/14/22 08:24:32.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:32.492
Dec 14 08:24:32.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 12/14/22 08:24:32.492
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:32.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:32.506
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 12/14/22 08:24:32.51
STEP: wait for the container to reach Succeeded 12/14/22 08:24:32.519
STEP: get the container status 12/14/22 08:24:35.536
STEP: the container should be terminated 12/14/22 08:24:35.54
STEP: the termination message should be set 12/14/22 08:24:35.54
Dec 14 08:24:35.540: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 12/14/22 08:24:35.54
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 14 08:24:35.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4234" for this suite. 12/14/22 08:24:35.554
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":71,"skipped":1314,"failed":0}
------------------------------
• [3.066 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:32.492
    Dec 14 08:24:32.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 12/14/22 08:24:32.492
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:32.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:32.506
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 12/14/22 08:24:32.51
    STEP: wait for the container to reach Succeeded 12/14/22 08:24:32.519
    STEP: get the container status 12/14/22 08:24:35.536
    STEP: the container should be terminated 12/14/22 08:24:35.54
    STEP: the termination message should be set 12/14/22 08:24:35.54
    Dec 14 08:24:35.540: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 12/14/22 08:24:35.54
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 14 08:24:35.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4234" for this suite. 12/14/22 08:24:35.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:35.557
Dec 14 08:24:35.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables 12/14/22 08:24:35.558
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:35.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:35.571
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Dec 14 08:24:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1823" for this suite. 12/14/22 08:24:35.583
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":72,"skipped":1322,"failed":0}
------------------------------
• [0.029 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:35.557
    Dec 14 08:24:35.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename tables 12/14/22 08:24:35.558
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:35.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:35.571
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Dec 14 08:24:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-1823" for this suite. 12/14/22 08:24:35.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:35.587
Dec 14 08:24:35.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 08:24:35.587
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:35.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:35.6
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 12/14/22 08:24:35.604
STEP: delete the rc 12/14/22 08:24:40.612
STEP: wait for all pods to be garbage collected 12/14/22 08:24:40.616
STEP: Gathering metrics 12/14/22 08:24:45.622
W1214 08:24:45.633686    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 08:24:45.633: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 08:24:45.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3270" for this suite. 12/14/22 08:24:45.637
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":73,"skipped":1333,"failed":0}
------------------------------
• [10.054 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:35.587
    Dec 14 08:24:35.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 08:24:35.587
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:35.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:35.6
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 12/14/22 08:24:35.604
    STEP: delete the rc 12/14/22 08:24:40.612
    STEP: wait for all pods to be garbage collected 12/14/22 08:24:40.616
    STEP: Gathering metrics 12/14/22 08:24:45.622
    W1214 08:24:45.633686    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 14 08:24:45.633: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 08:24:45.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3270" for this suite. 12/14/22 08:24:45.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:45.641
Dec 14 08:24:45.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 12/14/22 08:24:45.642
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:45.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:45.656
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Dec 14 08:24:45.662: INFO: Got root ca configmap in namespace "svcaccounts-7450"
Dec 14 08:24:45.666: INFO: Deleted root ca configmap in namespace "svcaccounts-7450"
STEP: waiting for a new root ca configmap created 12/14/22 08:24:46.166
Dec 14 08:24:46.170: INFO: Recreated root ca configmap in namespace "svcaccounts-7450"
Dec 14 08:24:46.175: INFO: Updated root ca configmap in namespace "svcaccounts-7450"
STEP: waiting for the root ca configmap reconciled 12/14/22 08:24:46.675
Dec 14 08:24:46.679: INFO: Reconciled root ca configmap in namespace "svcaccounts-7450"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 14 08:24:46.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7450" for this suite. 12/14/22 08:24:46.684
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":74,"skipped":1344,"failed":0}
------------------------------
• [1.048 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:45.641
    Dec 14 08:24:45.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 08:24:45.642
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:45.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:45.656
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Dec 14 08:24:45.662: INFO: Got root ca configmap in namespace "svcaccounts-7450"
    Dec 14 08:24:45.666: INFO: Deleted root ca configmap in namespace "svcaccounts-7450"
    STEP: waiting for a new root ca configmap created 12/14/22 08:24:46.166
    Dec 14 08:24:46.170: INFO: Recreated root ca configmap in namespace "svcaccounts-7450"
    Dec 14 08:24:46.175: INFO: Updated root ca configmap in namespace "svcaccounts-7450"
    STEP: waiting for the root ca configmap reconciled 12/14/22 08:24:46.675
    Dec 14 08:24:46.679: INFO: Reconciled root ca configmap in namespace "svcaccounts-7450"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 14 08:24:46.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7450" for this suite. 12/14/22 08:24:46.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:24:46.69
Dec 14 08:24:46.690: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 08:24:46.691
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:46.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:46.704
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 12/14/22 08:24:46.708
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local;sleep 1; done
 12/14/22 08:24:46.712
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local;sleep 1; done
 12/14/22 08:24:46.712
STEP: creating a pod to probe DNS 12/14/22 08:24:46.712
STEP: submitting the pod to kubernetes 12/14/22 08:24:46.712
Dec 14 08:24:46.722: INFO: Waiting up to 15m0s for pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051" in namespace "dns-3244" to be "running"
Dec 14 08:24:46.726: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09492ms
Dec 14 08:24:48.731: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008350758s
Dec 14 08:24:50.729: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006634541s
Dec 14 08:24:52.730: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Running", Reason="", readiness=true. Elapsed: 6.007893765s
Dec 14 08:24:52.730: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051" satisfied condition "running"
STEP: retrieving the pod 12/14/22 08:24:52.73
STEP: looking for the results for each expected name from probers 12/14/22 08:24:52.734
Dec 14 08:24:52.842: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.887: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.894: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.901: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.908: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.916: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.923: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.930: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:52.930: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

Dec 14 08:24:57.940: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:57.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.024: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.031: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.038: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.044: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.050: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.057: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:24:58.057: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

Dec 14 08:25:02.942: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:02.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:02.994: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:03.001: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:03.008: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:03.015: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:03.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:03.029: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:03.029: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

Dec 14 08:25:07.939: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:07.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:07.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:08.001: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:08.008: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:08.015: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:08.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:08.029: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:08.029: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

Dec 14 08:25:12.945: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:12.990: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:12.997: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:13.003: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:13.011: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:13.017: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:13.024: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:13.031: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:13.031: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

Dec 14 08:25:17.940: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:17.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:17.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:18.002: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:18.010: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:18.018: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:18.024: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:18.033: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
Dec 14 08:25:18.033: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

Dec 14 08:25:23.034: INFO: DNS probes using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 succeeded

STEP: deleting the pod 12/14/22 08:25:23.034
STEP: deleting the test headless service 12/14/22 08:25:23.045
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 08:25:23.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3244" for this suite. 12/14/22 08:25:23.06
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":75,"skipped":1351,"failed":0}
------------------------------
• [36.375 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:24:46.69
    Dec 14 08:24:46.690: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 08:24:46.691
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:24:46.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:24:46.704
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 12/14/22 08:24:46.708
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local;sleep 1; done
     12/14/22 08:24:46.712
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local;sleep 1; done
     12/14/22 08:24:46.712
    STEP: creating a pod to probe DNS 12/14/22 08:24:46.712
    STEP: submitting the pod to kubernetes 12/14/22 08:24:46.712
    Dec 14 08:24:46.722: INFO: Waiting up to 15m0s for pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051" in namespace "dns-3244" to be "running"
    Dec 14 08:24:46.726: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09492ms
    Dec 14 08:24:48.731: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008350758s
    Dec 14 08:24:50.729: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006634541s
    Dec 14 08:24:52.730: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051": Phase="Running", Reason="", readiness=true. Elapsed: 6.007893765s
    Dec 14 08:24:52.730: INFO: Pod "dns-test-f0774916-919d-43bf-adfa-45d1708a8051" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 08:24:52.73
    STEP: looking for the results for each expected name from probers 12/14/22 08:24:52.734
    Dec 14 08:24:52.842: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.887: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.894: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.901: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.908: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.916: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.923: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.930: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:52.930: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

    Dec 14 08:24:57.940: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:57.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.024: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.031: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.038: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.044: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.050: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.057: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:24:58.057: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

    Dec 14 08:25:02.942: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:02.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:02.994: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:03.001: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:03.008: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:03.015: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:03.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:03.029: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:03.029: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

    Dec 14 08:25:07.939: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:07.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:07.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:08.001: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:08.008: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:08.015: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:08.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:08.029: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:08.029: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

    Dec 14 08:25:12.945: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:12.990: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:12.997: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:13.003: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:13.011: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:13.017: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:13.024: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:13.031: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:13.031: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

    Dec 14 08:25:17.940: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:17.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:17.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:18.002: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:18.010: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:18.018: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:18.024: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:18.033: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local from pod dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051: the server could not find the requested resource (get pods dns-test-f0774916-919d-43bf-adfa-45d1708a8051)
    Dec 14 08:25:18.033: INFO: Lookups using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3244.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3244.svc.cluster.local jessie_udp@dns-test-service-2.dns-3244.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3244.svc.cluster.local]

    Dec 14 08:25:23.034: INFO: DNS probes using dns-3244/dns-test-f0774916-919d-43bf-adfa-45d1708a8051 succeeded

    STEP: deleting the pod 12/14/22 08:25:23.034
    STEP: deleting the test headless service 12/14/22 08:25:23.045
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 08:25:23.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3244" for this suite. 12/14/22 08:25:23.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:25:23.065
Dec 14 08:25:23.066: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:25:23.066
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:25:23.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:25:23.082
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:25:23.097
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:25:23.549
STEP: Deploying the webhook pod 12/14/22 08:25:23.555
STEP: Wait for the deployment to be ready 12/14/22 08:25:23.566
Dec 14 08:25:23.572: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 08:25:25.584
STEP: Verifying the service has paired with the endpoint 12/14/22 08:25:25.593
Dec 14 08:25:26.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Dec 14 08:25:26.599: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9361-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 08:25:27.11
STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 08:25:27.261
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:25:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7037" for this suite. 12/14/22 08:25:30.024
STEP: Destroying namespace "webhook-7037-markers" for this suite. 12/14/22 08:25:30.031
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":76,"skipped":1367,"failed":0}
------------------------------
• [6.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:25:23.065
    Dec 14 08:25:23.066: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:25:23.066
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:25:23.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:25:23.082
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:25:23.097
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:25:23.549
    STEP: Deploying the webhook pod 12/14/22 08:25:23.555
    STEP: Wait for the deployment to be ready 12/14/22 08:25:23.566
    Dec 14 08:25:23.572: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 08:25:25.584
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:25:25.593
    Dec 14 08:25:26.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Dec 14 08:25:26.599: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9361-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 08:25:27.11
    STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 08:25:27.261
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:25:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7037" for this suite. 12/14/22 08:25:30.024
    STEP: Destroying namespace "webhook-7037-markers" for this suite. 12/14/22 08:25:30.031
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:25:30.06
Dec 14 08:25:30.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 08:25:30.061
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:25:30.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:25:30.097
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-947b93c4-320b-4a72-971d-9958e8d90007 in namespace container-probe-4966 12/14/22 08:25:30.102
Dec 14 08:25:30.113: INFO: Waiting up to 5m0s for pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007" in namespace "container-probe-4966" to be "not pending"
Dec 14 08:25:30.116: INFO: Pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992323ms
Dec 14 08:25:32.121: INFO: Pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352123s
Dec 14 08:25:32.121: INFO: Pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007" satisfied condition "not pending"
Dec 14 08:25:32.121: INFO: Started pod busybox-947b93c4-320b-4a72-971d-9958e8d90007 in namespace container-probe-4966
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:25:32.121
Dec 14 08:25:32.125: INFO: Initial restart count of pod busybox-947b93c4-320b-4a72-971d-9958e8d90007 is 0
STEP: deleting the pod 12/14/22 08:29:32.815
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 08:29:32.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4966" for this suite. 12/14/22 08:29:32.83
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":77,"skipped":1367,"failed":0}
------------------------------
• [242.773 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:25:30.06
    Dec 14 08:25:30.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 08:25:30.061
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:25:30.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:25:30.097
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-947b93c4-320b-4a72-971d-9958e8d90007 in namespace container-probe-4966 12/14/22 08:25:30.102
    Dec 14 08:25:30.113: INFO: Waiting up to 5m0s for pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007" in namespace "container-probe-4966" to be "not pending"
    Dec 14 08:25:30.116: INFO: Pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992323ms
    Dec 14 08:25:32.121: INFO: Pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352123s
    Dec 14 08:25:32.121: INFO: Pod "busybox-947b93c4-320b-4a72-971d-9958e8d90007" satisfied condition "not pending"
    Dec 14 08:25:32.121: INFO: Started pod busybox-947b93c4-320b-4a72-971d-9958e8d90007 in namespace container-probe-4966
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:25:32.121
    Dec 14 08:25:32.125: INFO: Initial restart count of pod busybox-947b93c4-320b-4a72-971d-9958e8d90007 is 0
    STEP: deleting the pod 12/14/22 08:29:32.815
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 08:29:32.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4966" for this suite. 12/14/22 08:29:32.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:29:32.835
Dec 14 08:29:32.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 08:29:32.836
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:32.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:32.852
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 12/14/22 08:29:32.857
STEP: submitting the pod to kubernetes 12/14/22 08:29:32.857
Dec 14 08:29:32.893: INFO: Waiting up to 5m0s for pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" in namespace "pods-4332" to be "running and ready"
Dec 14 08:29:32.896: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19606ms
Dec 14 08:29:32.896: INFO: The phase of Pod pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:29:34.900: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007734526s
Dec 14 08:29:34.901: INFO: The phase of Pod pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0 is Running (Ready = true)
Dec 14 08:29:34.901: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/14/22 08:29:34.904
STEP: updating the pod 12/14/22 08:29:34.907
Dec 14 08:29:35.420: INFO: Successfully updated pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0"
Dec 14 08:29:35.420: INFO: Waiting up to 5m0s for pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" in namespace "pods-4332" to be "running"
Dec 14 08:29:35.423: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0": Phase="Running", Reason="", readiness=true. Elapsed: 3.827807ms
Dec 14 08:29:35.423: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 12/14/22 08:29:35.423
Dec 14 08:29:35.427: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 08:29:35.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4332" for this suite. 12/14/22 08:29:35.432
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":78,"skipped":1397,"failed":0}
------------------------------
• [2.602 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:29:32.835
    Dec 14 08:29:32.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 08:29:32.836
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:32.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:32.852
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 12/14/22 08:29:32.857
    STEP: submitting the pod to kubernetes 12/14/22 08:29:32.857
    Dec 14 08:29:32.893: INFO: Waiting up to 5m0s for pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" in namespace "pods-4332" to be "running and ready"
    Dec 14 08:29:32.896: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19606ms
    Dec 14 08:29:32.896: INFO: The phase of Pod pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:29:34.900: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007734526s
    Dec 14 08:29:34.901: INFO: The phase of Pod pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0 is Running (Ready = true)
    Dec 14 08:29:34.901: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/14/22 08:29:34.904
    STEP: updating the pod 12/14/22 08:29:34.907
    Dec 14 08:29:35.420: INFO: Successfully updated pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0"
    Dec 14 08:29:35.420: INFO: Waiting up to 5m0s for pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" in namespace "pods-4332" to be "running"
    Dec 14 08:29:35.423: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0": Phase="Running", Reason="", readiness=true. Elapsed: 3.827807ms
    Dec 14 08:29:35.423: INFO: Pod "pod-update-9b2bb1eb-37e8-4659-849b-43af327afbc0" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 12/14/22 08:29:35.423
    Dec 14 08:29:35.427: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 08:29:35.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4332" for this suite. 12/14/22 08:29:35.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:29:35.438
Dec 14 08:29:35.438: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:29:35.438
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:35.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:35.454
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-115d2d86-7b70-4ae9-afa9-faf5d17fd8df 12/14/22 08:29:35.458
STEP: Creating secret with name secret-projected-all-test-volume-fb5a42d8-1858-48ca-9355-84dc196ff782 12/14/22 08:29:35.462
STEP: Creating a pod to test Check all projections for projected volume plugin 12/14/22 08:29:35.466
Dec 14 08:29:35.475: INFO: Waiting up to 5m0s for pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5" in namespace "projected-8069" to be "Succeeded or Failed"
Dec 14 08:29:35.479: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313972ms
Dec 14 08:29:37.483: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5": Phase="Running", Reason="", readiness=false. Elapsed: 2.008057863s
Dec 14 08:29:39.485: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009241884s
STEP: Saw pod success 12/14/22 08:29:39.485
Dec 14 08:29:39.485: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5" satisfied condition "Succeeded or Failed"
Dec 14 08:29:39.488: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5 container projected-all-volume-test: <nil>
STEP: delete the pod 12/14/22 08:29:39.5
Dec 14 08:29:39.508: INFO: Waiting for pod projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5 to disappear
Dec 14 08:29:39.511: INFO: Pod projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Dec 14 08:29:39.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8069" for this suite. 12/14/22 08:29:39.519
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":79,"skipped":1422,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:29:35.438
    Dec 14 08:29:35.438: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:29:35.438
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:35.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:35.454
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-115d2d86-7b70-4ae9-afa9-faf5d17fd8df 12/14/22 08:29:35.458
    STEP: Creating secret with name secret-projected-all-test-volume-fb5a42d8-1858-48ca-9355-84dc196ff782 12/14/22 08:29:35.462
    STEP: Creating a pod to test Check all projections for projected volume plugin 12/14/22 08:29:35.466
    Dec 14 08:29:35.475: INFO: Waiting up to 5m0s for pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5" in namespace "projected-8069" to be "Succeeded or Failed"
    Dec 14 08:29:35.479: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313972ms
    Dec 14 08:29:37.483: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5": Phase="Running", Reason="", readiness=false. Elapsed: 2.008057863s
    Dec 14 08:29:39.485: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009241884s
    STEP: Saw pod success 12/14/22 08:29:39.485
    Dec 14 08:29:39.485: INFO: Pod "projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5" satisfied condition "Succeeded or Failed"
    Dec 14 08:29:39.488: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5 container projected-all-volume-test: <nil>
    STEP: delete the pod 12/14/22 08:29:39.5
    Dec 14 08:29:39.508: INFO: Waiting for pod projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5 to disappear
    Dec 14 08:29:39.511: INFO: Pod projected-volume-f55490d2-14f0-4dfe-94c2-672a0ed37ee5 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Dec 14 08:29:39.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8069" for this suite. 12/14/22 08:29:39.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:29:39.527
Dec 14 08:29:39.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:29:39.528
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:39.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:39.543
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-85a9cf75-fc0f-404c-bb3e-289a9af56553 12/14/22 08:29:39.547
STEP: Creating a pod to test consume configMaps 12/14/22 08:29:39.551
Dec 14 08:29:39.560: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27" in namespace "projected-7451" to be "Succeeded or Failed"
Dec 14 08:29:39.564: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402983ms
Dec 14 08:29:41.570: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010204606s
Dec 14 08:29:43.570: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010506908s
STEP: Saw pod success 12/14/22 08:29:43.57
Dec 14 08:29:43.570: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27" satisfied condition "Succeeded or Failed"
Dec 14 08:29:43.575: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:29:43.584
Dec 14 08:29:43.591: INFO: Waiting for pod pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27 to disappear
Dec 14 08:29:43.594: INFO: Pod pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 08:29:43.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7451" for this suite. 12/14/22 08:29:43.6
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":80,"skipped":1435,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:29:39.527
    Dec 14 08:29:39.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:29:39.528
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:39.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:39.543
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-85a9cf75-fc0f-404c-bb3e-289a9af56553 12/14/22 08:29:39.547
    STEP: Creating a pod to test consume configMaps 12/14/22 08:29:39.551
    Dec 14 08:29:39.560: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27" in namespace "projected-7451" to be "Succeeded or Failed"
    Dec 14 08:29:39.564: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402983ms
    Dec 14 08:29:41.570: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010204606s
    Dec 14 08:29:43.570: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010506908s
    STEP: Saw pod success 12/14/22 08:29:43.57
    Dec 14 08:29:43.570: INFO: Pod "pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27" satisfied condition "Succeeded or Failed"
    Dec 14 08:29:43.575: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:29:43.584
    Dec 14 08:29:43.591: INFO: Waiting for pod pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27 to disappear
    Dec 14 08:29:43.594: INFO: Pod pod-projected-configmaps-3a603760-be6e-4612-a872-de6ab836fc27 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 08:29:43.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7451" for this suite. 12/14/22 08:29:43.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:29:43.606
Dec 14 08:29:43.606: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:29:43.607
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:43.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:43.623
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 12/14/22 08:29:43.629
Dec 14 08:29:43.638: INFO: Waiting up to 5m0s for pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc" in namespace "downward-api-7130" to be "Succeeded or Failed"
Dec 14 08:29:43.642: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885197ms
Dec 14 08:29:45.647: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008501019s
Dec 14 08:29:47.649: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010644382s
STEP: Saw pod success 12/14/22 08:29:47.649
Dec 14 08:29:47.650: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc" satisfied condition "Succeeded or Failed"
Dec 14 08:29:47.653: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc container dapi-container: <nil>
STEP: delete the pod 12/14/22 08:29:47.663
Dec 14 08:29:47.670: INFO: Waiting for pod downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc to disappear
Dec 14 08:29:47.673: INFO: Pod downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 14 08:29:47.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7130" for this suite. 12/14/22 08:29:47.678
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":81,"skipped":1489,"failed":0}
------------------------------
• [4.077 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:29:43.606
    Dec 14 08:29:43.606: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:29:43.607
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:43.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:43.623
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 12/14/22 08:29:43.629
    Dec 14 08:29:43.638: INFO: Waiting up to 5m0s for pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc" in namespace "downward-api-7130" to be "Succeeded or Failed"
    Dec 14 08:29:43.642: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885197ms
    Dec 14 08:29:45.647: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008501019s
    Dec 14 08:29:47.649: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010644382s
    STEP: Saw pod success 12/14/22 08:29:47.649
    Dec 14 08:29:47.650: INFO: Pod "downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc" satisfied condition "Succeeded or Failed"
    Dec 14 08:29:47.653: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc container dapi-container: <nil>
    STEP: delete the pod 12/14/22 08:29:47.663
    Dec 14 08:29:47.670: INFO: Waiting for pod downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc to disappear
    Dec 14 08:29:47.673: INFO: Pod downward-api-d9eaf0d5-699b-4b21-9304-4fc83dbd3dbc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 14 08:29:47.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7130" for this suite. 12/14/22 08:29:47.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:29:47.684
Dec 14 08:29:47.684: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 08:29:47.684
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:47.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:47.699
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9584 12/14/22 08:29:47.704
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 12/14/22 08:29:47.708
Dec 14 08:29:47.716: INFO: Found 0 stateful pods, waiting for 3
Dec 14 08:29:57.724: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:29:57.724: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:29:57.724: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:29:57.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:29:58.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:29:58.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:29:58.015: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/14/22 08:30:08.035
Dec 14 08:30:08.060: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/14/22 08:30:08.06
STEP: Updating Pods in reverse ordinal order 12/14/22 08:30:18.078
Dec 14 08:30:18.082: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:30:18.369: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:30:18.369: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:30:18.369: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:30:28.391: INFO: Waiting for StatefulSet statefulset-9584/ss2 to complete update
STEP: Rolling back to a previous revision 12/14/22 08:30:38.402
Dec 14 08:30:38.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:30:38.845: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:30:38.845: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:30:38.845: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:30:48.881: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 12/14/22 08:30:58.9
Dec 14 08:30:58.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:30:59.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:30:59.264: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:30:59.265: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 08:31:09.296: INFO: Deleting all statefulset in ns statefulset-9584
Dec 14 08:31:09.299: INFO: Scaling statefulset ss2 to 0
Dec 14 08:31:19.320: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:31:19.323: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 08:31:19.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9584" for this suite. 12/14/22 08:31:19.339
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":82,"skipped":1516,"failed":0}
------------------------------
• [91.659 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:29:47.684
    Dec 14 08:29:47.684: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 08:29:47.684
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:29:47.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:29:47.699
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9584 12/14/22 08:29:47.704
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 12/14/22 08:29:47.708
    Dec 14 08:29:47.716: INFO: Found 0 stateful pods, waiting for 3
    Dec 14 08:29:57.724: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:29:57.724: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:29:57.724: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:29:57.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:29:58.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:29:58.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:29:58.015: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/14/22 08:30:08.035
    Dec 14 08:30:08.060: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/14/22 08:30:08.06
    STEP: Updating Pods in reverse ordinal order 12/14/22 08:30:18.078
    Dec 14 08:30:18.082: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:30:18.369: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 08:30:18.369: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:30:18.369: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:30:28.391: INFO: Waiting for StatefulSet statefulset-9584/ss2 to complete update
    STEP: Rolling back to a previous revision 12/14/22 08:30:38.402
    Dec 14 08:30:38.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:30:38.845: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:30:38.845: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:30:38.845: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:30:48.881: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 12/14/22 08:30:58.9
    Dec 14 08:30:58.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9584 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:30:59.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 08:30:59.264: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:30:59.265: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 08:31:09.296: INFO: Deleting all statefulset in ns statefulset-9584
    Dec 14 08:31:09.299: INFO: Scaling statefulset ss2 to 0
    Dec 14 08:31:19.320: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:31:19.323: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 08:31:19.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9584" for this suite. 12/14/22 08:31:19.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:31:19.343
Dec 14 08:31:19.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/14/22 08:31:19.344
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:31:19.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:31:19.359
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 12/14/22 08:31:19.364
STEP: patching the Namespace 12/14/22 08:31:19.375
STEP: get the Namespace and ensuring it has the label 12/14/22 08:31:19.379
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:31:19.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1992" for this suite. 12/14/22 08:31:19.386
STEP: Destroying namespace "nspatchtest-9e183a2b-bb07-46b6-8ce5-7c8a88a4abf9-3157" for this suite. 12/14/22 08:31:19.39
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":83,"skipped":1529,"failed":0}
------------------------------
• [0.051 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:31:19.343
    Dec 14 08:31:19.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/14/22 08:31:19.344
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:31:19.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:31:19.359
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 12/14/22 08:31:19.364
    STEP: patching the Namespace 12/14/22 08:31:19.375
    STEP: get the Namespace and ensuring it has the label 12/14/22 08:31:19.379
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:31:19.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1992" for this suite. 12/14/22 08:31:19.386
    STEP: Destroying namespace "nspatchtest-9e183a2b-bb07-46b6-8ce5-7c8a88a4abf9-3157" for this suite. 12/14/22 08:31:19.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:31:19.396
Dec 14 08:31:19.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 12/14/22 08:31:19.396
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:31:19.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:31:19.412
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 12/14/22 08:31:19.417
STEP: Ensuring a job is scheduled 12/14/22 08:31:19.421
STEP: Ensuring exactly one is scheduled 12/14/22 08:32:01.426
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 08:32:01.429
STEP: Ensuring no more jobs are scheduled 12/14/22 08:32:01.432
STEP: Removing cronjob 12/14/22 08:37:01.442
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 14 08:37:01.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2551" for this suite. 12/14/22 08:37:01.453
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":84,"skipped":1585,"failed":0}
------------------------------
• [SLOW TEST] [342.061 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:31:19.396
    Dec 14 08:31:19.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 12/14/22 08:31:19.396
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:31:19.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:31:19.412
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 12/14/22 08:31:19.417
    STEP: Ensuring a job is scheduled 12/14/22 08:31:19.421
    STEP: Ensuring exactly one is scheduled 12/14/22 08:32:01.426
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 08:32:01.429
    STEP: Ensuring no more jobs are scheduled 12/14/22 08:32:01.432
    STEP: Removing cronjob 12/14/22 08:37:01.442
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 14 08:37:01.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2551" for this suite. 12/14/22 08:37:01.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:01.459
Dec 14 08:37:01.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:37:01.46
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:01.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:01.474
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 12/14/22 08:37:01.479
Dec 14 08:37:01.498: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0" in namespace "emptydir-9292" to be "running"
Dec 14 08:37:01.505: INFO: Pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555829ms
Dec 14 08:37:03.511: INFO: Pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012173136s
Dec 14 08:37:03.511: INFO: Pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0" satisfied condition "running"
STEP: Reading file content from the nginx-container 12/14/22 08:37:03.511
Dec 14 08:37:03.511: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9292 PodName:pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 08:37:03.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:37:03.511: INFO: ExecWithOptions: Clientset creation
Dec 14 08:37:03.511: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-9292/pods/pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Dec 14 08:37:03.943: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:37:03.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9292" for this suite. 12/14/22 08:37:03.949
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":85,"skipped":1631,"failed":0}
------------------------------
• [2.495 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:01.459
    Dec 14 08:37:01.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:37:01.46
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:01.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:01.474
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 12/14/22 08:37:01.479
    Dec 14 08:37:01.498: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0" in namespace "emptydir-9292" to be "running"
    Dec 14 08:37:01.505: INFO: Pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555829ms
    Dec 14 08:37:03.511: INFO: Pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012173136s
    Dec 14 08:37:03.511: INFO: Pod "pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0" satisfied condition "running"
    STEP: Reading file content from the nginx-container 12/14/22 08:37:03.511
    Dec 14 08:37:03.511: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9292 PodName:pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 08:37:03.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:37:03.511: INFO: ExecWithOptions: Clientset creation
    Dec 14 08:37:03.511: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-9292/pods/pod-sharedvolume-bd6cdc1d-1a46-4029-98e1-162cd9e5d7c0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Dec 14 08:37:03.943: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:37:03.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9292" for this suite. 12/14/22 08:37:03.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:03.954
Dec 14 08:37:03.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:37:03.955
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:03.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:03.971
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-8036 12/14/22 08:37:03.976
STEP: creating service affinity-nodeport in namespace services-8036 12/14/22 08:37:03.976
STEP: creating replication controller affinity-nodeport in namespace services-8036 12/14/22 08:37:03.988
I1214 08:37:03.991843    4635 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8036, replica count: 3
I1214 08:37:07.042038    4635 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 08:37:07.056: INFO: Creating new exec pod
Dec 14 08:37:07.064: INFO: Waiting up to 5m0s for pod "execpod-affinitybs5rg" in namespace "services-8036" to be "running"
Dec 14 08:37:07.068: INFO: Pod "execpod-affinitybs5rg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195219ms
Dec 14 08:37:09.073: INFO: Pod "execpod-affinitybs5rg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009017134s
Dec 14 08:37:09.073: INFO: Pod "execpod-affinitybs5rg" satisfied condition "running"
Dec 14 08:37:10.080: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Dec 14 08:37:10.572: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 14 08:37:10.572: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:37:10.572: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.109.238.179 80'
Dec 14 08:37:10.935: INFO: stderr: "+ nc -v -t -w 2 100.109.238.179 80\n+ echo hostName\nConnection to 100.109.238.179 80 port [tcp/http] succeeded!\n"
Dec 14 08:37:10.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:37:10.935: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30829'
Dec 14 08:37:11.427: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30829\nConnection to 10.250.3.58 30829 port [tcp/*] succeeded!\n"
Dec 14 08:37:11.427: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:37:11.427: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30829'
Dec 14 08:37:11.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30829\nConnection to 10.250.3.210 30829 port [tcp/*] succeeded!\n"
Dec 14 08:37:11.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:37:11.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30829/ ; done'
Dec 14 08:37:12.399: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n"
Dec 14 08:37:12.400: INFO: stdout: "\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg"
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
Dec 14 08:37:12.400: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8036, will wait for the garbage collector to delete the pods 12/14/22 08:37:12.408
Dec 14 08:37:12.468: INFO: Deleting ReplicationController affinity-nodeport took: 5.481481ms
Dec 14 08:37:12.568: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.09218ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 08:37:14.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8036" for this suite. 12/14/22 08:37:14.791
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":86,"skipped":1649,"failed":0}
------------------------------
• [10.841 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:03.954
    Dec 14 08:37:03.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:37:03.955
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:03.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:03.971
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-8036 12/14/22 08:37:03.976
    STEP: creating service affinity-nodeport in namespace services-8036 12/14/22 08:37:03.976
    STEP: creating replication controller affinity-nodeport in namespace services-8036 12/14/22 08:37:03.988
    I1214 08:37:03.991843    4635 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8036, replica count: 3
    I1214 08:37:07.042038    4635 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 08:37:07.056: INFO: Creating new exec pod
    Dec 14 08:37:07.064: INFO: Waiting up to 5m0s for pod "execpod-affinitybs5rg" in namespace "services-8036" to be "running"
    Dec 14 08:37:07.068: INFO: Pod "execpod-affinitybs5rg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195219ms
    Dec 14 08:37:09.073: INFO: Pod "execpod-affinitybs5rg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009017134s
    Dec 14 08:37:09.073: INFO: Pod "execpod-affinitybs5rg" satisfied condition "running"
    Dec 14 08:37:10.080: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Dec 14 08:37:10.572: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Dec 14 08:37:10.572: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:37:10.572: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.109.238.179 80'
    Dec 14 08:37:10.935: INFO: stderr: "+ nc -v -t -w 2 100.109.238.179 80\n+ echo hostName\nConnection to 100.109.238.179 80 port [tcp/http] succeeded!\n"
    Dec 14 08:37:10.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:37:10.935: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30829'
    Dec 14 08:37:11.427: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30829\nConnection to 10.250.3.58 30829 port [tcp/*] succeeded!\n"
    Dec 14 08:37:11.427: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:37:11.427: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30829'
    Dec 14 08:37:11.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30829\nConnection to 10.250.3.210 30829 port [tcp/*] succeeded!\n"
    Dec 14 08:37:11.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:37:11.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8036 exec execpod-affinitybs5rg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30829/ ; done'
    Dec 14 08:37:12.399: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30829/\n"
    Dec 14 08:37:12.400: INFO: stdout: "\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg\naffinity-nodeport-k8zxg"
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Received response from host: affinity-nodeport-k8zxg
    Dec 14 08:37:12.400: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-8036, will wait for the garbage collector to delete the pods 12/14/22 08:37:12.408
    Dec 14 08:37:12.468: INFO: Deleting ReplicationController affinity-nodeport took: 5.481481ms
    Dec 14 08:37:12.568: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.09218ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 08:37:14.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8036" for this suite. 12/14/22 08:37:14.791
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:14.798
Dec 14 08:37:14.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:37:14.798
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:14.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:14.813
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:37:14.829
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:37:15.124
STEP: Deploying the webhook pod 12/14/22 08:37:15.131
STEP: Wait for the deployment to be ready 12/14/22 08:37:15.14
Dec 14 08:37:15.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:37:17.162
STEP: Verifying the service has paired with the endpoint 12/14/22 08:37:17.171
Dec 14 08:37:18.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 12/14/22 08:37:18.175
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/14/22 08:37:18.178
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 08:37:18.178
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/14/22 08:37:18.178
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/14/22 08:37:18.181
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 08:37:18.181
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 08:37:18.183
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:37:18.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4314" for this suite. 12/14/22 08:37:18.189
STEP: Destroying namespace "webhook-4314-markers" for this suite. 12/14/22 08:37:18.194
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":87,"skipped":1706,"failed":0}
------------------------------
• [3.426 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:14.798
    Dec 14 08:37:14.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:37:14.798
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:14.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:14.813
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:37:14.829
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:37:15.124
    STEP: Deploying the webhook pod 12/14/22 08:37:15.131
    STEP: Wait for the deployment to be ready 12/14/22 08:37:15.14
    Dec 14 08:37:15.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:37:17.162
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:37:17.171
    Dec 14 08:37:18.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 12/14/22 08:37:18.175
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/14/22 08:37:18.178
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 08:37:18.178
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/14/22 08:37:18.178
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/14/22 08:37:18.181
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 08:37:18.181
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 08:37:18.183
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:37:18.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4314" for this suite. 12/14/22 08:37:18.189
    STEP: Destroying namespace "webhook-4314-markers" for this suite. 12/14/22 08:37:18.194
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:18.226
Dec 14 08:37:18.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 12/14/22 08:37:18.226
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:18.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:18.24
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Dec 14 08:37:18.258: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 08:37:23.265: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 08:37:23.265
STEP: Scaling up "test-rs" replicaset  12/14/22 08:37:23.265
Dec 14 08:37:23.274: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 12/14/22 08:37:23.274
W1214 08:37:23.280962    4635 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 14 08:37:23.283: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 08:37:23.286: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 08:37:23.301: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 08:37:23.304: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 08:37:24.105: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 2, AvailableReplicas 2
Dec 14 08:37:24.726: INFO: observed Replicaset test-rs in namespace replicaset-5699 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 14 08:37:24.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5699" for this suite. 12/14/22 08:37:24.732
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":88,"skipped":1748,"failed":0}
------------------------------
• [6.512 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:18.226
    Dec 14 08:37:18.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 12/14/22 08:37:18.226
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:18.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:18.24
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Dec 14 08:37:18.258: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 08:37:23.265: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 08:37:23.265
    STEP: Scaling up "test-rs" replicaset  12/14/22 08:37:23.265
    Dec 14 08:37:23.274: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 12/14/22 08:37:23.274
    W1214 08:37:23.280962    4635 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 14 08:37:23.283: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 08:37:23.286: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 08:37:23.301: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 08:37:23.304: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 08:37:24.105: INFO: observed ReplicaSet test-rs in namespace replicaset-5699 with ReadyReplicas 2, AvailableReplicas 2
    Dec 14 08:37:24.726: INFO: observed Replicaset test-rs in namespace replicaset-5699 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 14 08:37:24.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5699" for this suite. 12/14/22 08:37:24.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:24.738
Dec 14 08:37:24.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:37:24.739
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:24.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:24.756
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 08:37:24.76
Dec 14 08:37:24.769: INFO: Waiting up to 5m0s for pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39" in namespace "emptydir-3975" to be "Succeeded or Failed"
Dec 14 08:37:24.772: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960448ms
Dec 14 08:37:26.778: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00907998s
Dec 14 08:37:28.778: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008275302s
STEP: Saw pod success 12/14/22 08:37:28.778
Dec 14 08:37:28.778: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39" satisfied condition "Succeeded or Failed"
Dec 14 08:37:28.781: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39 container test-container: <nil>
STEP: delete the pod 12/14/22 08:37:28.806
Dec 14 08:37:28.814: INFO: Waiting for pod pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39 to disappear
Dec 14 08:37:28.818: INFO: Pod pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:37:28.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3975" for this suite. 12/14/22 08:37:28.823
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":89,"skipped":1765,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:24.738
    Dec 14 08:37:24.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:37:24.739
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:24.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:24.756
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 08:37:24.76
    Dec 14 08:37:24.769: INFO: Waiting up to 5m0s for pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39" in namespace "emptydir-3975" to be "Succeeded or Failed"
    Dec 14 08:37:24.772: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960448ms
    Dec 14 08:37:26.778: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00907998s
    Dec 14 08:37:28.778: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008275302s
    STEP: Saw pod success 12/14/22 08:37:28.778
    Dec 14 08:37:28.778: INFO: Pod "pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39" satisfied condition "Succeeded or Failed"
    Dec 14 08:37:28.781: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:37:28.806
    Dec 14 08:37:28.814: INFO: Waiting for pod pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39 to disappear
    Dec 14 08:37:28.818: INFO: Pod pod-f4ab7b6c-ae94-4bc9-8b5c-3ed1a1aa5d39 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:37:28.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3975" for this suite. 12/14/22 08:37:28.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:28.829
Dec 14 08:37:28.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:37:28.829
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:28.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:28.843
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-e4234dd0-77c2-4275-9ecb-705dec1881fd 12/14/22 08:37:28.854
STEP: Creating configMap with name cm-test-opt-upd-ff79c491-13f8-407b-8359-b17ffbe5b82c 12/14/22 08:37:28.857
STEP: Creating the pod 12/14/22 08:37:28.861
Dec 14 08:37:28.871: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23" in namespace "projected-8816" to be "running and ready"
Dec 14 08:37:28.875: INFO: Pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.304485ms
Dec 14 08:37:28.875: INFO: The phase of Pod pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:37:30.881: INFO: Pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23": Phase="Running", Reason="", readiness=true. Elapsed: 2.009871621s
Dec 14 08:37:30.881: INFO: The phase of Pod pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23 is Running (Ready = true)
Dec 14 08:37:30.881: INFO: Pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-e4234dd0-77c2-4275-9ecb-705dec1881fd 12/14/22 08:37:31.07
STEP: Updating configmap cm-test-opt-upd-ff79c491-13f8-407b-8359-b17ffbe5b82c 12/14/22 08:37:31.075
STEP: Creating configMap with name cm-test-opt-create-bb307c5e-58b6-4983-95ed-35088d5d1627 12/14/22 08:37:31.079
STEP: waiting to observe update in volume 12/14/22 08:37:31.083
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 08:37:33.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8816" for this suite. 12/14/22 08:37:33.345
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":90,"skipped":1808,"failed":0}
------------------------------
• [4.523 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:28.829
    Dec 14 08:37:28.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:37:28.829
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:28.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:28.843
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-e4234dd0-77c2-4275-9ecb-705dec1881fd 12/14/22 08:37:28.854
    STEP: Creating configMap with name cm-test-opt-upd-ff79c491-13f8-407b-8359-b17ffbe5b82c 12/14/22 08:37:28.857
    STEP: Creating the pod 12/14/22 08:37:28.861
    Dec 14 08:37:28.871: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23" in namespace "projected-8816" to be "running and ready"
    Dec 14 08:37:28.875: INFO: Pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.304485ms
    Dec 14 08:37:28.875: INFO: The phase of Pod pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:37:30.881: INFO: Pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23": Phase="Running", Reason="", readiness=true. Elapsed: 2.009871621s
    Dec 14 08:37:30.881: INFO: The phase of Pod pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23 is Running (Ready = true)
    Dec 14 08:37:30.881: INFO: Pod "pod-projected-configmaps-0053e66f-57ef-4b20-aadb-bd63a5afbf23" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-e4234dd0-77c2-4275-9ecb-705dec1881fd 12/14/22 08:37:31.07
    STEP: Updating configmap cm-test-opt-upd-ff79c491-13f8-407b-8359-b17ffbe5b82c 12/14/22 08:37:31.075
    STEP: Creating configMap with name cm-test-opt-create-bb307c5e-58b6-4983-95ed-35088d5d1627 12/14/22 08:37:31.079
    STEP: waiting to observe update in volume 12/14/22 08:37:31.083
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 08:37:33.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8816" for this suite. 12/14/22 08:37:33.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:33.352
Dec 14 08:37:33.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 08:37:33.353
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:33.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:33.368
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1543 12/14/22 08:37:33.373
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 12/14/22 08:37:33.378
STEP: Creating pod with conflicting port in namespace statefulset-1543 12/14/22 08:37:33.382
STEP: Waiting until pod test-pod will start running in namespace statefulset-1543 12/14/22 08:37:33.394
Dec 14 08:37:33.394: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1543" to be "running"
Dec 14 08:37:33.397: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448692ms
Dec 14 08:37:35.403: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009388221s
Dec 14 08:37:35.403: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-1543 12/14/22 08:37:35.403
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1543 12/14/22 08:37:35.409
Dec 14 08:37:35.421: INFO: Observed stateful pod in namespace: statefulset-1543, name: ss-0, uid: 77b39d69-18c4-4ce8-a275-b440f90dc895, status phase: Pending. Waiting for statefulset controller to delete.
Dec 14 08:37:35.431: INFO: Observed stateful pod in namespace: statefulset-1543, name: ss-0, uid: 77b39d69-18c4-4ce8-a275-b440f90dc895, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 08:37:35.434: INFO: Observed stateful pod in namespace: statefulset-1543, name: ss-0, uid: 77b39d69-18c4-4ce8-a275-b440f90dc895, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 08:37:35.436: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1543
STEP: Removing pod with conflicting port in namespace statefulset-1543 12/14/22 08:37:35.436
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1543 and will be in running state 12/14/22 08:37:35.443
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 08:37:37.451: INFO: Deleting all statefulset in ns statefulset-1543
Dec 14 08:37:37.454: INFO: Scaling statefulset ss to 0
Dec 14 08:37:47.475: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:37:47.479: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 08:37:47.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1543" for this suite. 12/14/22 08:37:47.495
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":91,"skipped":1821,"failed":0}
------------------------------
• [14.148 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:33.352
    Dec 14 08:37:33.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 08:37:33.353
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:33.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:33.368
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1543 12/14/22 08:37:33.373
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 12/14/22 08:37:33.378
    STEP: Creating pod with conflicting port in namespace statefulset-1543 12/14/22 08:37:33.382
    STEP: Waiting until pod test-pod will start running in namespace statefulset-1543 12/14/22 08:37:33.394
    Dec 14 08:37:33.394: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1543" to be "running"
    Dec 14 08:37:33.397: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448692ms
    Dec 14 08:37:35.403: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009388221s
    Dec 14 08:37:35.403: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-1543 12/14/22 08:37:35.403
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1543 12/14/22 08:37:35.409
    Dec 14 08:37:35.421: INFO: Observed stateful pod in namespace: statefulset-1543, name: ss-0, uid: 77b39d69-18c4-4ce8-a275-b440f90dc895, status phase: Pending. Waiting for statefulset controller to delete.
    Dec 14 08:37:35.431: INFO: Observed stateful pod in namespace: statefulset-1543, name: ss-0, uid: 77b39d69-18c4-4ce8-a275-b440f90dc895, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 14 08:37:35.434: INFO: Observed stateful pod in namespace: statefulset-1543, name: ss-0, uid: 77b39d69-18c4-4ce8-a275-b440f90dc895, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 14 08:37:35.436: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1543
    STEP: Removing pod with conflicting port in namespace statefulset-1543 12/14/22 08:37:35.436
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1543 and will be in running state 12/14/22 08:37:35.443
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 08:37:37.451: INFO: Deleting all statefulset in ns statefulset-1543
    Dec 14 08:37:37.454: INFO: Scaling statefulset ss to 0
    Dec 14 08:37:47.475: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:37:47.479: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 08:37:47.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1543" for this suite. 12/14/22 08:37:47.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:47.5
Dec 14 08:37:47.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:37:47.501
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:47.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:47.516
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 12/14/22 08:37:47.52
Dec 14 08:37:47.529: INFO: Waiting up to 5m0s for pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075" in namespace "emptydir-2345" to be "Succeeded or Failed"
Dec 14 08:37:47.532: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010447ms
Dec 14 08:37:49.540: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075": Phase="Running", Reason="", readiness=false. Elapsed: 2.011204396s
Dec 14 08:37:51.536: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007519717s
STEP: Saw pod success 12/14/22 08:37:51.536
Dec 14 08:37:51.536: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075" satisfied condition "Succeeded or Failed"
Dec 14 08:37:51.540: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075 container test-container: <nil>
STEP: delete the pod 12/14/22 08:37:51.549
Dec 14 08:37:51.556: INFO: Waiting for pod pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075 to disappear
Dec 14 08:37:51.558: INFO: Pod pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:37:51.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2345" for this suite. 12/14/22 08:37:51.563
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":92,"skipped":1829,"failed":0}
------------------------------
• [4.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:47.5
    Dec 14 08:37:47.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:37:47.501
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:47.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:47.516
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 12/14/22 08:37:47.52
    Dec 14 08:37:47.529: INFO: Waiting up to 5m0s for pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075" in namespace "emptydir-2345" to be "Succeeded or Failed"
    Dec 14 08:37:47.532: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010447ms
    Dec 14 08:37:49.540: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075": Phase="Running", Reason="", readiness=false. Elapsed: 2.011204396s
    Dec 14 08:37:51.536: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007519717s
    STEP: Saw pod success 12/14/22 08:37:51.536
    Dec 14 08:37:51.536: INFO: Pod "pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075" satisfied condition "Succeeded or Failed"
    Dec 14 08:37:51.540: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:37:51.549
    Dec 14 08:37:51.556: INFO: Waiting for pod pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075 to disappear
    Dec 14 08:37:51.558: INFO: Pod pod-5758362e-34a4-40ca-ab9d-7f3ac6b46075 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:37:51.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2345" for this suite. 12/14/22 08:37:51.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:51.569
Dec 14 08:37:51.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:37:51.57
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:51.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:51.584
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Dec 14 08:37:51.597: INFO: Waiting up to 5m0s for pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70" in namespace "kubelet-test-2311" to be "running and ready"
Dec 14 08:37:51.600: INFO: Pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194103ms
Dec 14 08:37:51.600: INFO: The phase of Pod busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:37:53.608: INFO: Pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70": Phase="Running", Reason="", readiness=true. Elapsed: 2.010795769s
Dec 14 08:37:53.608: INFO: The phase of Pod busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70 is Running (Ready = true)
Dec 14 08:37:53.608: INFO: Pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 14 08:37:53.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2311" for this suite. 12/14/22 08:37:53.632
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":93,"skipped":1855,"failed":0}
------------------------------
• [2.068 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:51.569
    Dec 14 08:37:51.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:37:51.57
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:51.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:51.584
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Dec 14 08:37:51.597: INFO: Waiting up to 5m0s for pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70" in namespace "kubelet-test-2311" to be "running and ready"
    Dec 14 08:37:51.600: INFO: Pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194103ms
    Dec 14 08:37:51.600: INFO: The phase of Pod busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:37:53.608: INFO: Pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70": Phase="Running", Reason="", readiness=true. Elapsed: 2.010795769s
    Dec 14 08:37:53.608: INFO: The phase of Pod busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70 is Running (Ready = true)
    Dec 14 08:37:53.608: INFO: Pod "busybox-scheduling-9ed7e76d-76bf-4bbb-b1f8-afe50a310d70" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 14 08:37:53.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2311" for this suite. 12/14/22 08:37:53.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:37:53.637
Dec 14 08:37:53.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 12/14/22 08:37:53.638
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:53.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:53.654
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 12/14/22 08:37:53.658
Dec 14 08:37:53.658: INFO: PodSpec: initContainers in spec.initContainers
Dec 14 08:38:38.283: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a4009fcd-5c82-4c6c-9718-3f9c4a49f4bc", GenerateName:"", Namespace:"init-container-6502", SelfLink:"", UID:"f5076c83-f597-46f4-a1bd-c463cede78b8", ResourceVersion:"20914", Generation:0, CreationTimestamp:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"658872780"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0d57211cc5978d6958a3dfc4ffc8796277c40dfab6a399441df54d2c61bce769", "cni.projectcalico.org/podIP":"100.64.1.165/32", "cni.projectcalico.org/podIPs":"100.64.1.165/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dd3110), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 8, 37, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dd3140), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 8, 38, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dd3170), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tf98s", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003a78da0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tm5on-jne.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tf98s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tm5on-jne.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tf98s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tm5on-jne.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tf98s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00156b540), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0006f2460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00156b5c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00156b5e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00156b5e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00156b5ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000025930), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.3.210", PodIP:"100.64.1.165", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.165"}}, StartTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f2540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f25b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://4825314bcc0480ab3ba5c82232c1d4a6300bbed3c3abaff4cf166ddaf12748f8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a78e20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a78e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00156b66f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 08:38:38.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6502" for this suite. 12/14/22 08:38:38.289
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":94,"skipped":1876,"failed":0}
------------------------------
• [44.656 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:37:53.637
    Dec 14 08:37:53.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 12/14/22 08:37:53.638
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:37:53.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:37:53.654
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 12/14/22 08:37:53.658
    Dec 14 08:37:53.658: INFO: PodSpec: initContainers in spec.initContainers
    Dec 14 08:38:38.283: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a4009fcd-5c82-4c6c-9718-3f9c4a49f4bc", GenerateName:"", Namespace:"init-container-6502", SelfLink:"", UID:"f5076c83-f597-46f4-a1bd-c463cede78b8", ResourceVersion:"20914", Generation:0, CreationTimestamp:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"658872780"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0d57211cc5978d6958a3dfc4ffc8796277c40dfab6a399441df54d2c61bce769", "cni.projectcalico.org/podIP":"100.64.1.165/32", "cni.projectcalico.org/podIPs":"100.64.1.165/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dd3110), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 8, 37, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dd3140), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 8, 38, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dd3170), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tf98s", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003a78da0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tm5on-jne.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tf98s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tm5on-jne.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tf98s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tm5on-jne.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tf98s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00156b540), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0006f2460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00156b5c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00156b5e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00156b5e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00156b5ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000025930), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.3.210", PodIP:"100.64.1.165", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.165"}}, StartTime:time.Date(2022, time.December, 14, 8, 37, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f2540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f25b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://4825314bcc0480ab3ba5c82232c1d4a6300bbed3c3abaff4cf166ddaf12748f8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a78e20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a78e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00156b66f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 08:38:38.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6502" for this suite. 12/14/22 08:38:38.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:38:38.294
Dec 14 08:38:38.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:38:38.295
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:38.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:38.325
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:38:38.341
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:38:38.61
STEP: Deploying the webhook pod 12/14/22 08:38:38.618
STEP: Wait for the deployment to be ready 12/14/22 08:38:38.628
Dec 14 08:38:38.634: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 08:38:40.647
STEP: Verifying the service has paired with the endpoint 12/14/22 08:38:40.655
Dec 14 08:38:41.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Dec 14 08:38:41.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/14/22 08:38:42.172
STEP: Creating a custom resource that should be denied by the webhook 12/14/22 08:38:42.294
STEP: Creating a custom resource whose deletion would be denied by the webhook 12/14/22 08:38:44.428
STEP: Updating the custom resource with disallowed data should be denied 12/14/22 08:38:44.533
STEP: Deleting the custom resource should be denied 12/14/22 08:38:44.649
STEP: Remove the offending key and value from the custom resource data 12/14/22 08:38:44.709
STEP: Deleting the updated custom resource should be successful 12/14/22 08:38:44.825
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:38:45.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8072" for this suite. 12/14/22 08:38:45.445
STEP: Destroying namespace "webhook-8072-markers" for this suite. 12/14/22 08:38:45.45
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":95,"skipped":1899,"failed":0}
------------------------------
• [7.186 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:38:38.294
    Dec 14 08:38:38.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:38:38.295
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:38.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:38.325
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:38:38.341
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:38:38.61
    STEP: Deploying the webhook pod 12/14/22 08:38:38.618
    STEP: Wait for the deployment to be ready 12/14/22 08:38:38.628
    Dec 14 08:38:38.634: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 08:38:40.647
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:38:40.655
    Dec 14 08:38:41.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Dec 14 08:38:41.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/14/22 08:38:42.172
    STEP: Creating a custom resource that should be denied by the webhook 12/14/22 08:38:42.294
    STEP: Creating a custom resource whose deletion would be denied by the webhook 12/14/22 08:38:44.428
    STEP: Updating the custom resource with disallowed data should be denied 12/14/22 08:38:44.533
    STEP: Deleting the custom resource should be denied 12/14/22 08:38:44.649
    STEP: Remove the offending key and value from the custom resource data 12/14/22 08:38:44.709
    STEP: Deleting the updated custom resource should be successful 12/14/22 08:38:44.825
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:38:45.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8072" for this suite. 12/14/22 08:38:45.445
    STEP: Destroying namespace "webhook-8072-markers" for this suite. 12/14/22 08:38:45.45
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:38:45.482
Dec 14 08:38:45.482: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 12/14/22 08:38:45.483
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:45.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:45.498
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 12/14/22 08:38:45.503
Dec 14 08:38:45.513: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6611" to be "running and ready"
Dec 14 08:38:45.516: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999181ms
Dec 14 08:38:45.516: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:38:47.522: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008782907s
Dec 14 08:38:47.522: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Dec 14 08:38:47.522: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 12/14/22 08:38:47.526
STEP: Then the orphan pod is adopted 12/14/22 08:38:47.531
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 14 08:38:48.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6611" for this suite. 12/14/22 08:38:48.544
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":96,"skipped":1937,"failed":0}
------------------------------
• [3.067 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:38:45.482
    Dec 14 08:38:45.482: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 12/14/22 08:38:45.483
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:45.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:45.498
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 12/14/22 08:38:45.503
    Dec 14 08:38:45.513: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6611" to be "running and ready"
    Dec 14 08:38:45.516: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999181ms
    Dec 14 08:38:45.516: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:38:47.522: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008782907s
    Dec 14 08:38:47.522: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Dec 14 08:38:47.522: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 12/14/22 08:38:47.526
    STEP: Then the orphan pod is adopted 12/14/22 08:38:47.531
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 14 08:38:48.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6611" for this suite. 12/14/22 08:38:48.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:38:48.55
Dec 14 08:38:48.550: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 08:38:48.55
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:48.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:48.567
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-1df4ffd7-35d4-4503-a6f2-c7e829e63dd4 12/14/22 08:38:48.571
STEP: Creating a pod to test consume configMaps 12/14/22 08:38:48.575
Dec 14 08:38:48.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af" in namespace "configmap-7003" to be "Succeeded or Failed"
Dec 14 08:38:48.618: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841738ms
Dec 14 08:38:50.622: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008616677s
Dec 14 08:38:52.623: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009649009s
STEP: Saw pod success 12/14/22 08:38:52.623
Dec 14 08:38:52.624: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af" satisfied condition "Succeeded or Failed"
Dec 14 08:38:52.628: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:38:52.639
Dec 14 08:38:52.647: INFO: Waiting for pod pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af to disappear
Dec 14 08:38:52.650: INFO: Pod pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 08:38:52.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7003" for this suite. 12/14/22 08:38:52.655
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1942,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:38:48.55
    Dec 14 08:38:48.550: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 08:38:48.55
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:48.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:48.567
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-1df4ffd7-35d4-4503-a6f2-c7e829e63dd4 12/14/22 08:38:48.571
    STEP: Creating a pod to test consume configMaps 12/14/22 08:38:48.575
    Dec 14 08:38:48.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af" in namespace "configmap-7003" to be "Succeeded or Failed"
    Dec 14 08:38:48.618: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841738ms
    Dec 14 08:38:50.622: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008616677s
    Dec 14 08:38:52.623: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009649009s
    STEP: Saw pod success 12/14/22 08:38:52.623
    Dec 14 08:38:52.624: INFO: Pod "pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af" satisfied condition "Succeeded or Failed"
    Dec 14 08:38:52.628: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:38:52.639
    Dec 14 08:38:52.647: INFO: Waiting for pod pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af to disappear
    Dec 14 08:38:52.650: INFO: Pod pod-configmaps-1bd82a37-c06d-4866-9b16-bc735ab9e2af no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 08:38:52.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7003" for this suite. 12/14/22 08:38:52.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:38:52.661
Dec 14 08:38:52.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 12/14/22 08:38:52.661
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:52.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:52.676
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 12/14/22 08:38:52.681
Dec 14 08:38:52.691: INFO: Waiting up to 5m0s for pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90" in namespace "containers-5105" to be "Succeeded or Failed"
Dec 14 08:38:52.695: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721544ms
Dec 14 08:38:54.700: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008550232s
Dec 14 08:38:56.701: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010281953s
STEP: Saw pod success 12/14/22 08:38:56.701
Dec 14 08:38:56.701: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90" satisfied condition "Succeeded or Failed"
Dec 14 08:38:56.705: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:38:56.719
Dec 14 08:38:56.729: INFO: Waiting for pod client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90 to disappear
Dec 14 08:38:56.733: INFO: Pod client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 14 08:38:56.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5105" for this suite. 12/14/22 08:38:56.741
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":98,"skipped":1956,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:38:52.661
    Dec 14 08:38:52.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 12/14/22 08:38:52.661
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:52.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:52.676
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 12/14/22 08:38:52.681
    Dec 14 08:38:52.691: INFO: Waiting up to 5m0s for pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90" in namespace "containers-5105" to be "Succeeded or Failed"
    Dec 14 08:38:52.695: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721544ms
    Dec 14 08:38:54.700: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008550232s
    Dec 14 08:38:56.701: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010281953s
    STEP: Saw pod success 12/14/22 08:38:56.701
    Dec 14 08:38:56.701: INFO: Pod "client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90" satisfied condition "Succeeded or Failed"
    Dec 14 08:38:56.705: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:38:56.719
    Dec 14 08:38:56.729: INFO: Waiting for pod client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90 to disappear
    Dec 14 08:38:56.733: INFO: Pod client-containers-d8d62268-3939-4b1c-8efd-2e11b22b0a90 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 14 08:38:56.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5105" for this suite. 12/14/22 08:38:56.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:38:56.748
Dec 14 08:38:56.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 08:38:56.749
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:56.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:56.771
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 12/14/22 08:38:56.779
STEP: Creating a ResourceQuota 12/14/22 08:39:01.783
STEP: Ensuring resource quota status is calculated 12/14/22 08:39:01.788
STEP: Creating a ReplicationController 12/14/22 08:39:03.794
STEP: Ensuring resource quota status captures replication controller creation 12/14/22 08:39:03.802
STEP: Deleting a ReplicationController 12/14/22 08:39:05.807
STEP: Ensuring resource quota status released usage 12/14/22 08:39:05.812
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 08:39:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8147" for this suite. 12/14/22 08:39:07.823
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":99,"skipped":2001,"failed":0}
------------------------------
• [11.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:38:56.748
    Dec 14 08:38:56.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 08:38:56.749
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:38:56.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:38:56.771
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 12/14/22 08:38:56.779
    STEP: Creating a ResourceQuota 12/14/22 08:39:01.783
    STEP: Ensuring resource quota status is calculated 12/14/22 08:39:01.788
    STEP: Creating a ReplicationController 12/14/22 08:39:03.794
    STEP: Ensuring resource quota status captures replication controller creation 12/14/22 08:39:03.802
    STEP: Deleting a ReplicationController 12/14/22 08:39:05.807
    STEP: Ensuring resource quota status released usage 12/14/22 08:39:05.812
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 08:39:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8147" for this suite. 12/14/22 08:39:07.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:39:07.829
Dec 14 08:39:07.830: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:39:07.83
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:07.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:07.85
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 12/14/22 08:39:07.855
Dec 14 08:39:07.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 create -f -'
Dec 14 08:39:08.856: INFO: stderr: ""
Dec 14 08:39:08.856: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:39:08.856
Dec 14 08:39:08.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:39:08.922: INFO: stderr: ""
Dec 14 08:39:08.922: INFO: stdout: "update-demo-nautilus-lp92r update-demo-nautilus-wmlb4 "
Dec 14 08:39:08.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-lp92r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:39:08.984: INFO: stderr: ""
Dec 14 08:39:08.984: INFO: stdout: ""
Dec 14 08:39:08.984: INFO: update-demo-nautilus-lp92r is created but not running
Dec 14 08:39:13.988: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:39:14.054: INFO: stderr: ""
Dec 14 08:39:14.054: INFO: stdout: "update-demo-nautilus-lp92r update-demo-nautilus-wmlb4 "
Dec 14 08:39:14.054: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-lp92r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:39:14.125: INFO: stderr: ""
Dec 14 08:39:14.125: INFO: stdout: "true"
Dec 14 08:39:14.125: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-lp92r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:39:14.189: INFO: stderr: ""
Dec 14 08:39:14.189: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:39:14.189: INFO: validating pod update-demo-nautilus-lp92r
Dec 14 08:39:14.302: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:39:14.302: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:39:14.302: INFO: update-demo-nautilus-lp92r is verified up and running
Dec 14 08:39:14.302: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-wmlb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:39:14.369: INFO: stderr: ""
Dec 14 08:39:14.369: INFO: stdout: "true"
Dec 14 08:39:14.369: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-wmlb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:39:14.430: INFO: stderr: ""
Dec 14 08:39:14.430: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:39:14.430: INFO: validating pod update-demo-nautilus-wmlb4
Dec 14 08:39:14.531: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:39:14.531: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:39:14.531: INFO: update-demo-nautilus-wmlb4 is verified up and running
STEP: using delete to clean up resources 12/14/22 08:39:14.531
Dec 14 08:39:14.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 delete --grace-period=0 --force -f -'
Dec 14 08:39:14.605: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:39:14.605: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 08:39:14.605: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get rc,svc -l name=update-demo --no-headers'
Dec 14 08:39:14.676: INFO: stderr: "No resources found in kubectl-1645 namespace.\n"
Dec 14 08:39:14.676: INFO: stdout: ""
Dec 14 08:39:14.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 08:39:14.743: INFO: stderr: ""
Dec 14 08:39:14.743: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:39:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1645" for this suite. 12/14/22 08:39:14.749
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":100,"skipped":2012,"failed":0}
------------------------------
• [6.925 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:39:07.829
    Dec 14 08:39:07.830: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:39:07.83
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:07.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:07.85
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 12/14/22 08:39:07.855
    Dec 14 08:39:07.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 create -f -'
    Dec 14 08:39:08.856: INFO: stderr: ""
    Dec 14 08:39:08.856: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:39:08.856
    Dec 14 08:39:08.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:39:08.922: INFO: stderr: ""
    Dec 14 08:39:08.922: INFO: stdout: "update-demo-nautilus-lp92r update-demo-nautilus-wmlb4 "
    Dec 14 08:39:08.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-lp92r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:39:08.984: INFO: stderr: ""
    Dec 14 08:39:08.984: INFO: stdout: ""
    Dec 14 08:39:08.984: INFO: update-demo-nautilus-lp92r is created but not running
    Dec 14 08:39:13.988: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:39:14.054: INFO: stderr: ""
    Dec 14 08:39:14.054: INFO: stdout: "update-demo-nautilus-lp92r update-demo-nautilus-wmlb4 "
    Dec 14 08:39:14.054: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-lp92r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:39:14.125: INFO: stderr: ""
    Dec 14 08:39:14.125: INFO: stdout: "true"
    Dec 14 08:39:14.125: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-lp92r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:39:14.189: INFO: stderr: ""
    Dec 14 08:39:14.189: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:39:14.189: INFO: validating pod update-demo-nautilus-lp92r
    Dec 14 08:39:14.302: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:39:14.302: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:39:14.302: INFO: update-demo-nautilus-lp92r is verified up and running
    Dec 14 08:39:14.302: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-wmlb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:39:14.369: INFO: stderr: ""
    Dec 14 08:39:14.369: INFO: stdout: "true"
    Dec 14 08:39:14.369: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods update-demo-nautilus-wmlb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:39:14.430: INFO: stderr: ""
    Dec 14 08:39:14.430: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:39:14.430: INFO: validating pod update-demo-nautilus-wmlb4
    Dec 14 08:39:14.531: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:39:14.531: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:39:14.531: INFO: update-demo-nautilus-wmlb4 is verified up and running
    STEP: using delete to clean up resources 12/14/22 08:39:14.531
    Dec 14 08:39:14.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 delete --grace-period=0 --force -f -'
    Dec 14 08:39:14.605: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:39:14.605: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 14 08:39:14.605: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get rc,svc -l name=update-demo --no-headers'
    Dec 14 08:39:14.676: INFO: stderr: "No resources found in kubectl-1645 namespace.\n"
    Dec 14 08:39:14.676: INFO: stdout: ""
    Dec 14 08:39:14.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1645 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 14 08:39:14.743: INFO: stderr: ""
    Dec 14 08:39:14.743: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:39:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1645" for this suite. 12/14/22 08:39:14.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:39:14.755
Dec 14 08:39:14.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:39:14.756
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:14.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:14.773
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-1bd3daa9-4dad-44f8-a3eb-884d32b5a469 12/14/22 08:39:14.778
STEP: Creating a pod to test consume configMaps 12/14/22 08:39:14.782
Dec 14 08:39:14.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4" in namespace "projected-9482" to be "Succeeded or Failed"
Dec 14 08:39:14.796: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944259ms
Dec 14 08:39:16.802: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009140078s
Dec 14 08:39:18.802: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00920287s
STEP: Saw pod success 12/14/22 08:39:18.802
Dec 14 08:39:18.802: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4" satisfied condition "Succeeded or Failed"
Dec 14 08:39:18.807: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:39:18.825
Dec 14 08:39:18.834: INFO: Waiting for pod pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4 to disappear
Dec 14 08:39:18.838: INFO: Pod pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 08:39:18.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9482" for this suite. 12/14/22 08:39:18.844
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":101,"skipped":2020,"failed":0}
------------------------------
• [4.094 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:39:14.755
    Dec 14 08:39:14.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:39:14.756
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:14.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:14.773
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-1bd3daa9-4dad-44f8-a3eb-884d32b5a469 12/14/22 08:39:14.778
    STEP: Creating a pod to test consume configMaps 12/14/22 08:39:14.782
    Dec 14 08:39:14.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4" in namespace "projected-9482" to be "Succeeded or Failed"
    Dec 14 08:39:14.796: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944259ms
    Dec 14 08:39:16.802: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009140078s
    Dec 14 08:39:18.802: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00920287s
    STEP: Saw pod success 12/14/22 08:39:18.802
    Dec 14 08:39:18.802: INFO: Pod "pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4" satisfied condition "Succeeded or Failed"
    Dec 14 08:39:18.807: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:39:18.825
    Dec 14 08:39:18.834: INFO: Waiting for pod pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4 to disappear
    Dec 14 08:39:18.838: INFO: Pod pod-projected-configmaps-163d0e9d-04b9-4a55-8acf-71aa330bf7f4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 08:39:18.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9482" for this suite. 12/14/22 08:39:18.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:39:18.85
Dec 14 08:39:18.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 08:39:18.85
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:18.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:18.867
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 12/14/22 08:39:18.873
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 12/14/22 08:39:18.878
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 12/14/22 08:39:18.878
STEP: creating a pod to probe DNS 12/14/22 08:39:18.878
STEP: submitting the pod to kubernetes 12/14/22 08:39:18.878
Dec 14 08:39:18.891: INFO: Waiting up to 15m0s for pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861" in namespace "dns-9703" to be "running"
Dec 14 08:39:18.896: INFO: Pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898182ms
Dec 14 08:39:20.901: INFO: Pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861": Phase="Running", Reason="", readiness=true. Elapsed: 2.010306846s
Dec 14 08:39:20.901: INFO: Pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861" satisfied condition "running"
STEP: retrieving the pod 12/14/22 08:39:20.901
STEP: looking for the results for each expected name from probers 12/14/22 08:39:20.906
Dec 14 08:39:21.087: INFO: DNS probes using dns-9703/dns-test-35fca113-10bf-41ef-a1b0-268a8d741861 succeeded

STEP: deleting the pod 12/14/22 08:39:21.087
STEP: deleting the test headless service 12/14/22 08:39:21.095
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 08:39:21.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9703" for this suite. 12/14/22 08:39:21.108
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":102,"skipped":2033,"failed":0}
------------------------------
• [2.263 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:39:18.85
    Dec 14 08:39:18.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 08:39:18.85
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:18.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:18.867
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 12/14/22 08:39:18.873
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     12/14/22 08:39:18.878
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9703.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     12/14/22 08:39:18.878
    STEP: creating a pod to probe DNS 12/14/22 08:39:18.878
    STEP: submitting the pod to kubernetes 12/14/22 08:39:18.878
    Dec 14 08:39:18.891: INFO: Waiting up to 15m0s for pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861" in namespace "dns-9703" to be "running"
    Dec 14 08:39:18.896: INFO: Pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898182ms
    Dec 14 08:39:20.901: INFO: Pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861": Phase="Running", Reason="", readiness=true. Elapsed: 2.010306846s
    Dec 14 08:39:20.901: INFO: Pod "dns-test-35fca113-10bf-41ef-a1b0-268a8d741861" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 08:39:20.901
    STEP: looking for the results for each expected name from probers 12/14/22 08:39:20.906
    Dec 14 08:39:21.087: INFO: DNS probes using dns-9703/dns-test-35fca113-10bf-41ef-a1b0-268a8d741861 succeeded

    STEP: deleting the pod 12/14/22 08:39:21.087
    STEP: deleting the test headless service 12/14/22 08:39:21.095
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 08:39:21.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9703" for this suite. 12/14/22 08:39:21.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:39:21.113
Dec 14 08:39:21.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:39:21.113
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:21.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:21.127
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 12/14/22 08:39:21.132
Dec 14 08:39:21.132: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd 12/14/22 08:39:29.677
STEP: check the unserved version gets removed 12/14/22 08:39:29.696
STEP: check the other version is not changed 12/14/22 08:39:32.714
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:39:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8914" for this suite. 12/14/22 08:39:38.191
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":103,"skipped":2041,"failed":0}
------------------------------
• [17.083 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:39:21.113
    Dec 14 08:39:21.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:39:21.113
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:21.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:21.127
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 12/14/22 08:39:21.132
    Dec 14 08:39:21.132: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: mark a version not serverd 12/14/22 08:39:29.677
    STEP: check the unserved version gets removed 12/14/22 08:39:29.696
    STEP: check the other version is not changed 12/14/22 08:39:32.714
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:39:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8914" for this suite. 12/14/22 08:39:38.191
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:39:38.196
Dec 14 08:39:38.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 08:39:38.196
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:38.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:38.209
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7562 12/14/22 08:39:38.213
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7562 12/14/22 08:39:38.217
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7562 12/14/22 08:39:38.222
Dec 14 08:39:38.224: INFO: Found 0 stateful pods, waiting for 1
Dec 14 08:39:48.232: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/14/22 08:39:48.232
Dec 14 08:39:48.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:39:48.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:39:48.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:39:48.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:39:48.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 08:39:58.701: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:39:58.701: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:39:58.714: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Dec 14 08:39:58.714: INFO: ss-0  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  }]
Dec 14 08:39:58.714: INFO: 
Dec 14 08:39:58.714: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 14 08:39:59.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995802566s
Dec 14 08:40:00.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989271302s
Dec 14 08:40:01.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984213046s
Dec 14 08:40:02.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97868065s
Dec 14 08:40:03.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974016065s
Dec 14 08:40:04.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964909539s
Dec 14 08:40:05.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959776193s
Dec 14 08:40:06.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954864306s
Dec 14 08:40:08.125: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.653859ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7562 12/14/22 08:40:09.125
Dec 14 08:40:09.130: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:40:09.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 08:40:09.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:40:09.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:40:09.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:40:09.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 08:40:09.764: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:40:09.764: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:40:09.764: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 08:40:10.183: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 08:40:10.183: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 08:40:10.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 08:40:10.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:40:10.187: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:40:10.187: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 12/14/22 08:40:10.187
Dec 14 08:40:10.191: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:40:10.648: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:40:10.648: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:40:10.648: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:40:10.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:40:11.095: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:40:11.095: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:40:11.095: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:40:11.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 08:40:11.463: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 08:40:11.463: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 08:40:11.463: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 08:40:11.463: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:40:11.467: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 14 08:40:21.475: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:40:21.475: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:40:21.475: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 08:40:21.487: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Dec 14 08:40:21.487: INFO: ss-0  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  }]
Dec 14 08:40:21.487: INFO: ss-1  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  }]
Dec 14 08:40:21.487: INFO: ss-2  shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  }]
Dec 14 08:40:21.487: INFO: 
Dec 14 08:40:21.487: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 08:40:22.492: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Dec 14 08:40:22.492: INFO: ss-0  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  }]
Dec 14 08:40:22.492: INFO: ss-1  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  }]
Dec 14 08:40:22.492: INFO: 
Dec 14 08:40:22.492: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 14 08:40:23.497: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99112083s
Dec 14 08:40:24.501: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986089853s
Dec 14 08:40:25.506: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981701222s
Dec 14 08:40:26.510: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977295054s
Dec 14 08:40:27.514: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973359765s
Dec 14 08:40:28.519: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968890133s
Dec 14 08:40:29.523: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964522089s
Dec 14 08:40:30.528: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.841282ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7562 12/14/22 08:40:31.529
Dec 14 08:40:31.533: INFO: Scaling statefulset ss to 0
Dec 14 08:40:31.545: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 08:40:31.548: INFO: Deleting all statefulset in ns statefulset-7562
Dec 14 08:40:31.551: INFO: Scaling statefulset ss to 0
Dec 14 08:40:31.562: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:40:31.564: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 08:40:31.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7562" for this suite. 12/14/22 08:40:31.58
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":104,"skipped":2044,"failed":0}
------------------------------
• [53.399 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:39:38.196
    Dec 14 08:39:38.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 08:39:38.196
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:39:38.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:39:38.209
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7562 12/14/22 08:39:38.213
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7562 12/14/22 08:39:38.217
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7562 12/14/22 08:39:38.222
    Dec 14 08:39:38.224: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 08:39:48.232: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/14/22 08:39:48.232
    Dec 14 08:39:48.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:39:48.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:39:48.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:39:48.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:39:48.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 14 08:39:58.701: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:39:58.701: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:39:58.714: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Dec 14 08:39:58.714: INFO: ss-0  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  }]
    Dec 14 08:39:58.714: INFO: 
    Dec 14 08:39:58.714: INFO: StatefulSet ss has not reached scale 3, at 1
    Dec 14 08:39:59.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995802566s
    Dec 14 08:40:00.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989271302s
    Dec 14 08:40:01.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984213046s
    Dec 14 08:40:02.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97868065s
    Dec 14 08:40:03.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974016065s
    Dec 14 08:40:04.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964909539s
    Dec 14 08:40:05.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959776193s
    Dec 14 08:40:06.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954864306s
    Dec 14 08:40:08.125: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.653859ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7562 12/14/22 08:40:09.125
    Dec 14 08:40:09.130: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:40:09.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 08:40:09.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:40:09.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:40:09.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:40:09.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 14 08:40:09.764: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:40:09.764: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:40:09.764: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 08:40:10.183: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 14 08:40:10.183: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 08:40:10.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 08:40:10.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:40:10.187: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:40:10.187: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 12/14/22 08:40:10.187
    Dec 14 08:40:10.191: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:40:10.648: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:40:10.648: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:40:10.648: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:40:10.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:40:11.095: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:40:11.095: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:40:11.095: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:40:11.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7562 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 08:40:11.463: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 08:40:11.463: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 08:40:11.463: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 08:40:11.463: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:40:11.467: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Dec 14 08:40:21.475: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:40:21.475: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:40:21.475: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 08:40:21.487: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Dec 14 08:40:21.487: INFO: ss-0  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  }]
    Dec 14 08:40:21.487: INFO: ss-1  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  }]
    Dec 14 08:40:21.487: INFO: ss-2  shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  }]
    Dec 14 08:40:21.487: INFO: 
    Dec 14 08:40:21.487: INFO: StatefulSet ss has not reached scale 0, at 3
    Dec 14 08:40:22.492: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Dec 14 08:40:22.492: INFO: ss-0  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:38 +0000 UTC  }]
    Dec 14 08:40:22.492: INFO: ss-1  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:40:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:39:58 +0000 UTC  }]
    Dec 14 08:40:22.492: INFO: 
    Dec 14 08:40:22.492: INFO: StatefulSet ss has not reached scale 0, at 2
    Dec 14 08:40:23.497: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99112083s
    Dec 14 08:40:24.501: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986089853s
    Dec 14 08:40:25.506: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981701222s
    Dec 14 08:40:26.510: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977295054s
    Dec 14 08:40:27.514: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973359765s
    Dec 14 08:40:28.519: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968890133s
    Dec 14 08:40:29.523: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964522089s
    Dec 14 08:40:30.528: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.841282ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7562 12/14/22 08:40:31.529
    Dec 14 08:40:31.533: INFO: Scaling statefulset ss to 0
    Dec 14 08:40:31.545: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 08:40:31.548: INFO: Deleting all statefulset in ns statefulset-7562
    Dec 14 08:40:31.551: INFO: Scaling statefulset ss to 0
    Dec 14 08:40:31.562: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:40:31.564: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 08:40:31.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7562" for this suite. 12/14/22 08:40:31.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:31.597
Dec 14 08:40:31.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csistoragecapacity 12/14/22 08:40:31.598
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:31.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:31.612
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 12/14/22 08:40:31.616
STEP: getting /apis/storage.k8s.io 12/14/22 08:40:31.62
STEP: getting /apis/storage.k8s.io/v1 12/14/22 08:40:31.621
STEP: creating 12/14/22 08:40:31.623
STEP: watching 12/14/22 08:40:31.636
Dec 14 08:40:31.636: INFO: starting watch
STEP: getting 12/14/22 08:40:31.644
STEP: listing in namespace 12/14/22 08:40:31.647
STEP: listing across namespaces 12/14/22 08:40:31.65
STEP: patching 12/14/22 08:40:31.653
STEP: updating 12/14/22 08:40:31.657
Dec 14 08:40:31.661: INFO: waiting for watch events with expected annotations in namespace
Dec 14 08:40:31.661: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 12/14/22 08:40:31.661
STEP: deleting a collection 12/14/22 08:40:31.672
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Dec 14 08:40:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-8765" for this suite. 12/14/22 08:40:31.688
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":105,"skipped":2185,"failed":0}
------------------------------
• [0.097 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:31.597
    Dec 14 08:40:31.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csistoragecapacity 12/14/22 08:40:31.598
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:31.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:31.612
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 12/14/22 08:40:31.616
    STEP: getting /apis/storage.k8s.io 12/14/22 08:40:31.62
    STEP: getting /apis/storage.k8s.io/v1 12/14/22 08:40:31.621
    STEP: creating 12/14/22 08:40:31.623
    STEP: watching 12/14/22 08:40:31.636
    Dec 14 08:40:31.636: INFO: starting watch
    STEP: getting 12/14/22 08:40:31.644
    STEP: listing in namespace 12/14/22 08:40:31.647
    STEP: listing across namespaces 12/14/22 08:40:31.65
    STEP: patching 12/14/22 08:40:31.653
    STEP: updating 12/14/22 08:40:31.657
    Dec 14 08:40:31.661: INFO: waiting for watch events with expected annotations in namespace
    Dec 14 08:40:31.661: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 12/14/22 08:40:31.661
    STEP: deleting a collection 12/14/22 08:40:31.672
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Dec 14 08:40:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-8765" for this suite. 12/14/22 08:40:31.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:31.695
Dec 14 08:40:31.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:40:31.696
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:31.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:31.712
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:40:31.716
Dec 14 08:40:31.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1" in namespace "projected-3323" to be "Succeeded or Failed"
Dec 14 08:40:31.737: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2675ms
Dec 14 08:40:33.741: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01321039s
Dec 14 08:40:35.743: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015428092s
STEP: Saw pod success 12/14/22 08:40:35.743
Dec 14 08:40:35.744: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1" satisfied condition "Succeeded or Failed"
Dec 14 08:40:35.748: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1 container client-container: <nil>
STEP: delete the pod 12/14/22 08:40:35.802
Dec 14 08:40:35.810: INFO: Waiting for pod downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1 to disappear
Dec 14 08:40:35.812: INFO: Pod downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 08:40:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3323" for this suite. 12/14/22 08:40:35.818
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":106,"skipped":2202,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:31.695
    Dec 14 08:40:31.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:40:31.696
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:31.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:31.712
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:40:31.716
    Dec 14 08:40:31.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1" in namespace "projected-3323" to be "Succeeded or Failed"
    Dec 14 08:40:31.737: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2675ms
    Dec 14 08:40:33.741: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01321039s
    Dec 14 08:40:35.743: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015428092s
    STEP: Saw pod success 12/14/22 08:40:35.743
    Dec 14 08:40:35.744: INFO: Pod "downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1" satisfied condition "Succeeded or Failed"
    Dec 14 08:40:35.748: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1 container client-container: <nil>
    STEP: delete the pod 12/14/22 08:40:35.802
    Dec 14 08:40:35.810: INFO: Waiting for pod downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1 to disappear
    Dec 14 08:40:35.812: INFO: Pod downwardapi-volume-f0597f86-fce6-4eed-96f0-994ab1ecd3e1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 08:40:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3323" for this suite. 12/14/22 08:40:35.818
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:35.823
Dec 14 08:40:35.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 08:40:35.823
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:35.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:35.838
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 12/14/22 08:40:35.842
STEP: Creating a ResourceQuota 12/14/22 08:40:40.864
STEP: Ensuring resource quota status is calculated 12/14/22 08:40:40.871
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 08:40:42.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4656" for this suite. 12/14/22 08:40:42.882
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":107,"skipped":2204,"failed":0}
------------------------------
• [7.065 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:35.823
    Dec 14 08:40:35.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 08:40:35.823
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:35.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:35.838
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 12/14/22 08:40:35.842
    STEP: Creating a ResourceQuota 12/14/22 08:40:40.864
    STEP: Ensuring resource quota status is calculated 12/14/22 08:40:40.871
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 08:40:42.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4656" for this suite. 12/14/22 08:40:42.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:42.888
Dec 14 08:40:42.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:40:42.889
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:42.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:42.906
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 12/14/22 08:40:42.91
Dec 14 08:40:42.922: INFO: Waiting up to 5m0s for pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6" in namespace "emptydir-5216" to be "Succeeded or Failed"
Dec 14 08:40:42.927: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475448ms
Dec 14 08:40:44.932: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009777786s
Dec 14 08:40:46.933: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010656464s
STEP: Saw pod success 12/14/22 08:40:46.933
Dec 14 08:40:46.933: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6" satisfied condition "Succeeded or Failed"
Dec 14 08:40:46.937: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-8d708b5a-45b9-426d-8674-d2306bdb26b6 container test-container: <nil>
STEP: delete the pod 12/14/22 08:40:46.981
Dec 14 08:40:46.989: INFO: Waiting for pod pod-8d708b5a-45b9-426d-8674-d2306bdb26b6 to disappear
Dec 14 08:40:46.992: INFO: Pod pod-8d708b5a-45b9-426d-8674-d2306bdb26b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:40:46.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5216" for this suite. 12/14/22 08:40:46.998
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":108,"skipped":2209,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:42.888
    Dec 14 08:40:42.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:40:42.889
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:42.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:42.906
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 12/14/22 08:40:42.91
    Dec 14 08:40:42.922: INFO: Waiting up to 5m0s for pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6" in namespace "emptydir-5216" to be "Succeeded or Failed"
    Dec 14 08:40:42.927: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475448ms
    Dec 14 08:40:44.932: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009777786s
    Dec 14 08:40:46.933: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010656464s
    STEP: Saw pod success 12/14/22 08:40:46.933
    Dec 14 08:40:46.933: INFO: Pod "pod-8d708b5a-45b9-426d-8674-d2306bdb26b6" satisfied condition "Succeeded or Failed"
    Dec 14 08:40:46.937: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-8d708b5a-45b9-426d-8674-d2306bdb26b6 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:40:46.981
    Dec 14 08:40:46.989: INFO: Waiting for pod pod-8d708b5a-45b9-426d-8674-d2306bdb26b6 to disappear
    Dec 14 08:40:46.992: INFO: Pod pod-8d708b5a-45b9-426d-8674-d2306bdb26b6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:40:46.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5216" for this suite. 12/14/22 08:40:46.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:47.002
Dec 14 08:40:47.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:40:47.003
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:47.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:47.017
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-b704a19d-4f86-43f1-992d-1241b6e2d399 12/14/22 08:40:47.022
STEP: Creating a pod to test consume configMaps 12/14/22 08:40:47.025
Dec 14 08:40:47.035: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625" in namespace "projected-5547" to be "Succeeded or Failed"
Dec 14 08:40:47.038: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942685ms
Dec 14 08:40:49.043: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625": Phase="Running", Reason="", readiness=false. Elapsed: 2.008141017s
Dec 14 08:40:51.044: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008751345s
STEP: Saw pod success 12/14/22 08:40:51.044
Dec 14 08:40:51.044: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625" satisfied condition "Succeeded or Failed"
Dec 14 08:40:51.047: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:40:51.057
Dec 14 08:40:51.065: INFO: Waiting for pod pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625 to disappear
Dec 14 08:40:51.068: INFO: Pod pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 08:40:51.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5547" for this suite. 12/14/22 08:40:51.074
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":109,"skipped":2217,"failed":0}
------------------------------
• [4.076 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:47.002
    Dec 14 08:40:47.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:40:47.003
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:47.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:47.017
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-b704a19d-4f86-43f1-992d-1241b6e2d399 12/14/22 08:40:47.022
    STEP: Creating a pod to test consume configMaps 12/14/22 08:40:47.025
    Dec 14 08:40:47.035: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625" in namespace "projected-5547" to be "Succeeded or Failed"
    Dec 14 08:40:47.038: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942685ms
    Dec 14 08:40:49.043: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625": Phase="Running", Reason="", readiness=false. Elapsed: 2.008141017s
    Dec 14 08:40:51.044: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008751345s
    STEP: Saw pod success 12/14/22 08:40:51.044
    Dec 14 08:40:51.044: INFO: Pod "pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625" satisfied condition "Succeeded or Failed"
    Dec 14 08:40:51.047: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:40:51.057
    Dec 14 08:40:51.065: INFO: Waiting for pod pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625 to disappear
    Dec 14 08:40:51.068: INFO: Pod pod-projected-configmaps-7a645fb8-e319-4132-9f02-e2784b047625 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 08:40:51.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5547" for this suite. 12/14/22 08:40:51.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:51.079
Dec 14 08:40:51.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:40:51.08
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:51.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:51.096
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 12/14/22 08:40:51.101
Dec 14 08:40:51.101: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 create -f -'
Dec 14 08:40:51.703: INFO: stderr: ""
Dec 14 08:40:51.703: INFO: stdout: "pod/pause created\n"
Dec 14 08:40:51.703: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 14 08:40:51.703: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6943" to be "running and ready"
Dec 14 08:40:51.707: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.626771ms
Dec 14 08:40:51.707: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb' to be 'Running' but was 'Pending'
Dec 14 08:40:53.713: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010256024s
Dec 14 08:40:53.713: INFO: Pod "pause" satisfied condition "running and ready"
Dec 14 08:40:53.713: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 12/14/22 08:40:53.713
Dec 14 08:40:53.714: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 label pods pause testing-label=testing-label-value'
Dec 14 08:40:53.789: INFO: stderr: ""
Dec 14 08:40:53.789: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 12/14/22 08:40:53.789
Dec 14 08:40:53.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get pod pause -L testing-label'
Dec 14 08:40:53.848: INFO: stderr: ""
Dec 14 08:40:53.848: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 12/14/22 08:40:53.848
Dec 14 08:40:53.848: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 label pods pause testing-label-'
Dec 14 08:40:53.921: INFO: stderr: ""
Dec 14 08:40:53.921: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 12/14/22 08:40:53.921
Dec 14 08:40:53.921: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get pod pause -L testing-label'
Dec 14 08:40:53.981: INFO: stderr: ""
Dec 14 08:40:53.981: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 12/14/22 08:40:53.981
Dec 14 08:40:53.981: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 delete --grace-period=0 --force -f -'
Dec 14 08:40:54.046: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:40:54.046: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 14 08:40:54.046: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get rc,svc -l name=pause --no-headers'
Dec 14 08:40:54.113: INFO: stderr: "No resources found in kubectl-6943 namespace.\n"
Dec 14 08:40:54.113: INFO: stdout: ""
Dec 14 08:40:54.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 08:40:54.173: INFO: stderr: ""
Dec 14 08:40:54.173: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:40:54.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6943" for this suite. 12/14/22 08:40:54.18
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":110,"skipped":2232,"failed":0}
------------------------------
• [3.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:51.079
    Dec 14 08:40:51.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:40:51.08
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:51.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:51.096
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 12/14/22 08:40:51.101
    Dec 14 08:40:51.101: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 create -f -'
    Dec 14 08:40:51.703: INFO: stderr: ""
    Dec 14 08:40:51.703: INFO: stdout: "pod/pause created\n"
    Dec 14 08:40:51.703: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Dec 14 08:40:51.703: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6943" to be "running and ready"
    Dec 14 08:40:51.707: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.626771ms
    Dec 14 08:40:51.707: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb' to be 'Running' but was 'Pending'
    Dec 14 08:40:53.713: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010256024s
    Dec 14 08:40:53.713: INFO: Pod "pause" satisfied condition "running and ready"
    Dec 14 08:40:53.713: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 12/14/22 08:40:53.713
    Dec 14 08:40:53.714: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 label pods pause testing-label=testing-label-value'
    Dec 14 08:40:53.789: INFO: stderr: ""
    Dec 14 08:40:53.789: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 12/14/22 08:40:53.789
    Dec 14 08:40:53.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get pod pause -L testing-label'
    Dec 14 08:40:53.848: INFO: stderr: ""
    Dec 14 08:40:53.848: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 12/14/22 08:40:53.848
    Dec 14 08:40:53.848: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 label pods pause testing-label-'
    Dec 14 08:40:53.921: INFO: stderr: ""
    Dec 14 08:40:53.921: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 12/14/22 08:40:53.921
    Dec 14 08:40:53.921: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get pod pause -L testing-label'
    Dec 14 08:40:53.981: INFO: stderr: ""
    Dec 14 08:40:53.981: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 12/14/22 08:40:53.981
    Dec 14 08:40:53.981: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 delete --grace-period=0 --force -f -'
    Dec 14 08:40:54.046: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:40:54.046: INFO: stdout: "pod \"pause\" force deleted\n"
    Dec 14 08:40:54.046: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get rc,svc -l name=pause --no-headers'
    Dec 14 08:40:54.113: INFO: stderr: "No resources found in kubectl-6943 namespace.\n"
    Dec 14 08:40:54.113: INFO: stdout: ""
    Dec 14 08:40:54.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6943 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 14 08:40:54.173: INFO: stderr: ""
    Dec 14 08:40:54.173: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:40:54.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6943" for this suite. 12/14/22 08:40:54.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:40:54.185
Dec 14 08:40:54.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 08:40:54.186
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:54.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:54.202
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 12/14/22 08:40:54.207
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
 12/14/22 08:40:54.211
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
 12/14/22 08:40:54.211
STEP: creating a pod to probe DNS 12/14/22 08:40:54.211
STEP: submitting the pod to kubernetes 12/14/22 08:40:54.211
Dec 14 08:40:54.223: INFO: Waiting up to 15m0s for pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e" in namespace "dns-4577" to be "running"
Dec 14 08:40:54.226: INFO: Pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165777ms
Dec 14 08:40:56.230: INFO: Pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007363843s
Dec 14 08:40:56.230: INFO: Pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e" satisfied condition "running"
STEP: retrieving the pod 12/14/22 08:40:56.23
STEP: looking for the results for each expected name from probers 12/14/22 08:40:56.234
Dec 14 08:40:56.387: INFO: DNS probes using dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e succeeded

STEP: deleting the pod 12/14/22 08:40:56.387
STEP: changing the externalName to bar.example.com 12/14/22 08:40:56.398
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
 12/14/22 08:40:56.407
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
 12/14/22 08:40:56.407
STEP: creating a second pod to probe DNS 12/14/22 08:40:56.407
STEP: submitting the pod to kubernetes 12/14/22 08:40:56.407
Dec 14 08:40:56.417: INFO: Waiting up to 15m0s for pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b" in namespace "dns-4577" to be "running"
Dec 14 08:40:56.422: INFO: Pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228556ms
Dec 14 08:40:58.428: INFO: Pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010299672s
Dec 14 08:40:58.428: INFO: Pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b" satisfied condition "running"
STEP: retrieving the pod 12/14/22 08:40:58.428
STEP: looking for the results for each expected name from probers 12/14/22 08:40:58.432
Dec 14 08:40:58.539: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:40:58.584: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:40:58.584: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

Dec 14 08:41:03.594: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:03.644: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:03.644: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

Dec 14 08:41:08.597: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:08.643: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:08.644: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

Dec 14 08:41:13.595: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:13.641: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:13.641: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

Dec 14 08:41:18.595: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:18.640: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:18.640: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

Dec 14 08:41:23.596: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:23.649: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 08:41:23.649: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

Dec 14 08:41:28.648: INFO: DNS probes using dns-test-450f2e90-8b15-4e1f-9166-9d746138676b succeeded

STEP: deleting the pod 12/14/22 08:41:28.648
STEP: changing the service to type=ClusterIP 12/14/22 08:41:28.658
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
 12/14/22 08:41:28.672
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
 12/14/22 08:41:28.672
STEP: creating a third pod to probe DNS 12/14/22 08:41:28.672
STEP: submitting the pod to kubernetes 12/14/22 08:41:28.676
Dec 14 08:41:28.689: INFO: Waiting up to 15m0s for pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2" in namespace "dns-4577" to be "running"
Dec 14 08:41:28.693: INFO: Pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353102ms
Dec 14 08:41:30.700: INFO: Pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010861393s
Dec 14 08:41:30.702: INFO: Pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2" satisfied condition "running"
STEP: retrieving the pod 12/14/22 08:41:30.702
STEP: looking for the results for each expected name from probers 12/14/22 08:41:30.706
Dec 14 08:41:30.788: INFO: DNS probes using dns-test-ecd63511-8e68-4279-867f-eb435701aca2 succeeded

STEP: deleting the pod 12/14/22 08:41:30.788
STEP: deleting the test externalName service 12/14/22 08:41:30.799
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 08:41:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4577" for this suite. 12/14/22 08:41:30.819
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":111,"skipped":2238,"failed":0}
------------------------------
• [36.641 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:40:54.185
    Dec 14 08:40:54.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 08:40:54.186
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:40:54.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:40:54.202
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 12/14/22 08:40:54.207
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
     12/14/22 08:40:54.211
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
     12/14/22 08:40:54.211
    STEP: creating a pod to probe DNS 12/14/22 08:40:54.211
    STEP: submitting the pod to kubernetes 12/14/22 08:40:54.211
    Dec 14 08:40:54.223: INFO: Waiting up to 15m0s for pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e" in namespace "dns-4577" to be "running"
    Dec 14 08:40:54.226: INFO: Pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165777ms
    Dec 14 08:40:56.230: INFO: Pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007363843s
    Dec 14 08:40:56.230: INFO: Pod "dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 08:40:56.23
    STEP: looking for the results for each expected name from probers 12/14/22 08:40:56.234
    Dec 14 08:40:56.387: INFO: DNS probes using dns-test-da5ba8ea-4f86-4e30-9645-a4c878f3ec3e succeeded

    STEP: deleting the pod 12/14/22 08:40:56.387
    STEP: changing the externalName to bar.example.com 12/14/22 08:40:56.398
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
     12/14/22 08:40:56.407
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
     12/14/22 08:40:56.407
    STEP: creating a second pod to probe DNS 12/14/22 08:40:56.407
    STEP: submitting the pod to kubernetes 12/14/22 08:40:56.407
    Dec 14 08:40:56.417: INFO: Waiting up to 15m0s for pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b" in namespace "dns-4577" to be "running"
    Dec 14 08:40:56.422: INFO: Pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228556ms
    Dec 14 08:40:58.428: INFO: Pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010299672s
    Dec 14 08:40:58.428: INFO: Pod "dns-test-450f2e90-8b15-4e1f-9166-9d746138676b" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 08:40:58.428
    STEP: looking for the results for each expected name from probers 12/14/22 08:40:58.432
    Dec 14 08:40:58.539: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:40:58.584: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:40:58.584: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

    Dec 14 08:41:03.594: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:03.644: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:03.644: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

    Dec 14 08:41:08.597: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:08.643: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:08.644: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

    Dec 14 08:41:13.595: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:13.641: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:13.641: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

    Dec 14 08:41:18.595: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:18.640: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:18.640: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

    Dec 14 08:41:23.596: INFO: File wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:23.649: INFO: File jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local from pod  dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 08:41:23.649: INFO: Lookups using dns-4577/dns-test-450f2e90-8b15-4e1f-9166-9d746138676b failed for: [wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local]

    Dec 14 08:41:28.648: INFO: DNS probes using dns-test-450f2e90-8b15-4e1f-9166-9d746138676b succeeded

    STEP: deleting the pod 12/14/22 08:41:28.648
    STEP: changing the service to type=ClusterIP 12/14/22 08:41:28.658
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
     12/14/22 08:41:28.672
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4577.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4577.svc.cluster.local; sleep 1; done
     12/14/22 08:41:28.672
    STEP: creating a third pod to probe DNS 12/14/22 08:41:28.672
    STEP: submitting the pod to kubernetes 12/14/22 08:41:28.676
    Dec 14 08:41:28.689: INFO: Waiting up to 15m0s for pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2" in namespace "dns-4577" to be "running"
    Dec 14 08:41:28.693: INFO: Pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353102ms
    Dec 14 08:41:30.700: INFO: Pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010861393s
    Dec 14 08:41:30.702: INFO: Pod "dns-test-ecd63511-8e68-4279-867f-eb435701aca2" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 08:41:30.702
    STEP: looking for the results for each expected name from probers 12/14/22 08:41:30.706
    Dec 14 08:41:30.788: INFO: DNS probes using dns-test-ecd63511-8e68-4279-867f-eb435701aca2 succeeded

    STEP: deleting the pod 12/14/22 08:41:30.788
    STEP: deleting the test externalName service 12/14/22 08:41:30.799
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 08:41:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4577" for this suite. 12/14/22 08:41:30.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:41:30.827
Dec 14 08:41:30.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:41:30.827
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:30.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:30.864
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 12/14/22 08:41:30.87
Dec 14 08:41:30.884: INFO: Waiting up to 5m0s for pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b" in namespace "downward-api-1686" to be "Succeeded or Failed"
Dec 14 08:41:30.889: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742841ms
Dec 14 08:41:32.895: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010605223s
Dec 14 08:41:34.898: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014188572s
STEP: Saw pod success 12/14/22 08:41:34.898
Dec 14 08:41:34.899: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b" satisfied condition "Succeeded or Failed"
Dec 14 08:41:34.902: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b container dapi-container: <nil>
STEP: delete the pod 12/14/22 08:41:34.913
Dec 14 08:41:34.922: INFO: Waiting for pod downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b to disappear
Dec 14 08:41:34.925: INFO: Pod downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 14 08:41:34.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1686" for this suite. 12/14/22 08:41:34.93
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":112,"skipped":2246,"failed":0}
------------------------------
• [4.109 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:41:30.827
    Dec 14 08:41:30.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:41:30.827
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:30.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:30.864
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 12/14/22 08:41:30.87
    Dec 14 08:41:30.884: INFO: Waiting up to 5m0s for pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b" in namespace "downward-api-1686" to be "Succeeded or Failed"
    Dec 14 08:41:30.889: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742841ms
    Dec 14 08:41:32.895: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010605223s
    Dec 14 08:41:34.898: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014188572s
    STEP: Saw pod success 12/14/22 08:41:34.898
    Dec 14 08:41:34.899: INFO: Pod "downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b" satisfied condition "Succeeded or Failed"
    Dec 14 08:41:34.902: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b container dapi-container: <nil>
    STEP: delete the pod 12/14/22 08:41:34.913
    Dec 14 08:41:34.922: INFO: Waiting for pod downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b to disappear
    Dec 14 08:41:34.925: INFO: Pod downward-api-5be9b3c9-ba05-4154-b052-7fa211bdd56b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 14 08:41:34.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1686" for this suite. 12/14/22 08:41:34.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:41:34.936
Dec 14 08:41:34.936: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 08:41:34.937
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:34.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:34.954
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/14/22 08:41:34.959
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/14/22 08:41:34.959
STEP: creating a pod to probe DNS 12/14/22 08:41:34.959
STEP: submitting the pod to kubernetes 12/14/22 08:41:34.959
Dec 14 08:41:34.972: INFO: Waiting up to 15m0s for pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645" in namespace "dns-3515" to be "running"
Dec 14 08:41:34.976: INFO: Pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514578ms
Dec 14 08:41:36.982: INFO: Pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645": Phase="Running", Reason="", readiness=true. Elapsed: 2.010480971s
Dec 14 08:41:36.982: INFO: Pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645" satisfied condition "running"
STEP: retrieving the pod 12/14/22 08:41:36.982
STEP: looking for the results for each expected name from probers 12/14/22 08:41:36.986
Dec 14 08:41:37.073: INFO: DNS probes using dns-3515/dns-test-50835cae-5868-4b3e-8c84-d6fb110df645 succeeded

STEP: deleting the pod 12/14/22 08:41:37.073
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 08:41:37.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3515" for this suite. 12/14/22 08:41:37.087
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":113,"skipped":2251,"failed":0}
------------------------------
• [2.157 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:41:34.936
    Dec 14 08:41:34.936: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 08:41:34.937
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:34.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:34.954
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/14/22 08:41:34.959
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/14/22 08:41:34.959
    STEP: creating a pod to probe DNS 12/14/22 08:41:34.959
    STEP: submitting the pod to kubernetes 12/14/22 08:41:34.959
    Dec 14 08:41:34.972: INFO: Waiting up to 15m0s for pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645" in namespace "dns-3515" to be "running"
    Dec 14 08:41:34.976: INFO: Pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514578ms
    Dec 14 08:41:36.982: INFO: Pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645": Phase="Running", Reason="", readiness=true. Elapsed: 2.010480971s
    Dec 14 08:41:36.982: INFO: Pod "dns-test-50835cae-5868-4b3e-8c84-d6fb110df645" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 08:41:36.982
    STEP: looking for the results for each expected name from probers 12/14/22 08:41:36.986
    Dec 14 08:41:37.073: INFO: DNS probes using dns-3515/dns-test-50835cae-5868-4b3e-8c84-d6fb110df645 succeeded

    STEP: deleting the pod 12/14/22 08:41:37.073
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 08:41:37.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3515" for this suite. 12/14/22 08:41:37.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:41:37.093
Dec 14 08:41:37.093: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 12/14/22 08:41:37.094
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:37.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:37.111
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 12/14/22 08:41:37.121
STEP: Waiting for all pods to be running 12/14/22 08:41:39.16
Dec 14 08:41:39.165: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 14 08:41:41.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5644" for this suite. 12/14/22 08:41:41.18
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":114,"skipped":2265,"failed":0}
------------------------------
• [4.092 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:41:37.093
    Dec 14 08:41:37.093: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 12/14/22 08:41:37.094
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:37.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:37.111
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 12/14/22 08:41:37.121
    STEP: Waiting for all pods to be running 12/14/22 08:41:39.16
    Dec 14 08:41:39.165: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 14 08:41:41.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5644" for this suite. 12/14/22 08:41:41.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:41:41.186
Dec 14 08:41:41.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator 12/14/22 08:41:41.187
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:41.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:41.204
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Dec 14 08:41:41.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 12/14/22 08:41:41.209
Dec 14 08:41:41.923: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 14 08:41:43.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:45.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:47.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:49.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:51.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:53.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:55.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:57.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:41:59.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:42:01.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 08:42:04.198: INFO: Waited 224.362653ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 12/14/22 08:42:04.714
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/14/22 08:42:04.719
STEP: List APIServices 12/14/22 08:42:04.724
Dec 14 08:42:04.732: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Dec 14 08:42:04.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5310" for this suite. 12/14/22 08:42:04.953
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":115,"skipped":2300,"failed":0}
------------------------------
• [23.772 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:41:41.186
    Dec 14 08:41:41.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename aggregator 12/14/22 08:41:41.187
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:41:41.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:41:41.204
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Dec 14 08:41:41.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 12/14/22 08:41:41.209
    Dec 14 08:41:41.923: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Dec 14 08:41:43.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:45.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:47.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:49.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:51.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:53.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:55.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:57.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:41:59.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:42:01.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 41, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 08:42:04.198: INFO: Waited 224.362653ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 12/14/22 08:42:04.714
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/14/22 08:42:04.719
    STEP: List APIServices 12/14/22 08:42:04.724
    Dec 14 08:42:04.732: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Dec 14 08:42:04.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-5310" for this suite. 12/14/22 08:42:04.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:04.96
Dec 14 08:42:04.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 12/14/22 08:42:04.96
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:04.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:04.975
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 12/14/22 08:42:04.99
STEP: creating replication controller proxy-service-mmv5p in namespace proxy-7573 12/14/22 08:42:04.99
I1214 08:42:04.996113    4635 runners.go:193] Created replication controller with name: proxy-service-mmv5p, namespace: proxy-7573, replica count: 1
I1214 08:42:06.047408    4635 runners.go:193] proxy-service-mmv5p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 08:42:07.047531    4635 runners.go:193] proxy-service-mmv5p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 08:42:07.052: INFO: setup took 2.072269282s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/14/22 08:42:07.052
Dec 14 08:42:07.080: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 27.506002ms)
Dec 14 08:42:07.080: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 27.579538ms)
Dec 14 08:42:07.080: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 27.684989ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 28.379848ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 28.331613ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 28.316818ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 28.304811ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 28.486806ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 28.592369ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 28.619136ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 28.709839ms)
Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 28.698178ms)
Dec 14 08:42:07.082: INFO: (0) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 29.34271ms)
Dec 14 08:42:07.083: INFO: (0) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 30.180329ms)
Dec 14 08:42:07.085: INFO: (0) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 32.870931ms)
Dec 14 08:42:07.085: INFO: (0) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 32.8938ms)
Dec 14 08:42:07.096: INFO: (1) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.330491ms)
Dec 14 08:42:07.096: INFO: (1) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.222042ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.181846ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 11.29788ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.350255ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.298713ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.214365ms)
Dec 14 08:42:07.096: INFO: (1) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.219137ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.244206ms)
Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.302339ms)
Dec 14 08:42:07.099: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 13.495572ms)
Dec 14 08:42:07.099: INFO: (1) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 13.415783ms)
Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.874586ms)
Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.9915ms)
Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.980113ms)
Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 14.916365ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.47764ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.443516ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.544259ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.773436ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.596977ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.687223ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.720234ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.760885ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.747813ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.411041ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.791338ms)
Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.595298ms)
Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 16.510283ms)
Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 16.615183ms)
Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 16.653671ms)
Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 16.485756ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.445729ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.351538ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.360612ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.45135ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.477099ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.432647ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.448379ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.568203ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.582796ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 10.52843ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.616289ms)
Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.536974ms)
Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 13.9472ms)
Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 13.95893ms)
Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 14.058915ms)
Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.027858ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 12.92099ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 12.750791ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 12.870252ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 13.064045ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 13.10743ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 13.19434ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 13.245887ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 12.876035ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 12.854525ms)
Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 13.456233ms)
Dec 14 08:42:07.146: INFO: (4) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 14.115901ms)
Dec 14 08:42:07.146: INFO: (4) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 13.945202ms)
Dec 14 08:42:07.147: INFO: (4) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 15.707928ms)
Dec 14 08:42:07.149: INFO: (4) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 17.244113ms)
Dec 14 08:42:07.149: INFO: (4) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 17.509051ms)
Dec 14 08:42:07.149: INFO: (4) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 17.172467ms)
Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 7.292495ms)
Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 7.383963ms)
Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 7.536988ms)
Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 7.621636ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.880821ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.092463ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.14992ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.276754ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.428236ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.049339ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.864342ms)
Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.945937ms)
Dec 14 08:42:07.161: INFO: (5) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 12.13118ms)
Dec 14 08:42:07.165: INFO: (5) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 15.558132ms)
Dec 14 08:42:07.165: INFO: (5) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 16.054902ms)
Dec 14 08:42:07.165: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 16.156462ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.951314ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 12.282856ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 12.009722ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 11.977264ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 12.067824ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.945485ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 12.225517ms)
Dec 14 08:42:07.178: INFO: (6) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 12.088605ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 12.109845ms)
Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.987947ms)
Dec 14 08:42:07.178: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 12.031841ms)
Dec 14 08:42:07.178: INFO: (6) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 12.07909ms)
Dec 14 08:42:07.180: INFO: (6) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.199935ms)
Dec 14 08:42:07.182: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 16.791705ms)
Dec 14 08:42:07.182: INFO: (6) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 16.649342ms)
Dec 14 08:42:07.182: INFO: (6) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 16.886969ms)
Dec 14 08:42:07.191: INFO: (7) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 8.65809ms)
Dec 14 08:42:07.191: INFO: (7) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 8.700038ms)
Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.864806ms)
Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.723424ms)
Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.770058ms)
Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.813337ms)
Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.125301ms)
Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.167196ms)
Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.20231ms)
Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.280037ms)
Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.382702ms)
Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.552794ms)
Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 15.429086ms)
Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 15.376327ms)
Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 15.411058ms)
Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 15.322921ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.691417ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.765654ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.828466ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.742061ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.767501ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.790312ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.908333ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.00961ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.059834ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.954627ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 10.873237ms)
Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.22214ms)
Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 14.514862ms)
Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.556823ms)
Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.617361ms)
Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.727604ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.662198ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.867926ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 9.530386ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.518973ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 9.632394ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 9.966343ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.052155ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.702516ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.648908ms)
Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 9.808393ms)
Dec 14 08:42:07.225: INFO: (9) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 12.641085ms)
Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 13.57963ms)
Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.3798ms)
Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.357206ms)
Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 13.495109ms)
Dec 14 08:42:07.227: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 13.694599ms)
Dec 14 08:42:07.233: INFO: (10) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 6.928302ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.578979ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.550984ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.570541ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.555676ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.681803ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.651595ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.705236ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.639594ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.69828ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.651223ms)
Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.669038ms)
Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 16.644188ms)
Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 16.77174ms)
Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 16.710647ms)
Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 16.757773ms)
Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 10.038445ms)
Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.817542ms)
Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 10.013388ms)
Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 9.861625ms)
Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.328927ms)
Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.390667ms)
Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.375055ms)
Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.591224ms)
Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.386179ms)
Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.439757ms)
Dec 14 08:42:07.256: INFO: (11) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 12.111777ms)
Dec 14 08:42:07.256: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 12.229274ms)
Dec 14 08:42:07.257: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.347067ms)
Dec 14 08:42:07.257: INFO: (11) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.388824ms)
Dec 14 08:42:07.257: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 13.328378ms)
Dec 14 08:42:07.259: INFO: (11) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 15.087233ms)
Dec 14 08:42:07.266: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 6.603268ms)
Dec 14 08:42:07.266: INFO: (12) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 7.162531ms)
Dec 14 08:42:07.266: INFO: (12) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 6.898967ms)
Dec 14 08:42:07.267: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 7.623815ms)
Dec 14 08:42:07.267: INFO: (12) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 7.809038ms)
Dec 14 08:42:07.267: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 7.841871ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.933482ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 10.155095ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.267852ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.944601ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.116428ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.0377ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 10.363731ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.901602ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.970146ms)
Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.306701ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.825225ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 9.945929ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.986894ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.817191ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.032197ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.213213ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.106946ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.750264ms)
Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.966492ms)
Dec 14 08:42:07.280: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.832584ms)
Dec 14 08:42:07.280: INFO: (13) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.920081ms)
Dec 14 08:42:07.280: INFO: (13) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.03889ms)
Dec 14 08:42:07.281: INFO: (13) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.885054ms)
Dec 14 08:42:07.284: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.379027ms)
Dec 14 08:42:07.284: INFO: (13) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.310254ms)
Dec 14 08:42:07.284: INFO: (13) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.573385ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.311251ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.27381ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.383354ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.312001ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.33596ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 11.436741ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.363892ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.469239ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.390287ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.397568ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.420948ms)
Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.492841ms)
Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 15.963019ms)
Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 15.802786ms)
Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 15.868172ms)
Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 16.012999ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.409667ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.43701ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.419124ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.411568ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.444013ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.497913ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.512567ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.479808ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.630502ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.613694ms)
Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.540256ms)
Dec 14 08:42:07.311: INFO: (15) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.482154ms)
Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.836767ms)
Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.791529ms)
Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.764913ms)
Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 14.759678ms)
Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 21.463764ms)
Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 21.436106ms)
Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 21.429488ms)
Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 21.478589ms)
Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 21.32174ms)
Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 21.390512ms)
Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 21.867159ms)
Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 21.757771ms)
Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 21.776815ms)
Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 21.884013ms)
Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 21.788791ms)
Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 21.877876ms)
Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 26.08131ms)
Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 26.103357ms)
Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 26.161864ms)
Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 26.149789ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.737482ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 9.622681ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 9.644073ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.598232ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.609607ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.702565ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.648231ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 9.717387ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.755778ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.747301ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.824356ms)
Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.3622ms)
Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.105739ms)
Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.006277ms)
Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 14.102796ms)
Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.17001ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 10.12652ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.90567ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.627152ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.071788ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.056898ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.795578ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.902614ms)
Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.976662ms)
Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.350112ms)
Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.473987ms)
Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.418986ms)
Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.525928ms)
Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.450078ms)
Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 14.590708ms)
Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.150891ms)
Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.388277ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.119647ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.429241ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.321743ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 9.371436ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.558799ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.217703ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 9.326263ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.167754ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.746721ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.445237ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 9.557427ms)
Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.630222ms)
Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 13.143813ms)
Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 12.807911ms)
Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 13.02391ms)
Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 13.130617ms)
STEP: deleting ReplicationController proxy-service-mmv5p in namespace proxy-7573, will wait for the garbage collector to delete the pods 12/14/22 08:42:07.383
Dec 14 08:42:07.445: INFO: Deleting ReplicationController proxy-service-mmv5p took: 7.338786ms
Dec 14 08:42:07.546: INFO: Terminating ReplicationController proxy-service-mmv5p pods took: 100.834585ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Dec 14 08:42:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7573" for this suite. 12/14/22 08:42:09.953
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":116,"skipped":2345,"failed":0}
------------------------------
• [4.999 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:04.96
    Dec 14 08:42:04.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 12/14/22 08:42:04.96
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:04.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:04.975
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 12/14/22 08:42:04.99
    STEP: creating replication controller proxy-service-mmv5p in namespace proxy-7573 12/14/22 08:42:04.99
    I1214 08:42:04.996113    4635 runners.go:193] Created replication controller with name: proxy-service-mmv5p, namespace: proxy-7573, replica count: 1
    I1214 08:42:06.047408    4635 runners.go:193] proxy-service-mmv5p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I1214 08:42:07.047531    4635 runners.go:193] proxy-service-mmv5p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 08:42:07.052: INFO: setup took 2.072269282s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/14/22 08:42:07.052
    Dec 14 08:42:07.080: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 27.506002ms)
    Dec 14 08:42:07.080: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 27.579538ms)
    Dec 14 08:42:07.080: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 27.684989ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 28.379848ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 28.331613ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 28.316818ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 28.304811ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 28.486806ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 28.592369ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 28.619136ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 28.709839ms)
    Dec 14 08:42:07.081: INFO: (0) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 28.698178ms)
    Dec 14 08:42:07.082: INFO: (0) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 29.34271ms)
    Dec 14 08:42:07.083: INFO: (0) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 30.180329ms)
    Dec 14 08:42:07.085: INFO: (0) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 32.870931ms)
    Dec 14 08:42:07.085: INFO: (0) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 32.8938ms)
    Dec 14 08:42:07.096: INFO: (1) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.330491ms)
    Dec 14 08:42:07.096: INFO: (1) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.222042ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.181846ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 11.29788ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.350255ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.298713ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.214365ms)
    Dec 14 08:42:07.096: INFO: (1) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.219137ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.244206ms)
    Dec 14 08:42:07.097: INFO: (1) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.302339ms)
    Dec 14 08:42:07.099: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 13.495572ms)
    Dec 14 08:42:07.099: INFO: (1) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 13.415783ms)
    Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.874586ms)
    Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.9915ms)
    Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.980113ms)
    Dec 14 08:42:07.100: INFO: (1) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 14.916365ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.47764ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.443516ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.544259ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.773436ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.596977ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.687223ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.720234ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.760885ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.747813ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.411041ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.791338ms)
    Dec 14 08:42:07.112: INFO: (2) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.595298ms)
    Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 16.510283ms)
    Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 16.615183ms)
    Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 16.653671ms)
    Dec 14 08:42:07.117: INFO: (2) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 16.485756ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.445729ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.351538ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.360612ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.45135ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.477099ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.432647ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.448379ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.568203ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.582796ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 10.52843ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.616289ms)
    Dec 14 08:42:07.128: INFO: (3) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.536974ms)
    Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 13.9472ms)
    Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 13.95893ms)
    Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 14.058915ms)
    Dec 14 08:42:07.131: INFO: (3) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.027858ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 12.92099ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 12.750791ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 12.870252ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 13.064045ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 13.10743ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 13.19434ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 13.245887ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 12.876035ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 12.854525ms)
    Dec 14 08:42:07.145: INFO: (4) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 13.456233ms)
    Dec 14 08:42:07.146: INFO: (4) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 14.115901ms)
    Dec 14 08:42:07.146: INFO: (4) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 13.945202ms)
    Dec 14 08:42:07.147: INFO: (4) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 15.707928ms)
    Dec 14 08:42:07.149: INFO: (4) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 17.244113ms)
    Dec 14 08:42:07.149: INFO: (4) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 17.509051ms)
    Dec 14 08:42:07.149: INFO: (4) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 17.172467ms)
    Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 7.292495ms)
    Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 7.383963ms)
    Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 7.536988ms)
    Dec 14 08:42:07.157: INFO: (5) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 7.621636ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.880821ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.092463ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.14992ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.276754ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.428236ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.049339ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.864342ms)
    Dec 14 08:42:07.160: INFO: (5) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.945937ms)
    Dec 14 08:42:07.161: INFO: (5) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 12.13118ms)
    Dec 14 08:42:07.165: INFO: (5) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 15.558132ms)
    Dec 14 08:42:07.165: INFO: (5) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 16.054902ms)
    Dec 14 08:42:07.165: INFO: (5) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 16.156462ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.951314ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 12.282856ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 12.009722ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 11.977264ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 12.067824ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.945485ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 12.225517ms)
    Dec 14 08:42:07.178: INFO: (6) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 12.088605ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 12.109845ms)
    Dec 14 08:42:07.177: INFO: (6) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.987947ms)
    Dec 14 08:42:07.178: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 12.031841ms)
    Dec 14 08:42:07.178: INFO: (6) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 12.07909ms)
    Dec 14 08:42:07.180: INFO: (6) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.199935ms)
    Dec 14 08:42:07.182: INFO: (6) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 16.791705ms)
    Dec 14 08:42:07.182: INFO: (6) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 16.649342ms)
    Dec 14 08:42:07.182: INFO: (6) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 16.886969ms)
    Dec 14 08:42:07.191: INFO: (7) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 8.65809ms)
    Dec 14 08:42:07.191: INFO: (7) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 8.700038ms)
    Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.864806ms)
    Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.723424ms)
    Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.770058ms)
    Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.813337ms)
    Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.125301ms)
    Dec 14 08:42:07.193: INFO: (7) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.167196ms)
    Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.20231ms)
    Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.280037ms)
    Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.382702ms)
    Dec 14 08:42:07.194: INFO: (7) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.552794ms)
    Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 15.429086ms)
    Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 15.376327ms)
    Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 15.411058ms)
    Dec 14 08:42:07.198: INFO: (7) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 15.322921ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.691417ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.765654ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.828466ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.742061ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.767501ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.790312ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.908333ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.00961ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.059834ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.954627ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 10.873237ms)
    Dec 14 08:42:07.209: INFO: (8) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.22214ms)
    Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 14.514862ms)
    Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.556823ms)
    Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.617361ms)
    Dec 14 08:42:07.213: INFO: (8) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.727604ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.662198ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.867926ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 9.530386ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.518973ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 9.632394ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 9.966343ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.052155ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.702516ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.648908ms)
    Dec 14 08:42:07.223: INFO: (9) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 9.808393ms)
    Dec 14 08:42:07.225: INFO: (9) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 12.641085ms)
    Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 13.57963ms)
    Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.3798ms)
    Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.357206ms)
    Dec 14 08:42:07.226: INFO: (9) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 13.495109ms)
    Dec 14 08:42:07.227: INFO: (9) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 13.694599ms)
    Dec 14 08:42:07.233: INFO: (10) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 6.928302ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.578979ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.550984ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.570541ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.555676ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.681803ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.651595ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.705236ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.639594ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.69828ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 11.651223ms)
    Dec 14 08:42:07.238: INFO: (10) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.669038ms)
    Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 16.644188ms)
    Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 16.77174ms)
    Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 16.710647ms)
    Dec 14 08:42:07.243: INFO: (10) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 16.757773ms)
    Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 10.038445ms)
    Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.817542ms)
    Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 10.013388ms)
    Dec 14 08:42:07.253: INFO: (11) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 9.861625ms)
    Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.328927ms)
    Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.390667ms)
    Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.375055ms)
    Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.591224ms)
    Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.386179ms)
    Dec 14 08:42:07.254: INFO: (11) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.439757ms)
    Dec 14 08:42:07.256: INFO: (11) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 12.111777ms)
    Dec 14 08:42:07.256: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 12.229274ms)
    Dec 14 08:42:07.257: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.347067ms)
    Dec 14 08:42:07.257: INFO: (11) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 13.388824ms)
    Dec 14 08:42:07.257: INFO: (11) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 13.328378ms)
    Dec 14 08:42:07.259: INFO: (11) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 15.087233ms)
    Dec 14 08:42:07.266: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 6.603268ms)
    Dec 14 08:42:07.266: INFO: (12) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 7.162531ms)
    Dec 14 08:42:07.266: INFO: (12) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 6.898967ms)
    Dec 14 08:42:07.267: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 7.623815ms)
    Dec 14 08:42:07.267: INFO: (12) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 7.809038ms)
    Dec 14 08:42:07.267: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 7.841871ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.933482ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 10.155095ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.267852ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.944601ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.116428ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.0377ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 10.363731ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.901602ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.970146ms)
    Dec 14 08:42:07.269: INFO: (12) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.306701ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.825225ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 9.945929ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.986894ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.817191ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.032197ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.213213ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.106946ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.750264ms)
    Dec 14 08:42:07.279: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.966492ms)
    Dec 14 08:42:07.280: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.832584ms)
    Dec 14 08:42:07.280: INFO: (13) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.920081ms)
    Dec 14 08:42:07.280: INFO: (13) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.03889ms)
    Dec 14 08:42:07.281: INFO: (13) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.885054ms)
    Dec 14 08:42:07.284: INFO: (13) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.379027ms)
    Dec 14 08:42:07.284: INFO: (13) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.310254ms)
    Dec 14 08:42:07.284: INFO: (13) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.573385ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 11.311251ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.27381ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 11.383354ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 11.312001ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 11.33596ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 11.436741ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 11.363892ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 11.469239ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 11.390287ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 11.397568ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 11.420948ms)
    Dec 14 08:42:07.295: INFO: (14) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 11.492841ms)
    Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 15.963019ms)
    Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 15.802786ms)
    Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 15.868172ms)
    Dec 14 08:42:07.300: INFO: (14) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 16.012999ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.409667ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.43701ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.419124ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.411568ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 10.444013ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.497913ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.512567ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 10.479808ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.630502ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 10.613694ms)
    Dec 14 08:42:07.310: INFO: (15) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 10.540256ms)
    Dec 14 08:42:07.311: INFO: (15) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 11.482154ms)
    Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.836767ms)
    Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 14.791529ms)
    Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.764913ms)
    Dec 14 08:42:07.315: INFO: (15) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 14.759678ms)
    Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 21.463764ms)
    Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 21.436106ms)
    Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 21.429488ms)
    Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 21.478589ms)
    Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 21.32174ms)
    Dec 14 08:42:07.336: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 21.390512ms)
    Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 21.867159ms)
    Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 21.757771ms)
    Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 21.776815ms)
    Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 21.884013ms)
    Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 21.788791ms)
    Dec 14 08:42:07.337: INFO: (16) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 21.877876ms)
    Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 26.08131ms)
    Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 26.103357ms)
    Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 26.161864ms)
    Dec 14 08:42:07.341: INFO: (16) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 26.149789ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 9.737482ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 9.622681ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 9.644073ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.598232ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.609607ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.702565ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.648231ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 9.717387ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.755778ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.747301ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.824356ms)
    Dec 14 08:42:07.351: INFO: (17) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.3622ms)
    Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.105739ms)
    Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.006277ms)
    Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 14.102796ms)
    Dec 14 08:42:07.355: INFO: (17) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.17001ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 10.12652ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.90567ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.627152ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 10.071788ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 10.056898ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.795578ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.902614ms)
    Dec 14 08:42:07.365: INFO: (18) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.976662ms)
    Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 10.350112ms)
    Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 10.473987ms)
    Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 10.418986ms)
    Dec 14 08:42:07.366: INFO: (18) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 10.525928ms)
    Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 14.450078ms)
    Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 14.590708ms)
    Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 14.150891ms)
    Dec 14 08:42:07.370: INFO: (18) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 14.388277ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t/proxy/rewriteme">test</a> (200; 9.119647ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">... (200; 9.429241ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.321743ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:1080/proxy/rewriteme">test<... (200; 9.371436ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.558799ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname2/proxy/: tls qux (200; 9.217703ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:462/proxy/: tls qux (200; 9.326263ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:160/proxy/: foo (200; 9.167754ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/http:proxy-service-mmv5p-kjh5t:162/proxy/: bar (200; 9.746721ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/: <a href="/api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:443/proxy/tlsrewritem... (200; 9.445237ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/pods/https:proxy-service-mmv5p-kjh5t:460/proxy/: tls baz (200; 9.557427ms)
    Dec 14 08:42:07.380: INFO: (19) /api/v1/namespaces/proxy-7573/services/https:proxy-service-mmv5p:tlsportname1/proxy/: tls baz (200; 9.630222ms)
    Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname2/proxy/: bar (200; 13.143813ms)
    Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/proxy-service-mmv5p:portname1/proxy/: foo (200; 12.807911ms)
    Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname1/proxy/: foo (200; 13.02391ms)
    Dec 14 08:42:07.383: INFO: (19) /api/v1/namespaces/proxy-7573/services/http:proxy-service-mmv5p:portname2/proxy/: bar (200; 13.130617ms)
    STEP: deleting ReplicationController proxy-service-mmv5p in namespace proxy-7573, will wait for the garbage collector to delete the pods 12/14/22 08:42:07.383
    Dec 14 08:42:07.445: INFO: Deleting ReplicationController proxy-service-mmv5p took: 7.338786ms
    Dec 14 08:42:07.546: INFO: Terminating ReplicationController proxy-service-mmv5p pods took: 100.834585ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Dec 14 08:42:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7573" for this suite. 12/14/22 08:42:09.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:09.962
Dec 14 08:42:09.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:42:09.963
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:09.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:09.979
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Dec 14 08:42:09.984: INFO: Creating deployment "test-recreate-deployment"
Dec 14 08:42:09.989: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 14 08:42:09.996: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 14 08:42:12.006: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 14 08:42:12.010: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 14 08:42:12.022: INFO: Updating deployment test-recreate-deployment
Dec 14 08:42:12.022: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:42:12.110: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7655  45306158-3be6-469d-a0a7-c425778b3eba 22763 2 2022-12-14 08:42:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00376b508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 08:42:12 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-12-14 08:42:12 +0000 UTC,LastTransitionTime:2022-12-14 08:42:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 08:42:12.113: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-7655  d1639b7c-4cb1-4eec-b317-295bcda29507 22761 1 2022-12-14 08:42:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 45306158-3be6-469d-a0a7-c425778b3eba 0xc0039f7260 0xc0039f7261}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45306158-3be6-469d-a0a7-c425778b3eba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f7308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:42:12.113: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 14 08:42:12.113: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-7655  0ec6bacf-718b-4e95-9a09-8e26dfa4996b 22754 2 2022-12-14 08:42:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 45306158-3be6-469d-a0a7-c425778b3eba 0xc0039f7147 0xc0039f7148}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45306158-3be6-469d-a0a7-c425778b3eba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f71f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:42:12.117: INFO: Pod "test-recreate-deployment-9d58999df-nthb9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-nthb9 test-recreate-deployment-9d58999df- deployment-7655  d346c9f1-b1da-47c7-80fc-fc87414a6a04 22762 0 2022-12-14 08:42:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df d1639b7c-4cb1-4eec-b317-295bcda29507 0xc000d33a70 0xc000d33a71}] [] [{kube-controller-manager Update v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1639b7c-4cb1-4eec-b317-295bcda29507\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkfzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkfzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:,StartTime:2022-12-14 08:42:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:42:12.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7655" for this suite. 12/14/22 08:42:12.123
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":117,"skipped":2368,"failed":0}
------------------------------
• [2.165 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:09.962
    Dec 14 08:42:09.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:42:09.963
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:09.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:09.979
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Dec 14 08:42:09.984: INFO: Creating deployment "test-recreate-deployment"
    Dec 14 08:42:09.989: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Dec 14 08:42:09.996: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Dec 14 08:42:12.006: INFO: Waiting deployment "test-recreate-deployment" to complete
    Dec 14 08:42:12.010: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Dec 14 08:42:12.022: INFO: Updating deployment test-recreate-deployment
    Dec 14 08:42:12.022: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:42:12.110: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7655  45306158-3be6-469d-a0a7-c425778b3eba 22763 2 2022-12-14 08:42:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00376b508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 08:42:12 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-12-14 08:42:12 +0000 UTC,LastTransitionTime:2022-12-14 08:42:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 14 08:42:12.113: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-7655  d1639b7c-4cb1-4eec-b317-295bcda29507 22761 1 2022-12-14 08:42:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 45306158-3be6-469d-a0a7-c425778b3eba 0xc0039f7260 0xc0039f7261}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45306158-3be6-469d-a0a7-c425778b3eba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f7308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:42:12.113: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Dec 14 08:42:12.113: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-7655  0ec6bacf-718b-4e95-9a09-8e26dfa4996b 22754 2 2022-12-14 08:42:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 45306158-3be6-469d-a0a7-c425778b3eba 0xc0039f7147 0xc0039f7148}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45306158-3be6-469d-a0a7-c425778b3eba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f71f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:42:12.117: INFO: Pod "test-recreate-deployment-9d58999df-nthb9" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-nthb9 test-recreate-deployment-9d58999df- deployment-7655  d346c9f1-b1da-47c7-80fc-fc87414a6a04 22762 0 2022-12-14 08:42:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df d1639b7c-4cb1-4eec-b317-295bcda29507 0xc000d33a70 0xc000d33a71}] [] [{kube-controller-manager Update v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1639b7c-4cb1-4eec-b317-295bcda29507\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 08:42:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkfzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkfzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:42:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:,StartTime:2022-12-14 08:42:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:42:12.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7655" for this suite. 12/14/22 08:42:12.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:12.128
Dec 14 08:42:12.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:42:12.129
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:12.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:12.145
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 12/14/22 08:42:12.155
STEP: waiting for Deployment to be created 12/14/22 08:42:12.16
STEP: waiting for all Replicas to be Ready 12/14/22 08:42:12.163
Dec 14 08:42:12.166: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.166: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.167: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.167: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.179: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.179: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.193: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.193: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 08:42:12.892: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 08:42:12.892: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 08:42:12.903: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 12/14/22 08:42:12.903
W1214 08:42:12.913031    4635 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 14 08:42:12.916: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 12/14/22 08:42:12.916
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.921: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.921: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.932: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.932: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:12.941: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:12.941: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:12.948: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:12.948: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:13.903: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:13.903: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:13.917: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
STEP: listing Deployments 12/14/22 08:42:13.917
Dec 14 08:42:13.922: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 12/14/22 08:42:13.922
Dec 14 08:42:13.941: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 12/14/22 08:42:13.941
Dec 14 08:42:13.951: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:13.951: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:13.951: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:13.959: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:13.973: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:14.910: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:14.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:14.923: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:14.938: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 08:42:16.426: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 12/14/22 08:42:16.438
STEP: fetching the DeploymentStatus 12/14/22 08:42:16.446
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
Dec 14 08:42:16.453: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 3
STEP: deleting the Deployment 12/14/22 08:42:16.453
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
Dec 14 08:42:16.462: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:42:16.466: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:42:16.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-728" for this suite. 12/14/22 08:42:16.475
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":118,"skipped":2398,"failed":0}
------------------------------
• [4.352 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:12.128
    Dec 14 08:42:12.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:42:12.129
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:12.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:12.145
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 12/14/22 08:42:12.155
    STEP: waiting for Deployment to be created 12/14/22 08:42:12.16
    STEP: waiting for all Replicas to be Ready 12/14/22 08:42:12.163
    Dec 14 08:42:12.166: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.166: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.167: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.167: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.179: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.179: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.193: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.193: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.892: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.892: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 14 08:42:12.903: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 12/14/22 08:42:12.903
    W1214 08:42:12.913031    4635 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 14 08:42:12.916: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 12/14/22 08:42:12.916
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 0
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.921: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.921: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.932: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.932: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:12.941: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:12.941: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:12.948: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:12.948: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:13.903: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:13.903: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:13.917: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    STEP: listing Deployments 12/14/22 08:42:13.917
    Dec 14 08:42:13.922: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 12/14/22 08:42:13.922
    Dec 14 08:42:13.941: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 12/14/22 08:42:13.941
    Dec 14 08:42:13.951: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:13.951: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:13.951: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:13.959: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:13.973: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:14.910: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:14.919: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:14.923: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:14.938: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 08:42:16.426: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 12/14/22 08:42:16.438
    STEP: fetching the DeploymentStatus 12/14/22 08:42:16.446
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 1
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:16.452: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 2
    Dec 14 08:42:16.453: INFO: observed Deployment test-deployment in namespace deployment-728 with ReadyReplicas 3
    STEP: deleting the Deployment 12/14/22 08:42:16.453
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    Dec 14 08:42:16.462: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:42:16.466: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:42:16.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-728" for this suite. 12/14/22 08:42:16.475
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:16.48
Dec 14 08:42:16.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:42:16.481
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:16.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:16.496
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:42:16.505
Dec 14 08:42:16.515: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9344" to be "running and ready"
Dec 14 08:42:16.520: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356908ms
Dec 14 08:42:16.520: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:42:18.526: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010410681s
Dec 14 08:42:18.526: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 08:42:18.526: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 12/14/22 08:42:18.53
Dec 14 08:42:18.538: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9344" to be "running and ready"
Dec 14 08:42:18.546: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555143ms
Dec 14 08:42:18.546: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:42:20.550: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012249545s
Dec 14 08:42:20.550: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Dec 14 08:42:20.550: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/14/22 08:42:20.559
STEP: delete the pod with lifecycle hook 12/14/22 08:42:20.618
Dec 14 08:42:20.624: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 08:42:20.629: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 08:42:22.629: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 08:42:22.634: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 14 08:42:22.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9344" for this suite. 12/14/22 08:42:22.641
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":119,"skipped":2400,"failed":0}
------------------------------
• [6.167 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:16.48
    Dec 14 08:42:16.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:42:16.481
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:16.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:16.496
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:42:16.505
    Dec 14 08:42:16.515: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9344" to be "running and ready"
    Dec 14 08:42:16.520: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356908ms
    Dec 14 08:42:16.520: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:42:18.526: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010410681s
    Dec 14 08:42:18.526: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 08:42:18.526: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 12/14/22 08:42:18.53
    Dec 14 08:42:18.538: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9344" to be "running and ready"
    Dec 14 08:42:18.546: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555143ms
    Dec 14 08:42:18.546: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:42:20.550: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012249545s
    Dec 14 08:42:20.550: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Dec 14 08:42:20.550: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/14/22 08:42:20.559
    STEP: delete the pod with lifecycle hook 12/14/22 08:42:20.618
    Dec 14 08:42:20.624: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 14 08:42:20.629: INFO: Pod pod-with-poststart-http-hook still exists
    Dec 14 08:42:22.629: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 14 08:42:22.634: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 14 08:42:22.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9344" for this suite. 12/14/22 08:42:22.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:22.648
Dec 14 08:42:22.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 12/14/22 08:42:22.649
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:22.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:22.667
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 12/14/22 08:42:22.672
STEP: listing events in all namespaces 12/14/22 08:42:22.676
STEP: listing events in test namespace 12/14/22 08:42:22.686
STEP: listing events with field selection filtering on source 12/14/22 08:42:22.689
STEP: listing events with field selection filtering on reportingController 12/14/22 08:42:22.693
STEP: getting the test event 12/14/22 08:42:22.696
STEP: patching the test event 12/14/22 08:42:22.698
STEP: getting the test event 12/14/22 08:42:22.706
STEP: updating the test event 12/14/22 08:42:22.709
STEP: getting the test event 12/14/22 08:42:22.713
STEP: deleting the test event 12/14/22 08:42:22.717
STEP: listing events in all namespaces 12/14/22 08:42:22.722
STEP: listing events in test namespace 12/14/22 08:42:22.733
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Dec 14 08:42:22.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6604" for this suite. 12/14/22 08:42:22.742
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":120,"skipped":2433,"failed":0}
------------------------------
• [0.098 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:22.648
    Dec 14 08:42:22.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 12/14/22 08:42:22.649
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:22.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:22.667
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 12/14/22 08:42:22.672
    STEP: listing events in all namespaces 12/14/22 08:42:22.676
    STEP: listing events in test namespace 12/14/22 08:42:22.686
    STEP: listing events with field selection filtering on source 12/14/22 08:42:22.689
    STEP: listing events with field selection filtering on reportingController 12/14/22 08:42:22.693
    STEP: getting the test event 12/14/22 08:42:22.696
    STEP: patching the test event 12/14/22 08:42:22.698
    STEP: getting the test event 12/14/22 08:42:22.706
    STEP: updating the test event 12/14/22 08:42:22.709
    STEP: getting the test event 12/14/22 08:42:22.713
    STEP: deleting the test event 12/14/22 08:42:22.717
    STEP: listing events in all namespaces 12/14/22 08:42:22.722
    STEP: listing events in test namespace 12/14/22 08:42:22.733
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Dec 14 08:42:22.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6604" for this suite. 12/14/22 08:42:22.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:22.747
Dec 14 08:42:22.747: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:42:22.748
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:22.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:22.764
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:42:22.78
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:42:23.19
STEP: Deploying the webhook pod 12/14/22 08:42:23.197
STEP: Wait for the deployment to be ready 12/14/22 08:42:23.206
Dec 14 08:42:23.215: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:42:25.228
STEP: Verifying the service has paired with the endpoint 12/14/22 08:42:25.237
Dec 14 08:42:26.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 08:42:26.244
STEP: create a pod 12/14/22 08:42:26.379
Dec 14 08:42:26.389: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6464" to be "running"
Dec 14 08:42:26.393: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718954ms
Dec 14 08:42:28.399: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009950642s
Dec 14 08:42:28.399: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 12/14/22 08:42:28.399
Dec 14 08:42:28.399: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-6464 attach --namespace=webhook-6464 to-be-attached-pod -i -c=container1'
Dec 14 08:42:28.632: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:42:28.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6464" for this suite. 12/14/22 08:42:28.643
STEP: Destroying namespace "webhook-6464-markers" for this suite. 12/14/22 08:42:28.649
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":121,"skipped":2454,"failed":0}
------------------------------
• [5.931 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:22.747
    Dec 14 08:42:22.747: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:42:22.748
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:22.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:22.764
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:42:22.78
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:42:23.19
    STEP: Deploying the webhook pod 12/14/22 08:42:23.197
    STEP: Wait for the deployment to be ready 12/14/22 08:42:23.206
    Dec 14 08:42:23.215: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:42:25.228
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:42:25.237
    Dec 14 08:42:26.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 08:42:26.244
    STEP: create a pod 12/14/22 08:42:26.379
    Dec 14 08:42:26.389: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6464" to be "running"
    Dec 14 08:42:26.393: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718954ms
    Dec 14 08:42:28.399: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009950642s
    Dec 14 08:42:28.399: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 12/14/22 08:42:28.399
    Dec 14 08:42:28.399: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-6464 attach --namespace=webhook-6464 to-be-attached-pod -i -c=container1'
    Dec 14 08:42:28.632: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:42:28.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6464" for this suite. 12/14/22 08:42:28.643
    STEP: Destroying namespace "webhook-6464-markers" for this suite. 12/14/22 08:42:28.649
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:28.678
Dec 14 08:42:28.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 08:42:28.679
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:28.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:28.697
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Dec 14 08:42:28.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:42:29.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-565" for this suite. 12/14/22 08:42:29.733
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":122,"skipped":2459,"failed":0}
------------------------------
• [1.061 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:28.678
    Dec 14 08:42:28.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 08:42:28.679
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:28.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:28.697
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Dec 14 08:42:28.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:42:29.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-565" for this suite. 12/14/22 08:42:29.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:29.74
Dec 14 08:42:29.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/14/22 08:42:29.741
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:29.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:29.758
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 12/14/22 08:42:29.763
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:29.775
STEP: Creating a pod in the namespace 12/14/22 08:42:29.78
STEP: Waiting for the pod to have running status 12/14/22 08:42:29.79
Dec 14 08:42:29.790: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5286" to be "running"
Dec 14 08:42:29.795: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689034ms
Dec 14 08:42:31.800: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009396339s
Dec 14 08:42:31.800: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 12/14/22 08:42:31.8
STEP: Waiting for the namespace to be removed. 12/14/22 08:42:31.805
STEP: Recreating the namespace 12/14/22 08:42:42.81
STEP: Verifying there are no pods in the namespace 12/14/22 08:42:42.822
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:42:42.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4426" for this suite. 12/14/22 08:42:42.831
STEP: Destroying namespace "nsdeletetest-5286" for this suite. 12/14/22 08:42:42.836
Dec 14 08:42:42.841: INFO: Namespace nsdeletetest-5286 was already deleted
STEP: Destroying namespace "nsdeletetest-5549" for this suite. 12/14/22 08:42:42.841
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":123,"skipped":2495,"failed":0}
------------------------------
• [13.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:29.74
    Dec 14 08:42:29.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/14/22 08:42:29.741
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:29.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:29.758
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 12/14/22 08:42:29.763
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:29.775
    STEP: Creating a pod in the namespace 12/14/22 08:42:29.78
    STEP: Waiting for the pod to have running status 12/14/22 08:42:29.79
    Dec 14 08:42:29.790: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5286" to be "running"
    Dec 14 08:42:29.795: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689034ms
    Dec 14 08:42:31.800: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009396339s
    Dec 14 08:42:31.800: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 12/14/22 08:42:31.8
    STEP: Waiting for the namespace to be removed. 12/14/22 08:42:31.805
    STEP: Recreating the namespace 12/14/22 08:42:42.81
    STEP: Verifying there are no pods in the namespace 12/14/22 08:42:42.822
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:42:42.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4426" for this suite. 12/14/22 08:42:42.831
    STEP: Destroying namespace "nsdeletetest-5286" for this suite. 12/14/22 08:42:42.836
    Dec 14 08:42:42.841: INFO: Namespace nsdeletetest-5286 was already deleted
    STEP: Destroying namespace "nsdeletetest-5549" for this suite. 12/14/22 08:42:42.841
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:42.846
Dec 14 08:42:42.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 12/14/22 08:42:42.847
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:42.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:42.862
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 14 08:42:44.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3451" for this suite. 12/14/22 08:42:44.913
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":124,"skipped":2497,"failed":0}
------------------------------
• [2.072 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:42.846
    Dec 14 08:42:42.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 12/14/22 08:42:42.847
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:42.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:42.862
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 14 08:42:44.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3451" for this suite. 12/14/22 08:42:44.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:44.92
Dec 14 08:42:44.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 12/14/22 08:42:44.921
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:44.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:44.937
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Dec 14 08:42:44.957: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1446 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 14 08:42:44.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1446" for this suite. 12/14/22 08:42:44.971
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":125,"skipped":2576,"failed":0}
------------------------------
• [0.057 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:44.92
    Dec 14 08:42:44.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 08:42:44.921
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:44.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:44.937
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Dec 14 08:42:44.957: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1446 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 14 08:42:44.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1446" for this suite. 12/14/22 08:42:44.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:44.977
Dec 14 08:42:44.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:42:44.978
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:44.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:44.992
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Dec 14 08:42:44.997: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6371 version'
Dec 14 08:42:45.053: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Dec 14 08:42:45.053: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:42:45.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6371" for this suite. 12/14/22 08:42:45.058
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":126,"skipped":2592,"failed":0}
------------------------------
• [0.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:44.977
    Dec 14 08:42:44.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:42:44.978
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:44.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:44.992
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Dec 14 08:42:44.997: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6371 version'
    Dec 14 08:42:45.053: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Dec 14 08:42:45.053: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:42:45.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6371" for this suite. 12/14/22 08:42:45.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:42:45.063
Dec 14 08:42:45.063: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 08:42:45.063
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:45.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:45.079
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8821 12/14/22 08:42:45.084
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 12/14/22 08:42:45.089
Dec 14 08:42:45.097: INFO: Found 0 stateful pods, waiting for 3
Dec 14 08:42:55.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:42:55.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:42:55.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/14/22 08:42:55.119
Dec 14 08:42:55.138: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/14/22 08:42:55.138
STEP: Not applying an update when the partition is greater than the number of replicas 12/14/22 08:43:05.156
STEP: Performing a canary update 12/14/22 08:43:05.156
Dec 14 08:43:05.176: INFO: Updating stateful set ss2
Dec 14 08:43:05.183: INFO: Waiting for Pod statefulset-8821/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 12/14/22 08:43:15.193
Dec 14 08:43:15.212: INFO: Found 1 stateful pods, waiting for 3
Dec 14 08:43:25.218: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:43:25.218: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 08:43:25.218: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 12/14/22 08:43:25.226
Dec 14 08:43:25.247: INFO: Updating stateful set ss2
Dec 14 08:43:25.255: INFO: Waiting for Pod statefulset-8821/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Dec 14 08:43:35.285: INFO: Updating stateful set ss2
Dec 14 08:43:35.297: INFO: Waiting for StatefulSet statefulset-8821/ss2 to complete update
Dec 14 08:43:35.297: INFO: Waiting for Pod statefulset-8821/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 08:43:45.306: INFO: Deleting all statefulset in ns statefulset-8821
Dec 14 08:43:45.310: INFO: Scaling statefulset ss2 to 0
Dec 14 08:43:55.328: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:43:55.332: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 08:43:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8821" for this suite. 12/14/22 08:43:55.351
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":127,"skipped":2598,"failed":0}
------------------------------
• [70.294 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:42:45.063
    Dec 14 08:42:45.063: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 08:42:45.063
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:42:45.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:42:45.079
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8821 12/14/22 08:42:45.084
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 12/14/22 08:42:45.089
    Dec 14 08:42:45.097: INFO: Found 0 stateful pods, waiting for 3
    Dec 14 08:42:55.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:42:55.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:42:55.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/14/22 08:42:55.119
    Dec 14 08:42:55.138: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/14/22 08:42:55.138
    STEP: Not applying an update when the partition is greater than the number of replicas 12/14/22 08:43:05.156
    STEP: Performing a canary update 12/14/22 08:43:05.156
    Dec 14 08:43:05.176: INFO: Updating stateful set ss2
    Dec 14 08:43:05.183: INFO: Waiting for Pod statefulset-8821/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 12/14/22 08:43:15.193
    Dec 14 08:43:15.212: INFO: Found 1 stateful pods, waiting for 3
    Dec 14 08:43:25.218: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:43:25.218: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 08:43:25.218: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 12/14/22 08:43:25.226
    Dec 14 08:43:25.247: INFO: Updating stateful set ss2
    Dec 14 08:43:25.255: INFO: Waiting for Pod statefulset-8821/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Dec 14 08:43:35.285: INFO: Updating stateful set ss2
    Dec 14 08:43:35.297: INFO: Waiting for StatefulSet statefulset-8821/ss2 to complete update
    Dec 14 08:43:35.297: INFO: Waiting for Pod statefulset-8821/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 08:43:45.306: INFO: Deleting all statefulset in ns statefulset-8821
    Dec 14 08:43:45.310: INFO: Scaling statefulset ss2 to 0
    Dec 14 08:43:55.328: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:43:55.332: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 08:43:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8821" for this suite. 12/14/22 08:43:55.351
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:43:55.357
Dec 14 08:43:55.357: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 12/14/22 08:43:55.357
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:43:55.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:43:55.373
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Dec 14 08:43:55.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7143" for this suite. 12/14/22 08:43:55.41
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":128,"skipped":2601,"failed":0}
------------------------------
• [0.057 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:43:55.357
    Dec 14 08:43:55.357: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 12/14/22 08:43:55.357
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:43:55.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:43:55.373
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Dec 14 08:43:55.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7143" for this suite. 12/14/22 08:43:55.41
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:43:55.414
Dec 14 08:43:55.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:43:55.415
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:43:55.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:43:55.43
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:43:55.444
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:43:55.945
STEP: Deploying the webhook pod 12/14/22 08:43:55.951
STEP: Wait for the deployment to be ready 12/14/22 08:43:55.961
Dec 14 08:43:55.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:43:57.982
STEP: Verifying the service has paired with the endpoint 12/14/22 08:43:57.993
Dec 14 08:43:58.994: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Dec 14 08:43:58.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8124-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 08:43:59.514
STEP: Creating a custom resource while v1 is storage version 12/14/22 08:43:59.639
STEP: Patching Custom Resource Definition to set v2 as storage 12/14/22 08:44:01.847
STEP: Patching the custom resource while v2 is storage version 12/14/22 08:44:01.864
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:44:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6466" for this suite. 12/14/22 08:44:02.444
STEP: Destroying namespace "webhook-6466-markers" for this suite. 12/14/22 08:44:02.449
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":129,"skipped":2602,"failed":0}
------------------------------
• [7.065 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:43:55.414
    Dec 14 08:43:55.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:43:55.415
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:43:55.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:43:55.43
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:43:55.444
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:43:55.945
    STEP: Deploying the webhook pod 12/14/22 08:43:55.951
    STEP: Wait for the deployment to be ready 12/14/22 08:43:55.961
    Dec 14 08:43:55.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:43:57.982
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:43:57.993
    Dec 14 08:43:58.994: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Dec 14 08:43:58.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8124-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 08:43:59.514
    STEP: Creating a custom resource while v1 is storage version 12/14/22 08:43:59.639
    STEP: Patching Custom Resource Definition to set v2 as storage 12/14/22 08:44:01.847
    STEP: Patching the custom resource while v2 is storage version 12/14/22 08:44:01.864
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:44:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6466" for this suite. 12/14/22 08:44:02.444
    STEP: Destroying namespace "webhook-6466-markers" for this suite. 12/14/22 08:44:02.449
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:02.48
Dec 14 08:44:02.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 12/14/22 08:44:02.481
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:02.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:02.496
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Dec 14 08:44:02.501: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/14/22 08:44:02.508
STEP: Checking rc "condition-test" has the desired failure condition set 12/14/22 08:44:02.513
STEP: Scaling down rc "condition-test" to satisfy pod quota 12/14/22 08:44:03.521
Dec 14 08:44:03.532: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 12/14/22 08:44:03.532
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 14 08:44:03.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1969" for this suite. 12/14/22 08:44:03.542
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":130,"skipped":2605,"failed":0}
------------------------------
• [1.068 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:02.48
    Dec 14 08:44:02.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 12/14/22 08:44:02.481
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:02.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:02.496
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Dec 14 08:44:02.501: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/14/22 08:44:02.508
    STEP: Checking rc "condition-test" has the desired failure condition set 12/14/22 08:44:02.513
    STEP: Scaling down rc "condition-test" to satisfy pod quota 12/14/22 08:44:03.521
    Dec 14 08:44:03.532: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 12/14/22 08:44:03.532
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 14 08:44:03.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1969" for this suite. 12/14/22 08:44:03.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:03.548
Dec 14 08:44:03.548: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 08:44:03.549
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:03.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:03.565
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 12/14/22 08:44:03.57
STEP: submitting the pod to kubernetes 12/14/22 08:44:03.57
Dec 14 08:44:03.594: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" in namespace "pods-8901" to be "running and ready"
Dec 14 08:44:03.598: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003123ms
Dec 14 08:44:03.598: INFO: The phase of Pod pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:44:05.604: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009224119s
Dec 14 08:44:05.604: INFO: The phase of Pod pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a is Running (Ready = true)
Dec 14 08:44:05.604: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/14/22 08:44:05.608
STEP: updating the pod 12/14/22 08:44:05.611
Dec 14 08:44:06.129: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a"
Dec 14 08:44:06.129: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" in namespace "pods-8901" to be "terminated with reason DeadlineExceeded"
Dec 14 08:44:06.133: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=true. Elapsed: 4.055463ms
Dec 14 08:44:08.138: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581339s
Dec 14 08:44:10.139: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=false. Elapsed: 4.009291356s
Dec 14 08:44:12.138: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.008526919s
Dec 14 08:44:12.138: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 08:44:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8901" for this suite. 12/14/22 08:44:12.144
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":131,"skipped":2611,"failed":0}
------------------------------
• [8.601 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:03.548
    Dec 14 08:44:03.548: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 08:44:03.549
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:03.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:03.565
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 12/14/22 08:44:03.57
    STEP: submitting the pod to kubernetes 12/14/22 08:44:03.57
    Dec 14 08:44:03.594: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" in namespace "pods-8901" to be "running and ready"
    Dec 14 08:44:03.598: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003123ms
    Dec 14 08:44:03.598: INFO: The phase of Pod pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:44:05.604: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009224119s
    Dec 14 08:44:05.604: INFO: The phase of Pod pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a is Running (Ready = true)
    Dec 14 08:44:05.604: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/14/22 08:44:05.608
    STEP: updating the pod 12/14/22 08:44:05.611
    Dec 14 08:44:06.129: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a"
    Dec 14 08:44:06.129: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" in namespace "pods-8901" to be "terminated with reason DeadlineExceeded"
    Dec 14 08:44:06.133: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=true. Elapsed: 4.055463ms
    Dec 14 08:44:08.138: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581339s
    Dec 14 08:44:10.139: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Running", Reason="", readiness=false. Elapsed: 4.009291356s
    Dec 14 08:44:12.138: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.008526919s
    Dec 14 08:44:12.138: INFO: Pod "pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 08:44:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8901" for this suite. 12/14/22 08:44:12.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:12.15
Dec 14 08:44:12.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 08:44:12.151
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:12.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:12.166
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-1076b6c6-2d6c-4e41-9ad8-fc02b63a7581 12/14/22 08:44:12.171
STEP: Creating a pod to test consume configMaps 12/14/22 08:44:12.175
Dec 14 08:44:12.186: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82" in namespace "projected-8393" to be "Succeeded or Failed"
Dec 14 08:44:12.190: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.485672ms
Dec 14 08:44:14.196: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009900103s
Dec 14 08:44:16.195: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009297375s
STEP: Saw pod success 12/14/22 08:44:16.195
Dec 14 08:44:16.195: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82" satisfied condition "Succeeded or Failed"
Dec 14 08:44:16.199: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:44:16.212
Dec 14 08:44:16.219: INFO: Waiting for pod pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82 to disappear
Dec 14 08:44:16.223: INFO: Pod pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 08:44:16.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8393" for this suite. 12/14/22 08:44:16.228
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":132,"skipped":2623,"failed":0}
------------------------------
• [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:12.15
    Dec 14 08:44:12.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 08:44:12.151
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:12.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:12.166
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-1076b6c6-2d6c-4e41-9ad8-fc02b63a7581 12/14/22 08:44:12.171
    STEP: Creating a pod to test consume configMaps 12/14/22 08:44:12.175
    Dec 14 08:44:12.186: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82" in namespace "projected-8393" to be "Succeeded or Failed"
    Dec 14 08:44:12.190: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.485672ms
    Dec 14 08:44:14.196: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009900103s
    Dec 14 08:44:16.195: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009297375s
    STEP: Saw pod success 12/14/22 08:44:16.195
    Dec 14 08:44:16.195: INFO: Pod "pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82" satisfied condition "Succeeded or Failed"
    Dec 14 08:44:16.199: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:44:16.212
    Dec 14 08:44:16.219: INFO: Waiting for pod pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82 to disappear
    Dec 14 08:44:16.223: INFO: Pod pod-projected-configmaps-ea19c464-3920-429a-8b9f-aa7fd22abd82 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 08:44:16.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8393" for this suite. 12/14/22 08:44:16.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:16.233
Dec 14 08:44:16.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/14/22 08:44:16.234
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:16.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:16.249
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 08:44:16.254: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 08:44:16.264: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 08:44:16.267: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
Dec 14 08:44:16.278: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 14 08:44:16.278: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec 14 08:44:16.278: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container proxy ready: true, restart count 0
Dec 14 08:44:16.278: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 08:44:16.278: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 08:44:16.278: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 08:44:16.278: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container calico-typha ready: true, restart count 0
Dec 14 08:44:16.278: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 08:44:16.278: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 08:44:16.278: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container coredns ready: true, restart count 0
Dec 14 08:44:16.278: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container coredns ready: true, restart count 0
Dec 14 08:44:16.278: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 08:44:16.278: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 08:44:16.278: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 08:44:16.278: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 08:44:16.278: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 08:44:16.278: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 08:44:16.278: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 08:44:16.278: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 08:44:16.278: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 08:44:16.278: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 08:44:16.278: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 08:44:16.278: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec 14 08:44:16.278: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 14 08:44:16.278: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.278: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 08:44:16.278: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
Dec 14 08:44:16.290: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container proxy ready: true, restart count 0
Dec 14 08:44:16.290: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 08:44:16.290: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 08:44:16.290: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 08:44:16.290: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 08:44:16.290: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 08:44:16.290: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 08:44:16.290: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 08:44:16.290: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 08:44:16.290: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 08:44:16.290: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 08:44:16.290: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container metrics-server ready: true, restart count 2
Dec 14 08:44:16.290: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container metrics-server ready: true, restart count 1
Dec 14 08:44:16.290: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 08:44:16.290: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 08:44:16.290: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 08:44:16.290: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 08:44:16.290: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 08:44:16.290: INFO: pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a from pods-8901 started at 2022-12-14 08:44:03 +0000 UTC (1 container statuses recorded)
Dec 14 08:44:16.290: INFO: 	Container pause ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 08:44:16.29
Dec 14 08:44:16.301: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3020" to be "running"
Dec 14 08:44:16.305: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115441ms
Dec 14 08:44:18.318: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017160116s
Dec 14 08:44:18.318: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 08:44:18.323
STEP: Trying to apply a random label on the found node. 12/14/22 08:44:18.329
STEP: verifying the node has the label kubernetes.io/e2e-f1066181-89b4-4e33-8512-395f86aa7a58 42 12/14/22 08:44:18.342
STEP: Trying to relaunch the pod, now with labels. 12/14/22 08:44:18.346
Dec 14 08:44:18.355: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3020" to be "not pending"
Dec 14 08:44:18.358: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263611ms
Dec 14 08:44:20.363: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008194126s
Dec 14 08:44:20.363: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f1066181-89b4-4e33-8512-395f86aa7a58 off the node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 08:44:20.367
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f1066181-89b4-4e33-8512-395f86aa7a58 12/14/22 08:44:20.383
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:44:20.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3020" for this suite. 12/14/22 08:44:20.391
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":133,"skipped":2633,"failed":0}
------------------------------
• [4.163 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:16.233
    Dec 14 08:44:16.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/14/22 08:44:16.234
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:16.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:16.249
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 14 08:44:16.254: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 08:44:16.264: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 08:44:16.267: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
    Dec 14 08:44:16.278: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container calico-typha ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 08:44:16.278: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.278: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec 14 08:44:16.278: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
    Dec 14 08:44:16.290: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 08:44:16.290: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container metrics-server ready: true, restart count 2
    Dec 14 08:44:16.290: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container metrics-server ready: true, restart count 1
    Dec 14 08:44:16.290: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 08:44:16.290: INFO: pod-update-activedeadlineseconds-8808d1d3-2c19-4552-81eb-994692d08e0a from pods-8901 started at 2022-12-14 08:44:03 +0000 UTC (1 container statuses recorded)
    Dec 14 08:44:16.290: INFO: 	Container pause ready: false, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 08:44:16.29
    Dec 14 08:44:16.301: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3020" to be "running"
    Dec 14 08:44:16.305: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115441ms
    Dec 14 08:44:18.318: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017160116s
    Dec 14 08:44:18.318: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 08:44:18.323
    STEP: Trying to apply a random label on the found node. 12/14/22 08:44:18.329
    STEP: verifying the node has the label kubernetes.io/e2e-f1066181-89b4-4e33-8512-395f86aa7a58 42 12/14/22 08:44:18.342
    STEP: Trying to relaunch the pod, now with labels. 12/14/22 08:44:18.346
    Dec 14 08:44:18.355: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3020" to be "not pending"
    Dec 14 08:44:18.358: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263611ms
    Dec 14 08:44:20.363: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008194126s
    Dec 14 08:44:20.363: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f1066181-89b4-4e33-8512-395f86aa7a58 off the node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 08:44:20.367
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f1066181-89b4-4e33-8512-395f86aa7a58 12/14/22 08:44:20.383
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:44:20.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3020" for this suite. 12/14/22 08:44:20.391
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:20.397
Dec 14 08:44:20.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:44:20.398
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:20.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:20.413
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Dec 14 08:44:20.427: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48" in namespace "kubelet-test-2132" to be "running and ready"
Dec 14 08:44:20.431: INFO: Pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51091ms
Dec 14 08:44:20.431: INFO: The phase of Pod busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:44:22.436: INFO: Pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008117424s
Dec 14 08:44:22.436: INFO: The phase of Pod busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48 is Running (Ready = true)
Dec 14 08:44:22.436: INFO: Pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 14 08:44:22.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2132" for this suite. 12/14/22 08:44:22.455
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":134,"skipped":2676,"failed":0}
------------------------------
• [2.063 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:20.397
    Dec 14 08:44:20.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:44:20.398
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:20.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:20.413
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Dec 14 08:44:20.427: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48" in namespace "kubelet-test-2132" to be "running and ready"
    Dec 14 08:44:20.431: INFO: Pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51091ms
    Dec 14 08:44:20.431: INFO: The phase of Pod busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:44:22.436: INFO: Pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008117424s
    Dec 14 08:44:22.436: INFO: The phase of Pod busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48 is Running (Ready = true)
    Dec 14 08:44:22.436: INFO: Pod "busybox-readonly-fs170dd36c-b6c1-468d-bb55-2ad24e2c2b48" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 14 08:44:22.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2132" for this suite. 12/14/22 08:44:22.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:22.46
Dec 14 08:44:22.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:44:22.461
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:22.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:22.482
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:44:22.491
Dec 14 08:44:22.500: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9179" to be "running and ready"
Dec 14 08:44:22.504: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050153ms
Dec 14 08:44:22.504: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:44:24.509: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009116984s
Dec 14 08:44:24.509: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 08:44:24.509: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 12/14/22 08:44:24.513
Dec 14 08:44:24.522: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9179" to be "running and ready"
Dec 14 08:44:24.526: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683644ms
Dec 14 08:44:24.526: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:44:26.547: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.025229632s
Dec 14 08:44:26.547: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Dec 14 08:44:26.547: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/14/22 08:44:26.552
Dec 14 08:44:26.559: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 08:44:26.563: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 08:44:28.563: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 08:44:28.568: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 12/14/22 08:44:28.568
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 14 08:44:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9179" for this suite. 12/14/22 08:44:28.598
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":135,"skipped":2681,"failed":0}
------------------------------
• [6.142 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:22.46
    Dec 14 08:44:22.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 08:44:22.461
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:22.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:22.482
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 08:44:22.491
    Dec 14 08:44:22.500: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9179" to be "running and ready"
    Dec 14 08:44:22.504: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050153ms
    Dec 14 08:44:22.504: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:44:24.509: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009116984s
    Dec 14 08:44:24.509: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 08:44:24.509: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 12/14/22 08:44:24.513
    Dec 14 08:44:24.522: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9179" to be "running and ready"
    Dec 14 08:44:24.526: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683644ms
    Dec 14 08:44:24.526: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:44:26.547: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.025229632s
    Dec 14 08:44:26.547: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Dec 14 08:44:26.547: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/14/22 08:44:26.552
    Dec 14 08:44:26.559: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 14 08:44:26.563: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 14 08:44:28.563: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 14 08:44:28.568: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 12/14/22 08:44:28.568
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 14 08:44:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9179" for this suite. 12/14/22 08:44:28.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:28.603
Dec 14 08:44:28.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:44:28.604
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:28.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:28.621
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 12/14/22 08:44:28.626
Dec 14 08:44:28.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 create -f -'
Dec 14 08:44:29.519: INFO: stderr: ""
Dec 14 08:44:29.519: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:44:29.519
Dec 14 08:44:29.519: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:44:29.617: INFO: stderr: ""
Dec 14 08:44:29.617: INFO: stdout: "update-demo-nautilus-62pdf update-demo-nautilus-kgjn9 "
Dec 14 08:44:29.617: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-62pdf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:44:29.757: INFO: stderr: ""
Dec 14 08:44:29.757: INFO: stdout: ""
Dec 14 08:44:29.757: INFO: update-demo-nautilus-62pdf is created but not running
Dec 14 08:44:34.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:44:34.837: INFO: stderr: ""
Dec 14 08:44:34.837: INFO: stdout: "update-demo-nautilus-62pdf update-demo-nautilus-kgjn9 "
Dec 14 08:44:34.837: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-62pdf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:44:34.915: INFO: stderr: ""
Dec 14 08:44:34.915: INFO: stdout: "true"
Dec 14 08:44:34.915: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-62pdf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:44:35.010: INFO: stderr: ""
Dec 14 08:44:35.010: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:44:35.010: INFO: validating pod update-demo-nautilus-62pdf
Dec 14 08:44:35.111: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:44:35.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:44:35.111: INFO: update-demo-nautilus-62pdf is verified up and running
Dec 14 08:44:35.111: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:44:35.218: INFO: stderr: ""
Dec 14 08:44:35.218: INFO: stdout: "true"
Dec 14 08:44:35.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:44:35.319: INFO: stderr: ""
Dec 14 08:44:35.319: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:44:35.319: INFO: validating pod update-demo-nautilus-kgjn9
Dec 14 08:44:35.423: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:44:35.423: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:44:35.423: INFO: update-demo-nautilus-kgjn9 is verified up and running
STEP: scaling down the replication controller 12/14/22 08:44:35.423
Dec 14 08:44:35.424: INFO: scanned /root for discovery docs: <nil>
Dec 14 08:44:35.424: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 14 08:44:36.523: INFO: stderr: ""
Dec 14 08:44:36.523: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:44:36.523
Dec 14 08:44:36.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:44:36.613: INFO: stderr: ""
Dec 14 08:44:36.613: INFO: stdout: "update-demo-nautilus-62pdf update-demo-nautilus-kgjn9 "
STEP: Replicas for name=update-demo: expected=1 actual=2 12/14/22 08:44:36.613
Dec 14 08:44:41.614: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:44:41.706: INFO: stderr: ""
Dec 14 08:44:41.706: INFO: stdout: "update-demo-nautilus-kgjn9 "
Dec 14 08:44:41.706: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:44:41.789: INFO: stderr: ""
Dec 14 08:44:41.789: INFO: stdout: "true"
Dec 14 08:44:41.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:44:41.865: INFO: stderr: ""
Dec 14 08:44:41.865: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:44:41.865: INFO: validating pod update-demo-nautilus-kgjn9
Dec 14 08:44:41.872: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:44:41.872: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:44:41.872: INFO: update-demo-nautilus-kgjn9 is verified up and running
STEP: scaling up the replication controller 12/14/22 08:44:41.872
Dec 14 08:44:41.874: INFO: scanned /root for discovery docs: <nil>
Dec 14 08:44:41.874: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 14 08:44:42.969: INFO: stderr: ""
Dec 14 08:44:42.969: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:44:42.969
Dec 14 08:44:42.969: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 08:44:43.045: INFO: stderr: ""
Dec 14 08:44:43.045: INFO: stdout: "update-demo-nautilus-kgjn9 update-demo-nautilus-tpbzf "
Dec 14 08:44:43.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:44:43.129: INFO: stderr: ""
Dec 14 08:44:43.129: INFO: stdout: "true"
Dec 14 08:44:43.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:44:43.219: INFO: stderr: ""
Dec 14 08:44:43.219: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:44:43.219: INFO: validating pod update-demo-nautilus-kgjn9
Dec 14 08:44:43.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:44:43.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:44:43.268: INFO: update-demo-nautilus-kgjn9 is verified up and running
Dec 14 08:44:43.268: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-tpbzf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 08:44:43.359: INFO: stderr: ""
Dec 14 08:44:43.359: INFO: stdout: "true"
Dec 14 08:44:43.359: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-tpbzf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 08:44:43.449: INFO: stderr: ""
Dec 14 08:44:43.449: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 14 08:44:43.449: INFO: validating pod update-demo-nautilus-tpbzf
Dec 14 08:44:43.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 08:44:43.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 08:44:43.559: INFO: update-demo-nautilus-tpbzf is verified up and running
STEP: using delete to clean up resources 12/14/22 08:44:43.559
Dec 14 08:44:43.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 delete --grace-period=0 --force -f -'
Dec 14 08:44:43.635: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 08:44:43.635: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 08:44:43.635: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get rc,svc -l name=update-demo --no-headers'
Dec 14 08:44:43.722: INFO: stderr: "No resources found in kubectl-1898 namespace.\n"
Dec 14 08:44:43.722: INFO: stdout: ""
Dec 14 08:44:43.722: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 08:44:43.816: INFO: stderr: ""
Dec 14 08:44:43.816: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:44:43.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1898" for this suite. 12/14/22 08:44:43.822
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":136,"skipped":2710,"failed":0}
------------------------------
• [15.224 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:28.603
    Dec 14 08:44:28.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:44:28.604
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:28.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:28.621
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 12/14/22 08:44:28.626
    Dec 14 08:44:28.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 create -f -'
    Dec 14 08:44:29.519: INFO: stderr: ""
    Dec 14 08:44:29.519: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:44:29.519
    Dec 14 08:44:29.519: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:44:29.617: INFO: stderr: ""
    Dec 14 08:44:29.617: INFO: stdout: "update-demo-nautilus-62pdf update-demo-nautilus-kgjn9 "
    Dec 14 08:44:29.617: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-62pdf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:44:29.757: INFO: stderr: ""
    Dec 14 08:44:29.757: INFO: stdout: ""
    Dec 14 08:44:29.757: INFO: update-demo-nautilus-62pdf is created but not running
    Dec 14 08:44:34.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:44:34.837: INFO: stderr: ""
    Dec 14 08:44:34.837: INFO: stdout: "update-demo-nautilus-62pdf update-demo-nautilus-kgjn9 "
    Dec 14 08:44:34.837: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-62pdf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:44:34.915: INFO: stderr: ""
    Dec 14 08:44:34.915: INFO: stdout: "true"
    Dec 14 08:44:34.915: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-62pdf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:44:35.010: INFO: stderr: ""
    Dec 14 08:44:35.010: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:44:35.010: INFO: validating pod update-demo-nautilus-62pdf
    Dec 14 08:44:35.111: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:44:35.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:44:35.111: INFO: update-demo-nautilus-62pdf is verified up and running
    Dec 14 08:44:35.111: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:44:35.218: INFO: stderr: ""
    Dec 14 08:44:35.218: INFO: stdout: "true"
    Dec 14 08:44:35.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:44:35.319: INFO: stderr: ""
    Dec 14 08:44:35.319: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:44:35.319: INFO: validating pod update-demo-nautilus-kgjn9
    Dec 14 08:44:35.423: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:44:35.423: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:44:35.423: INFO: update-demo-nautilus-kgjn9 is verified up and running
    STEP: scaling down the replication controller 12/14/22 08:44:35.423
    Dec 14 08:44:35.424: INFO: scanned /root for discovery docs: <nil>
    Dec 14 08:44:35.424: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Dec 14 08:44:36.523: INFO: stderr: ""
    Dec 14 08:44:36.523: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:44:36.523
    Dec 14 08:44:36.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:44:36.613: INFO: stderr: ""
    Dec 14 08:44:36.613: INFO: stdout: "update-demo-nautilus-62pdf update-demo-nautilus-kgjn9 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 12/14/22 08:44:36.613
    Dec 14 08:44:41.614: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:44:41.706: INFO: stderr: ""
    Dec 14 08:44:41.706: INFO: stdout: "update-demo-nautilus-kgjn9 "
    Dec 14 08:44:41.706: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:44:41.789: INFO: stderr: ""
    Dec 14 08:44:41.789: INFO: stdout: "true"
    Dec 14 08:44:41.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:44:41.865: INFO: stderr: ""
    Dec 14 08:44:41.865: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:44:41.865: INFO: validating pod update-demo-nautilus-kgjn9
    Dec 14 08:44:41.872: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:44:41.872: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:44:41.872: INFO: update-demo-nautilus-kgjn9 is verified up and running
    STEP: scaling up the replication controller 12/14/22 08:44:41.872
    Dec 14 08:44:41.874: INFO: scanned /root for discovery docs: <nil>
    Dec 14 08:44:41.874: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Dec 14 08:44:42.969: INFO: stderr: ""
    Dec 14 08:44:42.969: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 08:44:42.969
    Dec 14 08:44:42.969: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 08:44:43.045: INFO: stderr: ""
    Dec 14 08:44:43.045: INFO: stdout: "update-demo-nautilus-kgjn9 update-demo-nautilus-tpbzf "
    Dec 14 08:44:43.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:44:43.129: INFO: stderr: ""
    Dec 14 08:44:43.129: INFO: stdout: "true"
    Dec 14 08:44:43.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-kgjn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:44:43.219: INFO: stderr: ""
    Dec 14 08:44:43.219: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:44:43.219: INFO: validating pod update-demo-nautilus-kgjn9
    Dec 14 08:44:43.268: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:44:43.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:44:43.268: INFO: update-demo-nautilus-kgjn9 is verified up and running
    Dec 14 08:44:43.268: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-tpbzf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 08:44:43.359: INFO: stderr: ""
    Dec 14 08:44:43.359: INFO: stdout: "true"
    Dec 14 08:44:43.359: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods update-demo-nautilus-tpbzf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 08:44:43.449: INFO: stderr: ""
    Dec 14 08:44:43.449: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 14 08:44:43.449: INFO: validating pod update-demo-nautilus-tpbzf
    Dec 14 08:44:43.559: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 08:44:43.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 08:44:43.559: INFO: update-demo-nautilus-tpbzf is verified up and running
    STEP: using delete to clean up resources 12/14/22 08:44:43.559
    Dec 14 08:44:43.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 delete --grace-period=0 --force -f -'
    Dec 14 08:44:43.635: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 08:44:43.635: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 14 08:44:43.635: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get rc,svc -l name=update-demo --no-headers'
    Dec 14 08:44:43.722: INFO: stderr: "No resources found in kubectl-1898 namespace.\n"
    Dec 14 08:44:43.722: INFO: stdout: ""
    Dec 14 08:44:43.722: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1898 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 14 08:44:43.816: INFO: stderr: ""
    Dec 14 08:44:43.816: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:44:43.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1898" for this suite. 12/14/22 08:44:43.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:43.827
Dec 14 08:44:43.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:44:43.828
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:43.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:43.844
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Dec 14 08:44:43.848: INFO: Creating deployment "webserver-deployment"
Dec 14 08:44:43.853: INFO: Waiting for observed generation 1
Dec 14 08:44:45.861: INFO: Waiting for all required pods to come up
Dec 14 08:44:45.869: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 12/14/22 08:44:45.869
Dec 14 08:44:45.869: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 14 08:44:45.876: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 14 08:44:45.885: INFO: Updating deployment webserver-deployment
Dec 14 08:44:45.885: INFO: Waiting for observed generation 2
Dec 14 08:44:47.894: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 14 08:44:47.898: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 14 08:44:47.901: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 08:44:47.911: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 14 08:44:47.911: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 14 08:44:47.914: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 08:44:47.920: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 14 08:44:47.920: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 14 08:44:47.929: INFO: Updating deployment webserver-deployment
Dec 14 08:44:47.929: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 14 08:44:47.935: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 14 08:44:47.939: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:44:47.946: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3742  8e9d0d07-9a4c-43e0-b86a-181042e7335b 24517 3 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000715618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-12-14 08:44:45 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 08:44:47 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 14 08:44:47.952: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-3742  8da53502-aa5f-4dd6-a55e-a84de92ef64c 24516 3 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8e9d0d07-9a4c-43e0-b86a-181042e7335b 0xc003324307 0xc003324308}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e9d0d07-9a4c-43e0-b86a-181042e7335b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033243a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:44:47.952: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 14 08:44:47.952: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-3742  af9d6e4e-649e-4a40-a161-0bbf51449eea 24515 3 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8e9d0d07-9a4c-43e0-b86a-181042e7335b 0xc003324407 0xc003324408}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e9d0d07-9a4c-43e0-b86a-181042e7335b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003324498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:44:47.961: INFO: Pod "webserver-deployment-69b7448995-944ks" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-944ks webserver-deployment-69b7448995- deployment-3742  66aba89c-9361-4711-899a-5775e5f36c45 24508 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fb056d668ac2ea5850d0794cb6f544b3ce3bb89393c3a497249bb6e2667e651b cni.projectcalico.org/podIP:100.64.1.220/32 cni.projectcalico.org/podIPs:100.64.1.220/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003940d67 0xc003940d68}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5tk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5tk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.220,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.961: INFO: Pod "webserver-deployment-69b7448995-fnlv9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-fnlv9 webserver-deployment-69b7448995- deployment-3742  7f05da3d-799a-4e7f-928d-b1a91cea7f68 24526 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003940f97 0xc003940f98}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pm6ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pm6ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.961: INFO: Pod "webserver-deployment-69b7448995-jsbcg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jsbcg webserver-deployment-69b7448995- deployment-3742  cb6264bd-c1f2-4985-bf71-10d061be20f4 24495 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:dbac06bc18fb76aa3a8df19b1b3112ac46da5971b31048c845dc19257651da8c cni.projectcalico.org/podIP:100.64.1.221/32 cni.projectcalico.org/podIPs:100.64.1.221/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941107 0xc003941108}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t66td,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t66td,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-kmslj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kmslj webserver-deployment-69b7448995- deployment-3742  d05c0b73-06d3-4c14-b8e2-632ad1969994 24528 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941317 0xc003941318}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hng2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hng2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-q67km" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-q67km webserver-deployment-69b7448995- deployment-3742  f8ec8611-6ecb-4110-a237-22148d9b0c79 24512 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a5e0c9b73b72ded990364b38d73f6ec82e888c8441c8bf1a3653f00d29d821bb cni.projectcalico.org/podIP:100.64.0.103/32 cni.projectcalico.org/podIPs:100.64.0.103/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc0039414a0 0xc0039414a1}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssc8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssc8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.103,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-qdrz5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qdrz5 webserver-deployment-69b7448995- deployment-3742  66666241-a83f-40b5-aa31-9f8a748ee48f 24498 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e2ce9739f1b144cea9eab25ee9e27353977ef0031fdb5118bd3d90ea2d1dfddb cni.projectcalico.org/podIP:100.64.1.222/32 cni.projectcalico.org/podIPs:100.64.1.222/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc0039416f0 0xc0039416f1}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzw89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzw89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-rbph8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rbph8 webserver-deployment-69b7448995- deployment-3742  051b9b8e-54a3-4874-9eb6-8ab048de62d2 24511 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:91bfe4c2e54283be178d51e9de8f7d5506cfd4ca39fbc1d1759739d50c84700d cni.projectcalico.org/podIP:100.64.0.102/32 cni.projectcalico.org/podIPs:100.64.0.102/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941917 0xc003941918}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lj2jt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lj2jt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.102,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-tsgqt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tsgqt webserver-deployment-69b7448995- deployment-3742  49e0db22-82bb-4ea4-a427-c6798cc437f4 24521 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941b40 0xc003941b41}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-km4bn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-km4bn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-845c8977d9-dpnvn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dpnvn webserver-deployment-845c8977d9- deployment-3742  fd3dab10-1cd2-42ef-9105-c8a927a6a2f1 24527 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc003941ec0 0xc003941ec1}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tltxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tltxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-845c8977d9-f7v7z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f7v7z webserver-deployment-845c8977d9- deployment-3742  b54b3740-9aaa-4885-8f30-4241f7f2a5f8 24436 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:44e397b8d0480c84988fdf43e225f7ba18e9beac64ec0a818840f431b80ed87c cni.projectcalico.org/podIP:100.64.1.218/32 cni.projectcalico.org/podIPs:100.64.1.218/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc00060c740 0xc00060c741}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk9kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk9kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.218,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3ba3f027a4f46a8a036eeec5d12252116321066204193df048aea9187327bd7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-fq5bk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fq5bk webserver-deployment-845c8977d9- deployment-3742  cc7059f8-4876-4e16-b85b-56327df5a414 24445 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:851b531173968e78a7cd88b7b1d131603ecadb95185f01669d479c25d30062c2 cni.projectcalico.org/podIP:100.64.0.100/32 cni.projectcalico.org/podIPs:100.64.0.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc00060d197 0xc00060d198}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-df6j5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-df6j5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.100,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://400ae44d7495eb7589a75a100904055ff13260eb98544d23409d65ba5fbd913f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-g25tw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-g25tw webserver-deployment-845c8977d9- deployment-3742  e6aa0073-a844-459f-b48f-9f0bf5c83722 24433 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:848e130af85e888326b5ea944c24e85756669ffb9dbee8c701d2117c8eaf3be4 cni.projectcalico.org/podIP:100.64.1.217/32 cni.projectcalico.org/podIPs:100.64.1.217/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc00060dae7 0xc00060dae8}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mpjl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mpjl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.217,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://50b801a22561f94c5b06764229b2a749110229360c08d5e16d3667000850a887,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-ggpjm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ggpjm webserver-deployment-845c8977d9- deployment-3742  cb04b459-1551-4c73-a75d-de3c7a496a36 24448 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0b0af96e1531624d6ac30449e807e6853ce36de1d5b7639f73d6b79036085c91 cni.projectcalico.org/podIP:100.64.0.99/32 cni.projectcalico.org/podIPs:100.64.0.99/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f81d7 0xc0036f81d8}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rrp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rrp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.99,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://891d355627973905eb16664547799ab95260d815d32bdf260cb4beedce97bb34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-klrnn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-klrnn webserver-deployment-845c8977d9- deployment-3742  8db1713a-49d5-4bdc-9ce7-6c6b0dae544a 24454 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9db38e6929e67a5d6e636d350dc4fc2a1bb5b9542fc8409a29fcb1612cf34398 cni.projectcalico.org/podIP:100.64.0.97/32 cni.projectcalico.org/podIPs:100.64.0.97/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8420 0xc0036f8421}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd9pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd9pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.97,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9e7354de3da67fc956c011e5ef956c0b3cdf3e0e7ac4a7b698ce54d8f0cec5cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-kr86s" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kr86s webserver-deployment-845c8977d9- deployment-3742  63a8e5f5-83cf-4863-b90c-5f7637e647b8 24430 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9799342fcdaf2c8a9529f00c7118b165deafc59cea70c41d9055b35f5ddb7e56 cni.projectcalico.org/podIP:100.64.1.219/32 cni.projectcalico.org/podIPs:100.64.1.219/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8640 0xc0036f8641}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqrqd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqrqd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.219,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aedd3e6ccba28cd713ec8290dc3b56fe6c77620443fcb78dbf5de29def1ca06a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-mhv25" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhv25 webserver-deployment-845c8977d9- deployment-3742  918e061c-fd32-4f68-a137-db5dfa5a99ef 24451 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8ca2123b42732ad3f68220dbafde4102da65560c46d4d03ef4e7eca268cb8c77 cni.projectcalico.org/podIP:100.64.0.101/32 cni.projectcalico.org/podIPs:100.64.0.101/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8877 0xc0036f8878}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7vfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7vfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.101,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f86fabb7ae1cb9085e4724d99b04933760c51b39cdf609580d6e46d8054415b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-qhcnn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhcnn webserver-deployment-845c8977d9- deployment-3742  e4fcfcab-3eb9-49ba-99c5-5569494961e2 24520 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8a87 0xc0036f8a88}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tnwcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tnwcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.964: INFO: Pod "webserver-deployment-845c8977d9-w25q6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w25q6 webserver-deployment-845c8977d9- deployment-3742  79bcbc24-3b3f-45c4-a68d-66f720fe0085 24419 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:58fbf3c87f6e2131626ef372ddcf2bf8a708b7d0293073f9694e26de4bbc5e8d cni.projectcalico.org/podIP:100.64.0.98/32 cni.projectcalico.org/podIPs:100.64.0.98/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8c00 0xc0036f8c01}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2w2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2w2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.98,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec39682a42d92d12670361dc5c9744cea87d0aac09b4eff1235e0a8f880b9cd9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 08:44:47.964: INFO: Pod "webserver-deployment-845c8977d9-xs2kc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xs2kc webserver-deployment-845c8977d9- deployment-3742  a2f1274f-fb08-4150-8219-8b4b6fd20748 24529 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8e10 0xc0036f8e11}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2jrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2jrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:44:47.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3742" for this suite. 12/14/22 08:44:47.971
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":137,"skipped":2715,"failed":0}
------------------------------
• [4.152 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:43.827
    Dec 14 08:44:43.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:44:43.828
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:43.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:43.844
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Dec 14 08:44:43.848: INFO: Creating deployment "webserver-deployment"
    Dec 14 08:44:43.853: INFO: Waiting for observed generation 1
    Dec 14 08:44:45.861: INFO: Waiting for all required pods to come up
    Dec 14 08:44:45.869: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 12/14/22 08:44:45.869
    Dec 14 08:44:45.869: INFO: Waiting for deployment "webserver-deployment" to complete
    Dec 14 08:44:45.876: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Dec 14 08:44:45.885: INFO: Updating deployment webserver-deployment
    Dec 14 08:44:45.885: INFO: Waiting for observed generation 2
    Dec 14 08:44:47.894: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Dec 14 08:44:47.898: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Dec 14 08:44:47.901: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 14 08:44:47.911: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Dec 14 08:44:47.911: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Dec 14 08:44:47.914: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 14 08:44:47.920: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Dec 14 08:44:47.920: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Dec 14 08:44:47.929: INFO: Updating deployment webserver-deployment
    Dec 14 08:44:47.929: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Dec 14 08:44:47.935: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Dec 14 08:44:47.939: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:44:47.946: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3742  8e9d0d07-9a4c-43e0-b86a-181042e7335b 24517 3 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000715618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-12-14 08:44:45 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 08:44:47 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Dec 14 08:44:47.952: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-3742  8da53502-aa5f-4dd6-a55e-a84de92ef64c 24516 3 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8e9d0d07-9a4c-43e0-b86a-181042e7335b 0xc003324307 0xc003324308}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e9d0d07-9a4c-43e0-b86a-181042e7335b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033243a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:44:47.952: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Dec 14 08:44:47.952: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-3742  af9d6e4e-649e-4a40-a161-0bbf51449eea 24515 3 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8e9d0d07-9a4c-43e0-b86a-181042e7335b 0xc003324407 0xc003324408}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e9d0d07-9a4c-43e0-b86a-181042e7335b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003324498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:44:47.961: INFO: Pod "webserver-deployment-69b7448995-944ks" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-944ks webserver-deployment-69b7448995- deployment-3742  66aba89c-9361-4711-899a-5775e5f36c45 24508 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fb056d668ac2ea5850d0794cb6f544b3ce3bb89393c3a497249bb6e2667e651b cni.projectcalico.org/podIP:100.64.1.220/32 cni.projectcalico.org/podIPs:100.64.1.220/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003940d67 0xc003940d68}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5tk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5tk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.220,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.961: INFO: Pod "webserver-deployment-69b7448995-fnlv9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-fnlv9 webserver-deployment-69b7448995- deployment-3742  7f05da3d-799a-4e7f-928d-b1a91cea7f68 24526 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003940f97 0xc003940f98}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pm6ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pm6ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.961: INFO: Pod "webserver-deployment-69b7448995-jsbcg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jsbcg webserver-deployment-69b7448995- deployment-3742  cb6264bd-c1f2-4985-bf71-10d061be20f4 24495 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:dbac06bc18fb76aa3a8df19b1b3112ac46da5971b31048c845dc19257651da8c cni.projectcalico.org/podIP:100.64.1.221/32 cni.projectcalico.org/podIPs:100.64.1.221/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941107 0xc003941108}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t66td,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t66td,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-kmslj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kmslj webserver-deployment-69b7448995- deployment-3742  d05c0b73-06d3-4c14-b8e2-632ad1969994 24528 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941317 0xc003941318}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hng2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hng2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-q67km" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-q67km webserver-deployment-69b7448995- deployment-3742  f8ec8611-6ecb-4110-a237-22148d9b0c79 24512 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a5e0c9b73b72ded990364b38d73f6ec82e888c8441c8bf1a3653f00d29d821bb cni.projectcalico.org/podIP:100.64.0.103/32 cni.projectcalico.org/podIPs:100.64.0.103/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc0039414a0 0xc0039414a1}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssc8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssc8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.103,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-qdrz5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qdrz5 webserver-deployment-69b7448995- deployment-3742  66666241-a83f-40b5-aa31-9f8a748ee48f 24498 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e2ce9739f1b144cea9eab25ee9e27353977ef0031fdb5118bd3d90ea2d1dfddb cni.projectcalico.org/podIP:100.64.1.222/32 cni.projectcalico.org/podIPs:100.64.1.222/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc0039416f0 0xc0039416f1}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzw89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzw89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-rbph8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rbph8 webserver-deployment-69b7448995- deployment-3742  051b9b8e-54a3-4874-9eb6-8ab048de62d2 24511 0 2022-12-14 08:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:91bfe4c2e54283be178d51e9de8f7d5506cfd4ca39fbc1d1759739d50c84700d cni.projectcalico.org/podIP:100.64.0.102/32 cni.projectcalico.org/podIPs:100.64.0.102/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941917 0xc003941918}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lj2jt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lj2jt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.102,StartTime:2022-12-14 08:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-69b7448995-tsgqt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tsgqt webserver-deployment-69b7448995- deployment-3742  49e0db22-82bb-4ea4-a427-c6798cc437f4 24521 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 8da53502-aa5f-4dd6-a55e-a84de92ef64c 0xc003941b40 0xc003941b41}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8da53502-aa5f-4dd6-a55e-a84de92ef64c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-km4bn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-km4bn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-845c8977d9-dpnvn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dpnvn webserver-deployment-845c8977d9- deployment-3742  fd3dab10-1cd2-42ef-9105-c8a927a6a2f1 24527 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc003941ec0 0xc003941ec1}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tltxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tltxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.962: INFO: Pod "webserver-deployment-845c8977d9-f7v7z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f7v7z webserver-deployment-845c8977d9- deployment-3742  b54b3740-9aaa-4885-8f30-4241f7f2a5f8 24436 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:44e397b8d0480c84988fdf43e225f7ba18e9beac64ec0a818840f431b80ed87c cni.projectcalico.org/podIP:100.64.1.218/32 cni.projectcalico.org/podIPs:100.64.1.218/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc00060c740 0xc00060c741}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk9kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk9kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.218,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3ba3f027a4f46a8a036eeec5d12252116321066204193df048aea9187327bd7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-fq5bk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fq5bk webserver-deployment-845c8977d9- deployment-3742  cc7059f8-4876-4e16-b85b-56327df5a414 24445 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:851b531173968e78a7cd88b7b1d131603ecadb95185f01669d479c25d30062c2 cni.projectcalico.org/podIP:100.64.0.100/32 cni.projectcalico.org/podIPs:100.64.0.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc00060d197 0xc00060d198}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-df6j5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-df6j5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.100,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://400ae44d7495eb7589a75a100904055ff13260eb98544d23409d65ba5fbd913f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-g25tw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-g25tw webserver-deployment-845c8977d9- deployment-3742  e6aa0073-a844-459f-b48f-9f0bf5c83722 24433 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:848e130af85e888326b5ea944c24e85756669ffb9dbee8c701d2117c8eaf3be4 cni.projectcalico.org/podIP:100.64.1.217/32 cni.projectcalico.org/podIPs:100.64.1.217/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc00060dae7 0xc00060dae8}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mpjl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mpjl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.217,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://50b801a22561f94c5b06764229b2a749110229360c08d5e16d3667000850a887,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-ggpjm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ggpjm webserver-deployment-845c8977d9- deployment-3742  cb04b459-1551-4c73-a75d-de3c7a496a36 24448 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0b0af96e1531624d6ac30449e807e6853ce36de1d5b7639f73d6b79036085c91 cni.projectcalico.org/podIP:100.64.0.99/32 cni.projectcalico.org/podIPs:100.64.0.99/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f81d7 0xc0036f81d8}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rrp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rrp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.99,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://891d355627973905eb16664547799ab95260d815d32bdf260cb4beedce97bb34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-klrnn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-klrnn webserver-deployment-845c8977d9- deployment-3742  8db1713a-49d5-4bdc-9ce7-6c6b0dae544a 24454 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9db38e6929e67a5d6e636d350dc4fc2a1bb5b9542fc8409a29fcb1612cf34398 cni.projectcalico.org/podIP:100.64.0.97/32 cni.projectcalico.org/podIPs:100.64.0.97/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8420 0xc0036f8421}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd9pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd9pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.97,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9e7354de3da67fc956c011e5ef956c0b3cdf3e0e7ac4a7b698ce54d8f0cec5cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-kr86s" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kr86s webserver-deployment-845c8977d9- deployment-3742  63a8e5f5-83cf-4863-b90c-5f7637e647b8 24430 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9799342fcdaf2c8a9529f00c7118b165deafc59cea70c41d9055b35f5ddb7e56 cni.projectcalico.org/podIP:100.64.1.219/32 cni.projectcalico.org/podIPs:100.64.1.219/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8640 0xc0036f8641}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqrqd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqrqd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.219,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aedd3e6ccba28cd713ec8290dc3b56fe6c77620443fcb78dbf5de29def1ca06a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-mhv25" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhv25 webserver-deployment-845c8977d9- deployment-3742  918e061c-fd32-4f68-a137-db5dfa5a99ef 24451 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8ca2123b42732ad3f68220dbafde4102da65560c46d4d03ef4e7eca268cb8c77 cni.projectcalico.org/podIP:100.64.0.101/32 cni.projectcalico.org/podIPs:100.64.0.101/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8877 0xc0036f8878}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7vfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7vfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.101,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f86fabb7ae1cb9085e4724d99b04933760c51b39cdf609580d6e46d8054415b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.963: INFO: Pod "webserver-deployment-845c8977d9-qhcnn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhcnn webserver-deployment-845c8977d9- deployment-3742  e4fcfcab-3eb9-49ba-99c5-5569494961e2 24520 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8a87 0xc0036f8a88}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tnwcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tnwcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.964: INFO: Pod "webserver-deployment-845c8977d9-w25q6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w25q6 webserver-deployment-845c8977d9- deployment-3742  79bcbc24-3b3f-45c4-a68d-66f720fe0085 24419 0 2022-12-14 08:44:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:58fbf3c87f6e2131626ef372ddcf2bf8a708b7d0293073f9694e26de4bbc5e8d cni.projectcalico.org/podIP:100.64.0.98/32 cni.projectcalico.org/podIPs:100.64.0.98/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8c00 0xc0036f8c01}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:44:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2w2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2w2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.58,PodIP:100.64.0.98,StartTime:2022-12-14 08:44:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:44:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec39682a42d92d12670361dc5c9744cea87d0aac09b4eff1235e0a8f880b9cd9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 08:44:47.964: INFO: Pod "webserver-deployment-845c8977d9-xs2kc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xs2kc webserver-deployment-845c8977d9- deployment-3742  a2f1274f-fb08-4150-8219-8b4b6fd20748 24529 0 2022-12-14 08:44:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 af9d6e4e-649e-4a40-a161-0bbf51449eea 0xc0036f8e10 0xc0036f8e11}] [] [{kube-controller-manager Update v1 2022-12-14 08:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af9d6e4e-649e-4a40-a161-0bbf51449eea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2jrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2jrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:44:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:44:47.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3742" for this suite. 12/14/22 08:44:47.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:47.98
Dec 14 08:44:47.980: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 08:44:47.98
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:47.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:47.997
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-17cb5414-8653-4267-bd39-0ecac38205cb 12/14/22 08:44:48.011
STEP: Creating the pod 12/14/22 08:44:48.015
Dec 14 08:44:48.025: INFO: Waiting up to 5m0s for pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f" in namespace "configmap-261" to be "running and ready"
Dec 14 08:44:48.029: INFO: Pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124297ms
Dec 14 08:44:48.029: INFO: The phase of Pod pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:44:50.034: INFO: Pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009095709s
Dec 14 08:44:50.034: INFO: The phase of Pod pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f is Running (Ready = true)
Dec 14 08:44:50.034: INFO: Pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-17cb5414-8653-4267-bd39-0ecac38205cb 12/14/22 08:44:50.047
STEP: waiting to observe update in volume 12/14/22 08:44:50.052
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 08:44:52.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-261" for this suite. 12/14/22 08:44:52.15
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":138,"skipped":2720,"failed":0}
------------------------------
• [4.175 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:47.98
    Dec 14 08:44:47.980: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 08:44:47.98
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:47.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:47.997
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-17cb5414-8653-4267-bd39-0ecac38205cb 12/14/22 08:44:48.011
    STEP: Creating the pod 12/14/22 08:44:48.015
    Dec 14 08:44:48.025: INFO: Waiting up to 5m0s for pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f" in namespace "configmap-261" to be "running and ready"
    Dec 14 08:44:48.029: INFO: Pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124297ms
    Dec 14 08:44:48.029: INFO: The phase of Pod pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:44:50.034: INFO: Pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009095709s
    Dec 14 08:44:50.034: INFO: The phase of Pod pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f is Running (Ready = true)
    Dec 14 08:44:50.034: INFO: Pod "pod-configmaps-966a8487-faf4-441d-8b12-f2f85ef0a09f" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-17cb5414-8653-4267-bd39-0ecac38205cb 12/14/22 08:44:50.047
    STEP: waiting to observe update in volume 12/14/22 08:44:50.052
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 08:44:52.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-261" for this suite. 12/14/22 08:44:52.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:52.156
Dec 14 08:44:52.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:44:52.156
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:52.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:52.171
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:44:52.184
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:44:52.428
STEP: Deploying the webhook pod 12/14/22 08:44:52.434
STEP: Wait for the deployment to be ready 12/14/22 08:44:52.449
Dec 14 08:44:52.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:44:54.47
STEP: Verifying the service has paired with the endpoint 12/14/22 08:44:54.481
Dec 14 08:44:55.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/14/22 08:44:55.487
STEP: create a pod that should be updated by the webhook 12/14/22 08:44:55.603
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:44:55.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6509" for this suite. 12/14/22 08:44:55.722
STEP: Destroying namespace "webhook-6509-markers" for this suite. 12/14/22 08:44:55.726
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":139,"skipped":2746,"failed":0}
------------------------------
• [3.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:52.156
    Dec 14 08:44:52.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:44:52.156
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:52.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:52.171
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:44:52.184
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:44:52.428
    STEP: Deploying the webhook pod 12/14/22 08:44:52.434
    STEP: Wait for the deployment to be ready 12/14/22 08:44:52.449
    Dec 14 08:44:52.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:44:54.47
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:44:54.481
    Dec 14 08:44:55.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/14/22 08:44:55.487
    STEP: create a pod that should be updated by the webhook 12/14/22 08:44:55.603
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:44:55.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6509" for this suite. 12/14/22 08:44:55.722
    STEP: Destroying namespace "webhook-6509-markers" for this suite. 12/14/22 08:44:55.726
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:55.754
Dec 14 08:44:55.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context 12/14/22 08:44:55.755
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:55.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:55.772
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 08:44:55.779
Dec 14 08:44:55.790: INFO: Waiting up to 5m0s for pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb" in namespace "security-context-2122" to be "Succeeded or Failed"
Dec 14 08:44:55.794: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00077ms
Dec 14 08:44:57.800: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010052803s
Dec 14 08:44:59.799: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008716583s
STEP: Saw pod success 12/14/22 08:44:59.799
Dec 14 08:44:59.799: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb" satisfied condition "Succeeded or Failed"
Dec 14 08:44:59.803: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb container test-container: <nil>
STEP: delete the pod 12/14/22 08:44:59.855
Dec 14 08:44:59.864: INFO: Waiting for pod security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb to disappear
Dec 14 08:44:59.868: INFO: Pod security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 14 08:44:59.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2122" for this suite. 12/14/22 08:44:59.874
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":140,"skipped":2766,"failed":0}
------------------------------
• [4.126 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:55.754
    Dec 14 08:44:55.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context 12/14/22 08:44:55.755
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:55.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:55.772
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 08:44:55.779
    Dec 14 08:44:55.790: INFO: Waiting up to 5m0s for pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb" in namespace "security-context-2122" to be "Succeeded or Failed"
    Dec 14 08:44:55.794: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00077ms
    Dec 14 08:44:57.800: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010052803s
    Dec 14 08:44:59.799: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008716583s
    STEP: Saw pod success 12/14/22 08:44:59.799
    Dec 14 08:44:59.799: INFO: Pod "security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb" satisfied condition "Succeeded or Failed"
    Dec 14 08:44:59.803: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb container test-container: <nil>
    STEP: delete the pod 12/14/22 08:44:59.855
    Dec 14 08:44:59.864: INFO: Waiting for pod security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb to disappear
    Dec 14 08:44:59.868: INFO: Pod security-context-e3088af2-652a-435e-afd2-5d0d2136e4fb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 14 08:44:59.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2122" for this suite. 12/14/22 08:44:59.874
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:59.88
Dec 14 08:44:59.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:44:59.881
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:59.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:59.897
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 12/14/22 08:44:59.902
Dec 14 08:44:59.902: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9109 api-versions'
Dec 14 08:44:59.963: INFO: stderr: ""
Dec 14 08:44:59.963: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:44:59.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9109" for this suite. 12/14/22 08:44:59.968
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":141,"skipped":2767,"failed":0}
------------------------------
• [0.093 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:59.88
    Dec 14 08:44:59.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:44:59.881
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:59.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:44:59.897
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 12/14/22 08:44:59.902
    Dec 14 08:44:59.902: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9109 api-versions'
    Dec 14 08:44:59.963: INFO: stderr: ""
    Dec 14 08:44:59.963: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:44:59.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9109" for this suite. 12/14/22 08:44:59.968
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:44:59.973
Dec 14 08:44:59.973: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 12/14/22 08:44:59.974
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:59.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:00
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Dec 14 08:45:00.021: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2082 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 14 08:45:00.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2082" for this suite. 12/14/22 08:45:00.054
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":142,"skipped":2768,"failed":0}
------------------------------
• [0.088 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:44:59.973
    Dec 14 08:44:59.973: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 08:44:59.974
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:44:59.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:00
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Dec 14 08:45:00.021: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2082 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 14 08:45:00.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2082" for this suite. 12/14/22 08:45:00.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:00.061
Dec 14 08:45:00.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:45:00.062
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:00.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:00.08
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/14/22 08:45:00.084
Dec 14 08:45:00.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:45:02.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:45:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9305" for this suite. 12/14/22 08:45:14.206
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":143,"skipped":2778,"failed":0}
------------------------------
• [14.151 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:00.061
    Dec 14 08:45:00.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:45:00.062
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:00.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:00.08
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/14/22 08:45:00.084
    Dec 14 08:45:00.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:45:02.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:45:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9305" for this suite. 12/14/22 08:45:14.206
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:14.213
Dec 14 08:45:14.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 08:45:14.213
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:14.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:14.227
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-7f50bd07-885e-42e5-9554-f19147f27076 12/14/22 08:45:14.23
STEP: Creating a pod to test consume secrets 12/14/22 08:45:14.234
Dec 14 08:45:14.243: INFO: Waiting up to 5m0s for pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65" in namespace "secrets-1562" to be "Succeeded or Failed"
Dec 14 08:45:14.246: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.430632ms
Dec 14 08:45:16.250: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006581489s
Dec 14 08:45:18.251: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007527342s
STEP: Saw pod success 12/14/22 08:45:18.251
Dec 14 08:45:18.251: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65" satisfied condition "Succeeded or Failed"
Dec 14 08:45:18.254: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 08:45:18.263
Dec 14 08:45:18.270: INFO: Waiting for pod pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65 to disappear
Dec 14 08:45:18.273: INFO: Pod pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 08:45:18.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1562" for this suite. 12/14/22 08:45:18.277
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":144,"skipped":2782,"failed":0}
------------------------------
• [4.068 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:14.213
    Dec 14 08:45:14.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 08:45:14.213
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:14.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:14.227
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-7f50bd07-885e-42e5-9554-f19147f27076 12/14/22 08:45:14.23
    STEP: Creating a pod to test consume secrets 12/14/22 08:45:14.234
    Dec 14 08:45:14.243: INFO: Waiting up to 5m0s for pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65" in namespace "secrets-1562" to be "Succeeded or Failed"
    Dec 14 08:45:14.246: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.430632ms
    Dec 14 08:45:16.250: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006581489s
    Dec 14 08:45:18.251: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007527342s
    STEP: Saw pod success 12/14/22 08:45:18.251
    Dec 14 08:45:18.251: INFO: Pod "pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65" satisfied condition "Succeeded or Failed"
    Dec 14 08:45:18.254: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 08:45:18.263
    Dec 14 08:45:18.270: INFO: Waiting for pod pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65 to disappear
    Dec 14 08:45:18.273: INFO: Pod pod-secrets-380936b5-96c6-4e83-9fbb-634bf4921b65 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 08:45:18.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1562" for this suite. 12/14/22 08:45:18.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:18.282
Dec 14 08:45:18.282: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 12/14/22 08:45:18.282
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:18.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:18.295
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 12/14/22 08:45:18.298
STEP: getting /apis/discovery.k8s.io 12/14/22 08:45:18.301
STEP: getting /apis/discovery.k8s.iov1 12/14/22 08:45:18.303
STEP: creating 12/14/22 08:45:18.304
STEP: getting 12/14/22 08:45:18.314
STEP: listing 12/14/22 08:45:18.317
STEP: watching 12/14/22 08:45:18.32
Dec 14 08:45:18.320: INFO: starting watch
STEP: cluster-wide listing 12/14/22 08:45:18.322
STEP: cluster-wide watching 12/14/22 08:45:18.325
Dec 14 08:45:18.325: INFO: starting watch
STEP: patching 12/14/22 08:45:18.327
STEP: updating 12/14/22 08:45:18.331
Dec 14 08:45:18.337: INFO: waiting for watch events with expected annotations
Dec 14 08:45:18.337: INFO: saw patched and updated annotations
STEP: deleting 12/14/22 08:45:18.337
STEP: deleting a collection 12/14/22 08:45:18.345
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 14 08:45:18.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3321" for this suite. 12/14/22 08:45:18.361
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":145,"skipped":2803,"failed":0}
------------------------------
• [0.083 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:18.282
    Dec 14 08:45:18.282: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 12/14/22 08:45:18.282
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:18.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:18.295
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 12/14/22 08:45:18.298
    STEP: getting /apis/discovery.k8s.io 12/14/22 08:45:18.301
    STEP: getting /apis/discovery.k8s.iov1 12/14/22 08:45:18.303
    STEP: creating 12/14/22 08:45:18.304
    STEP: getting 12/14/22 08:45:18.314
    STEP: listing 12/14/22 08:45:18.317
    STEP: watching 12/14/22 08:45:18.32
    Dec 14 08:45:18.320: INFO: starting watch
    STEP: cluster-wide listing 12/14/22 08:45:18.322
    STEP: cluster-wide watching 12/14/22 08:45:18.325
    Dec 14 08:45:18.325: INFO: starting watch
    STEP: patching 12/14/22 08:45:18.327
    STEP: updating 12/14/22 08:45:18.331
    Dec 14 08:45:18.337: INFO: waiting for watch events with expected annotations
    Dec 14 08:45:18.337: INFO: saw patched and updated annotations
    STEP: deleting 12/14/22 08:45:18.337
    STEP: deleting a collection 12/14/22 08:45:18.345
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 14 08:45:18.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3321" for this suite. 12/14/22 08:45:18.361
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:18.364
Dec 14 08:45:18.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 12/14/22 08:45:18.365
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:18.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:18.377
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 12/14/22 08:45:18.38
Dec 14 08:45:18.380: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 08:45:21.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4848" for this suite. 12/14/22 08:45:21.433
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":146,"skipped":2803,"failed":0}
------------------------------
• [3.072 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:18.364
    Dec 14 08:45:18.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 12/14/22 08:45:18.365
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:18.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:18.377
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 12/14/22 08:45:18.38
    Dec 14 08:45:18.380: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 08:45:21.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4848" for this suite. 12/14/22 08:45:21.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:21.437
Dec 14 08:45:21.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 08:45:21.438
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:21.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:21.449
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-4102613e-bcc4-4431-ae8d-3bfb1c14bef5 12/14/22 08:45:21.452
STEP: Creating a pod to test consume configMaps 12/14/22 08:45:21.455
Dec 14 08:45:21.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea" in namespace "configmap-4698" to be "Succeeded or Failed"
Dec 14 08:45:21.466: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946317ms
Dec 14 08:45:23.470: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006317024s
Dec 14 08:45:25.470: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006464388s
STEP: Saw pod success 12/14/22 08:45:25.47
Dec 14 08:45:25.470: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea" satisfied condition "Succeeded or Failed"
Dec 14 08:45:25.474: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea container agnhost-container: <nil>
STEP: delete the pod 12/14/22 08:45:25.482
Dec 14 08:45:25.490: INFO: Waiting for pod pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea to disappear
Dec 14 08:45:25.492: INFO: Pod pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 08:45:25.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4698" for this suite. 12/14/22 08:45:25.496
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":147,"skipped":2843,"failed":0}
------------------------------
• [4.063 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:21.437
    Dec 14 08:45:21.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 08:45:21.438
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:21.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:21.449
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-4102613e-bcc4-4431-ae8d-3bfb1c14bef5 12/14/22 08:45:21.452
    STEP: Creating a pod to test consume configMaps 12/14/22 08:45:21.455
    Dec 14 08:45:21.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea" in namespace "configmap-4698" to be "Succeeded or Failed"
    Dec 14 08:45:21.466: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946317ms
    Dec 14 08:45:23.470: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006317024s
    Dec 14 08:45:25.470: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006464388s
    STEP: Saw pod success 12/14/22 08:45:25.47
    Dec 14 08:45:25.470: INFO: Pod "pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea" satisfied condition "Succeeded or Failed"
    Dec 14 08:45:25.474: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 08:45:25.482
    Dec 14 08:45:25.490: INFO: Waiting for pod pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea to disappear
    Dec 14 08:45:25.492: INFO: Pod pod-configmaps-e30cdb4c-02c9-46a5-8fc9-d389bea568ea no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 08:45:25.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4698" for this suite. 12/14/22 08:45:25.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:25.501
Dec 14 08:45:25.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 08:45:25.502
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:25.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:25.514
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 12/14/22 08:45:42.522
STEP: Creating a ResourceQuota 12/14/22 08:45:47.527
STEP: Ensuring resource quota status is calculated 12/14/22 08:45:47.531
STEP: Creating a ConfigMap 12/14/22 08:45:49.538
STEP: Ensuring resource quota status captures configMap creation 12/14/22 08:45:49.545
STEP: Deleting a ConfigMap 12/14/22 08:45:51.549
STEP: Ensuring resource quota status released usage 12/14/22 08:45:51.553
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 08:45:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-798" for this suite. 12/14/22 08:45:53.582
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":148,"skipped":2887,"failed":0}
------------------------------
• [28.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:25.501
    Dec 14 08:45:25.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 08:45:25.502
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:25.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:25.514
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 12/14/22 08:45:42.522
    STEP: Creating a ResourceQuota 12/14/22 08:45:47.527
    STEP: Ensuring resource quota status is calculated 12/14/22 08:45:47.531
    STEP: Creating a ConfigMap 12/14/22 08:45:49.538
    STEP: Ensuring resource quota status captures configMap creation 12/14/22 08:45:49.545
    STEP: Deleting a ConfigMap 12/14/22 08:45:51.549
    STEP: Ensuring resource quota status released usage 12/14/22 08:45:51.553
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 08:45:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-798" for this suite. 12/14/22 08:45:53.582
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:53.603
Dec 14 08:45:53.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 12/14/22 08:45:53.604
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:53.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:53.653
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 12/14/22 08:45:53.663
STEP: starting a background goroutine to produce watch events 12/14/22 08:45:53.67
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/14/22 08:45:53.67
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 14 08:45:56.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5549" for this suite. 12/14/22 08:45:56.468
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":149,"skipped":2888,"failed":0}
------------------------------
• [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:53.603
    Dec 14 08:45:53.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 12/14/22 08:45:53.604
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:53.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:53.653
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 12/14/22 08:45:53.663
    STEP: starting a background goroutine to produce watch events 12/14/22 08:45:53.67
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/14/22 08:45:53.67
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 14 08:45:56.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5549" for this suite. 12/14/22 08:45:56.468
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:56.511
Dec 14 08:45:56.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery 12/14/22 08:45:56.511
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:56.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:56.524
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 12/14/22 08:45:56.533
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Dec 14 08:45:57.079: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 14 08:45:57.081: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 14 08:45:57.081: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec 14 08:45:57.081: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 14 08:45:57.081: INFO: Checking APIGroup: apps
Dec 14 08:45:57.083: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 14 08:45:57.083: INFO: Versions found [{apps/v1 v1}]
Dec 14 08:45:57.083: INFO: apps/v1 matches apps/v1
Dec 14 08:45:57.083: INFO: Checking APIGroup: events.k8s.io
Dec 14 08:45:57.085: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 14 08:45:57.085: INFO: Versions found [{events.k8s.io/v1 v1}]
Dec 14 08:45:57.085: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 14 08:45:57.085: INFO: Checking APIGroup: authentication.k8s.io
Dec 14 08:45:57.087: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 14 08:45:57.087: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec 14 08:45:57.087: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 14 08:45:57.087: INFO: Checking APIGroup: authorization.k8s.io
Dec 14 08:45:57.089: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 14 08:45:57.089: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec 14 08:45:57.089: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 14 08:45:57.089: INFO: Checking APIGroup: autoscaling
Dec 14 08:45:57.091: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Dec 14 08:45:57.091: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Dec 14 08:45:57.091: INFO: autoscaling/v2 matches autoscaling/v2
Dec 14 08:45:57.091: INFO: Checking APIGroup: batch
Dec 14 08:45:57.093: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 14 08:45:57.093: INFO: Versions found [{batch/v1 v1}]
Dec 14 08:45:57.093: INFO: batch/v1 matches batch/v1
Dec 14 08:45:57.093: INFO: Checking APIGroup: certificates.k8s.io
Dec 14 08:45:57.094: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 14 08:45:57.094: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec 14 08:45:57.094: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 14 08:45:57.094: INFO: Checking APIGroup: networking.k8s.io
Dec 14 08:45:57.096: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 14 08:45:57.096: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec 14 08:45:57.096: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 14 08:45:57.096: INFO: Checking APIGroup: policy
Dec 14 08:45:57.097: INFO: PreferredVersion.GroupVersion: policy/v1
Dec 14 08:45:57.097: INFO: Versions found [{policy/v1 v1}]
Dec 14 08:45:57.097: INFO: policy/v1 matches policy/v1
Dec 14 08:45:57.097: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 14 08:45:57.099: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 14 08:45:57.099: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec 14 08:45:57.099: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 14 08:45:57.099: INFO: Checking APIGroup: storage.k8s.io
Dec 14 08:45:57.101: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 14 08:45:57.101: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 14 08:45:57.101: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 14 08:45:57.101: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 14 08:45:57.103: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 14 08:45:57.103: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec 14 08:45:57.103: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 14 08:45:57.103: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 14 08:45:57.105: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 14 08:45:57.105: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec 14 08:45:57.105: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 14 08:45:57.105: INFO: Checking APIGroup: scheduling.k8s.io
Dec 14 08:45:57.107: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 14 08:45:57.107: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec 14 08:45:57.107: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 14 08:45:57.107: INFO: Checking APIGroup: coordination.k8s.io
Dec 14 08:45:57.109: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 14 08:45:57.109: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec 14 08:45:57.109: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 14 08:45:57.109: INFO: Checking APIGroup: node.k8s.io
Dec 14 08:45:57.110: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 14 08:45:57.110: INFO: Versions found [{node.k8s.io/v1 v1}]
Dec 14 08:45:57.110: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 14 08:45:57.110: INFO: Checking APIGroup: discovery.k8s.io
Dec 14 08:45:57.112: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec 14 08:45:57.112: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Dec 14 08:45:57.112: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec 14 08:45:57.112: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 14 08:45:57.113: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Dec 14 08:45:57.113: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec 14 08:45:57.113: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Dec 14 08:45:57.113: INFO: Checking APIGroup: autoscaling.k8s.io
Dec 14 08:45:57.115: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Dec 14 08:45:57.115: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Dec 14 08:45:57.115: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Dec 14 08:45:57.115: INFO: Checking APIGroup: crd.projectcalico.org
Dec 14 08:45:57.117: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 14 08:45:57.117: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 14 08:45:57.117: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Dec 14 08:45:57.117: INFO: Checking APIGroup: snapshot.storage.k8s.io
Dec 14 08:45:57.118: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Dec 14 08:45:57.118: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Dec 14 08:45:57.118: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Dec 14 08:45:57.118: INFO: Checking APIGroup: cert.gardener.cloud
Dec 14 08:45:57.120: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Dec 14 08:45:57.120: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Dec 14 08:45:57.120: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Dec 14 08:45:57.120: INFO: Checking APIGroup: dns.gardener.cloud
Dec 14 08:45:57.122: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Dec 14 08:45:57.122: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Dec 14 08:45:57.122: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Dec 14 08:45:57.122: INFO: Checking APIGroup: metrics.k8s.io
Dec 14 08:45:57.123: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Dec 14 08:45:57.123: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Dec 14 08:45:57.123: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Dec 14 08:45:57.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5281" for this suite. 12/14/22 08:45:57.128
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":150,"skipped":2889,"failed":0}
------------------------------
• [0.621 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:56.511
    Dec 14 08:45:56.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename discovery 12/14/22 08:45:56.511
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:56.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:56.524
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 12/14/22 08:45:56.533
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Dec 14 08:45:57.079: INFO: Checking APIGroup: apiregistration.k8s.io
    Dec 14 08:45:57.081: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Dec 14 08:45:57.081: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Dec 14 08:45:57.081: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Dec 14 08:45:57.081: INFO: Checking APIGroup: apps
    Dec 14 08:45:57.083: INFO: PreferredVersion.GroupVersion: apps/v1
    Dec 14 08:45:57.083: INFO: Versions found [{apps/v1 v1}]
    Dec 14 08:45:57.083: INFO: apps/v1 matches apps/v1
    Dec 14 08:45:57.083: INFO: Checking APIGroup: events.k8s.io
    Dec 14 08:45:57.085: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Dec 14 08:45:57.085: INFO: Versions found [{events.k8s.io/v1 v1}]
    Dec 14 08:45:57.085: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Dec 14 08:45:57.085: INFO: Checking APIGroup: authentication.k8s.io
    Dec 14 08:45:57.087: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Dec 14 08:45:57.087: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Dec 14 08:45:57.087: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Dec 14 08:45:57.087: INFO: Checking APIGroup: authorization.k8s.io
    Dec 14 08:45:57.089: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Dec 14 08:45:57.089: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Dec 14 08:45:57.089: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Dec 14 08:45:57.089: INFO: Checking APIGroup: autoscaling
    Dec 14 08:45:57.091: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Dec 14 08:45:57.091: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Dec 14 08:45:57.091: INFO: autoscaling/v2 matches autoscaling/v2
    Dec 14 08:45:57.091: INFO: Checking APIGroup: batch
    Dec 14 08:45:57.093: INFO: PreferredVersion.GroupVersion: batch/v1
    Dec 14 08:45:57.093: INFO: Versions found [{batch/v1 v1}]
    Dec 14 08:45:57.093: INFO: batch/v1 matches batch/v1
    Dec 14 08:45:57.093: INFO: Checking APIGroup: certificates.k8s.io
    Dec 14 08:45:57.094: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Dec 14 08:45:57.094: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Dec 14 08:45:57.094: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Dec 14 08:45:57.094: INFO: Checking APIGroup: networking.k8s.io
    Dec 14 08:45:57.096: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Dec 14 08:45:57.096: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Dec 14 08:45:57.096: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Dec 14 08:45:57.096: INFO: Checking APIGroup: policy
    Dec 14 08:45:57.097: INFO: PreferredVersion.GroupVersion: policy/v1
    Dec 14 08:45:57.097: INFO: Versions found [{policy/v1 v1}]
    Dec 14 08:45:57.097: INFO: policy/v1 matches policy/v1
    Dec 14 08:45:57.097: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Dec 14 08:45:57.099: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Dec 14 08:45:57.099: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Dec 14 08:45:57.099: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Dec 14 08:45:57.099: INFO: Checking APIGroup: storage.k8s.io
    Dec 14 08:45:57.101: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Dec 14 08:45:57.101: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Dec 14 08:45:57.101: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Dec 14 08:45:57.101: INFO: Checking APIGroup: admissionregistration.k8s.io
    Dec 14 08:45:57.103: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Dec 14 08:45:57.103: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Dec 14 08:45:57.103: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Dec 14 08:45:57.103: INFO: Checking APIGroup: apiextensions.k8s.io
    Dec 14 08:45:57.105: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Dec 14 08:45:57.105: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Dec 14 08:45:57.105: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Dec 14 08:45:57.105: INFO: Checking APIGroup: scheduling.k8s.io
    Dec 14 08:45:57.107: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Dec 14 08:45:57.107: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Dec 14 08:45:57.107: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Dec 14 08:45:57.107: INFO: Checking APIGroup: coordination.k8s.io
    Dec 14 08:45:57.109: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Dec 14 08:45:57.109: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Dec 14 08:45:57.109: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Dec 14 08:45:57.109: INFO: Checking APIGroup: node.k8s.io
    Dec 14 08:45:57.110: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Dec 14 08:45:57.110: INFO: Versions found [{node.k8s.io/v1 v1}]
    Dec 14 08:45:57.110: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Dec 14 08:45:57.110: INFO: Checking APIGroup: discovery.k8s.io
    Dec 14 08:45:57.112: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Dec 14 08:45:57.112: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Dec 14 08:45:57.112: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Dec 14 08:45:57.112: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Dec 14 08:45:57.113: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Dec 14 08:45:57.113: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Dec 14 08:45:57.113: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Dec 14 08:45:57.113: INFO: Checking APIGroup: autoscaling.k8s.io
    Dec 14 08:45:57.115: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
    Dec 14 08:45:57.115: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
    Dec 14 08:45:57.115: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
    Dec 14 08:45:57.115: INFO: Checking APIGroup: crd.projectcalico.org
    Dec 14 08:45:57.117: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Dec 14 08:45:57.117: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Dec 14 08:45:57.117: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Dec 14 08:45:57.117: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Dec 14 08:45:57.118: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Dec 14 08:45:57.118: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Dec 14 08:45:57.118: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Dec 14 08:45:57.118: INFO: Checking APIGroup: cert.gardener.cloud
    Dec 14 08:45:57.120: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
    Dec 14 08:45:57.120: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
    Dec 14 08:45:57.120: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
    Dec 14 08:45:57.120: INFO: Checking APIGroup: dns.gardener.cloud
    Dec 14 08:45:57.122: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
    Dec 14 08:45:57.122: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
    Dec 14 08:45:57.122: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
    Dec 14 08:45:57.122: INFO: Checking APIGroup: metrics.k8s.io
    Dec 14 08:45:57.123: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Dec 14 08:45:57.123: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Dec 14 08:45:57.123: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Dec 14 08:45:57.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-5281" for this suite. 12/14/22 08:45:57.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:45:57.133
Dec 14 08:45:57.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod 12/14/22 08:45:57.133
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:57.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:57.146
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Dec 14 08:45:57.150: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 08:46:57.185: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Dec 14 08:46:57.189: INFO: Starting informer...
STEP: Starting pod... 12/14/22 08:46:57.189
Dec 14 08:46:57.414: INFO: Pod is running on shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb. Tainting Node
STEP: Trying to apply a taint on the Node 12/14/22 08:46:57.414
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:46:57.427
STEP: Waiting short time to make sure Pod is queued for deletion 12/14/22 08:46:57.431
Dec 14 08:46:57.431: INFO: Pod wasn't evicted. Proceeding
Dec 14 08:46:57.431: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:46:57.446
STEP: Waiting some time to make sure that toleration time passed. 12/14/22 08:46:57.449
Dec 14 08:48:12.450: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Dec 14 08:48:12.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1648" for this suite. 12/14/22 08:48:12.457
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":151,"skipped":2932,"failed":0}
------------------------------
• [135.329 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:45:57.133
    Dec 14 08:45:57.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-single-pod 12/14/22 08:45:57.133
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:45:57.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:45:57.146
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Dec 14 08:45:57.150: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 08:46:57.185: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Dec 14 08:46:57.189: INFO: Starting informer...
    STEP: Starting pod... 12/14/22 08:46:57.189
    Dec 14 08:46:57.414: INFO: Pod is running on shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb. Tainting Node
    STEP: Trying to apply a taint on the Node 12/14/22 08:46:57.414
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:46:57.427
    STEP: Waiting short time to make sure Pod is queued for deletion 12/14/22 08:46:57.431
    Dec 14 08:46:57.431: INFO: Pod wasn't evicted. Proceeding
    Dec 14 08:46:57.431: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 08:46:57.446
    STEP: Waiting some time to make sure that toleration time passed. 12/14/22 08:46:57.449
    Dec 14 08:48:12.450: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 08:48:12.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-1648" for this suite. 12/14/22 08:48:12.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:48:12.462
Dec 14 08:48:12.462: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:48:12.463
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:12.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:12.477
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 12/14/22 08:48:12.482
Dec 14 08:48:12.491: INFO: Waiting up to 5m0s for pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b" in namespace "downward-api-879" to be "Succeeded or Failed"
Dec 14 08:48:12.494: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504121ms
Dec 14 08:48:14.500: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008976871s
Dec 14 08:48:16.501: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009962913s
STEP: Saw pod success 12/14/22 08:48:16.501
Dec 14 08:48:16.501: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b" satisfied condition "Succeeded or Failed"
Dec 14 08:48:16.505: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b container dapi-container: <nil>
STEP: delete the pod 12/14/22 08:48:16.515
Dec 14 08:48:16.523: INFO: Waiting for pod downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b to disappear
Dec 14 08:48:16.527: INFO: Pod downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 14 08:48:16.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-879" for this suite. 12/14/22 08:48:16.533
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":152,"skipped":2942,"failed":0}
------------------------------
• [4.075 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:48:12.462
    Dec 14 08:48:12.462: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:48:12.463
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:12.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:12.477
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 12/14/22 08:48:12.482
    Dec 14 08:48:12.491: INFO: Waiting up to 5m0s for pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b" in namespace "downward-api-879" to be "Succeeded or Failed"
    Dec 14 08:48:12.494: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504121ms
    Dec 14 08:48:14.500: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008976871s
    Dec 14 08:48:16.501: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009962913s
    STEP: Saw pod success 12/14/22 08:48:16.501
    Dec 14 08:48:16.501: INFO: Pod "downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b" satisfied condition "Succeeded or Failed"
    Dec 14 08:48:16.505: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b container dapi-container: <nil>
    STEP: delete the pod 12/14/22 08:48:16.515
    Dec 14 08:48:16.523: INFO: Waiting for pod downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b to disappear
    Dec 14 08:48:16.527: INFO: Pod downward-api-56f5f611-52f6-4d07-92e4-584f75258a6b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 14 08:48:16.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-879" for this suite. 12/14/22 08:48:16.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:48:16.538
Dec 14 08:48:16.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:48:16.538
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:16.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:16.552
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 08:48:16.556
Dec 14 08:48:16.565: INFO: Waiting up to 5m0s for pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88" in namespace "emptydir-4749" to be "Succeeded or Failed"
Dec 14 08:48:16.568: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935919ms
Dec 14 08:48:18.574: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008785927s
Dec 14 08:48:20.573: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007771982s
STEP: Saw pod success 12/14/22 08:48:20.573
Dec 14 08:48:20.573: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88" satisfied condition "Succeeded or Failed"
Dec 14 08:48:20.576: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-6bbb2647-ccce-4032-879f-7703394b2a88 container test-container: <nil>
STEP: delete the pod 12/14/22 08:48:20.587
Dec 14 08:48:20.594: INFO: Waiting for pod pod-6bbb2647-ccce-4032-879f-7703394b2a88 to disappear
Dec 14 08:48:20.597: INFO: Pod pod-6bbb2647-ccce-4032-879f-7703394b2a88 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:48:20.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4749" for this suite. 12/14/22 08:48:20.602
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":153,"skipped":2953,"failed":0}
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:48:16.538
    Dec 14 08:48:16.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:48:16.538
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:16.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:16.552
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 08:48:16.556
    Dec 14 08:48:16.565: INFO: Waiting up to 5m0s for pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88" in namespace "emptydir-4749" to be "Succeeded or Failed"
    Dec 14 08:48:16.568: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935919ms
    Dec 14 08:48:18.574: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008785927s
    Dec 14 08:48:20.573: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007771982s
    STEP: Saw pod success 12/14/22 08:48:20.573
    Dec 14 08:48:20.573: INFO: Pod "pod-6bbb2647-ccce-4032-879f-7703394b2a88" satisfied condition "Succeeded or Failed"
    Dec 14 08:48:20.576: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-6bbb2647-ccce-4032-879f-7703394b2a88 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:48:20.587
    Dec 14 08:48:20.594: INFO: Waiting for pod pod-6bbb2647-ccce-4032-879f-7703394b2a88 to disappear
    Dec 14 08:48:20.597: INFO: Pod pod-6bbb2647-ccce-4032-879f-7703394b2a88 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:48:20.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4749" for this suite. 12/14/22 08:48:20.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:48:20.607
Dec 14 08:48:20.607: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 12/14/22 08:48:20.608
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:20.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:20.622
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439 12/14/22 08:48:20.627
Dec 14 08:48:20.635: INFO: Pod name my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439: Found 0 pods out of 1
Dec 14 08:48:25.639: INFO: Pod name my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439: Found 1 pods out of 1
Dec 14 08:48:25.639: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439" are running
Dec 14 08:48:25.639: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb" in namespace "replication-controller-4196" to be "running"
Dec 14 08:48:25.642: INFO: Pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb": Phase="Running", Reason="", readiness=true. Elapsed: 3.335002ms
Dec 14 08:48:25.642: INFO: Pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb" satisfied condition "running"
Dec 14 08:48:25.642: INFO: Pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:20 +0000 UTC Reason: Message:}])
Dec 14 08:48:25.642: INFO: Trying to dial the pod
Dec 14 08:48:30.755: INFO: Controller my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439: Got expected result from replica 1 [my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb]: "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 14 08:48:30.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4196" for this suite. 12/14/22 08:48:30.761
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":154,"skipped":2974,"failed":0}
------------------------------
• [10.159 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:48:20.607
    Dec 14 08:48:20.607: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 12/14/22 08:48:20.608
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:20.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:20.622
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439 12/14/22 08:48:20.627
    Dec 14 08:48:20.635: INFO: Pod name my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439: Found 0 pods out of 1
    Dec 14 08:48:25.639: INFO: Pod name my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439: Found 1 pods out of 1
    Dec 14 08:48:25.639: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439" are running
    Dec 14 08:48:25.639: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb" in namespace "replication-controller-4196" to be "running"
    Dec 14 08:48:25.642: INFO: Pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb": Phase="Running", Reason="", readiness=true. Elapsed: 3.335002ms
    Dec 14 08:48:25.642: INFO: Pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb" satisfied condition "running"
    Dec 14 08:48:25.642: INFO: Pod "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 08:48:20 +0000 UTC Reason: Message:}])
    Dec 14 08:48:25.642: INFO: Trying to dial the pod
    Dec 14 08:48:30.755: INFO: Controller my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439: Got expected result from replica 1 [my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb]: "my-hostname-basic-e90f833d-3798-4ca4-83a5-9fe925f49439-97wrb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 14 08:48:30.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4196" for this suite. 12/14/22 08:48:30.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:48:30.768
Dec 14 08:48:30.768: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 08:48:30.768
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:30.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:30.784
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 12/14/22 08:48:30.789
Dec 14 08:48:30.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1792 create -f -'
Dec 14 08:48:31.321: INFO: stderr: ""
Dec 14 08:48:31.321: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 12/14/22 08:48:31.321
Dec 14 08:48:31.321: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1792 diff -f -'
Dec 14 08:48:31.865: INFO: rc: 1
Dec 14 08:48:31.865: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1792 delete -f -'
Dec 14 08:48:31.939: INFO: stderr: ""
Dec 14 08:48:31.939: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 08:48:31.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1792" for this suite. 12/14/22 08:48:31.944
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":155,"skipped":3009,"failed":0}
------------------------------
• [1.181 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:48:30.768
    Dec 14 08:48:30.768: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 08:48:30.768
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:30.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:30.784
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 12/14/22 08:48:30.789
    Dec 14 08:48:30.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1792 create -f -'
    Dec 14 08:48:31.321: INFO: stderr: ""
    Dec 14 08:48:31.321: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 12/14/22 08:48:31.321
    Dec 14 08:48:31.321: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1792 diff -f -'
    Dec 14 08:48:31.865: INFO: rc: 1
    Dec 14 08:48:31.865: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1792 delete -f -'
    Dec 14 08:48:31.939: INFO: stderr: ""
    Dec 14 08:48:31.939: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 08:48:31.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1792" for this suite. 12/14/22 08:48:31.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:48:31.949
Dec 14 08:48:31.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:48:31.95
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:31.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:31.964
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 12/14/22 08:48:31.969
Dec 14 08:48:31.979: INFO: Waiting up to 5m0s for pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704" in namespace "downward-api-924" to be "running and ready"
Dec 14 08:48:31.985: INFO: Pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704": Phase="Pending", Reason="", readiness=false. Elapsed: 6.531948ms
Dec 14 08:48:31.985: INFO: The phase of Pod labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:48:33.990: INFO: Pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704": Phase="Running", Reason="", readiness=true. Elapsed: 2.011380312s
Dec 14 08:48:33.990: INFO: The phase of Pod labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704 is Running (Ready = true)
Dec 14 08:48:33.990: INFO: Pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704" satisfied condition "running and ready"
Dec 14 08:48:34.515: INFO: Successfully updated pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 08:48:38.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-924" for this suite. 12/14/22 08:48:38.559
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":156,"skipped":3026,"failed":0}
------------------------------
• [6.615 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:48:31.949
    Dec 14 08:48:31.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:48:31.95
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:31.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:31.964
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 12/14/22 08:48:31.969
    Dec 14 08:48:31.979: INFO: Waiting up to 5m0s for pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704" in namespace "downward-api-924" to be "running and ready"
    Dec 14 08:48:31.985: INFO: Pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704": Phase="Pending", Reason="", readiness=false. Elapsed: 6.531948ms
    Dec 14 08:48:31.985: INFO: The phase of Pod labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:48:33.990: INFO: Pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704": Phase="Running", Reason="", readiness=true. Elapsed: 2.011380312s
    Dec 14 08:48:33.990: INFO: The phase of Pod labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704 is Running (Ready = true)
    Dec 14 08:48:33.990: INFO: Pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704" satisfied condition "running and ready"
    Dec 14 08:48:34.515: INFO: Successfully updated pod "labelsupdate3dcc353c-f460-4007-9d93-845c4bbec704"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 08:48:38.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-924" for this suite. 12/14/22 08:48:38.559
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:48:38.564
Dec 14 08:48:38.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:48:38.565
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:38.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:38.582
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/14/22 08:48:38.587
Dec 14 08:48:38.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/14/22 08:48:50.677
Dec 14 08:48:50.677: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 08:48:54.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:49:06.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8144" for this suite. 12/14/22 08:49:06.598
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":157,"skipped":3026,"failed":0}
------------------------------
• [28.038 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:48:38.564
    Dec 14 08:48:38.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 08:48:38.565
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:48:38.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:48:38.582
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/14/22 08:48:38.587
    Dec 14 08:48:38.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/14/22 08:48:50.677
    Dec 14 08:48:50.677: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 08:48:54.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:49:06.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8144" for this suite. 12/14/22 08:49:06.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:06.603
Dec 14 08:49:06.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 12/14/22 08:49:06.604
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:06.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:06.616
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Dec 14 08:49:06.628: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e" in namespace "security-context-test-6006" to be "Succeeded or Failed"
Dec 14 08:49:06.631: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.95528ms
Dec 14 08:49:08.636: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008202236s
Dec 14 08:49:10.636: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007501572s
Dec 14 08:49:10.636: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 14 08:49:10.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6006" for this suite. 12/14/22 08:49:10.641
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":3039,"failed":0}
------------------------------
• [4.042 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:06.603
    Dec 14 08:49:06.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 12/14/22 08:49:06.604
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:06.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:06.616
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Dec 14 08:49:06.628: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e" in namespace "security-context-test-6006" to be "Succeeded or Failed"
    Dec 14 08:49:06.631: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.95528ms
    Dec 14 08:49:08.636: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008202236s
    Dec 14 08:49:10.636: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007501572s
    Dec 14 08:49:10.636: INFO: Pod "busybox-user-65534-8d4b04d8-16c6-4af2-b41b-de900958340e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 14 08:49:10.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6006" for this suite. 12/14/22 08:49:10.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:10.646
Dec 14 08:49:10.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:49:10.647
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:10.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:10.66
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 14 08:49:10.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4933" for this suite. 12/14/22 08:49:10.678
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":159,"skipped":3061,"failed":0}
------------------------------
• [0.036 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:10.646
    Dec 14 08:49:10.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 08:49:10.647
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:10.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:10.66
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 14 08:49:10.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4933" for this suite. 12/14/22 08:49:10.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:10.683
Dec 14 08:49:10.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 12/14/22 08:49:10.683
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:10.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:10.695
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 08:49:10.699
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-x5wr 12/14/22 08:49:10.705
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 08:49:10.705
Dec 14 08:49:10.715: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x5wr" in namespace "subpath-968" to be "Succeeded or Failed"
Dec 14 08:49:10.717: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552241ms
Dec 14 08:49:12.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 2.007172313s
Dec 14 08:49:14.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 4.007160379s
Dec 14 08:49:16.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 6.006987133s
Dec 14 08:49:18.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 8.007572541s
Dec 14 08:49:20.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 10.008447094s
Dec 14 08:49:22.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 12.008462014s
Dec 14 08:49:24.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 14.006703748s
Dec 14 08:49:26.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 16.008036361s
Dec 14 08:49:28.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 18.007623062s
Dec 14 08:49:30.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 20.00763475s
Dec 14 08:49:32.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=false. Elapsed: 22.007668205s
Dec 14 08:49:34.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007115017s
STEP: Saw pod success 12/14/22 08:49:34.722
Dec 14 08:49:34.722: INFO: Pod "pod-subpath-test-configmap-x5wr" satisfied condition "Succeeded or Failed"
Dec 14 08:49:34.725: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-configmap-x5wr container test-container-subpath-configmap-x5wr: <nil>
STEP: delete the pod 12/14/22 08:49:34.737
Dec 14 08:49:34.744: INFO: Waiting for pod pod-subpath-test-configmap-x5wr to disappear
Dec 14 08:49:34.746: INFO: Pod pod-subpath-test-configmap-x5wr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-x5wr 12/14/22 08:49:34.746
Dec 14 08:49:34.747: INFO: Deleting pod "pod-subpath-test-configmap-x5wr" in namespace "subpath-968"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 14 08:49:34.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-968" for this suite. 12/14/22 08:49:34.753
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":160,"skipped":3083,"failed":0}
------------------------------
• [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:10.683
    Dec 14 08:49:10.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 12/14/22 08:49:10.683
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:10.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:10.695
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 08:49:10.699
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-x5wr 12/14/22 08:49:10.705
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 08:49:10.705
    Dec 14 08:49:10.715: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x5wr" in namespace "subpath-968" to be "Succeeded or Failed"
    Dec 14 08:49:10.717: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552241ms
    Dec 14 08:49:12.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 2.007172313s
    Dec 14 08:49:14.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 4.007160379s
    Dec 14 08:49:16.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 6.006987133s
    Dec 14 08:49:18.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 8.007572541s
    Dec 14 08:49:20.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 10.008447094s
    Dec 14 08:49:22.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 12.008462014s
    Dec 14 08:49:24.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 14.006703748s
    Dec 14 08:49:26.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 16.008036361s
    Dec 14 08:49:28.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 18.007623062s
    Dec 14 08:49:30.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=true. Elapsed: 20.00763475s
    Dec 14 08:49:32.723: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Running", Reason="", readiness=false. Elapsed: 22.007668205s
    Dec 14 08:49:34.722: INFO: Pod "pod-subpath-test-configmap-x5wr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007115017s
    STEP: Saw pod success 12/14/22 08:49:34.722
    Dec 14 08:49:34.722: INFO: Pod "pod-subpath-test-configmap-x5wr" satisfied condition "Succeeded or Failed"
    Dec 14 08:49:34.725: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-configmap-x5wr container test-container-subpath-configmap-x5wr: <nil>
    STEP: delete the pod 12/14/22 08:49:34.737
    Dec 14 08:49:34.744: INFO: Waiting for pod pod-subpath-test-configmap-x5wr to disappear
    Dec 14 08:49:34.746: INFO: Pod pod-subpath-test-configmap-x5wr no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-x5wr 12/14/22 08:49:34.746
    Dec 14 08:49:34.747: INFO: Deleting pod "pod-subpath-test-configmap-x5wr" in namespace "subpath-968"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 14 08:49:34.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-968" for this suite. 12/14/22 08:49:34.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:34.757
Dec 14 08:49:34.757: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 08:49:34.758
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:34.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:34.769
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 12/14/22 08:49:34.778
STEP: watching for Pod to be ready 12/14/22 08:49:34.787
Dec 14 08:49:34.789: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec 14 08:49:34.790: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
Dec 14 08:49:34.800: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
Dec 14 08:49:35.209: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
Dec 14 08:49:35.992: INFO: Found Pod pod-test in namespace pods-8095 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 12/14/22 08:49:35.995
STEP: getting the Pod and ensuring that it's patched 12/14/22 08:49:36.002
STEP: replacing the Pod's status Ready condition to False 12/14/22 08:49:36.004
STEP: check the Pod again to ensure its Ready conditions are False 12/14/22 08:49:36.013
STEP: deleting the Pod via a Collection with a LabelSelector 12/14/22 08:49:36.013
STEP: watching for the Pod to be deleted 12/14/22 08:49:36.019
Dec 14 08:49:36.020: INFO: observed event type MODIFIED
Dec 14 08:49:37.994: INFO: observed event type MODIFIED
Dec 14 08:49:38.141: INFO: observed event type MODIFIED
Dec 14 08:49:38.998: INFO: observed event type MODIFIED
Dec 14 08:49:39.003: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 08:49:39.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8095" for this suite. 12/14/22 08:49:39.013
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":161,"skipped":3097,"failed":0}
------------------------------
• [4.260 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:34.757
    Dec 14 08:49:34.757: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 08:49:34.758
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:34.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:34.769
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 12/14/22 08:49:34.778
    STEP: watching for Pod to be ready 12/14/22 08:49:34.787
    Dec 14 08:49:34.789: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Dec 14 08:49:34.790: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
    Dec 14 08:49:34.800: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
    Dec 14 08:49:35.209: INFO: observed Pod pod-test in namespace pods-8095 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
    Dec 14 08:49:35.992: INFO: Found Pod pod-test in namespace pods-8095 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 08:49:34 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 12/14/22 08:49:35.995
    STEP: getting the Pod and ensuring that it's patched 12/14/22 08:49:36.002
    STEP: replacing the Pod's status Ready condition to False 12/14/22 08:49:36.004
    STEP: check the Pod again to ensure its Ready conditions are False 12/14/22 08:49:36.013
    STEP: deleting the Pod via a Collection with a LabelSelector 12/14/22 08:49:36.013
    STEP: watching for the Pod to be deleted 12/14/22 08:49:36.019
    Dec 14 08:49:36.020: INFO: observed event type MODIFIED
    Dec 14 08:49:37.994: INFO: observed event type MODIFIED
    Dec 14 08:49:38.141: INFO: observed event type MODIFIED
    Dec 14 08:49:38.998: INFO: observed event type MODIFIED
    Dec 14 08:49:39.003: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 08:49:39.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8095" for this suite. 12/14/22 08:49:39.013
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:39.017
Dec 14 08:49:39.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 12/14/22 08:49:39.018
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:39.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:39.03
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/14/22 08:49:39.033
Dec 14 08:49:39.040: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4597" to be "running and ready"
Dec 14 08:49:39.045: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88146ms
Dec 14 08:49:39.045: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 14 08:49:41.050: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009781884s
Dec 14 08:49:41.050: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Dec 14 08:49:41.050: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 12/14/22 08:49:41.053
STEP: Then the orphan pod is adopted 12/14/22 08:49:41.057
STEP: When the matched label of one of its pods change 12/14/22 08:49:42.064
Dec 14 08:49:42.068: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 12/14/22 08:49:42.076
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 14 08:49:43.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4597" for this suite. 12/14/22 08:49:43.088
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":162,"skipped":3097,"failed":0}
------------------------------
• [4.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:39.017
    Dec 14 08:49:39.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 12/14/22 08:49:39.018
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:39.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:39.03
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/14/22 08:49:39.033
    Dec 14 08:49:39.040: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4597" to be "running and ready"
    Dec 14 08:49:39.045: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88146ms
    Dec 14 08:49:39.045: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 08:49:41.050: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009781884s
    Dec 14 08:49:41.050: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Dec 14 08:49:41.050: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 12/14/22 08:49:41.053
    STEP: Then the orphan pod is adopted 12/14/22 08:49:41.057
    STEP: When the matched label of one of its pods change 12/14/22 08:49:42.064
    Dec 14 08:49:42.068: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/14/22 08:49:42.076
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 14 08:49:43.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4597" for this suite. 12/14/22 08:49:43.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:43.092
Dec 14 08:49:43.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook 12/14/22 08:49:43.093
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:43.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:43.104
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/14/22 08:49:43.108
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 08:49:43.364
STEP: Deploying the custom resource conversion webhook pod 12/14/22 08:49:43.369
STEP: Wait for the deployment to be ready 12/14/22 08:49:43.377
Dec 14 08:49:43.382: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:49:45.392
STEP: Verifying the service has paired with the endpoint 12/14/22 08:49:45.401
Dec 14 08:49:46.402: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Dec 14 08:49:46.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource 12/14/22 08:49:49.202
STEP: v2 custom resource should be converted 12/14/22 08:49:49.206
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:49:49.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4397" for this suite. 12/14/22 08:49:49.731
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":163,"skipped":3115,"failed":0}
------------------------------
• [6.698 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:43.092
    Dec 14 08:49:43.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-webhook 12/14/22 08:49:43.093
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:43.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:43.104
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/14/22 08:49:43.108
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 08:49:43.364
    STEP: Deploying the custom resource conversion webhook pod 12/14/22 08:49:43.369
    STEP: Wait for the deployment to be ready 12/14/22 08:49:43.377
    Dec 14 08:49:43.382: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:49:45.392
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:49:45.401
    Dec 14 08:49:46.402: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Dec 14 08:49:46.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating a v1 custom resource 12/14/22 08:49:49.202
    STEP: v2 custom resource should be converted 12/14/22 08:49:49.206
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:49:49.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-4397" for this suite. 12/14/22 08:49:49.731
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:49.794
Dec 14 08:49:49.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 08:49:49.794
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:49.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:49.808
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 12/14/22 08:49:49.816
Dec 14 08:49:49.816: INFO: Creating simple deployment test-deployment-47wvl
Dec 14 08:49:49.826: INFO: deployment "test-deployment-47wvl" doesn't have the required revision set
STEP: Getting /status 12/14/22 08:49:51.837
Dec 14 08:49:51.840: INFO: Deployment test-deployment-47wvl has Conditions: [{Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 12/14/22 08:49:51.84
Dec 14 08:49:51.847: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 49, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 49, 49, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-47wvl-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 12/14/22 08:49:51.847
Dec 14 08:49:51.849: INFO: Observed &Deployment event: ADDED
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47wvl-777898ffcc" is progressing.}
Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
Dec 14 08:49:51.849: INFO: Found Deployment test-deployment-47wvl in namespace deployment-8630 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 08:49:51.849: INFO: Deployment test-deployment-47wvl has an updated status
STEP: patching the Statefulset Status 12/14/22 08:49:51.849
Dec 14 08:49:51.849: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 08:49:51.854: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 12/14/22 08:49:51.854
Dec 14 08:49:51.856: INFO: Observed &Deployment event: ADDED
Dec 14 08:49:51.856: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47wvl-777898ffcc" is progressing.}
Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
Dec 14 08:49:51.857: INFO: Found deployment test-deployment-47wvl in namespace deployment-8630 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec 14 08:49:51.857: INFO: Deployment test-deployment-47wvl has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 08:49:51.859: INFO: Deployment "test-deployment-47wvl":
&Deployment{ObjectMeta:{test-deployment-47wvl  deployment-8630  d7b02962-1094-45d8-a4d0-0486c329216b 26984 1 2022-12-14 08:49:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-14 08:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001c1cdc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-47wvl-777898ffcc",LastUpdateTime:2022-12-14 08:49:51 +0000 UTC,LastTransitionTime:2022-12-14 08:49:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 08:49:51.862: INFO: New ReplicaSet "test-deployment-47wvl-777898ffcc" of Deployment "test-deployment-47wvl":
&ReplicaSet{ObjectMeta:{test-deployment-47wvl-777898ffcc  deployment-8630  56cdf0b5-1416-43f7-8979-515ef5091b19 26980 1 2022-12-14 08:49:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-47wvl d7b02962-1094-45d8-a4d0-0486c329216b 0xc004ffdf50 0xc004ffdf51}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7b02962-1094-45d8-a4d0-0486c329216b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ffdff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 08:49:51.865: INFO: Pod "test-deployment-47wvl-777898ffcc-fnwn6" is available:
&Pod{ObjectMeta:{test-deployment-47wvl-777898ffcc-fnwn6 test-deployment-47wvl-777898ffcc- deployment-8630  1da7f670-342d-4ae3-96c5-cdfd9b496aab 26979 0 2022-12-14 08:49:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:a0867e39a26ec2a3670f549dcca43bf31ba4f256557865aee0ffc10c9cb4c20e cni.projectcalico.org/podIP:100.64.1.242/32 cni.projectcalico.org/podIPs:100.64.1.242/32] [{apps/v1 ReplicaSet test-deployment-47wvl-777898ffcc 56cdf0b5-1416-43f7-8979-515ef5091b19 0xc003726ff0 0xc003726ff1}] [] [{kube-controller-manager Update v1 2022-12-14 08:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56cdf0b5-1416-43f7-8979-515ef5091b19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79n5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79n5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.242,StartTime:2022-12-14 08:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea85e46b9bbe31e4e1aba807b89d6ba36ca163b3998cc7c69f8f528fd9b486ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 08:49:51.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8630" for this suite. 12/14/22 08:49:51.868
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":164,"skipped":3176,"failed":0}
------------------------------
• [2.079 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:49.794
    Dec 14 08:49:49.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 08:49:49.794
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:49.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:49.808
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 12/14/22 08:49:49.816
    Dec 14 08:49:49.816: INFO: Creating simple deployment test-deployment-47wvl
    Dec 14 08:49:49.826: INFO: deployment "test-deployment-47wvl" doesn't have the required revision set
    STEP: Getting /status 12/14/22 08:49:51.837
    Dec 14 08:49:51.840: INFO: Deployment test-deployment-47wvl has Conditions: [{Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 12/14/22 08:49:51.84
    Dec 14 08:49:51.847: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 49, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 8, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 8, 49, 49, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-47wvl-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 12/14/22 08:49:51.847
    Dec 14 08:49:51.849: INFO: Observed &Deployment event: ADDED
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
    Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47wvl-777898ffcc" is progressing.}
    Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
    Dec 14 08:49:51.849: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 08:49:51.849: INFO: Observed Deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
    Dec 14 08:49:51.849: INFO: Found Deployment test-deployment-47wvl in namespace deployment-8630 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 08:49:51.849: INFO: Deployment test-deployment-47wvl has an updated status
    STEP: patching the Statefulset Status 12/14/22 08:49:51.849
    Dec 14 08:49:51.849: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 14 08:49:51.854: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 12/14/22 08:49:51.854
    Dec 14 08:49:51.856: INFO: Observed &Deployment event: ADDED
    Dec 14 08:49:51.856: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
    Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47wvl-777898ffcc"}
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:49 +0000 UTC 2022-12-14 08:49:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47wvl-777898ffcc" is progressing.}
    Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
    Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 08:49:51 +0000 UTC 2022-12-14 08:49:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47wvl-777898ffcc" has successfully progressed.}
    Dec 14 08:49:51.857: INFO: Observed deployment test-deployment-47wvl in namespace deployment-8630 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 08:49:51.857: INFO: Observed &Deployment event: MODIFIED
    Dec 14 08:49:51.857: INFO: Found deployment test-deployment-47wvl in namespace deployment-8630 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Dec 14 08:49:51.857: INFO: Deployment test-deployment-47wvl has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 08:49:51.859: INFO: Deployment "test-deployment-47wvl":
    &Deployment{ObjectMeta:{test-deployment-47wvl  deployment-8630  d7b02962-1094-45d8-a4d0-0486c329216b 26984 1 2022-12-14 08:49:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-14 08:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001c1cdc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-47wvl-777898ffcc",LastUpdateTime:2022-12-14 08:49:51 +0000 UTC,LastTransitionTime:2022-12-14 08:49:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 08:49:51.862: INFO: New ReplicaSet "test-deployment-47wvl-777898ffcc" of Deployment "test-deployment-47wvl":
    &ReplicaSet{ObjectMeta:{test-deployment-47wvl-777898ffcc  deployment-8630  56cdf0b5-1416-43f7-8979-515ef5091b19 26980 1 2022-12-14 08:49:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-47wvl d7b02962-1094-45d8-a4d0-0486c329216b 0xc004ffdf50 0xc004ffdf51}] [] [{kube-controller-manager Update apps/v1 2022-12-14 08:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7b02962-1094-45d8-a4d0-0486c329216b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ffdff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 08:49:51.865: INFO: Pod "test-deployment-47wvl-777898ffcc-fnwn6" is available:
    &Pod{ObjectMeta:{test-deployment-47wvl-777898ffcc-fnwn6 test-deployment-47wvl-777898ffcc- deployment-8630  1da7f670-342d-4ae3-96c5-cdfd9b496aab 26979 0 2022-12-14 08:49:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:a0867e39a26ec2a3670f549dcca43bf31ba4f256557865aee0ffc10c9cb4c20e cni.projectcalico.org/podIP:100.64.1.242/32 cni.projectcalico.org/podIPs:100.64.1.242/32] [{apps/v1 ReplicaSet test-deployment-47wvl-777898ffcc 56cdf0b5-1416-43f7-8979-515ef5091b19 0xc003726ff0 0xc003726ff1}] [] [{kube-controller-manager Update v1 2022-12-14 08:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56cdf0b5-1416-43f7-8979-515ef5091b19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 08:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 08:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79n5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79n5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 08:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.242,StartTime:2022-12-14 08:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 08:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea85e46b9bbe31e4e1aba807b89d6ba36ca163b3998cc7c69f8f528fd9b486ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 08:49:51.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8630" for this suite. 12/14/22 08:49:51.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:51.875
Dec 14 08:49:51.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 08:49:51.876
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:51.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:51.887
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-559638bb-8588-4957-8c63-6e1580e73dc6 12/14/22 08:49:51.89
STEP: Creating a pod to test consume secrets 12/14/22 08:49:51.893
Dec 14 08:49:51.901: INFO: Waiting up to 5m0s for pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b" in namespace "secrets-8640" to be "Succeeded or Failed"
Dec 14 08:49:51.906: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.948181ms
Dec 14 08:49:53.910: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009643582s
Dec 14 08:49:55.917: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015944555s
STEP: Saw pod success 12/14/22 08:49:55.917
Dec 14 08:49:55.917: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b" satisfied condition "Succeeded or Failed"
Dec 14 08:49:55.920: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-a0e13863-f379-411b-9490-96554504999b container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 08:49:55.928
Dec 14 08:49:55.935: INFO: Waiting for pod pod-secrets-a0e13863-f379-411b-9490-96554504999b to disappear
Dec 14 08:49:55.938: INFO: Pod pod-secrets-a0e13863-f379-411b-9490-96554504999b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 08:49:55.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8640" for this suite. 12/14/22 08:49:55.941
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":165,"skipped":3323,"failed":0}
------------------------------
• [4.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:51.875
    Dec 14 08:49:51.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 08:49:51.876
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:51.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:51.887
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-559638bb-8588-4957-8c63-6e1580e73dc6 12/14/22 08:49:51.89
    STEP: Creating a pod to test consume secrets 12/14/22 08:49:51.893
    Dec 14 08:49:51.901: INFO: Waiting up to 5m0s for pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b" in namespace "secrets-8640" to be "Succeeded or Failed"
    Dec 14 08:49:51.906: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.948181ms
    Dec 14 08:49:53.910: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009643582s
    Dec 14 08:49:55.917: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015944555s
    STEP: Saw pod success 12/14/22 08:49:55.917
    Dec 14 08:49:55.917: INFO: Pod "pod-secrets-a0e13863-f379-411b-9490-96554504999b" satisfied condition "Succeeded or Failed"
    Dec 14 08:49:55.920: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-a0e13863-f379-411b-9490-96554504999b container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 08:49:55.928
    Dec 14 08:49:55.935: INFO: Waiting for pod pod-secrets-a0e13863-f379-411b-9490-96554504999b to disappear
    Dec 14 08:49:55.938: INFO: Pod pod-secrets-a0e13863-f379-411b-9490-96554504999b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 08:49:55.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8640" for this suite. 12/14/22 08:49:55.941
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:49:55.945
Dec 14 08:49:55.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 08:49:55.946
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:55.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:55.956
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be in namespace container-probe-4416 12/14/22 08:49:55.959
Dec 14 08:49:55.966: INFO: Waiting up to 5m0s for pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be" in namespace "container-probe-4416" to be "not pending"
Dec 14 08:49:55.970: INFO: Pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.145423ms
Dec 14 08:49:57.974: INFO: Pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be": Phase="Running", Reason="", readiness=true. Elapsed: 2.007610128s
Dec 14 08:49:57.974: INFO: Pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be" satisfied condition "not pending"
Dec 14 08:49:57.974: INFO: Started pod busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be in namespace container-probe-4416
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:49:57.974
Dec 14 08:49:57.977: INFO: Initial restart count of pod busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be is 0
Dec 14 08:50:48.096: INFO: Restart count of pod container-probe-4416/busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be is now 1 (50.119301119s elapsed)
STEP: deleting the pod 12/14/22 08:50:48.096
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 08:50:48.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4416" for this suite. 12/14/22 08:50:48.112
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":166,"skipped":3324,"failed":0}
------------------------------
• [52.172 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:49:55.945
    Dec 14 08:49:55.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 08:49:55.946
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:49:55.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:49:55.956
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be in namespace container-probe-4416 12/14/22 08:49:55.959
    Dec 14 08:49:55.966: INFO: Waiting up to 5m0s for pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be" in namespace "container-probe-4416" to be "not pending"
    Dec 14 08:49:55.970: INFO: Pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.145423ms
    Dec 14 08:49:57.974: INFO: Pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be": Phase="Running", Reason="", readiness=true. Elapsed: 2.007610128s
    Dec 14 08:49:57.974: INFO: Pod "busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be" satisfied condition "not pending"
    Dec 14 08:49:57.974: INFO: Started pod busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be in namespace container-probe-4416
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:49:57.974
    Dec 14 08:49:57.977: INFO: Initial restart count of pod busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be is 0
    Dec 14 08:50:48.096: INFO: Restart count of pod container-probe-4416/busybox-9dfb4b27-d906-4e53-a7b2-f625eeb622be is now 1 (50.119301119s elapsed)
    STEP: deleting the pod 12/14/22 08:50:48.096
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 08:50:48.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4416" for this suite. 12/14/22 08:50:48.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:50:48.118
Dec 14 08:50:48.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 12/14/22 08:50:48.118
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:50:48.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:50:48.134
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Dec 14 08:50:48.148: INFO: Endpoints addresses: [10.243.71.5] , ports: [443]
Dec 14 08:50:48.148: INFO: EndpointSlices addresses: [10.243.71.5] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 14 08:50:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7722" for this suite. 12/14/22 08:50:48.153
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":167,"skipped":3338,"failed":0}
------------------------------
• [0.039 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:50:48.118
    Dec 14 08:50:48.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 12/14/22 08:50:48.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:50:48.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:50:48.134
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Dec 14 08:50:48.148: INFO: Endpoints addresses: [10.243.71.5] , ports: [443]
    Dec 14 08:50:48.148: INFO: EndpointSlices addresses: [10.243.71.5] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 14 08:50:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7722" for this suite. 12/14/22 08:50:48.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:50:48.158
Dec 14 08:50:48.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:50:48.158
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:50:48.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:50:48.172
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-8573 12/14/22 08:50:48.177
STEP: creating service affinity-nodeport-transition in namespace services-8573 12/14/22 08:50:48.177
STEP: creating replication controller affinity-nodeport-transition in namespace services-8573 12/14/22 08:50:48.187
I1214 08:50:48.191613    4635 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8573, replica count: 3
I1214 08:50:51.243156    4635 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 08:50:51.255: INFO: Creating new exec pod
Dec 14 08:50:51.262: INFO: Waiting up to 5m0s for pod "execpod-affinityj5bzk" in namespace "services-8573" to be "running"
Dec 14 08:50:51.265: INFO: Pod "execpod-affinityj5bzk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.444499ms
Dec 14 08:50:53.269: INFO: Pod "execpod-affinityj5bzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006988093s
Dec 14 08:50:53.269: INFO: Pod "execpod-affinityj5bzk" satisfied condition "running"
Dec 14 08:50:54.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Dec 14 08:50:54.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 14 08:50:54.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:50:54.621: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.145.188 80'
Dec 14 08:50:55.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.145.188 80\nConnection to 100.105.145.188 80 port [tcp/http] succeeded!\n"
Dec 14 08:50:55.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:50:55.034: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30638'
Dec 14 08:50:55.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30638\nConnection to 10.250.3.58 30638 port [tcp/*] succeeded!\n"
Dec 14 08:50:55.501: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:50:55.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30638'
Dec 14 08:50:55.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30638\nConnection to 10.250.3.210 30638 port [tcp/*] succeeded!\n"
Dec 14 08:50:55.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 08:50:55.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30638/ ; done'
Dec 14 08:50:56.290: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n"
Dec 14 08:50:56.290: INFO: stdout: "\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-988qr\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-988qr\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-988qr"
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-988qr
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-988qr
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-988qr
Dec 14 08:50:56.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30638/ ; done'
Dec 14 08:50:56.790: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n"
Dec 14 08:50:56.790: INFO: stdout: "\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r"
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
Dec 14 08:50:56.790: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8573, will wait for the garbage collector to delete the pods 12/14/22 08:50:56.806
Dec 14 08:50:56.865: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.961447ms
Dec 14 08:50:56.965: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.388654ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 08:50:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8573" for this suite. 12/14/22 08:50:58.782
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":168,"skipped":3349,"failed":0}
------------------------------
• [10.629 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:50:48.158
    Dec 14 08:50:48.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:50:48.158
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:50:48.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:50:48.172
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-8573 12/14/22 08:50:48.177
    STEP: creating service affinity-nodeport-transition in namespace services-8573 12/14/22 08:50:48.177
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8573 12/14/22 08:50:48.187
    I1214 08:50:48.191613    4635 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8573, replica count: 3
    I1214 08:50:51.243156    4635 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 08:50:51.255: INFO: Creating new exec pod
    Dec 14 08:50:51.262: INFO: Waiting up to 5m0s for pod "execpod-affinityj5bzk" in namespace "services-8573" to be "running"
    Dec 14 08:50:51.265: INFO: Pod "execpod-affinityj5bzk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.444499ms
    Dec 14 08:50:53.269: INFO: Pod "execpod-affinityj5bzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006988093s
    Dec 14 08:50:53.269: INFO: Pod "execpod-affinityj5bzk" satisfied condition "running"
    Dec 14 08:50:54.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Dec 14 08:50:54.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Dec 14 08:50:54.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:50:54.621: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.145.188 80'
    Dec 14 08:50:55.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.145.188 80\nConnection to 100.105.145.188 80 port [tcp/http] succeeded!\n"
    Dec 14 08:50:55.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:50:55.034: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30638'
    Dec 14 08:50:55.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30638\nConnection to 10.250.3.58 30638 port [tcp/*] succeeded!\n"
    Dec 14 08:50:55.501: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:50:55.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30638'
    Dec 14 08:50:55.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30638\nConnection to 10.250.3.210 30638 port [tcp/*] succeeded!\n"
    Dec 14 08:50:55.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 08:50:55.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30638/ ; done'
    Dec 14 08:50:56.290: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n"
    Dec 14 08:50:56.290: INFO: stdout: "\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-988qr\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-988qr\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-4hwmm\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-988qr"
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-988qr
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-988qr
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-4hwmm
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.290: INFO: Received response from host: affinity-nodeport-transition-988qr
    Dec 14 08:50:56.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8573 exec execpod-affinityj5bzk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.3.58:30638/ ; done'
    Dec 14 08:50:56.790: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.3.58:30638/\n"
    Dec 14 08:50:56.790: INFO: stdout: "\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r\naffinity-nodeport-transition-dlr7r"
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Received response from host: affinity-nodeport-transition-dlr7r
    Dec 14 08:50:56.790: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8573, will wait for the garbage collector to delete the pods 12/14/22 08:50:56.806
    Dec 14 08:50:56.865: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.961447ms
    Dec 14 08:50:56.965: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.388654ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 08:50:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8573" for this suite. 12/14/22 08:50:58.782
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:50:58.786
Dec 14 08:50:58.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 08:50:58.787
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:50:58.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:50:58.8
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 08:50:58.813
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:50:59.138
STEP: Deploying the webhook pod 12/14/22 08:50:59.143
STEP: Wait for the deployment to be ready 12/14/22 08:50:59.152
Dec 14 08:50:59.159: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 08:51:01.172
STEP: Verifying the service has paired with the endpoint 12/14/22 08:51:01.18
Dec 14 08:51:02.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 12/14/22 08:51:02.185
STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/14/22 08:51:02.311
STEP: Creating a configMap that should not be mutated 12/14/22 08:51:02.316
STEP: Patching a mutating webhook configuration's rules to include the create operation 12/14/22 08:51:02.324
STEP: Creating a configMap that should be mutated 12/14/22 08:51:02.33
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 08:51:02.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1805" for this suite. 12/14/22 08:51:02.46
STEP: Destroying namespace "webhook-1805-markers" for this suite. 12/14/22 08:51:02.464
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":169,"skipped":3349,"failed":0}
------------------------------
• [3.703 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:50:58.786
    Dec 14 08:50:58.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 08:50:58.787
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:50:58.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:50:58.8
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 08:50:58.813
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 08:50:59.138
    STEP: Deploying the webhook pod 12/14/22 08:50:59.143
    STEP: Wait for the deployment to be ready 12/14/22 08:50:59.152
    Dec 14 08:50:59.159: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 08:51:01.172
    STEP: Verifying the service has paired with the endpoint 12/14/22 08:51:01.18
    Dec 14 08:51:02.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 12/14/22 08:51:02.185
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/14/22 08:51:02.311
    STEP: Creating a configMap that should not be mutated 12/14/22 08:51:02.316
    STEP: Patching a mutating webhook configuration's rules to include the create operation 12/14/22 08:51:02.324
    STEP: Creating a configMap that should be mutated 12/14/22 08:51:02.33
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 08:51:02.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1805" for this suite. 12/14/22 08:51:02.46
    STEP: Destroying namespace "webhook-1805-markers" for this suite. 12/14/22 08:51:02.464
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:51:02.49
Dec 14 08:51:02.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 08:51:02.49
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:51:02.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:51:02.505
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f in namespace container-probe-9445 12/14/22 08:51:02.509
Dec 14 08:51:02.517: INFO: Waiting up to 5m0s for pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f" in namespace "container-probe-9445" to be "not pending"
Dec 14 08:51:02.520: INFO: Pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794698ms
Dec 14 08:51:04.525: INFO: Pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007440924s
Dec 14 08:51:04.525: INFO: Pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f" satisfied condition "not pending"
Dec 14 08:51:04.525: INFO: Started pod test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f in namespace container-probe-9445
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:51:04.525
Dec 14 08:51:04.528: INFO: Initial restart count of pod test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f is 0
STEP: deleting the pod 12/14/22 08:55:05.199
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 08:55:05.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9445" for this suite. 12/14/22 08:55:05.213
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":170,"skipped":3355,"failed":0}
------------------------------
• [242.728 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:51:02.49
    Dec 14 08:51:02.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 08:51:02.49
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:51:02.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:51:02.505
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f in namespace container-probe-9445 12/14/22 08:51:02.509
    Dec 14 08:51:02.517: INFO: Waiting up to 5m0s for pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f" in namespace "container-probe-9445" to be "not pending"
    Dec 14 08:51:02.520: INFO: Pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794698ms
    Dec 14 08:51:04.525: INFO: Pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007440924s
    Dec 14 08:51:04.525: INFO: Pod "test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f" satisfied condition "not pending"
    Dec 14 08:51:04.525: INFO: Started pod test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f in namespace container-probe-9445
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:51:04.525
    Dec 14 08:51:04.528: INFO: Initial restart count of pod test-webserver-e7f00b36-e0e2-4f5e-92a0-0b5c6da0cd5f is 0
    STEP: deleting the pod 12/14/22 08:55:05.199
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 08:55:05.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9445" for this suite. 12/14/22 08:55:05.213
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:55:05.218
Dec 14 08:55:05.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 08:55:05.219
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:55:05.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:55:05.235
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 08:55:05.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6362" for this suite. 12/14/22 08:55:05.276
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":171,"skipped":3356,"failed":0}
------------------------------
• [0.063 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:55:05.218
    Dec 14 08:55:05.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 08:55:05.219
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:55:05.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:55:05.235
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 08:55:05.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6362" for this suite. 12/14/22 08:55:05.276
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:55:05.281
Dec 14 08:55:05.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 08:55:05.282
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:55:05.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:55:05.295
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 08:55:05.3
Dec 14 08:55:05.319: INFO: Waiting up to 5m0s for pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757" in namespace "emptydir-8404" to be "Succeeded or Failed"
Dec 14 08:55:05.323: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230217ms
Dec 14 08:55:07.329: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010030547s
Dec 14 08:55:09.328: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009246446s
STEP: Saw pod success 12/14/22 08:55:09.328
Dec 14 08:55:09.328: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757" satisfied condition "Succeeded or Failed"
Dec 14 08:55:09.332: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757 container test-container: <nil>
STEP: delete the pod 12/14/22 08:55:09.345
Dec 14 08:55:09.351: INFO: Waiting for pod pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757 to disappear
Dec 14 08:55:09.355: INFO: Pod pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 08:55:09.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8404" for this suite. 12/14/22 08:55:09.36
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":172,"skipped":3358,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:55:05.281
    Dec 14 08:55:05.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 08:55:05.282
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:55:05.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:55:05.295
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 08:55:05.3
    Dec 14 08:55:05.319: INFO: Waiting up to 5m0s for pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757" in namespace "emptydir-8404" to be "Succeeded or Failed"
    Dec 14 08:55:05.323: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230217ms
    Dec 14 08:55:07.329: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010030547s
    Dec 14 08:55:09.328: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009246446s
    STEP: Saw pod success 12/14/22 08:55:09.328
    Dec 14 08:55:09.328: INFO: Pod "pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757" satisfied condition "Succeeded or Failed"
    Dec 14 08:55:09.332: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757 container test-container: <nil>
    STEP: delete the pod 12/14/22 08:55:09.345
    Dec 14 08:55:09.351: INFO: Waiting for pod pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757 to disappear
    Dec 14 08:55:09.355: INFO: Pod pod-f1fbb22a-0d55-4e13-9e0d-558e079a4757 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 08:55:09.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8404" for this suite. 12/14/22 08:55:09.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:55:09.365
Dec 14 08:55:09.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 08:55:09.366
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:55:09.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:55:09.382
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-d92b7310-a286-418b-9abf-68d2d6229850 in namespace container-probe-3635 12/14/22 08:55:09.387
Dec 14 08:55:09.397: INFO: Waiting up to 5m0s for pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850" in namespace "container-probe-3635" to be "not pending"
Dec 14 08:55:09.400: INFO: Pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175344ms
Dec 14 08:55:11.405: INFO: Pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850": Phase="Running", Reason="", readiness=true. Elapsed: 2.008523214s
Dec 14 08:55:11.405: INFO: Pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850" satisfied condition "not pending"
Dec 14 08:55:11.405: INFO: Started pod liveness-d92b7310-a286-418b-9abf-68d2d6229850 in namespace container-probe-3635
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:55:11.405
Dec 14 08:55:11.409: INFO: Initial restart count of pod liveness-d92b7310-a286-418b-9abf-68d2d6229850 is 0
STEP: deleting the pod 12/14/22 08:59:12.085
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 08:59:12.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3635" for this suite. 12/14/22 08:59:12.1
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":173,"skipped":3366,"failed":0}
------------------------------
• [242.738 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:55:09.365
    Dec 14 08:55:09.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 08:55:09.366
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:55:09.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:55:09.382
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-d92b7310-a286-418b-9abf-68d2d6229850 in namespace container-probe-3635 12/14/22 08:55:09.387
    Dec 14 08:55:09.397: INFO: Waiting up to 5m0s for pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850" in namespace "container-probe-3635" to be "not pending"
    Dec 14 08:55:09.400: INFO: Pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175344ms
    Dec 14 08:55:11.405: INFO: Pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850": Phase="Running", Reason="", readiness=true. Elapsed: 2.008523214s
    Dec 14 08:55:11.405: INFO: Pod "liveness-d92b7310-a286-418b-9abf-68d2d6229850" satisfied condition "not pending"
    Dec 14 08:55:11.405: INFO: Started pod liveness-d92b7310-a286-418b-9abf-68d2d6229850 in namespace container-probe-3635
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 08:55:11.405
    Dec 14 08:55:11.409: INFO: Initial restart count of pod liveness-d92b7310-a286-418b-9abf-68d2d6229850 is 0
    STEP: deleting the pod 12/14/22 08:59:12.085
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 08:59:12.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3635" for this suite. 12/14/22 08:59:12.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:59:12.104
Dec 14 08:59:12.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 12/14/22 08:59:12.104
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:12.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:12.122
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 12/14/22 08:59:12.127
STEP: getting /apis/node.k8s.io 12/14/22 08:59:12.132
STEP: getting /apis/node.k8s.io/v1 12/14/22 08:59:12.134
STEP: creating 12/14/22 08:59:12.136
STEP: watching 12/14/22 08:59:12.149
Dec 14 08:59:12.149: INFO: starting watch
STEP: getting 12/14/22 08:59:12.156
STEP: listing 12/14/22 08:59:12.159
STEP: patching 12/14/22 08:59:12.163
STEP: updating 12/14/22 08:59:12.168
Dec 14 08:59:12.173: INFO: waiting for watch events with expected annotations
STEP: deleting 12/14/22 08:59:12.173
STEP: deleting a collection 12/14/22 08:59:12.184
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 14 08:59:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1707" for this suite. 12/14/22 08:59:12.2
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":174,"skipped":3374,"failed":0}
------------------------------
• [0.102 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:59:12.104
    Dec 14 08:59:12.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 08:59:12.104
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:12.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:12.122
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 12/14/22 08:59:12.127
    STEP: getting /apis/node.k8s.io 12/14/22 08:59:12.132
    STEP: getting /apis/node.k8s.io/v1 12/14/22 08:59:12.134
    STEP: creating 12/14/22 08:59:12.136
    STEP: watching 12/14/22 08:59:12.149
    Dec 14 08:59:12.149: INFO: starting watch
    STEP: getting 12/14/22 08:59:12.156
    STEP: listing 12/14/22 08:59:12.159
    STEP: patching 12/14/22 08:59:12.163
    STEP: updating 12/14/22 08:59:12.168
    Dec 14 08:59:12.173: INFO: waiting for watch events with expected annotations
    STEP: deleting 12/14/22 08:59:12.173
    STEP: deleting a collection 12/14/22 08:59:12.184
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 14 08:59:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1707" for this suite. 12/14/22 08:59:12.2
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:59:12.206
Dec 14 08:59:12.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:59:12.206
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:12.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:12.223
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 12/14/22 08:59:12.228
Dec 14 08:59:12.247: INFO: Waiting up to 5m0s for pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d" in namespace "downward-api-2841" to be "Succeeded or Failed"
Dec 14 08:59:12.251: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721896ms
Dec 14 08:59:14.255: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008277711s
Dec 14 08:59:16.255: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008499337s
STEP: Saw pod success 12/14/22 08:59:16.255
Dec 14 08:59:16.256: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d" satisfied condition "Succeeded or Failed"
Dec 14 08:59:16.259: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d container dapi-container: <nil>
STEP: delete the pod 12/14/22 08:59:16.271
Dec 14 08:59:16.280: INFO: Waiting for pod downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d to disappear
Dec 14 08:59:16.283: INFO: Pod downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 14 08:59:16.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2841" for this suite. 12/14/22 08:59:16.289
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":175,"skipped":3378,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:59:12.206
    Dec 14 08:59:12.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:59:12.206
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:12.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:12.223
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 12/14/22 08:59:12.228
    Dec 14 08:59:12.247: INFO: Waiting up to 5m0s for pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d" in namespace "downward-api-2841" to be "Succeeded or Failed"
    Dec 14 08:59:12.251: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721896ms
    Dec 14 08:59:14.255: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008277711s
    Dec 14 08:59:16.255: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008499337s
    STEP: Saw pod success 12/14/22 08:59:16.255
    Dec 14 08:59:16.256: INFO: Pod "downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d" satisfied condition "Succeeded or Failed"
    Dec 14 08:59:16.259: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d container dapi-container: <nil>
    STEP: delete the pod 12/14/22 08:59:16.271
    Dec 14 08:59:16.280: INFO: Waiting for pod downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d to disappear
    Dec 14 08:59:16.283: INFO: Pod downward-api-edd538cb-2e49-4a29-86f5-d7f569b0391d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 14 08:59:16.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2841" for this suite. 12/14/22 08:59:16.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:59:16.296
Dec 14 08:59:16.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 08:59:16.296
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:16.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:16.311
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3933 12/14/22 08:59:16.316
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-3933 12/14/22 08:59:16.321
Dec 14 08:59:16.328: INFO: Found 0 stateful pods, waiting for 1
Dec 14 08:59:26.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 12/14/22 08:59:26.343
STEP: updating a scale subresource 12/14/22 08:59:26.346
STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 08:59:26.351
STEP: Patch a scale subresource 12/14/22 08:59:26.355
STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 08:59:26.361
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 08:59:26.364: INFO: Deleting all statefulset in ns statefulset-3933
Dec 14 08:59:26.368: INFO: Scaling statefulset ss to 0
Dec 14 08:59:36.385: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 08:59:36.389: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 08:59:36.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3933" for this suite. 12/14/22 08:59:36.406
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":176,"skipped":3408,"failed":0}
------------------------------
• [20.116 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:59:16.296
    Dec 14 08:59:16.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 08:59:16.296
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:16.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:16.311
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3933 12/14/22 08:59:16.316
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-3933 12/14/22 08:59:16.321
    Dec 14 08:59:16.328: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 08:59:26.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 12/14/22 08:59:26.343
    STEP: updating a scale subresource 12/14/22 08:59:26.346
    STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 08:59:26.351
    STEP: Patch a scale subresource 12/14/22 08:59:26.355
    STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 08:59:26.361
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 08:59:26.364: INFO: Deleting all statefulset in ns statefulset-3933
    Dec 14 08:59:26.368: INFO: Scaling statefulset ss to 0
    Dec 14 08:59:36.385: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 08:59:36.389: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 08:59:36.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3933" for this suite. 12/14/22 08:59:36.406
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:59:36.412
Dec 14 08:59:36.412: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 08:59:36.412
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:36.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:36.428
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 12/14/22 08:59:36.432
Dec 14 08:59:36.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a" in namespace "downward-api-9665" to be "Succeeded or Failed"
Dec 14 08:59:36.448: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705948ms
Dec 14 08:59:38.453: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009915152s
Dec 14 08:59:40.452: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009198405s
STEP: Saw pod success 12/14/22 08:59:40.452
Dec 14 08:59:40.453: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a" satisfied condition "Succeeded or Failed"
Dec 14 08:59:40.457: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a container client-container: <nil>
STEP: delete the pod 12/14/22 08:59:40.468
Dec 14 08:59:40.475: INFO: Waiting for pod downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a to disappear
Dec 14 08:59:40.478: INFO: Pod downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 08:59:40.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9665" for this suite. 12/14/22 08:59:40.484
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":177,"skipped":3409,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:59:36.412
    Dec 14 08:59:36.412: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 08:59:36.412
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:36.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:36.428
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 12/14/22 08:59:36.432
    Dec 14 08:59:36.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a" in namespace "downward-api-9665" to be "Succeeded or Failed"
    Dec 14 08:59:36.448: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705948ms
    Dec 14 08:59:38.453: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009915152s
    Dec 14 08:59:40.452: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009198405s
    STEP: Saw pod success 12/14/22 08:59:40.452
    Dec 14 08:59:40.453: INFO: Pod "downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a" satisfied condition "Succeeded or Failed"
    Dec 14 08:59:40.457: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a container client-container: <nil>
    STEP: delete the pod 12/14/22 08:59:40.468
    Dec 14 08:59:40.475: INFO: Waiting for pod downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a to disappear
    Dec 14 08:59:40.478: INFO: Pod downwardapi-volume-4406ea92-10a2-43d8-915f-e09fb51aae6a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 08:59:40.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9665" for this suite. 12/14/22 08:59:40.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:59:40.49
Dec 14 08:59:40.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 12/14/22 08:59:40.49
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:40.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:40.506
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 12/14/22 08:59:40.511
STEP: Ensure pods equal to paralellism count is attached to the job 12/14/22 08:59:40.516
STEP: patching /status 12/14/22 08:59:58.521
STEP: updating /status 12/14/22 08:59:58.528
STEP: get /status 12/14/22 08:59:58.535
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 14 08:59:58.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4548" for this suite. 12/14/22 08:59:58.545
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":178,"skipped":3446,"failed":0}
------------------------------
• [18.060 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:59:40.49
    Dec 14 08:59:40.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 12/14/22 08:59:40.49
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:40.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:40.506
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 12/14/22 08:59:40.511
    STEP: Ensure pods equal to paralellism count is attached to the job 12/14/22 08:59:40.516
    STEP: patching /status 12/14/22 08:59:58.521
    STEP: updating /status 12/14/22 08:59:58.528
    STEP: get /status 12/14/22 08:59:58.535
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 14 08:59:58.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4548" for this suite. 12/14/22 08:59:58.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 08:59:58.55
Dec 14 08:59:58.550: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 08:59:58.551
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:58.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:58.566
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-7964 12/14/22 08:59:58.571
Dec 14 08:59:58.580: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7964" to be "running and ready"
Dec 14 08:59:58.583: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749761ms
Dec 14 08:59:58.583: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:00:00.596: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015933427s
Dec 14 09:00:00.596: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 14 09:00:00.596: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Dec 14 09:00:00.600: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 09:00:01.109: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 14 09:00:01.109: INFO: stdout: "iptables"
Dec 14 09:00:01.109: INFO: proxyMode: iptables
Dec 14 09:00:01.116: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 09:00:01.120: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7964 12/14/22 09:00:01.12
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7964 12/14/22 09:00:01.129
I1214 09:00:01.135657    4635 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7964, replica count: 3
I1214 09:00:04.186991    4635 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:00:04.195: INFO: Creating new exec pod
Dec 14 09:00:04.203: INFO: Waiting up to 5m0s for pod "execpod-affinity9wpjj" in namespace "services-7964" to be "running"
Dec 14 09:00:04.207: INFO: Pod "execpod-affinity9wpjj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433559ms
Dec 14 09:00:07.715: INFO: Pod "execpod-affinity9wpjj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511974745s
Dec 14 09:00:08.212: INFO: Pod "execpod-affinity9wpjj": Phase="Running", Reason="", readiness=true. Elapsed: 4.008143592s
Dec 14 09:00:08.212: INFO: Pod "execpod-affinity9wpjj" satisfied condition "running"
Dec 14 09:00:09.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Dec 14 09:00:10.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 09:00:10.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:00:10.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.108.48.164 80'
Dec 14 09:00:10.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.108.48.164 80\nConnection to 100.108.48.164 80 port [tcp/http] succeeded!\n"
Dec 14 09:00:10.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:00:10.994: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.48.164:80/ ; done'
Dec 14 09:00:11.501: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
Dec 14 09:00:11.501: INFO: stdout: "\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4"
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
Dec 14 09:00:11.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.108.48.164:80/'
Dec 14 09:00:11.860: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
Dec 14 09:00:11.860: INFO: stdout: "affinity-clusterip-timeout-flpj4"
Dec 14 09:00:31.861: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.108.48.164:80/'
Dec 14 09:00:32.404: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
Dec 14 09:00:32.404: INFO: stdout: "affinity-clusterip-timeout-flpj4"
Dec 14 09:00:52.405: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.108.48.164:80/'
Dec 14 09:00:52.780: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
Dec 14 09:00:52.780: INFO: stdout: "affinity-clusterip-timeout-xndn5"
Dec 14 09:00:52.780: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7964, will wait for the garbage collector to delete the pods 12/14/22 09:00:52.789
Dec 14 09:00:52.849: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.551486ms
Dec 14 09:00:52.950: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.021799ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:00:55.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7964" for this suite. 12/14/22 09:00:55.174
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":179,"skipped":3457,"failed":0}
------------------------------
• [56.631 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 08:59:58.55
    Dec 14 08:59:58.550: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 08:59:58.551
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 08:59:58.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 08:59:58.566
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-7964 12/14/22 08:59:58.571
    Dec 14 08:59:58.580: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7964" to be "running and ready"
    Dec 14 08:59:58.583: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749761ms
    Dec 14 08:59:58.583: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:00:00.596: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015933427s
    Dec 14 09:00:00.596: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Dec 14 09:00:00.596: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Dec 14 09:00:00.600: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Dec 14 09:00:01.109: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Dec 14 09:00:01.109: INFO: stdout: "iptables"
    Dec 14 09:00:01.109: INFO: proxyMode: iptables
    Dec 14 09:00:01.116: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Dec 14 09:00:01.120: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-7964 12/14/22 09:00:01.12
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-7964 12/14/22 09:00:01.129
    I1214 09:00:01.135657    4635 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7964, replica count: 3
    I1214 09:00:04.186991    4635 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:00:04.195: INFO: Creating new exec pod
    Dec 14 09:00:04.203: INFO: Waiting up to 5m0s for pod "execpod-affinity9wpjj" in namespace "services-7964" to be "running"
    Dec 14 09:00:04.207: INFO: Pod "execpod-affinity9wpjj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433559ms
    Dec 14 09:00:07.715: INFO: Pod "execpod-affinity9wpjj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511974745s
    Dec 14 09:00:08.212: INFO: Pod "execpod-affinity9wpjj": Phase="Running", Reason="", readiness=true. Elapsed: 4.008143592s
    Dec 14 09:00:08.212: INFO: Pod "execpod-affinity9wpjj" satisfied condition "running"
    Dec 14 09:00:09.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Dec 14 09:00:10.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Dec 14 09:00:10.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:00:10.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.108.48.164 80'
    Dec 14 09:00:10.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.108.48.164 80\nConnection to 100.108.48.164 80 port [tcp/http] succeeded!\n"
    Dec 14 09:00:10.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:00:10.994: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.48.164:80/ ; done'
    Dec 14 09:00:11.501: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
    Dec 14 09:00:11.501: INFO: stdout: "\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4\naffinity-clusterip-timeout-flpj4"
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Received response from host: affinity-clusterip-timeout-flpj4
    Dec 14 09:00:11.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.108.48.164:80/'
    Dec 14 09:00:11.860: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
    Dec 14 09:00:11.860: INFO: stdout: "affinity-clusterip-timeout-flpj4"
    Dec 14 09:00:31.861: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.108.48.164:80/'
    Dec 14 09:00:32.404: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
    Dec 14 09:00:32.404: INFO: stdout: "affinity-clusterip-timeout-flpj4"
    Dec 14 09:00:52.405: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7964 exec execpod-affinity9wpjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.108.48.164:80/'
    Dec 14 09:00:52.780: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.108.48.164:80/\n"
    Dec 14 09:00:52.780: INFO: stdout: "affinity-clusterip-timeout-xndn5"
    Dec 14 09:00:52.780: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7964, will wait for the garbage collector to delete the pods 12/14/22 09:00:52.789
    Dec 14 09:00:52.849: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.551486ms
    Dec 14 09:00:52.950: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.021799ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:00:55.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7964" for this suite. 12/14/22 09:00:55.174
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:00:55.182
Dec 14 09:00:55.183: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 09:00:55.183
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:00:55.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:00:55.197
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 09:00:55.202
Dec 14 09:00:55.212: INFO: Waiting up to 5m0s for pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6" in namespace "emptydir-4023" to be "Succeeded or Failed"
Dec 14 09:00:55.215: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.176264ms
Dec 14 09:00:57.221: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009386116s
Dec 14 09:00:59.222: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009638021s
STEP: Saw pod success 12/14/22 09:00:59.222
Dec 14 09:00:59.222: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6" satisfied condition "Succeeded or Failed"
Dec 14 09:00:59.226: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-b94aae11-f7fd-49f5-8ff0-c467703daad6 container test-container: <nil>
STEP: delete the pod 12/14/22 09:00:59.239
Dec 14 09:00:59.248: INFO: Waiting for pod pod-b94aae11-f7fd-49f5-8ff0-c467703daad6 to disappear
Dec 14 09:00:59.251: INFO: Pod pod-b94aae11-f7fd-49f5-8ff0-c467703daad6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 09:00:59.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4023" for this suite. 12/14/22 09:00:59.257
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":3482,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:00:55.182
    Dec 14 09:00:55.183: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 09:00:55.183
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:00:55.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:00:55.197
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 09:00:55.202
    Dec 14 09:00:55.212: INFO: Waiting up to 5m0s for pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6" in namespace "emptydir-4023" to be "Succeeded or Failed"
    Dec 14 09:00:55.215: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.176264ms
    Dec 14 09:00:57.221: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009386116s
    Dec 14 09:00:59.222: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009638021s
    STEP: Saw pod success 12/14/22 09:00:59.222
    Dec 14 09:00:59.222: INFO: Pod "pod-b94aae11-f7fd-49f5-8ff0-c467703daad6" satisfied condition "Succeeded or Failed"
    Dec 14 09:00:59.226: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-b94aae11-f7fd-49f5-8ff0-c467703daad6 container test-container: <nil>
    STEP: delete the pod 12/14/22 09:00:59.239
    Dec 14 09:00:59.248: INFO: Waiting for pod pod-b94aae11-f7fd-49f5-8ff0-c467703daad6 to disappear
    Dec 14 09:00:59.251: INFO: Pod pod-b94aae11-f7fd-49f5-8ff0-c467703daad6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:00:59.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4023" for this suite. 12/14/22 09:00:59.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:00:59.261
Dec 14 09:00:59.261: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 09:00:59.262
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:00:59.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:00:59.276
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 12/14/22 09:00:59.282
Dec 14 09:00:59.292: INFO: Waiting up to 2m0s for pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" in namespace "var-expansion-9071" to be "running"
Dec 14 09:00:59.297: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215033ms
Dec 14 09:01:01.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010215712s
Dec 14 09:01:03.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009590256s
Dec 14 09:01:05.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009226495s
Dec 14 09:01:07.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009141342s
Dec 14 09:01:09.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00999838s
Dec 14 09:01:11.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010241569s
Dec 14 09:01:13.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010967841s
Dec 14 09:01:15.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00911748s
Dec 14 09:01:17.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009852973s
Dec 14 09:01:19.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009477556s
Dec 14 09:01:21.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009919359s
Dec 14 09:01:23.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009580284s
Dec 14 09:01:25.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009206998s
Dec 14 09:01:27.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010842071s
Dec 14 09:01:29.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010992135s
Dec 14 09:01:31.304: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011532387s
Dec 14 09:01:33.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0096787s
Dec 14 09:01:35.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01001767s
Dec 14 09:01:37.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009768636s
Dec 14 09:01:39.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009957052s
Dec 14 09:01:41.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009924598s
Dec 14 09:01:43.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008809877s
Dec 14 09:01:45.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009339479s
Dec 14 09:01:47.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009905806s
Dec 14 09:01:49.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010367134s
Dec 14 09:01:51.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010879766s
Dec 14 09:01:53.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009719799s
Dec 14 09:01:55.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010100368s
Dec 14 09:01:57.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009864929s
Dec 14 09:01:59.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010308493s
Dec 14 09:02:01.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008945506s
Dec 14 09:02:03.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009942626s
Dec 14 09:02:05.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009808947s
Dec 14 09:02:07.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011093411s
Dec 14 09:02:09.340: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.047395534s
Dec 14 09:02:11.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00998526s
Dec 14 09:02:13.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010492462s
Dec 14 09:02:15.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009352002s
Dec 14 09:02:17.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010937854s
Dec 14 09:02:19.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009346746s
Dec 14 09:02:21.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009785226s
Dec 14 09:02:23.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010310887s
Dec 14 09:02:25.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00968907s
Dec 14 09:02:27.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010269915s
Dec 14 09:02:29.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009677496s
Dec 14 09:02:31.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009765223s
Dec 14 09:02:33.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009606059s
Dec 14 09:02:35.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009434501s
Dec 14 09:02:37.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010368167s
Dec 14 09:02:39.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010374778s
Dec 14 09:02:41.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010361407s
Dec 14 09:02:43.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009142403s
Dec 14 09:02:45.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009319934s
Dec 14 09:02:47.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011094187s
Dec 14 09:02:49.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01048432s
Dec 14 09:02:51.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009416152s
Dec 14 09:02:53.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009849024s
Dec 14 09:02:55.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.0095229s
Dec 14 09:02:57.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009752278s
Dec 14 09:02:59.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009990925s
Dec 14 09:02:59.306: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01341141s
STEP: updating the pod 12/14/22 09:02:59.306
Dec 14 09:02:59.817: INFO: Successfully updated pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8"
STEP: waiting for pod running 12/14/22 09:02:59.817
Dec 14 09:02:59.818: INFO: Waiting up to 2m0s for pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" in namespace "var-expansion-9071" to be "running"
Dec 14 09:02:59.821: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265686ms
Dec 14 09:03:01.826: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008849679s
Dec 14 09:03:01.827: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" satisfied condition "running"
STEP: deleting the pod gracefully 12/14/22 09:03:01.827
Dec 14 09:03:01.827: INFO: Deleting pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" in namespace "var-expansion-9071"
Dec 14 09:03:01.832: INFO: Wait up to 5m0s for pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 09:03:33.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9071" for this suite. 12/14/22 09:03:33.848
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":181,"skipped":3491,"failed":0}
------------------------------
• [154.592 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:00:59.261
    Dec 14 09:00:59.261: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 09:00:59.262
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:00:59.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:00:59.276
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 12/14/22 09:00:59.282
    Dec 14 09:00:59.292: INFO: Waiting up to 2m0s for pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" in namespace "var-expansion-9071" to be "running"
    Dec 14 09:00:59.297: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215033ms
    Dec 14 09:01:01.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010215712s
    Dec 14 09:01:03.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009590256s
    Dec 14 09:01:05.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009226495s
    Dec 14 09:01:07.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009141342s
    Dec 14 09:01:09.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00999838s
    Dec 14 09:01:11.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010241569s
    Dec 14 09:01:13.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010967841s
    Dec 14 09:01:15.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00911748s
    Dec 14 09:01:17.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009852973s
    Dec 14 09:01:19.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009477556s
    Dec 14 09:01:21.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009919359s
    Dec 14 09:01:23.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009580284s
    Dec 14 09:01:25.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009206998s
    Dec 14 09:01:27.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010842071s
    Dec 14 09:01:29.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010992135s
    Dec 14 09:01:31.304: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011532387s
    Dec 14 09:01:33.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0096787s
    Dec 14 09:01:35.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01001767s
    Dec 14 09:01:37.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009768636s
    Dec 14 09:01:39.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009957052s
    Dec 14 09:01:41.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009924598s
    Dec 14 09:01:43.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008809877s
    Dec 14 09:01:45.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009339479s
    Dec 14 09:01:47.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009905806s
    Dec 14 09:01:49.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010367134s
    Dec 14 09:01:51.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010879766s
    Dec 14 09:01:53.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009719799s
    Dec 14 09:01:55.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010100368s
    Dec 14 09:01:57.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009864929s
    Dec 14 09:01:59.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010308493s
    Dec 14 09:02:01.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008945506s
    Dec 14 09:02:03.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009942626s
    Dec 14 09:02:05.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009808947s
    Dec 14 09:02:07.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011093411s
    Dec 14 09:02:09.340: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.047395534s
    Dec 14 09:02:11.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00998526s
    Dec 14 09:02:13.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010492462s
    Dec 14 09:02:15.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009352002s
    Dec 14 09:02:17.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010937854s
    Dec 14 09:02:19.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009346746s
    Dec 14 09:02:21.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009785226s
    Dec 14 09:02:23.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010310887s
    Dec 14 09:02:25.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00968907s
    Dec 14 09:02:27.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010269915s
    Dec 14 09:02:29.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009677496s
    Dec 14 09:02:31.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009765223s
    Dec 14 09:02:33.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009606059s
    Dec 14 09:02:35.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009434501s
    Dec 14 09:02:37.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010368167s
    Dec 14 09:02:39.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010374778s
    Dec 14 09:02:41.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010361407s
    Dec 14 09:02:43.301: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009142403s
    Dec 14 09:02:45.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009319934s
    Dec 14 09:02:47.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011094187s
    Dec 14 09:02:49.303: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01048432s
    Dec 14 09:02:51.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009416152s
    Dec 14 09:02:53.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009849024s
    Dec 14 09:02:55.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.0095229s
    Dec 14 09:02:57.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009752278s
    Dec 14 09:02:59.302: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009990925s
    Dec 14 09:02:59.306: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01341141s
    STEP: updating the pod 12/14/22 09:02:59.306
    Dec 14 09:02:59.817: INFO: Successfully updated pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8"
    STEP: waiting for pod running 12/14/22 09:02:59.817
    Dec 14 09:02:59.818: INFO: Waiting up to 2m0s for pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" in namespace "var-expansion-9071" to be "running"
    Dec 14 09:02:59.821: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265686ms
    Dec 14 09:03:01.826: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008849679s
    Dec 14 09:03:01.827: INFO: Pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" satisfied condition "running"
    STEP: deleting the pod gracefully 12/14/22 09:03:01.827
    Dec 14 09:03:01.827: INFO: Deleting pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" in namespace "var-expansion-9071"
    Dec 14 09:03:01.832: INFO: Wait up to 5m0s for pod "var-expansion-e471789f-48b6-4774-a9e0-bd3f0a57bac8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 09:03:33.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9071" for this suite. 12/14/22 09:03:33.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:03:33.855
Dec 14 09:03:33.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 09:03:33.855
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:03:33.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:03:33.87
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 12/14/22 09:03:33.874
STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 09:03:33.878
STEP: delete the deployment 12/14/22 09:03:34.39
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/14/22 09:03:34.395
STEP: Gathering metrics 12/14/22 09:03:34.917
W1214 09:03:34.929459    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 09:03:34.929: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 09:03:34.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5537" for this suite. 12/14/22 09:03:34.936
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":182,"skipped":3533,"failed":0}
------------------------------
• [1.087 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:03:33.855
    Dec 14 09:03:33.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 09:03:33.855
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:03:33.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:03:33.87
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 12/14/22 09:03:33.874
    STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 09:03:33.878
    STEP: delete the deployment 12/14/22 09:03:34.39
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/14/22 09:03:34.395
    STEP: Gathering metrics 12/14/22 09:03:34.917
    W1214 09:03:34.929459    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 14 09:03:34.929: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 09:03:34.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5537" for this suite. 12/14/22 09:03:34.936
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:03:34.942
Dec 14 09:03:34.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 09:03:34.942
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:03:34.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:03:34.957
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 12/14/22 09:03:34.963
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_tcp@PTR;sleep 1; done
 12/14/22 09:03:34.974
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_tcp@PTR;sleep 1; done
 12/14/22 09:03:34.974
STEP: creating a pod to probe DNS 12/14/22 09:03:34.974
STEP: submitting the pod to kubernetes 12/14/22 09:03:34.974
Dec 14 09:03:34.985: INFO: Waiting up to 15m0s for pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b" in namespace "dns-4872" to be "running"
Dec 14 09:03:34.989: INFO: Pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.242553ms
Dec 14 09:03:36.994: INFO: Pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008908931s
Dec 14 09:03:36.994: INFO: Pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b" satisfied condition "running"
STEP: retrieving the pod 12/14/22 09:03:36.994
STEP: looking for the results for each expected name from probers 12/14/22 09:03:36.998
Dec 14 09:03:37.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.071: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.079: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.115: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.123: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.129: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.136: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:37.165: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:03:42.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.243: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.251: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.286: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.299: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.306: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:42.336: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:03:47.175: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.226: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.233: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.269: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.282: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:47.316: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:03:52.181: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.227: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.235: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.242: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.276: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.289: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.296: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:52.334: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:03:57.174: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.221: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.228: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.235: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.270: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.284: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.290: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:03:57.318: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:04:02.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.234: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.241: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.276: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.290: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.298: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:02.337: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:04:07.176: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.222: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.230: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.238: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.270: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.284: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.291: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
Dec 14 09:04:07.316: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

Dec 14 09:04:12.337: INFO: DNS probes using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b succeeded

STEP: deleting the pod 12/14/22 09:04:12.338
STEP: deleting the test service 12/14/22 09:04:12.346
STEP: deleting the test headless service 12/14/22 09:04:12.357
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 09:04:12.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4872" for this suite. 12/14/22 09:04:12.375
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":183,"skipped":3534,"failed":0}
------------------------------
• [37.437 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:03:34.942
    Dec 14 09:03:34.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 09:03:34.942
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:03:34.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:03:34.957
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 12/14/22 09:03:34.963
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_tcp@PTR;sleep 1; done
     12/14/22 09:03:34.974
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.249.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.249.157_tcp@PTR;sleep 1; done
     12/14/22 09:03:34.974
    STEP: creating a pod to probe DNS 12/14/22 09:03:34.974
    STEP: submitting the pod to kubernetes 12/14/22 09:03:34.974
    Dec 14 09:03:34.985: INFO: Waiting up to 15m0s for pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b" in namespace "dns-4872" to be "running"
    Dec 14 09:03:34.989: INFO: Pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.242553ms
    Dec 14 09:03:36.994: INFO: Pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008908931s
    Dec 14 09:03:36.994: INFO: Pod "dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 09:03:36.994
    STEP: looking for the results for each expected name from probers 12/14/22 09:03:36.998
    Dec 14 09:03:37.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.071: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.079: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.115: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.123: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.129: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.136: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:37.165: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:03:42.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.243: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.251: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.286: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.299: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.306: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:42.336: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:03:47.175: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.226: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.233: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.269: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.282: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:47.316: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:03:52.181: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.227: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.235: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.242: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.276: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.289: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.296: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:52.334: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:03:57.174: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.221: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.228: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.235: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.270: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.284: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.290: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:03:57.318: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:04:02.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.234: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.241: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.276: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.290: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.298: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:02.337: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:04:07.176: INFO: Unable to read wheezy_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.222: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.230: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.238: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.270: INFO: Unable to read jessie_udp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.284: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.291: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local from pod dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b: the server could not find the requested resource (get pods dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b)
    Dec 14 09:04:07.316: INFO: Lookups using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b failed for: [wheezy_udp@dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@dns-test-service.dns-4872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_udp@dns-test-service.dns-4872.svc.cluster.local jessie_tcp@dns-test-service.dns-4872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4872.svc.cluster.local]

    Dec 14 09:04:12.337: INFO: DNS probes using dns-4872/dns-test-b1348840-0647-41c8-b2e9-f71e08711c7b succeeded

    STEP: deleting the pod 12/14/22 09:04:12.338
    STEP: deleting the test service 12/14/22 09:04:12.346
    STEP: deleting the test headless service 12/14/22 09:04:12.357
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 09:04:12.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4872" for this suite. 12/14/22 09:04:12.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:12.381
Dec 14 09:04:12.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 09:04:12.382
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:12.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:12.399
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 12/14/22 09:04:12.404
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/14/22 09:04:12.407
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 09:04:12.407
STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/14/22 09:04:12.407
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/14/22 09:04:12.409
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 09:04:12.409
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 09:04:12.411
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:04:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-70" for this suite. 12/14/22 09:04:12.416
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":184,"skipped":3570,"failed":0}
------------------------------
• [0.039 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:12.381
    Dec 14 09:04:12.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 09:04:12.382
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:12.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:12.399
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 12/14/22 09:04:12.404
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/14/22 09:04:12.407
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 09:04:12.407
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/14/22 09:04:12.407
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/14/22 09:04:12.409
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 09:04:12.409
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 09:04:12.411
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:04:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-70" for this suite. 12/14/22 09:04:12.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:12.423
Dec 14 09:04:12.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:04:12.424
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:12.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:12.439
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-1103ced1-ca9c-438f-a7ad-1bd6dc7e908a 12/14/22 09:04:12.444
STEP: Creating a pod to test consume configMaps 12/14/22 09:04:12.448
Dec 14 09:04:12.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d" in namespace "configmap-8204" to be "Succeeded or Failed"
Dec 14 09:04:12.462: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532128ms
Dec 14 09:04:14.467: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009763501s
Dec 14 09:04:16.468: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01064004s
STEP: Saw pod success 12/14/22 09:04:16.468
Dec 14 09:04:16.468: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d" satisfied condition "Succeeded or Failed"
Dec 14 09:04:16.475: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:04:16.485
Dec 14 09:04:16.493: INFO: Waiting for pod pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d to disappear
Dec 14 09:04:16.496: INFO: Pod pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:04:16.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8204" for this suite. 12/14/22 09:04:16.502
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":185,"skipped":3619,"failed":0}
------------------------------
• [4.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:12.423
    Dec 14 09:04:12.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:04:12.424
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:12.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:12.439
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-1103ced1-ca9c-438f-a7ad-1bd6dc7e908a 12/14/22 09:04:12.444
    STEP: Creating a pod to test consume configMaps 12/14/22 09:04:12.448
    Dec 14 09:04:12.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d" in namespace "configmap-8204" to be "Succeeded or Failed"
    Dec 14 09:04:12.462: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532128ms
    Dec 14 09:04:14.467: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009763501s
    Dec 14 09:04:16.468: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01064004s
    STEP: Saw pod success 12/14/22 09:04:16.468
    Dec 14 09:04:16.468: INFO: Pod "pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d" satisfied condition "Succeeded or Failed"
    Dec 14 09:04:16.475: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:04:16.485
    Dec 14 09:04:16.493: INFO: Waiting for pod pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d to disappear
    Dec 14 09:04:16.496: INFO: Pod pod-configmaps-bdf32394-9e21-4005-a01e-505e38cd084d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:04:16.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8204" for this suite. 12/14/22 09:04:16.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:16.507
Dec 14 09:04:16.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:04:16.507
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:16.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:16.521
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 12/14/22 09:04:16.525
STEP: Counting existing ResourceQuota 12/14/22 09:04:21.53
STEP: Creating a ResourceQuota 12/14/22 09:04:26.535
STEP: Ensuring resource quota status is calculated 12/14/22 09:04:26.545
STEP: Creating a Secret 12/14/22 09:04:28.549
STEP: Ensuring resource quota status captures secret creation 12/14/22 09:04:28.556
STEP: Deleting a secret 12/14/22 09:04:30.562
STEP: Ensuring resource quota status released usage 12/14/22 09:04:30.568
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:04:32.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9165" for this suite. 12/14/22 09:04:32.58
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":186,"skipped":3632,"failed":0}
------------------------------
• [16.078 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:16.507
    Dec 14 09:04:16.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:04:16.507
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:16.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:16.521
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 12/14/22 09:04:16.525
    STEP: Counting existing ResourceQuota 12/14/22 09:04:21.53
    STEP: Creating a ResourceQuota 12/14/22 09:04:26.535
    STEP: Ensuring resource quota status is calculated 12/14/22 09:04:26.545
    STEP: Creating a Secret 12/14/22 09:04:28.549
    STEP: Ensuring resource quota status captures secret creation 12/14/22 09:04:28.556
    STEP: Deleting a secret 12/14/22 09:04:30.562
    STEP: Ensuring resource quota status released usage 12/14/22 09:04:30.568
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:04:32.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9165" for this suite. 12/14/22 09:04:32.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:32.586
Dec 14 09:04:32.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:04:32.587
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:32.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:32.602
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-0b5468d7-be00-4b5e-900a-22155c40c16e 12/14/22 09:04:32.606
STEP: Creating a pod to test consume secrets 12/14/22 09:04:32.61
Dec 14 09:04:32.620: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036" in namespace "projected-2505" to be "Succeeded or Failed"
Dec 14 09:04:32.624: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11562ms
Dec 14 09:04:34.630: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010104798s
Dec 14 09:04:36.630: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01047501s
STEP: Saw pod success 12/14/22 09:04:36.63
Dec 14 09:04:36.630: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036" satisfied condition "Succeeded or Failed"
Dec 14 09:04:36.634: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:04:36.643
Dec 14 09:04:36.651: INFO: Waiting for pod pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036 to disappear
Dec 14 09:04:36.655: INFO: Pod pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 09:04:36.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2505" for this suite. 12/14/22 09:04:36.661
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":187,"skipped":3699,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:32.586
    Dec 14 09:04:32.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:04:32.587
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:32.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:32.602
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-0b5468d7-be00-4b5e-900a-22155c40c16e 12/14/22 09:04:32.606
    STEP: Creating a pod to test consume secrets 12/14/22 09:04:32.61
    Dec 14 09:04:32.620: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036" in namespace "projected-2505" to be "Succeeded or Failed"
    Dec 14 09:04:32.624: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11562ms
    Dec 14 09:04:34.630: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010104798s
    Dec 14 09:04:36.630: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01047501s
    STEP: Saw pod success 12/14/22 09:04:36.63
    Dec 14 09:04:36.630: INFO: Pod "pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036" satisfied condition "Succeeded or Failed"
    Dec 14 09:04:36.634: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:04:36.643
    Dec 14 09:04:36.651: INFO: Waiting for pod pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036 to disappear
    Dec 14 09:04:36.655: INFO: Pod pod-projected-secrets-3afabf06-0e8d-4d00-bf5e-9c272bf81036 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 09:04:36.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2505" for this suite. 12/14/22 09:04:36.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:36.666
Dec 14 09:04:36.666: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 09:04:36.667
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:36.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:36.682
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 09:04:36.688
Dec 14 09:04:36.700: INFO: Waiting up to 5m0s for pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c" in namespace "emptydir-5507" to be "Succeeded or Failed"
Dec 14 09:04:36.705: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.003695ms
Dec 14 09:04:38.710: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010389598s
Dec 14 09:04:40.710: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010185738s
STEP: Saw pod success 12/14/22 09:04:40.71
Dec 14 09:04:40.710: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c" satisfied condition "Succeeded or Failed"
Dec 14 09:04:40.715: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c container test-container: <nil>
STEP: delete the pod 12/14/22 09:04:40.724
Dec 14 09:04:40.732: INFO: Waiting for pod pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c to disappear
Dec 14 09:04:40.736: INFO: Pod pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 09:04:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5507" for this suite. 12/14/22 09:04:40.743
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":188,"skipped":3709,"failed":0}
------------------------------
• [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:36.666
    Dec 14 09:04:36.666: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 09:04:36.667
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:36.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:36.682
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 09:04:36.688
    Dec 14 09:04:36.700: INFO: Waiting up to 5m0s for pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c" in namespace "emptydir-5507" to be "Succeeded or Failed"
    Dec 14 09:04:36.705: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.003695ms
    Dec 14 09:04:38.710: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010389598s
    Dec 14 09:04:40.710: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010185738s
    STEP: Saw pod success 12/14/22 09:04:40.71
    Dec 14 09:04:40.710: INFO: Pod "pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c" satisfied condition "Succeeded or Failed"
    Dec 14 09:04:40.715: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c container test-container: <nil>
    STEP: delete the pod 12/14/22 09:04:40.724
    Dec 14 09:04:40.732: INFO: Waiting for pod pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c to disappear
    Dec 14 09:04:40.736: INFO: Pod pod-981569b2-e1cb-44ba-a390-fc2fe4fc817c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:04:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5507" for this suite. 12/14/22 09:04:40.743
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:40.748
Dec 14 09:04:40.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:04:40.749
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:40.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:40.765
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/14/22 09:04:40.769
Dec 14 09:04:40.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:04:44.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:04:55.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4295" for this suite. 12/14/22 09:04:55.157
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":189,"skipped":3713,"failed":0}
------------------------------
• [14.413 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:40.748
    Dec 14 09:04:40.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:04:40.749
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:40.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:40.765
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/14/22 09:04:40.769
    Dec 14 09:04:40.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:04:44.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:04:55.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4295" for this suite. 12/14/22 09:04:55.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:55.161
Dec 14 09:04:55.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename conformance-tests 12/14/22 09:04:55.162
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:55.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:55.175
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 12/14/22 09:04:55.179
Dec 14 09:04:55.179: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Dec 14 09:04:55.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4849" for this suite. 12/14/22 09:04:55.189
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":190,"skipped":3720,"failed":0}
------------------------------
• [0.032 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:55.161
    Dec 14 09:04:55.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename conformance-tests 12/14/22 09:04:55.162
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:55.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:55.175
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 12/14/22 09:04:55.179
    Dec 14 09:04:55.179: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Dec 14 09:04:55.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-4849" for this suite. 12/14/22 09:04:55.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:04:55.195
Dec 14 09:04:55.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:04:55.195
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:55.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:55.207
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-e75a3b5e-d478-4d2d-8c78-16b7313f63a0 12/14/22 09:04:55.214
STEP: Creating configMap with name cm-test-opt-upd-b0b3c1de-50fd-4fb1-929a-6e98e5ae8b90 12/14/22 09:04:55.217
STEP: Creating the pod 12/14/22 09:04:55.22
Dec 14 09:04:55.231: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da" in namespace "configmap-1596" to be "running and ready"
Dec 14 09:04:55.234: INFO: Pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.848679ms
Dec 14 09:04:55.234: INFO: The phase of Pod pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:04:57.239: INFO: Pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.007627588s
Dec 14 09:04:57.239: INFO: The phase of Pod pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da is Running (Ready = true)
Dec 14 09:04:57.239: INFO: Pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-e75a3b5e-d478-4d2d-8c78-16b7313f63a0 12/14/22 09:04:57.429
STEP: Updating configmap cm-test-opt-upd-b0b3c1de-50fd-4fb1-929a-6e98e5ae8b90 12/14/22 09:04:57.434
STEP: Creating configMap with name cm-test-opt-create-005c5991-c528-4ce1-a007-37f5a99e5909 12/14/22 09:04:57.438
STEP: waiting to observe update in volume 12/14/22 09:04:57.44
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:05:01.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1596" for this suite. 12/14/22 09:05:01.759
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":191,"skipped":3764,"failed":0}
------------------------------
• [6.569 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:04:55.195
    Dec 14 09:04:55.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:04:55.195
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:04:55.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:04:55.207
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-e75a3b5e-d478-4d2d-8c78-16b7313f63a0 12/14/22 09:04:55.214
    STEP: Creating configMap with name cm-test-opt-upd-b0b3c1de-50fd-4fb1-929a-6e98e5ae8b90 12/14/22 09:04:55.217
    STEP: Creating the pod 12/14/22 09:04:55.22
    Dec 14 09:04:55.231: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da" in namespace "configmap-1596" to be "running and ready"
    Dec 14 09:04:55.234: INFO: Pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.848679ms
    Dec 14 09:04:55.234: INFO: The phase of Pod pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:04:57.239: INFO: Pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.007627588s
    Dec 14 09:04:57.239: INFO: The phase of Pod pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da is Running (Ready = true)
    Dec 14 09:04:57.239: INFO: Pod "pod-configmaps-4e625ba8-9055-44a0-8645-7ea48912f1da" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-e75a3b5e-d478-4d2d-8c78-16b7313f63a0 12/14/22 09:04:57.429
    STEP: Updating configmap cm-test-opt-upd-b0b3c1de-50fd-4fb1-929a-6e98e5ae8b90 12/14/22 09:04:57.434
    STEP: Creating configMap with name cm-test-opt-create-005c5991-c528-4ce1-a007-37f5a99e5909 12/14/22 09:04:57.438
    STEP: waiting to observe update in volume 12/14/22 09:04:57.44
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:05:01.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1596" for this suite. 12/14/22 09:05:01.759
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:01.764
Dec 14 09:05:01.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 12/14/22 09:05:01.765
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:01.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:01.778
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 09:05:01.782
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-nzms 12/14/22 09:05:01.79
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:05:01.79
Dec 14 09:05:01.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nzms" in namespace "subpath-9387" to be "Succeeded or Failed"
Dec 14 09:05:01.803: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Pending", Reason="", readiness=false. Elapsed: 2.676636ms
Dec 14 09:05:03.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 2.007692053s
Dec 14 09:05:05.807: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 4.007420001s
Dec 14 09:05:07.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 6.008443626s
Dec 14 09:05:09.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 8.007655324s
Dec 14 09:05:11.806: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 10.006544906s
Dec 14 09:05:13.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 12.007641812s
Dec 14 09:05:15.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 14.008123181s
Dec 14 09:05:17.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 16.007760264s
Dec 14 09:05:19.806: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 18.006289701s
Dec 14 09:05:21.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 20.00794363s
Dec 14 09:05:23.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=false. Elapsed: 22.007900797s
Dec 14 09:05:25.807: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007112843s
STEP: Saw pod success 12/14/22 09:05:25.807
Dec 14 09:05:25.807: INFO: Pod "pod-subpath-test-configmap-nzms" satisfied condition "Succeeded or Failed"
Dec 14 09:05:25.811: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-configmap-nzms container test-container-subpath-configmap-nzms: <nil>
STEP: delete the pod 12/14/22 09:05:25.82
Dec 14 09:05:25.829: INFO: Waiting for pod pod-subpath-test-configmap-nzms to disappear
Dec 14 09:05:25.832: INFO: Pod pod-subpath-test-configmap-nzms no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nzms 12/14/22 09:05:25.832
Dec 14 09:05:25.832: INFO: Deleting pod "pod-subpath-test-configmap-nzms" in namespace "subpath-9387"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 14 09:05:25.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9387" for this suite. 12/14/22 09:05:25.84
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":192,"skipped":3767,"failed":0}
------------------------------
• [24.079 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:01.764
    Dec 14 09:05:01.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 12/14/22 09:05:01.765
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:01.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:01.778
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 09:05:01.782
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-nzms 12/14/22 09:05:01.79
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:05:01.79
    Dec 14 09:05:01.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nzms" in namespace "subpath-9387" to be "Succeeded or Failed"
    Dec 14 09:05:01.803: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Pending", Reason="", readiness=false. Elapsed: 2.676636ms
    Dec 14 09:05:03.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 2.007692053s
    Dec 14 09:05:05.807: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 4.007420001s
    Dec 14 09:05:07.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 6.008443626s
    Dec 14 09:05:09.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 8.007655324s
    Dec 14 09:05:11.806: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 10.006544906s
    Dec 14 09:05:13.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 12.007641812s
    Dec 14 09:05:15.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 14.008123181s
    Dec 14 09:05:17.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 16.007760264s
    Dec 14 09:05:19.806: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 18.006289701s
    Dec 14 09:05:21.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=true. Elapsed: 20.00794363s
    Dec 14 09:05:23.808: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Running", Reason="", readiness=false. Elapsed: 22.007900797s
    Dec 14 09:05:25.807: INFO: Pod "pod-subpath-test-configmap-nzms": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007112843s
    STEP: Saw pod success 12/14/22 09:05:25.807
    Dec 14 09:05:25.807: INFO: Pod "pod-subpath-test-configmap-nzms" satisfied condition "Succeeded or Failed"
    Dec 14 09:05:25.811: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-configmap-nzms container test-container-subpath-configmap-nzms: <nil>
    STEP: delete the pod 12/14/22 09:05:25.82
    Dec 14 09:05:25.829: INFO: Waiting for pod pod-subpath-test-configmap-nzms to disappear
    Dec 14 09:05:25.832: INFO: Pod pod-subpath-test-configmap-nzms no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-nzms 12/14/22 09:05:25.832
    Dec 14 09:05:25.832: INFO: Deleting pod "pod-subpath-test-configmap-nzms" in namespace "subpath-9387"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 14 09:05:25.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9387" for this suite. 12/14/22 09:05:25.84
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:25.844
Dec 14 09:05:25.844: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:05:25.844
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:25.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:25.856
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 12/14/22 09:05:25.86
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:05:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5508" for this suite. 12/14/22 09:05:25.868
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":193,"skipped":3767,"failed":0}
------------------------------
• [0.028 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:25.844
    Dec 14 09:05:25.844: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:05:25.844
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:25.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:25.856
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 12/14/22 09:05:25.86
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:05:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5508" for this suite. 12/14/22 09:05:25.868
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:25.872
Dec 14 09:05:25.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:05:25.873
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:25.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:25.885
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8293 12/14/22 09:05:25.888
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 09:05:25.897
STEP: creating service externalsvc in namespace services-8293 12/14/22 09:05:25.897
STEP: creating replication controller externalsvc in namespace services-8293 12/14/22 09:05:25.929
I1214 09:05:25.933715    4635 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8293, replica count: 2
I1214 09:05:28.984425    4635 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 12/14/22 09:05:28.988
Dec 14 09:05:29.000: INFO: Creating new exec pod
Dec 14 09:05:29.008: INFO: Waiting up to 5m0s for pod "execpodwxtcm" in namespace "services-8293" to be "running"
Dec 14 09:05:29.011: INFO: Pod "execpodwxtcm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740278ms
Dec 14 09:05:31.016: INFO: Pod "execpodwxtcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008178146s
Dec 14 09:05:31.016: INFO: Pod "execpodwxtcm" satisfied condition "running"
Dec 14 09:05:31.016: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8293 exec execpodwxtcm -- /bin/sh -x -c nslookup clusterip-service.services-8293.svc.cluster.local'
Dec 14 09:05:31.535: INFO: stderr: "+ nslookup clusterip-service.services-8293.svc.cluster.local\n"
Dec 14 09:05:31.535: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-8293.svc.cluster.local\tcanonical name = externalsvc.services-8293.svc.cluster.local.\nName:\texternalsvc.services-8293.svc.cluster.local\nAddress: 100.109.10.124\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8293, will wait for the garbage collector to delete the pods 12/14/22 09:05:31.535
Dec 14 09:05:31.612: INFO: Deleting ReplicationController externalsvc took: 5.941205ms
Dec 14 09:05:31.712: INFO: Terminating ReplicationController externalsvc pods took: 100.624322ms
Dec 14 09:05:33.324: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:05:33.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8293" for this suite. 12/14/22 09:05:33.335
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":194,"skipped":3774,"failed":0}
------------------------------
• [7.467 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:25.872
    Dec 14 09:05:25.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:05:25.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:25.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:25.885
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8293 12/14/22 09:05:25.888
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 09:05:25.897
    STEP: creating service externalsvc in namespace services-8293 12/14/22 09:05:25.897
    STEP: creating replication controller externalsvc in namespace services-8293 12/14/22 09:05:25.929
    I1214 09:05:25.933715    4635 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8293, replica count: 2
    I1214 09:05:28.984425    4635 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 12/14/22 09:05:28.988
    Dec 14 09:05:29.000: INFO: Creating new exec pod
    Dec 14 09:05:29.008: INFO: Waiting up to 5m0s for pod "execpodwxtcm" in namespace "services-8293" to be "running"
    Dec 14 09:05:29.011: INFO: Pod "execpodwxtcm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740278ms
    Dec 14 09:05:31.016: INFO: Pod "execpodwxtcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008178146s
    Dec 14 09:05:31.016: INFO: Pod "execpodwxtcm" satisfied condition "running"
    Dec 14 09:05:31.016: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8293 exec execpodwxtcm -- /bin/sh -x -c nslookup clusterip-service.services-8293.svc.cluster.local'
    Dec 14 09:05:31.535: INFO: stderr: "+ nslookup clusterip-service.services-8293.svc.cluster.local\n"
    Dec 14 09:05:31.535: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-8293.svc.cluster.local\tcanonical name = externalsvc.services-8293.svc.cluster.local.\nName:\texternalsvc.services-8293.svc.cluster.local\nAddress: 100.109.10.124\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8293, will wait for the garbage collector to delete the pods 12/14/22 09:05:31.535
    Dec 14 09:05:31.612: INFO: Deleting ReplicationController externalsvc took: 5.941205ms
    Dec 14 09:05:31.712: INFO: Terminating ReplicationController externalsvc pods took: 100.624322ms
    Dec 14 09:05:33.324: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:05:33.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8293" for this suite. 12/14/22 09:05:33.335
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:33.34
Dec 14 09:05:33.340: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:05:33.34
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:33.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:33.353
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:05:33.358
Dec 14 09:05:33.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7" in namespace "downward-api-5302" to be "Succeeded or Failed"
Dec 14 09:05:33.371: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064377ms
Dec 14 09:05:35.375: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008640691s
Dec 14 09:05:37.376: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009479076s
STEP: Saw pod success 12/14/22 09:05:37.376
Dec 14 09:05:37.376: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7" satisfied condition "Succeeded or Failed"
Dec 14 09:05:37.380: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7 container client-container: <nil>
STEP: delete the pod 12/14/22 09:05:37.389
Dec 14 09:05:37.398: INFO: Waiting for pod downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7 to disappear
Dec 14 09:05:37.401: INFO: Pod downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 09:05:37.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5302" for this suite. 12/14/22 09:05:37.406
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":195,"skipped":3801,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:33.34
    Dec 14 09:05:33.340: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:05:33.34
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:33.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:33.353
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:05:33.358
    Dec 14 09:05:33.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7" in namespace "downward-api-5302" to be "Succeeded or Failed"
    Dec 14 09:05:33.371: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064377ms
    Dec 14 09:05:35.375: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008640691s
    Dec 14 09:05:37.376: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009479076s
    STEP: Saw pod success 12/14/22 09:05:37.376
    Dec 14 09:05:37.376: INFO: Pod "downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7" satisfied condition "Succeeded or Failed"
    Dec 14 09:05:37.380: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:05:37.389
    Dec 14 09:05:37.398: INFO: Waiting for pod downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7 to disappear
    Dec 14 09:05:37.401: INFO: Pod downwardapi-volume-76e4338c-4b3d-43af-9899-d7e727057db7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 09:05:37.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5302" for this suite. 12/14/22 09:05:37.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:37.411
Dec 14 09:05:37.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:05:37.412
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:37.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:37.427
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:05:37.431
Dec 14 09:05:37.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041" in namespace "downward-api-4764" to be "Succeeded or Failed"
Dec 14 09:05:37.444: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896381ms
Dec 14 09:05:39.449: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041": Phase="Running", Reason="", readiness=false. Elapsed: 2.007600483s
Dec 14 09:05:41.450: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0090945s
STEP: Saw pod success 12/14/22 09:05:41.45
Dec 14 09:05:41.450: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041" satisfied condition "Succeeded or Failed"
Dec 14 09:05:41.454: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041 container client-container: <nil>
STEP: delete the pod 12/14/22 09:05:41.464
Dec 14 09:05:41.472: INFO: Waiting for pod downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041 to disappear
Dec 14 09:05:41.475: INFO: Pod downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 09:05:41.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4764" for this suite. 12/14/22 09:05:41.48
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":196,"skipped":3807,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:37.411
    Dec 14 09:05:37.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:05:37.412
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:37.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:37.427
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:05:37.431
    Dec 14 09:05:37.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041" in namespace "downward-api-4764" to be "Succeeded or Failed"
    Dec 14 09:05:37.444: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896381ms
    Dec 14 09:05:39.449: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041": Phase="Running", Reason="", readiness=false. Elapsed: 2.007600483s
    Dec 14 09:05:41.450: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0090945s
    STEP: Saw pod success 12/14/22 09:05:41.45
    Dec 14 09:05:41.450: INFO: Pod "downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041" satisfied condition "Succeeded or Failed"
    Dec 14 09:05:41.454: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:05:41.464
    Dec 14 09:05:41.472: INFO: Waiting for pod downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041 to disappear
    Dec 14 09:05:41.475: INFO: Pod downwardapi-volume-c0d10507-f9b5-46b9-9881-7233a6964041 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 09:05:41.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4764" for this suite. 12/14/22 09:05:41.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:41.486
Dec 14 09:05:41.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:05:41.487
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:41.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:41.502
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-059d5c3a-9b24-41cc-a8be-fb917ad1ab8e 12/14/22 09:05:41.507
STEP: Creating a pod to test consume secrets 12/14/22 09:05:41.511
Dec 14 09:05:41.521: INFO: Waiting up to 5m0s for pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c" in namespace "secrets-605" to be "Succeeded or Failed"
Dec 14 09:05:41.524: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038678ms
Dec 14 09:05:43.528: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006692787s
Dec 14 09:05:45.529: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007584969s
STEP: Saw pod success 12/14/22 09:05:45.529
Dec 14 09:05:45.529: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c" satisfied condition "Succeeded or Failed"
Dec 14 09:05:45.532: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:05:45.541
Dec 14 09:05:45.546: INFO: Waiting for pod pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c to disappear
Dec 14 09:05:45.549: INFO: Pod pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:05:45.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-605" for this suite. 12/14/22 09:05:45.555
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3828,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:41.486
    Dec 14 09:05:41.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:05:41.487
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:41.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:41.502
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-059d5c3a-9b24-41cc-a8be-fb917ad1ab8e 12/14/22 09:05:41.507
    STEP: Creating a pod to test consume secrets 12/14/22 09:05:41.511
    Dec 14 09:05:41.521: INFO: Waiting up to 5m0s for pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c" in namespace "secrets-605" to be "Succeeded or Failed"
    Dec 14 09:05:41.524: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038678ms
    Dec 14 09:05:43.528: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006692787s
    Dec 14 09:05:45.529: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007584969s
    STEP: Saw pod success 12/14/22 09:05:45.529
    Dec 14 09:05:45.529: INFO: Pod "pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c" satisfied condition "Succeeded or Failed"
    Dec 14 09:05:45.532: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:05:45.541
    Dec 14 09:05:45.546: INFO: Waiting for pod pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c to disappear
    Dec 14 09:05:45.549: INFO: Pod pod-secrets-0a94a004-dd56-4a50-93b3-19a8beccc72c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:05:45.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-605" for this suite. 12/14/22 09:05:45.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:05:45.566
Dec 14 09:05:45.566: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 09:05:45.567
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:45.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:45.582
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Dec 14 09:05:45.600: INFO: Waiting up to 5m0s for pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2" in namespace "container-probe-3040" to be "running and ready"
Dec 14 09:05:45.604: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70995ms
Dec 14 09:05:45.604: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:05:47.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008452152s
Dec 14 09:05:47.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:05:49.618: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 4.017110036s
Dec 14 09:05:49.618: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:05:51.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 6.009407329s
Dec 14 09:05:51.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:05:53.611: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 8.010395106s
Dec 14 09:05:53.611: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:05:55.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 10.008378165s
Dec 14 09:05:55.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:05:57.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 12.009204978s
Dec 14 09:05:57.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:05:59.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 14.008737795s
Dec 14 09:05:59.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:06:01.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 16.00918008s
Dec 14 09:06:01.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:06:03.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 18.009553449s
Dec 14 09:06:03.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:06:05.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 20.008587421s
Dec 14 09:06:05.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
Dec 14 09:06:07.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=true. Elapsed: 22.008621809s
Dec 14 09:06:07.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = true)
Dec 14 09:06:07.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2" satisfied condition "running and ready"
Dec 14 09:06:07.613: INFO: Container started at 2022-12-14 09:05:46 +0000 UTC, pod became ready at 2022-12-14 09:06:05 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 09:06:07.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3040" for this suite. 12/14/22 09:06:07.619
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":198,"skipped":3862,"failed":0}
------------------------------
• [22.057 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:05:45.566
    Dec 14 09:05:45.566: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 09:05:45.567
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:05:45.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:05:45.582
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Dec 14 09:05:45.600: INFO: Waiting up to 5m0s for pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2" in namespace "container-probe-3040" to be "running and ready"
    Dec 14 09:05:45.604: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70995ms
    Dec 14 09:05:45.604: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:05:47.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008452152s
    Dec 14 09:05:47.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:05:49.618: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 4.017110036s
    Dec 14 09:05:49.618: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:05:51.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 6.009407329s
    Dec 14 09:05:51.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:05:53.611: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 8.010395106s
    Dec 14 09:05:53.611: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:05:55.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 10.008378165s
    Dec 14 09:05:55.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:05:57.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 12.009204978s
    Dec 14 09:05:57.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:05:59.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 14.008737795s
    Dec 14 09:05:59.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:06:01.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 16.00918008s
    Dec 14 09:06:01.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:06:03.610: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 18.009553449s
    Dec 14 09:06:03.610: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:06:05.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=false. Elapsed: 20.008587421s
    Dec 14 09:06:05.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = false)
    Dec 14 09:06:07.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2": Phase="Running", Reason="", readiness=true. Elapsed: 22.008621809s
    Dec 14 09:06:07.609: INFO: The phase of Pod test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2 is Running (Ready = true)
    Dec 14 09:06:07.609: INFO: Pod "test-webserver-d41dcade-0394-4a31-9ad0-c2f5ccc48ee2" satisfied condition "running and ready"
    Dec 14 09:06:07.613: INFO: Container started at 2022-12-14 09:05:46 +0000 UTC, pod became ready at 2022-12-14 09:06:05 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 09:06:07.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3040" for this suite. 12/14/22 09:06:07.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:07.624
Dec 14 09:06:07.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 12/14/22 09:06:07.625
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:07.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:07.642
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 12/14/22 09:06:07.649
STEP: Verify that the required pods have come up 12/14/22 09:06:07.653
Dec 14 09:06:07.657: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec 14 09:06:12.663: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 12/14/22 09:06:12.663
Dec 14 09:06:12.667: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 12/14/22 09:06:12.667
STEP: DeleteCollection of the ReplicaSets 12/14/22 09:06:12.671
STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/14/22 09:06:12.677
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 14 09:06:12.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2931" for this suite. 12/14/22 09:06:12.689
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":199,"skipped":3881,"failed":0}
------------------------------
• [5.071 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:07.624
    Dec 14 09:06:07.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 12/14/22 09:06:07.625
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:07.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:07.642
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 12/14/22 09:06:07.649
    STEP: Verify that the required pods have come up 12/14/22 09:06:07.653
    Dec 14 09:06:07.657: INFO: Pod name sample-pod: Found 0 pods out of 3
    Dec 14 09:06:12.663: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 12/14/22 09:06:12.663
    Dec 14 09:06:12.667: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 12/14/22 09:06:12.667
    STEP: DeleteCollection of the ReplicaSets 12/14/22 09:06:12.671
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/14/22 09:06:12.677
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 14 09:06:12.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2931" for this suite. 12/14/22 09:06:12.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:12.695
Dec 14 09:06:12.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:06:12.696
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:12.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:12.713
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-59a1e38f-b8b8-416f-b717-08b770e0f9cc 12/14/22 09:06:12.719
STEP: Creating a pod to test consume secrets 12/14/22 09:06:12.723
Dec 14 09:06:12.734: INFO: Waiting up to 5m0s for pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8" in namespace "secrets-8111" to be "Succeeded or Failed"
Dec 14 09:06:12.740: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.731122ms
Dec 14 09:06:14.745: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011452347s
Dec 14 09:06:16.746: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012083403s
STEP: Saw pod success 12/14/22 09:06:16.746
Dec 14 09:06:16.746: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8" satisfied condition "Succeeded or Failed"
Dec 14 09:06:16.749: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8 container secret-env-test: <nil>
STEP: delete the pod 12/14/22 09:06:16.762
Dec 14 09:06:16.770: INFO: Waiting for pod pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8 to disappear
Dec 14 09:06:16.773: INFO: Pod pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:06:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8111" for this suite. 12/14/22 09:06:16.778
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":200,"skipped":3888,"failed":0}
------------------------------
• [4.087 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:12.695
    Dec 14 09:06:12.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:06:12.696
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:12.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:12.713
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-59a1e38f-b8b8-416f-b717-08b770e0f9cc 12/14/22 09:06:12.719
    STEP: Creating a pod to test consume secrets 12/14/22 09:06:12.723
    Dec 14 09:06:12.734: INFO: Waiting up to 5m0s for pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8" in namespace "secrets-8111" to be "Succeeded or Failed"
    Dec 14 09:06:12.740: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.731122ms
    Dec 14 09:06:14.745: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011452347s
    Dec 14 09:06:16.746: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012083403s
    STEP: Saw pod success 12/14/22 09:06:16.746
    Dec 14 09:06:16.746: INFO: Pod "pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8" satisfied condition "Succeeded or Failed"
    Dec 14 09:06:16.749: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8 container secret-env-test: <nil>
    STEP: delete the pod 12/14/22 09:06:16.762
    Dec 14 09:06:16.770: INFO: Waiting for pod pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8 to disappear
    Dec 14 09:06:16.773: INFO: Pod pod-secrets-c13be0a7-ca79-4346-8210-50ffd32236e8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:06:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8111" for this suite. 12/14/22 09:06:16.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:16.784
Dec 14 09:06:16.784: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:06:16.784
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:16.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:16.8
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-5719 12/14/22 09:06:16.805
STEP: creating service affinity-clusterip-transition in namespace services-5719 12/14/22 09:06:16.805
STEP: creating replication controller affinity-clusterip-transition in namespace services-5719 12/14/22 09:06:16.813
I1214 09:06:16.817987    4635 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5719, replica count: 3
I1214 09:06:19.869161    4635 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:06:19.876: INFO: Creating new exec pod
Dec 14 09:06:19.883: INFO: Waiting up to 5m0s for pod "execpod-affinity4mrgf" in namespace "services-5719" to be "running"
Dec 14 09:06:19.886: INFO: Pod "execpod-affinity4mrgf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20876ms
Dec 14 09:06:21.890: INFO: Pod "execpod-affinity4mrgf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007128715s
Dec 14 09:06:21.890: INFO: Pod "execpod-affinity4mrgf" satisfied condition "running"
Dec 14 09:06:22.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Dec 14 09:06:23.295: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 14 09:06:23.295: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:06:23.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.109.184.96 80'
Dec 14 09:06:23.779: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.109.184.96 80\nConnection to 100.109.184.96 80 port [tcp/http] succeeded!\n"
Dec 14 09:06:23.779: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:06:23.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.109.184.96:80/ ; done'
Dec 14 09:06:24.252: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n"
Dec 14 09:06:24.252: INFO: stdout: "\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-6pk52\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-6pk52\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr"
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-6pk52
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-6pk52
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
Dec 14 09:06:24.260: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.109.184.96:80/ ; done'
Dec 14 09:06:24.754: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n"
Dec 14 09:06:24.754: INFO: stdout: "\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz"
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
Dec 14 09:06:24.755: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5719, will wait for the garbage collector to delete the pods 12/14/22 09:06:24.762
Dec 14 09:06:24.821: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.469552ms
Dec 14 09:06:24.922: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.471607ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:06:26.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5719" for this suite. 12/14/22 09:06:26.94
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":201,"skipped":3935,"failed":0}
------------------------------
• [10.162 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:16.784
    Dec 14 09:06:16.784: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:06:16.784
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:16.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:16.8
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-5719 12/14/22 09:06:16.805
    STEP: creating service affinity-clusterip-transition in namespace services-5719 12/14/22 09:06:16.805
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5719 12/14/22 09:06:16.813
    I1214 09:06:16.817987    4635 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5719, replica count: 3
    I1214 09:06:19.869161    4635 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:06:19.876: INFO: Creating new exec pod
    Dec 14 09:06:19.883: INFO: Waiting up to 5m0s for pod "execpod-affinity4mrgf" in namespace "services-5719" to be "running"
    Dec 14 09:06:19.886: INFO: Pod "execpod-affinity4mrgf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20876ms
    Dec 14 09:06:21.890: INFO: Pod "execpod-affinity4mrgf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007128715s
    Dec 14 09:06:21.890: INFO: Pod "execpod-affinity4mrgf" satisfied condition "running"
    Dec 14 09:06:22.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Dec 14 09:06:23.295: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Dec 14 09:06:23.295: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:06:23.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.109.184.96 80'
    Dec 14 09:06:23.779: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.109.184.96 80\nConnection to 100.109.184.96 80 port [tcp/http] succeeded!\n"
    Dec 14 09:06:23.779: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:06:23.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.109.184.96:80/ ; done'
    Dec 14 09:06:24.252: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n"
    Dec 14 09:06:24.252: INFO: stdout: "\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-6pk52\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-6pk52\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr\naffinity-clusterip-transition-g9zbr"
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-6pk52
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-6pk52
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.252: INFO: Received response from host: affinity-clusterip-transition-g9zbr
    Dec 14 09:06:24.260: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5719 exec execpod-affinity4mrgf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.109.184.96:80/ ; done'
    Dec 14 09:06:24.754: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.109.184.96:80/\n"
    Dec 14 09:06:24.754: INFO: stdout: "\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz\naffinity-clusterip-transition-zt7pz"
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.754: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Received response from host: affinity-clusterip-transition-zt7pz
    Dec 14 09:06:24.755: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5719, will wait for the garbage collector to delete the pods 12/14/22 09:06:24.762
    Dec 14 09:06:24.821: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.469552ms
    Dec 14 09:06:24.922: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.471607ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:06:26.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5719" for this suite. 12/14/22 09:06:26.94
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:26.946
Dec 14 09:06:26.946: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:06:26.946
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:26.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:26.963
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Dec 14 09:06:26.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 09:06:29.432
Dec 14 09:06:29.432: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 create -f -'
Dec 14 09:06:30.092: INFO: stderr: ""
Dec 14 09:06:30.092: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 09:06:30.092: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 delete e2e-test-crd-publish-openapi-5157-crds test-cr'
Dec 14 09:06:30.183: INFO: stderr: ""
Dec 14 09:06:30.183: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 14 09:06:30.183: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 apply -f -'
Dec 14 09:06:30.373: INFO: stderr: ""
Dec 14 09:06:30.373: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 09:06:30.373: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 delete e2e-test-crd-publish-openapi-5157-crds test-cr'
Dec 14 09:06:30.438: INFO: stderr: ""
Dec 14 09:06:30.438: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 12/14/22 09:06:30.438
Dec 14 09:06:30.438: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 explain e2e-test-crd-publish-openapi-5157-crds'
Dec 14 09:06:30.616: INFO: stderr: ""
Dec 14 09:06:30.616: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5157-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:06:33.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4140" for this suite. 12/14/22 09:06:33.101
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":202,"skipped":3944,"failed":0}
------------------------------
• [6.160 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:26.946
    Dec 14 09:06:26.946: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:06:26.946
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:26.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:26.963
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Dec 14 09:06:26.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 09:06:29.432
    Dec 14 09:06:29.432: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 create -f -'
    Dec 14 09:06:30.092: INFO: stderr: ""
    Dec 14 09:06:30.092: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 14 09:06:30.092: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 delete e2e-test-crd-publish-openapi-5157-crds test-cr'
    Dec 14 09:06:30.183: INFO: stderr: ""
    Dec 14 09:06:30.183: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Dec 14 09:06:30.183: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 apply -f -'
    Dec 14 09:06:30.373: INFO: stderr: ""
    Dec 14 09:06:30.373: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 14 09:06:30.373: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 --namespace=crd-publish-openapi-4140 delete e2e-test-crd-publish-openapi-5157-crds test-cr'
    Dec 14 09:06:30.438: INFO: stderr: ""
    Dec 14 09:06:30.438: INFO: stdout: "e2e-test-crd-publish-openapi-5157-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 12/14/22 09:06:30.438
    Dec 14 09:06:30.438: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4140 explain e2e-test-crd-publish-openapi-5157-crds'
    Dec 14 09:06:30.616: INFO: stderr: ""
    Dec 14 09:06:30.616: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5157-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:06:33.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4140" for this suite. 12/14/22 09:06:33.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:33.106
Dec 14 09:06:33.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:06:33.107
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:33.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:33.12
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-e3191df3-9b4e-4d81-a86a-0c81e0b52286 12/14/22 09:06:33.126
STEP: Creating the pod 12/14/22 09:06:33.129
Dec 14 09:06:33.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003" in namespace "configmap-1797" to be "running"
Dec 14 09:06:33.142: INFO: Pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796438ms
Dec 14 09:06:35.147: INFO: Pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003": Phase="Running", Reason="", readiness=false. Elapsed: 2.007948452s
Dec 14 09:06:35.147: INFO: Pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003" satisfied condition "running"
STEP: Waiting for pod with text data 12/14/22 09:06:35.147
STEP: Waiting for pod with binary data 12/14/22 09:06:35.156
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:06:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1797" for this suite. 12/14/22 09:06:35.258
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":203,"skipped":3964,"failed":0}
------------------------------
• [2.160 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:33.106
    Dec 14 09:06:33.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:06:33.107
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:33.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:33.12
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-e3191df3-9b4e-4d81-a86a-0c81e0b52286 12/14/22 09:06:33.126
    STEP: Creating the pod 12/14/22 09:06:33.129
    Dec 14 09:06:33.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003" in namespace "configmap-1797" to be "running"
    Dec 14 09:06:33.142: INFO: Pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796438ms
    Dec 14 09:06:35.147: INFO: Pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003": Phase="Running", Reason="", readiness=false. Elapsed: 2.007948452s
    Dec 14 09:06:35.147: INFO: Pod "pod-configmaps-10292e5f-d4cc-44a7-98ed-7f4a9f43d003" satisfied condition "running"
    STEP: Waiting for pod with text data 12/14/22 09:06:35.147
    STEP: Waiting for pod with binary data 12/14/22 09:06:35.156
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:06:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1797" for this suite. 12/14/22 09:06:35.258
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:35.267
Dec 14 09:06:35.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:06:35.267
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:35.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:35.279
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 12/14/22 09:06:35.283
Dec 14 09:06:35.292: INFO: Waiting up to 5m0s for pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d" in namespace "projected-8326" to be "running and ready"
Dec 14 09:06:35.296: INFO: Pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981189ms
Dec 14 09:06:35.296: INFO: The phase of Pod labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:06:37.300: INFO: Pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007982899s
Dec 14 09:06:37.300: INFO: The phase of Pod labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d is Running (Ready = true)
Dec 14 09:06:37.300: INFO: Pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d" satisfied condition "running and ready"
Dec 14 09:06:37.823: INFO: Successfully updated pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:06:41.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8326" for this suite. 12/14/22 09:06:41.862
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":204,"skipped":3967,"failed":0}
------------------------------
• [6.599 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:35.267
    Dec 14 09:06:35.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:06:35.267
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:35.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:35.279
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 12/14/22 09:06:35.283
    Dec 14 09:06:35.292: INFO: Waiting up to 5m0s for pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d" in namespace "projected-8326" to be "running and ready"
    Dec 14 09:06:35.296: INFO: Pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981189ms
    Dec 14 09:06:35.296: INFO: The phase of Pod labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:06:37.300: INFO: Pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007982899s
    Dec 14 09:06:37.300: INFO: The phase of Pod labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d is Running (Ready = true)
    Dec 14 09:06:37.300: INFO: Pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d" satisfied condition "running and ready"
    Dec 14 09:06:37.823: INFO: Successfully updated pod "labelsupdate42c359c4-36c2-4c99-be36-dee81b9f7e1d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:06:41.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8326" for this suite. 12/14/22 09:06:41.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:41.867
Dec 14 09:06:41.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:06:41.868
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:41.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:41.88
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 12/14/22 09:06:41.886
STEP: waiting for available Endpoint 12/14/22 09:06:41.89
STEP: listing all Endpoints 12/14/22 09:06:41.891
STEP: updating the Endpoint 12/14/22 09:06:41.894
STEP: fetching the Endpoint 12/14/22 09:06:41.899
STEP: patching the Endpoint 12/14/22 09:06:41.902
STEP: fetching the Endpoint 12/14/22 09:06:41.909
STEP: deleting the Endpoint by Collection 12/14/22 09:06:41.915
STEP: waiting for Endpoint deletion 12/14/22 09:06:41.92
STEP: fetching the Endpoint 12/14/22 09:06:41.922
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:06:41.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9132" for this suite. 12/14/22 09:06:41.929
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":205,"skipped":3994,"failed":0}
------------------------------
• [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:41.867
    Dec 14 09:06:41.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:06:41.868
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:41.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:41.88
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 12/14/22 09:06:41.886
    STEP: waiting for available Endpoint 12/14/22 09:06:41.89
    STEP: listing all Endpoints 12/14/22 09:06:41.891
    STEP: updating the Endpoint 12/14/22 09:06:41.894
    STEP: fetching the Endpoint 12/14/22 09:06:41.899
    STEP: patching the Endpoint 12/14/22 09:06:41.902
    STEP: fetching the Endpoint 12/14/22 09:06:41.909
    STEP: deleting the Endpoint by Collection 12/14/22 09:06:41.915
    STEP: waiting for Endpoint deletion 12/14/22 09:06:41.92
    STEP: fetching the Endpoint 12/14/22 09:06:41.922
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:06:41.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9132" for this suite. 12/14/22 09:06:41.929
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:41.934
Dec 14 09:06:41.934: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 12/14/22 09:06:41.934
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:41.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:41.948
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 12/14/22 09:06:41.951
STEP: wait for the container to reach Succeeded 12/14/22 09:06:41.965
STEP: get the container status 12/14/22 09:06:44.981
STEP: the container should be terminated 12/14/22 09:06:44.984
STEP: the termination message should be set 12/14/22 09:06:44.984
Dec 14 09:06:44.984: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/14/22 09:06:44.984
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 14 09:06:44.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1719" for this suite. 12/14/22 09:06:44.998
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":206,"skipped":4002,"failed":0}
------------------------------
• [3.068 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:41.934
    Dec 14 09:06:41.934: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 12/14/22 09:06:41.934
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:41.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:41.948
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 12/14/22 09:06:41.951
    STEP: wait for the container to reach Succeeded 12/14/22 09:06:41.965
    STEP: get the container status 12/14/22 09:06:44.981
    STEP: the container should be terminated 12/14/22 09:06:44.984
    STEP: the termination message should be set 12/14/22 09:06:44.984
    Dec 14 09:06:44.984: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/14/22 09:06:44.984
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 14 09:06:44.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1719" for this suite. 12/14/22 09:06:44.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:45.002
Dec 14 09:06:45.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:06:45.003
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:45.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:45.016
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 12/14/22 09:06:45.022
STEP: watching for the Service to be added 12/14/22 09:06:45.029
Dec 14 09:06:45.031: INFO: Found Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec 14 09:06:45.031: INFO: Service test-service-nlkk6 created
STEP: Getting /status 12/14/22 09:06:45.031
Dec 14 09:06:45.034: INFO: Service test-service-nlkk6 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 12/14/22 09:06:45.034
STEP: watching for the Service to be patched 12/14/22 09:06:45.038
Dec 14 09:06:45.039: INFO: observed Service test-service-nlkk6 in namespace services-3126 with annotations: map[] & LoadBalancer: {[]}
Dec 14 09:06:45.039: INFO: Found Service test-service-nlkk6 in namespace services-3126 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec 14 09:06:45.039: INFO: Service test-service-nlkk6 has service status patched
STEP: updating the ServiceStatus 12/14/22 09:06:45.039
Dec 14 09:06:45.045: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 12/14/22 09:06:45.045
Dec 14 09:06:45.047: INFO: Observed Service test-service-nlkk6 in namespace services-3126 with annotations: map[] & Conditions: {[]}
Dec 14 09:06:45.047: INFO: Observed event: &Service{ObjectMeta:{test-service-nlkk6  services-3126  87d62122-efd7-407a-9c78-317075cc101e 33578 0 2022-12-14 09:06:45 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.108.140.238,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.108.140.238],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec 14 09:06:45.047: INFO: Found Service test-service-nlkk6 in namespace services-3126 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 09:06:45.047: INFO: Service test-service-nlkk6 has service status updated
STEP: patching the service 12/14/22 09:06:45.047
STEP: watching for the Service to be patched 12/14/22 09:06:45.055
Dec 14 09:06:45.057: INFO: observed Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true]
Dec 14 09:06:45.057: INFO: observed Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true]
Dec 14 09:06:45.057: INFO: observed Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true]
Dec 14 09:06:45.057: INFO: Found Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service:patched test-service-static:true]
Dec 14 09:06:45.057: INFO: Service test-service-nlkk6 patched
STEP: deleting the service 12/14/22 09:06:45.057
STEP: watching for the Service to be deleted 12/14/22 09:06:45.065
Dec 14 09:06:45.066: INFO: Observed event: ADDED
Dec 14 09:06:45.066: INFO: Observed event: MODIFIED
Dec 14 09:06:45.066: INFO: Observed event: MODIFIED
Dec 14 09:06:45.067: INFO: Observed event: MODIFIED
Dec 14 09:06:45.067: INFO: Found Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec 14 09:06:45.067: INFO: Service test-service-nlkk6 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:06:45.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3126" for this suite. 12/14/22 09:06:45.07
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":207,"skipped":4008,"failed":0}
------------------------------
• [0.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:45.002
    Dec 14 09:06:45.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:06:45.003
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:45.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:45.016
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 12/14/22 09:06:45.022
    STEP: watching for the Service to be added 12/14/22 09:06:45.029
    Dec 14 09:06:45.031: INFO: Found Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Dec 14 09:06:45.031: INFO: Service test-service-nlkk6 created
    STEP: Getting /status 12/14/22 09:06:45.031
    Dec 14 09:06:45.034: INFO: Service test-service-nlkk6 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 12/14/22 09:06:45.034
    STEP: watching for the Service to be patched 12/14/22 09:06:45.038
    Dec 14 09:06:45.039: INFO: observed Service test-service-nlkk6 in namespace services-3126 with annotations: map[] & LoadBalancer: {[]}
    Dec 14 09:06:45.039: INFO: Found Service test-service-nlkk6 in namespace services-3126 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Dec 14 09:06:45.039: INFO: Service test-service-nlkk6 has service status patched
    STEP: updating the ServiceStatus 12/14/22 09:06:45.039
    Dec 14 09:06:45.045: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 12/14/22 09:06:45.045
    Dec 14 09:06:45.047: INFO: Observed Service test-service-nlkk6 in namespace services-3126 with annotations: map[] & Conditions: {[]}
    Dec 14 09:06:45.047: INFO: Observed event: &Service{ObjectMeta:{test-service-nlkk6  services-3126  87d62122-efd7-407a-9c78-317075cc101e 33578 0 2022-12-14 09:06:45 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.108.140.238,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.108.140.238],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Dec 14 09:06:45.047: INFO: Found Service test-service-nlkk6 in namespace services-3126 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 09:06:45.047: INFO: Service test-service-nlkk6 has service status updated
    STEP: patching the service 12/14/22 09:06:45.047
    STEP: watching for the Service to be patched 12/14/22 09:06:45.055
    Dec 14 09:06:45.057: INFO: observed Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true]
    Dec 14 09:06:45.057: INFO: observed Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true]
    Dec 14 09:06:45.057: INFO: observed Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service-static:true]
    Dec 14 09:06:45.057: INFO: Found Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service:patched test-service-static:true]
    Dec 14 09:06:45.057: INFO: Service test-service-nlkk6 patched
    STEP: deleting the service 12/14/22 09:06:45.057
    STEP: watching for the Service to be deleted 12/14/22 09:06:45.065
    Dec 14 09:06:45.066: INFO: Observed event: ADDED
    Dec 14 09:06:45.066: INFO: Observed event: MODIFIED
    Dec 14 09:06:45.066: INFO: Observed event: MODIFIED
    Dec 14 09:06:45.067: INFO: Observed event: MODIFIED
    Dec 14 09:06:45.067: INFO: Found Service test-service-nlkk6 in namespace services-3126 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Dec 14 09:06:45.067: INFO: Service test-service-nlkk6 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:06:45.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3126" for this suite. 12/14/22 09:06:45.07
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:45.077
Dec 14 09:06:45.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 12/14/22 09:06:45.078
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:45.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:45.089
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 12/14/22 09:06:45.093
STEP: modifying the configmap once 12/14/22 09:06:45.096
STEP: modifying the configmap a second time 12/14/22 09:06:45.101
STEP: deleting the configmap 12/14/22 09:06:45.106
STEP: creating a watch on configmaps from the resource version returned by the first update 12/14/22 09:06:45.109
STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/14/22 09:06:45.111
Dec 14 09:06:45.111: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6202  44f16747-858c-44c8-8255-822dd41a1a4f 33589 0 2022-12-14 09:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:06:45.111: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6202  44f16747-858c-44c8-8255-822dd41a1a4f 33590 0 2022-12-14 09:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 14 09:06:45.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6202" for this suite. 12/14/22 09:06:45.115
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":208,"skipped":4103,"failed":0}
------------------------------
• [0.042 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:45.077
    Dec 14 09:06:45.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 12/14/22 09:06:45.078
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:45.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:45.089
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 12/14/22 09:06:45.093
    STEP: modifying the configmap once 12/14/22 09:06:45.096
    STEP: modifying the configmap a second time 12/14/22 09:06:45.101
    STEP: deleting the configmap 12/14/22 09:06:45.106
    STEP: creating a watch on configmaps from the resource version returned by the first update 12/14/22 09:06:45.109
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/14/22 09:06:45.111
    Dec 14 09:06:45.111: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6202  44f16747-858c-44c8-8255-822dd41a1a4f 33589 0 2022-12-14 09:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:06:45.111: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6202  44f16747-858c-44c8-8255-822dd41a1a4f 33590 0 2022-12-14 09:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 09:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 14 09:06:45.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6202" for this suite. 12/14/22 09:06:45.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:45.122
Dec 14 09:06:45.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 09:06:45.123
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:45.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:45.135
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Dec 14 09:06:45.154: INFO: Waiting up to 5m0s for pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc" in namespace "emptydir-wrapper-5294" to be "running and ready"
Dec 14 09:06:45.157: INFO: Pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913085ms
Dec 14 09:06:45.157: INFO: The phase of Pod pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:06:47.161: INFO: Pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006874576s
Dec 14 09:06:47.161: INFO: The phase of Pod pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc is Running (Ready = true)
Dec 14 09:06:47.161: INFO: Pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc" satisfied condition "running and ready"
STEP: Cleaning up the secret 12/14/22 09:06:47.164
STEP: Cleaning up the configmap 12/14/22 09:06:47.168
STEP: Cleaning up the pod 12/14/22 09:06:47.172
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Dec 14 09:06:47.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5294" for this suite. 12/14/22 09:06:47.182
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":209,"skipped":4117,"failed":0}
------------------------------
• [2.063 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:45.122
    Dec 14 09:06:45.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 09:06:45.123
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:45.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:45.135
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Dec 14 09:06:45.154: INFO: Waiting up to 5m0s for pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc" in namespace "emptydir-wrapper-5294" to be "running and ready"
    Dec 14 09:06:45.157: INFO: Pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913085ms
    Dec 14 09:06:45.157: INFO: The phase of Pod pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:06:47.161: INFO: Pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006874576s
    Dec 14 09:06:47.161: INFO: The phase of Pod pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc is Running (Ready = true)
    Dec 14 09:06:47.161: INFO: Pod "pod-secrets-d834f970-4cdd-4e43-98bc-89e796203abc" satisfied condition "running and ready"
    STEP: Cleaning up the secret 12/14/22 09:06:47.164
    STEP: Cleaning up the configmap 12/14/22 09:06:47.168
    STEP: Cleaning up the pod 12/14/22 09:06:47.172
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:06:47.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5294" for this suite. 12/14/22 09:06:47.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:47.187
Dec 14 09:06:47.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version 12/14/22 09:06:47.188
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:47.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:47.199
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 12/14/22 09:06:47.203
STEP: Confirm major version 12/14/22 09:06:47.204
Dec 14 09:06:47.204: INFO: Major version: 1
STEP: Confirm minor version 12/14/22 09:06:47.204
Dec 14 09:06:47.205: INFO: cleanMinorVersion: 25
Dec 14 09:06:47.205: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Dec 14 09:06:47.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2718" for this suite. 12/14/22 09:06:47.208
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":210,"skipped":4149,"failed":0}
------------------------------
• [0.024 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:47.187
    Dec 14 09:06:47.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename server-version 12/14/22 09:06:47.188
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:47.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:47.199
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 12/14/22 09:06:47.203
    STEP: Confirm major version 12/14/22 09:06:47.204
    Dec 14 09:06:47.204: INFO: Major version: 1
    STEP: Confirm minor version 12/14/22 09:06:47.204
    Dec 14 09:06:47.205: INFO: cleanMinorVersion: 25
    Dec 14 09:06:47.205: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Dec 14 09:06:47.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-2718" for this suite. 12/14/22 09:06:47.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:47.212
Dec 14 09:06:47.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 12/14/22 09:06:47.213
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:47.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:47.224
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 12/14/22 09:06:47.227
Dec 14 09:06:47.227: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 09:06:51.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7989" for this suite. 12/14/22 09:06:51.24
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":211,"skipped":4171,"failed":0}
------------------------------
• [4.032 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:47.212
    Dec 14 09:06:47.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 12/14/22 09:06:47.213
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:47.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:47.224
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 12/14/22 09:06:47.227
    Dec 14 09:06:47.227: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 09:06:51.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7989" for this suite. 12/14/22 09:06:51.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:06:51.245
Dec 14 09:06:51.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:06:51.246
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:51.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:51.258
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-1462 12/14/22 09:06:51.262
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[] 12/14/22 09:06:51.27
Dec 14 09:06:51.277: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1462 12/14/22 09:06:51.277
Dec 14 09:06:51.285: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1462" to be "running and ready"
Dec 14 09:06:51.289: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566521ms
Dec 14 09:06:51.289: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:06:53.293: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007327066s
Dec 14 09:06:53.293: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 14 09:06:53.293: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[pod1:[100]] 12/14/22 09:06:53.295
Dec 14 09:06:53.304: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1462 12/14/22 09:06:53.304
Dec 14 09:06:53.311: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1462" to be "running and ready"
Dec 14 09:06:53.313: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214014ms
Dec 14 09:06:53.313: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:06:55.317: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005723031s
Dec 14 09:06:55.317: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 14 09:06:55.317: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[pod1:[100] pod2:[101]] 12/14/22 09:06:55.319
Dec 14 09:06:55.331: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 12/14/22 09:06:55.331
Dec 14 09:06:55.331: INFO: Creating new exec pod
Dec 14 09:06:55.337: INFO: Waiting up to 5m0s for pod "execpodjc27z" in namespace "services-1462" to be "running"
Dec 14 09:06:55.339: INFO: Pod "execpodjc27z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.659799ms
Dec 14 09:06:57.344: INFO: Pod "execpodjc27z": Phase="Running", Reason="", readiness=true. Elapsed: 2.007537297s
Dec 14 09:06:57.344: INFO: Pod "execpodjc27z" satisfied condition "running"
Dec 14 09:06:58.345: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Dec 14 09:06:58.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec 14 09:06:58.806: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:06:58.806: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.111.68.236 80'
Dec 14 09:06:59.169: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.111.68.236 80\nConnection to 100.111.68.236 80 port [tcp/http] succeeded!\n"
Dec 14 09:06:59.169: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:06:59.169: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Dec 14 09:06:59.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec 14 09:06:59.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:06:59.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.111.68.236 81'
Dec 14 09:06:59.902: INFO: stderr: "+ nc -v -t -w 2 100.111.68.236 81\nConnection to 100.111.68.236 81 port [tcp/*] succeeded!\n+ echo hostName\n"
Dec 14 09:06:59.902: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1462 12/14/22 09:06:59.902
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[pod2:[101]] 12/14/22 09:06:59.909
Dec 14 09:07:00.925: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1462 12/14/22 09:07:00.925
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[] 12/14/22 09:07:00.93
Dec 14 09:07:00.937: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:07:00.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1462" for this suite. 12/14/22 09:07:00.949
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":212,"skipped":4186,"failed":0}
------------------------------
• [9.708 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:06:51.245
    Dec 14 09:06:51.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:06:51.246
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:06:51.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:06:51.258
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-1462 12/14/22 09:06:51.262
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[] 12/14/22 09:06:51.27
    Dec 14 09:06:51.277: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1462 12/14/22 09:06:51.277
    Dec 14 09:06:51.285: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1462" to be "running and ready"
    Dec 14 09:06:51.289: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566521ms
    Dec 14 09:06:51.289: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:06:53.293: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007327066s
    Dec 14 09:06:53.293: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 14 09:06:53.293: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[pod1:[100]] 12/14/22 09:06:53.295
    Dec 14 09:06:53.304: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1462 12/14/22 09:06:53.304
    Dec 14 09:06:53.311: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1462" to be "running and ready"
    Dec 14 09:06:53.313: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214014ms
    Dec 14 09:06:53.313: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:06:55.317: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005723031s
    Dec 14 09:06:55.317: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 14 09:06:55.317: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[pod1:[100] pod2:[101]] 12/14/22 09:06:55.319
    Dec 14 09:06:55.331: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 12/14/22 09:06:55.331
    Dec 14 09:06:55.331: INFO: Creating new exec pod
    Dec 14 09:06:55.337: INFO: Waiting up to 5m0s for pod "execpodjc27z" in namespace "services-1462" to be "running"
    Dec 14 09:06:55.339: INFO: Pod "execpodjc27z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.659799ms
    Dec 14 09:06:57.344: INFO: Pod "execpodjc27z": Phase="Running", Reason="", readiness=true. Elapsed: 2.007537297s
    Dec 14 09:06:57.344: INFO: Pod "execpodjc27z" satisfied condition "running"
    Dec 14 09:06:58.345: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Dec 14 09:06:58.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Dec 14 09:06:58.806: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:06:58.806: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.111.68.236 80'
    Dec 14 09:06:59.169: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.111.68.236 80\nConnection to 100.111.68.236 80 port [tcp/http] succeeded!\n"
    Dec 14 09:06:59.169: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:06:59.169: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Dec 14 09:06:59.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Dec 14 09:06:59.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:06:59.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1462 exec execpodjc27z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.111.68.236 81'
    Dec 14 09:06:59.902: INFO: stderr: "+ nc -v -t -w 2 100.111.68.236 81\nConnection to 100.111.68.236 81 port [tcp/*] succeeded!\n+ echo hostName\n"
    Dec 14 09:06:59.902: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1462 12/14/22 09:06:59.902
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[pod2:[101]] 12/14/22 09:06:59.909
    Dec 14 09:07:00.925: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1462 12/14/22 09:07:00.925
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1462 to expose endpoints map[] 12/14/22 09:07:00.93
    Dec 14 09:07:00.937: INFO: successfully validated that service multi-endpoint-test in namespace services-1462 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:07:00.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1462" for this suite. 12/14/22 09:07:00.949
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:00.953
Dec 14 09:07:00.953: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:07:00.954
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:00.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:00.965
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 12/14/22 09:07:00.968
Dec 14 09:07:00.968: INFO: Creating e2e-svc-a-d5kkw
Dec 14 09:07:00.981: INFO: Creating e2e-svc-b-2bj8g
Dec 14 09:07:00.988: INFO: Creating e2e-svc-c-6sd6m
STEP: deleting service collection 12/14/22 09:07:00.996
Dec 14 09:07:01.011: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:07:01.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7249" for this suite. 12/14/22 09:07:01.014
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":213,"skipped":4200,"failed":0}
------------------------------
• [0.064 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:00.953
    Dec 14 09:07:00.953: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:07:00.954
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:00.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:00.965
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 12/14/22 09:07:00.968
    Dec 14 09:07:00.968: INFO: Creating e2e-svc-a-d5kkw
    Dec 14 09:07:00.981: INFO: Creating e2e-svc-b-2bj8g
    Dec 14 09:07:00.988: INFO: Creating e2e-svc-c-6sd6m
    STEP: deleting service collection 12/14/22 09:07:00.996
    Dec 14 09:07:01.011: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:07:01.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7249" for this suite. 12/14/22 09:07:01.014
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:01.019
Dec 14 09:07:01.019: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:07:01.02
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:01.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:01.042
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Dec 14 09:07:01.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/14/22 09:07:04.556
Dec 14 09:07:04.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
Dec 14 09:07:05.186: INFO: stderr: ""
Dec 14 09:07:05.186: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 09:07:05.187: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 delete e2e-test-crd-publish-openapi-179-crds test-foo'
Dec 14 09:07:05.250: INFO: stderr: ""
Dec 14 09:07:05.250: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 14 09:07:05.250: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 apply -f -'
Dec 14 09:07:05.434: INFO: stderr: ""
Dec 14 09:07:05.434: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 09:07:05.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 delete e2e-test-crd-publish-openapi-179-crds test-foo'
Dec 14 09:07:05.504: INFO: stderr: ""
Dec 14 09:07:05.504: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/14/22 09:07:05.504
Dec 14 09:07:05.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
Dec 14 09:07:06.079: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/14/22 09:07:06.079
Dec 14 09:07:06.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
Dec 14 09:07:06.258: INFO: rc: 1
Dec 14 09:07:06.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 apply -f -'
Dec 14 09:07:06.440: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/14/22 09:07:06.44
Dec 14 09:07:06.440: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
Dec 14 09:07:06.615: INFO: rc: 1
Dec 14 09:07:06.615: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 apply -f -'
Dec 14 09:07:06.805: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 12/14/22 09:07:06.805
Dec 14 09:07:06.806: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds'
Dec 14 09:07:06.987: INFO: stderr: ""
Dec 14 09:07:06.987: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 12/14/22 09:07:06.987
Dec 14 09:07:06.987: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.metadata'
Dec 14 09:07:07.174: INFO: stderr: ""
Dec 14 09:07:07.174: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 14 09:07:07.175: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.spec'
Dec 14 09:07:07.356: INFO: stderr: ""
Dec 14 09:07:07.356: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 14 09:07:07.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.spec.bars'
Dec 14 09:07:07.547: INFO: stderr: ""
Dec 14 09:07:07.547: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/14/22 09:07:07.547
Dec 14 09:07:07.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.spec.bars2'
Dec 14 09:07:07.745: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:07:10.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7707" for this suite. 12/14/22 09:07:10.331
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":214,"skipped":4260,"failed":0}
------------------------------
• [9.315 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:01.019
    Dec 14 09:07:01.019: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:07:01.02
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:01.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:01.042
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Dec 14 09:07:01.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/14/22 09:07:04.556
    Dec 14 09:07:04.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
    Dec 14 09:07:05.186: INFO: stderr: ""
    Dec 14 09:07:05.186: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 14 09:07:05.187: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 delete e2e-test-crd-publish-openapi-179-crds test-foo'
    Dec 14 09:07:05.250: INFO: stderr: ""
    Dec 14 09:07:05.250: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Dec 14 09:07:05.250: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 apply -f -'
    Dec 14 09:07:05.434: INFO: stderr: ""
    Dec 14 09:07:05.434: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 14 09:07:05.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 delete e2e-test-crd-publish-openapi-179-crds test-foo'
    Dec 14 09:07:05.504: INFO: stderr: ""
    Dec 14 09:07:05.504: INFO: stdout: "e2e-test-crd-publish-openapi-179-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/14/22 09:07:05.504
    Dec 14 09:07:05.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
    Dec 14 09:07:06.079: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/14/22 09:07:06.079
    Dec 14 09:07:06.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
    Dec 14 09:07:06.258: INFO: rc: 1
    Dec 14 09:07:06.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 apply -f -'
    Dec 14 09:07:06.440: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/14/22 09:07:06.44
    Dec 14 09:07:06.440: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 create -f -'
    Dec 14 09:07:06.615: INFO: rc: 1
    Dec 14 09:07:06.615: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 --namespace=crd-publish-openapi-7707 apply -f -'
    Dec 14 09:07:06.805: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 12/14/22 09:07:06.805
    Dec 14 09:07:06.806: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds'
    Dec 14 09:07:06.987: INFO: stderr: ""
    Dec 14 09:07:06.987: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 12/14/22 09:07:06.987
    Dec 14 09:07:06.987: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.metadata'
    Dec 14 09:07:07.174: INFO: stderr: ""
    Dec 14 09:07:07.174: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Dec 14 09:07:07.175: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.spec'
    Dec 14 09:07:07.356: INFO: stderr: ""
    Dec 14 09:07:07.356: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Dec 14 09:07:07.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.spec.bars'
    Dec 14 09:07:07.547: INFO: stderr: ""
    Dec 14 09:07:07.547: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-179-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/14/22 09:07:07.547
    Dec 14 09:07:07.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7707 explain e2e-test-crd-publish-openapi-179-crds.spec.bars2'
    Dec 14 09:07:07.745: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:07:10.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7707" for this suite. 12/14/22 09:07:10.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:10.337
Dec 14 09:07:10.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 12/14/22 09:07:10.338
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:10.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:10.35
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 09:07:10.353
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-fkmz 12/14/22 09:07:10.362
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:07:10.362
Dec 14 09:07:10.370: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fkmz" in namespace "subpath-9362" to be "Succeeded or Failed"
Dec 14 09:07:10.372: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428908ms
Dec 14 09:07:12.376: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005904155s
Dec 14 09:07:14.376: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 4.006285526s
Dec 14 09:07:16.376: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 6.006388339s
Dec 14 09:07:18.378: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 8.008043882s
Dec 14 09:07:20.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 10.006926732s
Dec 14 09:07:22.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 12.00734922s
Dec 14 09:07:24.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 14.007396979s
Dec 14 09:07:26.378: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 16.007934727s
Dec 14 09:07:28.378: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 18.007936777s
Dec 14 09:07:30.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 20.007232801s
Dec 14 09:07:32.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=false. Elapsed: 22.007464688s
Dec 14 09:07:34.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007512279s
STEP: Saw pod success 12/14/22 09:07:34.377
Dec 14 09:07:34.377: INFO: Pod "pod-subpath-test-secret-fkmz" satisfied condition "Succeeded or Failed"
Dec 14 09:07:34.380: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-secret-fkmz container test-container-subpath-secret-fkmz: <nil>
STEP: delete the pod 12/14/22 09:07:34.388
Dec 14 09:07:34.394: INFO: Waiting for pod pod-subpath-test-secret-fkmz to disappear
Dec 14 09:07:34.397: INFO: Pod pod-subpath-test-secret-fkmz no longer exists
STEP: Deleting pod pod-subpath-test-secret-fkmz 12/14/22 09:07:34.397
Dec 14 09:07:34.397: INFO: Deleting pod "pod-subpath-test-secret-fkmz" in namespace "subpath-9362"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 14 09:07:34.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9362" for this suite. 12/14/22 09:07:34.414
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":215,"skipped":4351,"failed":0}
------------------------------
• [24.083 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:10.337
    Dec 14 09:07:10.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 12/14/22 09:07:10.338
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:10.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:10.35
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 09:07:10.353
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-fkmz 12/14/22 09:07:10.362
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:07:10.362
    Dec 14 09:07:10.370: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fkmz" in namespace "subpath-9362" to be "Succeeded or Failed"
    Dec 14 09:07:10.372: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428908ms
    Dec 14 09:07:12.376: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005904155s
    Dec 14 09:07:14.376: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 4.006285526s
    Dec 14 09:07:16.376: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 6.006388339s
    Dec 14 09:07:18.378: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 8.008043882s
    Dec 14 09:07:20.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 10.006926732s
    Dec 14 09:07:22.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 12.00734922s
    Dec 14 09:07:24.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 14.007396979s
    Dec 14 09:07:26.378: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 16.007934727s
    Dec 14 09:07:28.378: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 18.007936777s
    Dec 14 09:07:30.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=true. Elapsed: 20.007232801s
    Dec 14 09:07:32.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Running", Reason="", readiness=false. Elapsed: 22.007464688s
    Dec 14 09:07:34.377: INFO: Pod "pod-subpath-test-secret-fkmz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007512279s
    STEP: Saw pod success 12/14/22 09:07:34.377
    Dec 14 09:07:34.377: INFO: Pod "pod-subpath-test-secret-fkmz" satisfied condition "Succeeded or Failed"
    Dec 14 09:07:34.380: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-secret-fkmz container test-container-subpath-secret-fkmz: <nil>
    STEP: delete the pod 12/14/22 09:07:34.388
    Dec 14 09:07:34.394: INFO: Waiting for pod pod-subpath-test-secret-fkmz to disappear
    Dec 14 09:07:34.397: INFO: Pod pod-subpath-test-secret-fkmz no longer exists
    STEP: Deleting pod pod-subpath-test-secret-fkmz 12/14/22 09:07:34.397
    Dec 14 09:07:34.397: INFO: Deleting pod "pod-subpath-test-secret-fkmz" in namespace "subpath-9362"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 14 09:07:34.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9362" for this suite. 12/14/22 09:07:34.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:34.421
Dec 14 09:07:34.421: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:07:34.422
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:34.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:34.433
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:07:34.444
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:07:34.794
STEP: Deploying the webhook pod 12/14/22 09:07:34.798
STEP: Wait for the deployment to be ready 12/14/22 09:07:34.805
Dec 14 09:07:34.810: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:07:36.82
STEP: Verifying the service has paired with the endpoint 12/14/22 09:07:36.828
Dec 14 09:07:37.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 12/14/22 09:07:37.884
STEP: Creating a configMap that should be mutated 12/14/22 09:07:37.957
STEP: Deleting the collection of validation webhooks 12/14/22 09:07:38.833
STEP: Creating a configMap that should not be mutated 12/14/22 09:07:38.854
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:07:38.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3331" for this suite. 12/14/22 09:07:38.865
STEP: Destroying namespace "webhook-3331-markers" for this suite. 12/14/22 09:07:38.868
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":216,"skipped":4359,"failed":0}
------------------------------
• [4.470 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:34.421
    Dec 14 09:07:34.421: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:07:34.422
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:34.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:34.433
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:07:34.444
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:07:34.794
    STEP: Deploying the webhook pod 12/14/22 09:07:34.798
    STEP: Wait for the deployment to be ready 12/14/22 09:07:34.805
    Dec 14 09:07:34.810: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:07:36.82
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:07:36.828
    Dec 14 09:07:37.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 12/14/22 09:07:37.884
    STEP: Creating a configMap that should be mutated 12/14/22 09:07:37.957
    STEP: Deleting the collection of validation webhooks 12/14/22 09:07:38.833
    STEP: Creating a configMap that should not be mutated 12/14/22 09:07:38.854
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:07:38.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3331" for this suite. 12/14/22 09:07:38.865
    STEP: Destroying namespace "webhook-3331-markers" for this suite. 12/14/22 09:07:38.868
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:38.892
Dec 14 09:07:38.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 09:07:38.893
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:38.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:38.913
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 12/14/22 09:07:38.917
STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 09:07:38.921
STEP: delete the deployment 12/14/22 09:07:38.924
STEP: wait for all rs to be garbage collected 12/14/22 09:07:38.928
STEP: expected 0 pods, got 2 pods 12/14/22 09:07:38.936
STEP: Gathering metrics 12/14/22 09:07:39.445
W1214 09:07:39.456869    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 09:07:39.456: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 09:07:39.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9004" for this suite. 12/14/22 09:07:39.46
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":217,"skipped":4367,"failed":0}
------------------------------
• [0.572 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:38.892
    Dec 14 09:07:38.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 09:07:38.893
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:38.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:38.913
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 12/14/22 09:07:38.917
    STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 09:07:38.921
    STEP: delete the deployment 12/14/22 09:07:38.924
    STEP: wait for all rs to be garbage collected 12/14/22 09:07:38.928
    STEP: expected 0 pods, got 2 pods 12/14/22 09:07:38.936
    STEP: Gathering metrics 12/14/22 09:07:39.445
    W1214 09:07:39.456869    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 14 09:07:39.456: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 09:07:39.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9004" for this suite. 12/14/22 09:07:39.46
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:39.464
Dec 14 09:07:39.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 09:07:39.465
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:39.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:39.478
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Dec 14 09:07:39.489: INFO: Waiting up to 2m0s for pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" in namespace "var-expansion-4728" to be "container 0 failed with reason CreateContainerConfigError"
Dec 14 09:07:39.491: INFO: Pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183137ms
Dec 14 09:07:41.496: INFO: Pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006630373s
Dec 14 09:07:41.496: INFO: Pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 14 09:07:41.496: INFO: Deleting pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" in namespace "var-expansion-4728"
Dec 14 09:07:41.501: INFO: Wait up to 5m0s for pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 09:07:43.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4728" for this suite. 12/14/22 09:07:43.512
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":218,"skipped":4368,"failed":0}
------------------------------
• [4.051 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:39.464
    Dec 14 09:07:39.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 09:07:39.465
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:39.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:39.478
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Dec 14 09:07:39.489: INFO: Waiting up to 2m0s for pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" in namespace "var-expansion-4728" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 14 09:07:39.491: INFO: Pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183137ms
    Dec 14 09:07:41.496: INFO: Pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006630373s
    Dec 14 09:07:41.496: INFO: Pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 14 09:07:41.496: INFO: Deleting pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" in namespace "var-expansion-4728"
    Dec 14 09:07:41.501: INFO: Wait up to 5m0s for pod "var-expansion-51b724e7-00e3-435b-85d0-4e6a4e45436b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 09:07:43.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4728" for this suite. 12/14/22 09:07:43.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:43.516
Dec 14 09:07:43.516: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:07:43.517
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:43.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:43.548
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:07:43.561
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:07:43.764
STEP: Deploying the webhook pod 12/14/22 09:07:43.768
STEP: Wait for the deployment to be ready 12/14/22 09:07:43.785
Dec 14 09:07:43.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:07:45.816
STEP: Verifying the service has paired with the endpoint 12/14/22 09:07:45.824
Dec 14 09:07:46.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 12/14/22 09:07:46.829
STEP: Creating a custom resource definition that should be denied by the webhook 12/14/22 09:07:46.899
Dec 14 09:07:46.899: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:07:46.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5050" for this suite. 12/14/22 09:07:46.977
STEP: Destroying namespace "webhook-5050-markers" for this suite. 12/14/22 09:07:46.981
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":219,"skipped":4392,"failed":0}
------------------------------
• [3.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:43.516
    Dec 14 09:07:43.516: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:07:43.517
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:43.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:43.548
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:07:43.561
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:07:43.764
    STEP: Deploying the webhook pod 12/14/22 09:07:43.768
    STEP: Wait for the deployment to be ready 12/14/22 09:07:43.785
    Dec 14 09:07:43.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:07:45.816
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:07:45.824
    Dec 14 09:07:46.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 12/14/22 09:07:46.829
    STEP: Creating a custom resource definition that should be denied by the webhook 12/14/22 09:07:46.899
    Dec 14 09:07:46.899: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:07:46.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5050" for this suite. 12/14/22 09:07:46.977
    STEP: Destroying namespace "webhook-5050-markers" for this suite. 12/14/22 09:07:46.981
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:47.004
Dec 14 09:07:47.004: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 09:07:47.005
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:47.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:47.017
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 12/14/22 09:07:47.02
Dec 14 09:07:47.033: INFO: Waiting up to 5m0s for pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b" in namespace "var-expansion-8878" to be "Succeeded or Failed"
Dec 14 09:07:47.041: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.603771ms
Dec 14 09:07:49.045: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012401504s
Dec 14 09:07:51.045: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012216648s
STEP: Saw pod success 12/14/22 09:07:51.045
Dec 14 09:07:51.045: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b" satisfied condition "Succeeded or Failed"
Dec 14 09:07:51.049: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b container dapi-container: <nil>
STEP: delete the pod 12/14/22 09:07:51.056
Dec 14 09:07:51.063: INFO: Waiting for pod var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b to disappear
Dec 14 09:07:51.066: INFO: Pod var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 09:07:51.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8878" for this suite. 12/14/22 09:07:51.071
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":220,"skipped":4416,"failed":0}
------------------------------
• [4.071 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:47.004
    Dec 14 09:07:47.004: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 09:07:47.005
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:47.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:47.017
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 12/14/22 09:07:47.02
    Dec 14 09:07:47.033: INFO: Waiting up to 5m0s for pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b" in namespace "var-expansion-8878" to be "Succeeded or Failed"
    Dec 14 09:07:47.041: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.603771ms
    Dec 14 09:07:49.045: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012401504s
    Dec 14 09:07:51.045: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012216648s
    STEP: Saw pod success 12/14/22 09:07:51.045
    Dec 14 09:07:51.045: INFO: Pod "var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b" satisfied condition "Succeeded or Failed"
    Dec 14 09:07:51.049: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b container dapi-container: <nil>
    STEP: delete the pod 12/14/22 09:07:51.056
    Dec 14 09:07:51.063: INFO: Waiting for pod var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b to disappear
    Dec 14 09:07:51.066: INFO: Pod var-expansion-ba78515a-e84e-400e-ac4d-c87b9389225b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 09:07:51.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8878" for this suite. 12/14/22 09:07:51.071
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:51.076
Dec 14 09:07:51.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass 12/14/22 09:07:51.076
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:51.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:51.088
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 12/14/22 09:07:51.091
STEP: getting /apis/networking.k8s.io 12/14/22 09:07:51.094
STEP: getting /apis/networking.k8s.iov1 12/14/22 09:07:51.096
STEP: creating 12/14/22 09:07:51.097
STEP: getting 12/14/22 09:07:51.107
STEP: listing 12/14/22 09:07:51.11
STEP: watching 12/14/22 09:07:51.113
Dec 14 09:07:51.113: INFO: starting watch
STEP: patching 12/14/22 09:07:51.114
STEP: updating 12/14/22 09:07:51.117
Dec 14 09:07:51.120: INFO: waiting for watch events with expected annotations
Dec 14 09:07:51.121: INFO: saw patched and updated annotations
STEP: deleting 12/14/22 09:07:51.121
STEP: deleting a collection 12/14/22 09:07:51.129
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Dec 14 09:07:51.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7243" for this suite. 12/14/22 09:07:51.14
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":221,"skipped":4420,"failed":0}
------------------------------
• [0.067 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:51.076
    Dec 14 09:07:51.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ingressclass 12/14/22 09:07:51.076
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:51.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:51.088
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 12/14/22 09:07:51.091
    STEP: getting /apis/networking.k8s.io 12/14/22 09:07:51.094
    STEP: getting /apis/networking.k8s.iov1 12/14/22 09:07:51.096
    STEP: creating 12/14/22 09:07:51.097
    STEP: getting 12/14/22 09:07:51.107
    STEP: listing 12/14/22 09:07:51.11
    STEP: watching 12/14/22 09:07:51.113
    Dec 14 09:07:51.113: INFO: starting watch
    STEP: patching 12/14/22 09:07:51.114
    STEP: updating 12/14/22 09:07:51.117
    Dec 14 09:07:51.120: INFO: waiting for watch events with expected annotations
    Dec 14 09:07:51.121: INFO: saw patched and updated annotations
    STEP: deleting 12/14/22 09:07:51.121
    STEP: deleting a collection 12/14/22 09:07:51.129
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Dec 14 09:07:51.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-7243" for this suite. 12/14/22 09:07:51.14
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:07:51.143
Dec 14 09:07:51.143: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:07:51.144
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:51.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:51.155
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 09:07:51.167: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 09:08:51.214: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:08:51.216
Dec 14 09:08:51.217: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 09:08:51.217
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:08:51.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:08:51.228
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Dec 14 09:08:51.240: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Dec 14 09:08:51.243: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Dec 14 09:08:51.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-414" for this suite. 12/14/22 09:08:51.259
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:08:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2684" for this suite. 12/14/22 09:08:51.271
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":222,"skipped":4423,"failed":0}
------------------------------
• [60.158 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:07:51.143
    Dec 14 09:07:51.143: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:07:51.144
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:07:51.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:07:51.155
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 14 09:07:51.167: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 09:08:51.214: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:08:51.216
    Dec 14 09:08:51.217: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 09:08:51.217
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:08:51.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:08:51.228
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Dec 14 09:08:51.240: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Dec 14 09:08:51.243: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Dec 14 09:08:51.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-414" for this suite. 12/14/22 09:08:51.259
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:08:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2684" for this suite. 12/14/22 09:08:51.271
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:08:51.302
Dec 14 09:08:51.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:08:51.302
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:08:51.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:08:51.313
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 12/14/22 09:08:51.316
Dec 14 09:08:51.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 14 09:08:51.439: INFO: stderr: ""
Dec 14 09:08:51.439: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 12/14/22 09:08:51.439
Dec 14 09:08:51.439: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 14 09:08:51.439: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9551" to be "running and ready, or succeeded"
Dec 14 09:08:51.442: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687481ms
Dec 14 09:08:51.442: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb' to be 'Running' but was 'Pending'
Dec 14 09:08:53.445: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006202002s
Dec 14 09:08:53.445: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 14 09:08:53.445: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 12/14/22 09:08:53.445
Dec 14 09:08:53.445: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator'
Dec 14 09:08:53.539: INFO: stderr: ""
Dec 14 09:08:53.539: INFO: stdout: "I1214 09:08:52.050547       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/mkc 547\nI1214 09:08:52.248833       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wmzl 274\nI1214 09:08:52.449281       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/wr24 312\nI1214 09:08:52.649527       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hhb 220\nI1214 09:08:52.848812       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gz6 250\nI1214 09:08:53.049127       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/zzfk 567\nI1214 09:08:53.249481       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xzkn 328\nI1214 09:08:53.449887       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/4f88 231\n"
STEP: limiting log lines 12/14/22 09:08:53.539
Dec 14 09:08:53.539: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --tail=1'
Dec 14 09:08:53.648: INFO: stderr: ""
Dec 14 09:08:53.648: INFO: stdout: "I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
Dec 14 09:08:53.648: INFO: got output "I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
STEP: limiting log bytes 12/14/22 09:08:53.648
Dec 14 09:08:53.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --limit-bytes=1'
Dec 14 09:08:53.735: INFO: stderr: ""
Dec 14 09:08:53.735: INFO: stdout: "I"
Dec 14 09:08:53.735: INFO: got output "I"
STEP: exposing timestamps 12/14/22 09:08:53.735
Dec 14 09:08:53.735: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 14 09:08:53.831: INFO: stderr: ""
Dec 14 09:08:53.831: INFO: stdout: "2022-12-14T09:08:53.649251358Z I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
Dec 14 09:08:53.831: INFO: got output "2022-12-14T09:08:53.649251358Z I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
STEP: restricting to a time range 12/14/22 09:08:53.831
Dec 14 09:08:56.332: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --since=1s'
Dec 14 09:08:56.429: INFO: stderr: ""
Dec 14 09:08:56.429: INFO: stdout: "I1214 09:08:55.449165       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/9rt4 574\nI1214 09:08:55.649458       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rx4q 340\nI1214 09:08:55.849846       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/6ml4 351\nI1214 09:08:56.049204       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/wt5 396\nI1214 09:08:56.249544       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/95s 515\n"
Dec 14 09:08:56.430: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --since=24h'
Dec 14 09:08:56.531: INFO: stderr: ""
Dec 14 09:08:56.531: INFO: stdout: "I1214 09:08:52.050547       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/mkc 547\nI1214 09:08:52.248833       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wmzl 274\nI1214 09:08:52.449281       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/wr24 312\nI1214 09:08:52.649527       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hhb 220\nI1214 09:08:52.848812       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gz6 250\nI1214 09:08:53.049127       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/zzfk 567\nI1214 09:08:53.249481       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xzkn 328\nI1214 09:08:53.449887       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/4f88 231\nI1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\nI1214 09:08:53.849530       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/p862 241\nI1214 09:08:54.048794       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bt8 390\nI1214 09:08:54.249131       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/zshx 585\nI1214 09:08:54.449497       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/n4l 294\nI1214 09:08:54.648753       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/d8cx 368\nI1214 09:08:54.849089       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/7qt 273\nI1214 09:08:55.049441       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/xg9c 279\nI1214 09:08:55.249865       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/j9p 559\nI1214 09:08:55.449165       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/9rt4 574\nI1214 09:08:55.649458       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rx4q 340\nI1214 09:08:55.849846       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/6ml4 351\nI1214 09:08:56.049204       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/wt5 396\nI1214 09:08:56.249544       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/95s 515\nI1214 09:08:56.448835       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/5xqh 563\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Dec 14 09:08:56.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 delete pod logs-generator'
Dec 14 09:08:57.561: INFO: stderr: ""
Dec 14 09:08:57.562: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:08:57.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9551" for this suite. 12/14/22 09:08:57.566
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":223,"skipped":4444,"failed":0}
------------------------------
• [6.268 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:08:51.302
    Dec 14 09:08:51.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:08:51.302
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:08:51.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:08:51.313
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 12/14/22 09:08:51.316
    Dec 14 09:08:51.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Dec 14 09:08:51.439: INFO: stderr: ""
    Dec 14 09:08:51.439: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 12/14/22 09:08:51.439
    Dec 14 09:08:51.439: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Dec 14 09:08:51.439: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9551" to be "running and ready, or succeeded"
    Dec 14 09:08:51.442: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687481ms
    Dec 14 09:08:51.442: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb' to be 'Running' but was 'Pending'
    Dec 14 09:08:53.445: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006202002s
    Dec 14 09:08:53.445: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Dec 14 09:08:53.445: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 12/14/22 09:08:53.445
    Dec 14 09:08:53.445: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator'
    Dec 14 09:08:53.539: INFO: stderr: ""
    Dec 14 09:08:53.539: INFO: stdout: "I1214 09:08:52.050547       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/mkc 547\nI1214 09:08:52.248833       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wmzl 274\nI1214 09:08:52.449281       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/wr24 312\nI1214 09:08:52.649527       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hhb 220\nI1214 09:08:52.848812       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gz6 250\nI1214 09:08:53.049127       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/zzfk 567\nI1214 09:08:53.249481       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xzkn 328\nI1214 09:08:53.449887       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/4f88 231\n"
    STEP: limiting log lines 12/14/22 09:08:53.539
    Dec 14 09:08:53.539: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --tail=1'
    Dec 14 09:08:53.648: INFO: stderr: ""
    Dec 14 09:08:53.648: INFO: stdout: "I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
    Dec 14 09:08:53.648: INFO: got output "I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
    STEP: limiting log bytes 12/14/22 09:08:53.648
    Dec 14 09:08:53.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --limit-bytes=1'
    Dec 14 09:08:53.735: INFO: stderr: ""
    Dec 14 09:08:53.735: INFO: stdout: "I"
    Dec 14 09:08:53.735: INFO: got output "I"
    STEP: exposing timestamps 12/14/22 09:08:53.735
    Dec 14 09:08:53.735: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --tail=1 --timestamps'
    Dec 14 09:08:53.831: INFO: stderr: ""
    Dec 14 09:08:53.831: INFO: stdout: "2022-12-14T09:08:53.649251358Z I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
    Dec 14 09:08:53.831: INFO: got output "2022-12-14T09:08:53.649251358Z I1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\n"
    STEP: restricting to a time range 12/14/22 09:08:53.831
    Dec 14 09:08:56.332: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --since=1s'
    Dec 14 09:08:56.429: INFO: stderr: ""
    Dec 14 09:08:56.429: INFO: stdout: "I1214 09:08:55.449165       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/9rt4 574\nI1214 09:08:55.649458       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rx4q 340\nI1214 09:08:55.849846       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/6ml4 351\nI1214 09:08:56.049204       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/wt5 396\nI1214 09:08:56.249544       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/95s 515\n"
    Dec 14 09:08:56.430: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 logs logs-generator logs-generator --since=24h'
    Dec 14 09:08:56.531: INFO: stderr: ""
    Dec 14 09:08:56.531: INFO: stdout: "I1214 09:08:52.050547       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/mkc 547\nI1214 09:08:52.248833       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wmzl 274\nI1214 09:08:52.449281       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/wr24 312\nI1214 09:08:52.649527       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hhb 220\nI1214 09:08:52.848812       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gz6 250\nI1214 09:08:53.049127       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/zzfk 567\nI1214 09:08:53.249481       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xzkn 328\nI1214 09:08:53.449887       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/4f88 231\nI1214 09:08:53.649150       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/sb5 371\nI1214 09:08:53.849530       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/p862 241\nI1214 09:08:54.048794       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bt8 390\nI1214 09:08:54.249131       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/zshx 585\nI1214 09:08:54.449497       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/n4l 294\nI1214 09:08:54.648753       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/d8cx 368\nI1214 09:08:54.849089       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/7qt 273\nI1214 09:08:55.049441       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/xg9c 279\nI1214 09:08:55.249865       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/j9p 559\nI1214 09:08:55.449165       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/9rt4 574\nI1214 09:08:55.649458       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rx4q 340\nI1214 09:08:55.849846       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/6ml4 351\nI1214 09:08:56.049204       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/wt5 396\nI1214 09:08:56.249544       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/95s 515\nI1214 09:08:56.448835       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/5xqh 563\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Dec 14 09:08:56.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9551 delete pod logs-generator'
    Dec 14 09:08:57.561: INFO: stderr: ""
    Dec 14 09:08:57.562: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:08:57.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9551" for this suite. 12/14/22 09:08:57.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:08:57.57
Dec 14 09:08:57.570: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:08:57.571
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:08:57.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:08:57.582
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8733 12/14/22 09:08:57.585
STEP: changing the ExternalName service to type=ClusterIP 12/14/22 09:08:57.588
STEP: creating replication controller externalname-service in namespace services-8733 12/14/22 09:08:57.596
I1214 09:08:57.600302    4635 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8733, replica count: 2
I1214 09:09:00.652192    4635 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:09:00.652: INFO: Creating new exec pod
Dec 14 09:09:00.659: INFO: Waiting up to 5m0s for pod "execpod6s68s" in namespace "services-8733" to be "running"
Dec 14 09:09:00.663: INFO: Pod "execpod6s68s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798287ms
Dec 14 09:09:02.667: INFO: Pod "execpod6s68s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008135948s
Dec 14 09:09:02.667: INFO: Pod "execpod6s68s" satisfied condition "running"
Dec 14 09:09:03.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8733 exec execpod6s68s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 09:09:04.143: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 09:09:04.143: INFO: stdout: "externalname-service-xxt4n"
Dec 14 09:09:04.143: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8733 exec execpod6s68s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.111.251.142 80'
Dec 14 09:09:04.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.111.251.142 80\nConnection to 100.111.251.142 80 port [tcp/http] succeeded!\n"
Dec 14 09:09:04.523: INFO: stdout: "externalname-service-jvwqh"
Dec 14 09:09:04.523: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:09:04.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8733" for this suite. 12/14/22 09:09:04.537
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":224,"skipped":4451,"failed":0}
------------------------------
• [6.971 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:08:57.57
    Dec 14 09:08:57.570: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:08:57.571
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:08:57.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:08:57.582
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8733 12/14/22 09:08:57.585
    STEP: changing the ExternalName service to type=ClusterIP 12/14/22 09:08:57.588
    STEP: creating replication controller externalname-service in namespace services-8733 12/14/22 09:08:57.596
    I1214 09:08:57.600302    4635 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8733, replica count: 2
    I1214 09:09:00.652192    4635 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:09:00.652: INFO: Creating new exec pod
    Dec 14 09:09:00.659: INFO: Waiting up to 5m0s for pod "execpod6s68s" in namespace "services-8733" to be "running"
    Dec 14 09:09:00.663: INFO: Pod "execpod6s68s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798287ms
    Dec 14 09:09:02.667: INFO: Pod "execpod6s68s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008135948s
    Dec 14 09:09:02.667: INFO: Pod "execpod6s68s" satisfied condition "running"
    Dec 14 09:09:03.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8733 exec execpod6s68s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Dec 14 09:09:04.143: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 14 09:09:04.143: INFO: stdout: "externalname-service-xxt4n"
    Dec 14 09:09:04.143: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8733 exec execpod6s68s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.111.251.142 80'
    Dec 14 09:09:04.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.111.251.142 80\nConnection to 100.111.251.142 80 port [tcp/http] succeeded!\n"
    Dec 14 09:09:04.523: INFO: stdout: "externalname-service-jvwqh"
    Dec 14 09:09:04.523: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:09:04.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8733" for this suite. 12/14/22 09:09:04.537
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:04.541
Dec 14 09:09:04.541: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 12/14/22 09:09:04.541
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:04.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:04.552
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Dec 14 09:09:04.563: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87" in namespace "security-context-test-3512" to be "Succeeded or Failed"
Dec 14 09:09:04.566: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631805ms
Dec 14 09:09:06.573: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009351624s
Dec 14 09:09:08.570: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006469126s
Dec 14 09:09:08.570: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87" satisfied condition "Succeeded or Failed"
Dec 14 09:09:08.578: INFO: Got logs for pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 14 09:09:08.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3512" for this suite. 12/14/22 09:09:08.582
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":225,"skipped":4452,"failed":0}
------------------------------
• [4.046 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:04.541
    Dec 14 09:09:04.541: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 12/14/22 09:09:04.541
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:04.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:04.552
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Dec 14 09:09:04.563: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87" in namespace "security-context-test-3512" to be "Succeeded or Failed"
    Dec 14 09:09:04.566: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631805ms
    Dec 14 09:09:06.573: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009351624s
    Dec 14 09:09:08.570: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006469126s
    Dec 14 09:09:08.570: INFO: Pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87" satisfied condition "Succeeded or Failed"
    Dec 14 09:09:08.578: INFO: Got logs for pod "busybox-privileged-false-67f6f0e6-345c-4efb-8294-f069bf4e9d87": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 14 09:09:08.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3512" for this suite. 12/14/22 09:09:08.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:08.587
Dec 14 09:09:08.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 09:09:08.588
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:08.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:08.6
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 12/14/22 09:09:08.603
Dec 14 09:09:08.612: INFO: Waiting up to 5m0s for pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a" in namespace "var-expansion-3639" to be "Succeeded or Failed"
Dec 14 09:09:08.615: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999991ms
Dec 14 09:09:10.619: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007890171s
Dec 14 09:09:12.620: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008324353s
STEP: Saw pod success 12/14/22 09:09:12.62
Dec 14 09:09:12.620: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a" satisfied condition "Succeeded or Failed"
Dec 14 09:09:12.623: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a container dapi-container: <nil>
STEP: delete the pod 12/14/22 09:09:12.634
Dec 14 09:09:12.640: INFO: Waiting for pod var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a to disappear
Dec 14 09:09:12.643: INFO: Pod var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 09:09:12.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3639" for this suite. 12/14/22 09:09:12.648
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":226,"skipped":4464,"failed":0}
------------------------------
• [4.064 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:08.587
    Dec 14 09:09:08.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 09:09:08.588
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:08.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:08.6
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 12/14/22 09:09:08.603
    Dec 14 09:09:08.612: INFO: Waiting up to 5m0s for pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a" in namespace "var-expansion-3639" to be "Succeeded or Failed"
    Dec 14 09:09:08.615: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999991ms
    Dec 14 09:09:10.619: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007890171s
    Dec 14 09:09:12.620: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008324353s
    STEP: Saw pod success 12/14/22 09:09:12.62
    Dec 14 09:09:12.620: INFO: Pod "var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a" satisfied condition "Succeeded or Failed"
    Dec 14 09:09:12.623: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a container dapi-container: <nil>
    STEP: delete the pod 12/14/22 09:09:12.634
    Dec 14 09:09:12.640: INFO: Waiting for pod var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a to disappear
    Dec 14 09:09:12.643: INFO: Pod var-expansion-a5f1f479-5815-40bc-a735-aa5f5f937a3a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 09:09:12.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3639" for this suite. 12/14/22 09:09:12.648
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:12.652
Dec 14 09:09:12.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:09:12.652
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:12.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:12.665
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 12/14/22 09:09:12.668
STEP: Getting a ResourceQuota 12/14/22 09:09:12.671
STEP: Updating a ResourceQuota 12/14/22 09:09:12.674
STEP: Verifying a ResourceQuota was modified 12/14/22 09:09:12.677
STEP: Deleting a ResourceQuota 12/14/22 09:09:12.679
STEP: Verifying the deleted ResourceQuota 12/14/22 09:09:12.683
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:09:12.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5333" for this suite. 12/14/22 09:09:12.689
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":227,"skipped":4468,"failed":0}
------------------------------
• [0.041 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:12.652
    Dec 14 09:09:12.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:09:12.652
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:12.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:12.665
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 12/14/22 09:09:12.668
    STEP: Getting a ResourceQuota 12/14/22 09:09:12.671
    STEP: Updating a ResourceQuota 12/14/22 09:09:12.674
    STEP: Verifying a ResourceQuota was modified 12/14/22 09:09:12.677
    STEP: Deleting a ResourceQuota 12/14/22 09:09:12.679
    STEP: Verifying the deleted ResourceQuota 12/14/22 09:09:12.683
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:09:12.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5333" for this suite. 12/14/22 09:09:12.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:12.693
Dec 14 09:09:12.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 12/14/22 09:09:12.693
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:12.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:12.705
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 12/14/22 09:09:12.708
STEP: Waiting for the pdb to be processed 12/14/22 09:09:12.712
STEP: updating the pdb 12/14/22 09:09:12.714
STEP: Waiting for the pdb to be processed 12/14/22 09:09:12.72
STEP: patching the pdb 12/14/22 09:09:12.723
STEP: Waiting for the pdb to be processed 12/14/22 09:09:12.73
STEP: Waiting for the pdb to be deleted 12/14/22 09:09:12.736
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 14 09:09:12.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1277" for this suite. 12/14/22 09:09:12.75
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":228,"skipped":4484,"failed":0}
------------------------------
• [0.061 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:12.693
    Dec 14 09:09:12.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 12/14/22 09:09:12.693
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:12.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:12.705
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 12/14/22 09:09:12.708
    STEP: Waiting for the pdb to be processed 12/14/22 09:09:12.712
    STEP: updating the pdb 12/14/22 09:09:12.714
    STEP: Waiting for the pdb to be processed 12/14/22 09:09:12.72
    STEP: patching the pdb 12/14/22 09:09:12.723
    STEP: Waiting for the pdb to be processed 12/14/22 09:09:12.73
    STEP: Waiting for the pdb to be deleted 12/14/22 09:09:12.736
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 14 09:09:12.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1277" for this suite. 12/14/22 09:09:12.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:12.755
Dec 14 09:09:12.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 12/14/22 09:09:12.755
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:12.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:12.768
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 14 09:09:16.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3748" for this suite. 12/14/22 09:09:16.791
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":229,"skipped":4498,"failed":0}
------------------------------
• [4.039 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:12.755
    Dec 14 09:09:12.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 09:09:12.755
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:12.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:12.768
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 14 09:09:16.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3748" for this suite. 12/14/22 09:09:16.791
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:16.794
Dec 14 09:09:16.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 12/14/22 09:09:16.795
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:16.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:16.806
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/14/22 09:09:16.81
Dec 14 09:09:16.816: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 09:09:21.819: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 09:09:21.819
STEP: getting scale subresource 12/14/22 09:09:21.819
STEP: updating a scale subresource 12/14/22 09:09:21.822
STEP: verifying the replicaset Spec.Replicas was modified 12/14/22 09:09:21.826
STEP: Patch a scale subresource 12/14/22 09:09:21.83
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 14 09:09:21.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8133" for this suite. 12/14/22 09:09:21.841
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":230,"skipped":4501,"failed":0}
------------------------------
• [5.052 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:16.794
    Dec 14 09:09:16.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 12/14/22 09:09:16.795
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:16.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:16.806
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/14/22 09:09:16.81
    Dec 14 09:09:16.816: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 09:09:21.819: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 09:09:21.819
    STEP: getting scale subresource 12/14/22 09:09:21.819
    STEP: updating a scale subresource 12/14/22 09:09:21.822
    STEP: verifying the replicaset Spec.Replicas was modified 12/14/22 09:09:21.826
    STEP: Patch a scale subresource 12/14/22 09:09:21.83
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 14 09:09:21.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8133" for this suite. 12/14/22 09:09:21.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:21.847
Dec 14 09:09:21.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 12/14/22 09:09:21.847
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:21.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:21.859
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 12/14/22 09:09:21.866
STEP: Verify that the required pods have come up. 12/14/22 09:09:21.869
Dec 14 09:09:21.873: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 09:09:26.877: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 09:09:26.878
STEP: Getting /status 12/14/22 09:09:26.878
Dec 14 09:09:26.881: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 12/14/22 09:09:26.881
Dec 14 09:09:26.888: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 12/14/22 09:09:26.888
Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: ADDED
Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.890: INFO: Found replicaset test-rs in namespace replicaset-1561 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 09:09:26.890: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 12/14/22 09:09:26.89
Dec 14 09:09:26.890: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 09:09:26.894: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 12/14/22 09:09:26.894
Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: ADDED
Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.896: INFO: Observed replicaset test-rs in namespace replicaset-1561 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 09:09:26.896: INFO: Found replicaset test-rs in namespace replicaset-1561 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec 14 09:09:26.896: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 14 09:09:26.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1561" for this suite. 12/14/22 09:09:26.901
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":231,"skipped":4512,"failed":0}
------------------------------
• [5.058 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:21.847
    Dec 14 09:09:21.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 12/14/22 09:09:21.847
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:21.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:21.859
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 12/14/22 09:09:21.866
    STEP: Verify that the required pods have come up. 12/14/22 09:09:21.869
    Dec 14 09:09:21.873: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 09:09:26.877: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 09:09:26.878
    STEP: Getting /status 12/14/22 09:09:26.878
    Dec 14 09:09:26.881: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 12/14/22 09:09:26.881
    Dec 14 09:09:26.888: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 12/14/22 09:09:26.888
    Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: ADDED
    Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.890: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.890: INFO: Found replicaset test-rs in namespace replicaset-1561 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 09:09:26.890: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 12/14/22 09:09:26.89
    Dec 14 09:09:26.890: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 14 09:09:26.894: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 12/14/22 09:09:26.894
    Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: ADDED
    Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.896: INFO: Observed replicaset test-rs in namespace replicaset-1561 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 09:09:26.896: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 09:09:26.896: INFO: Found replicaset test-rs in namespace replicaset-1561 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Dec 14 09:09:26.896: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 14 09:09:26.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1561" for this suite. 12/14/22 09:09:26.901
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:26.905
Dec 14 09:09:26.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 09:09:26.906
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:26.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:26.917
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 09:09:26.92
Dec 14 09:09:26.928: INFO: Waiting up to 5m0s for pod "pod-85768896-6d2d-4586-8743-17a30055b35e" in namespace "emptydir-8017" to be "Succeeded or Failed"
Dec 14 09:09:26.931: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821747ms
Dec 14 09:09:28.936: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007388295s
Dec 14 09:09:30.938: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009270783s
STEP: Saw pod success 12/14/22 09:09:30.938
Dec 14 09:09:30.938: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e" satisfied condition "Succeeded or Failed"
Dec 14 09:09:30.941: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-85768896-6d2d-4586-8743-17a30055b35e container test-container: <nil>
STEP: delete the pod 12/14/22 09:09:30.95
Dec 14 09:09:30.958: INFO: Waiting for pod pod-85768896-6d2d-4586-8743-17a30055b35e to disappear
Dec 14 09:09:30.960: INFO: Pod pod-85768896-6d2d-4586-8743-17a30055b35e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 09:09:30.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8017" for this suite. 12/14/22 09:09:30.966
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":232,"skipped":4514,"failed":0}
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:26.905
    Dec 14 09:09:26.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 09:09:26.906
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:26.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:26.917
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 09:09:26.92
    Dec 14 09:09:26.928: INFO: Waiting up to 5m0s for pod "pod-85768896-6d2d-4586-8743-17a30055b35e" in namespace "emptydir-8017" to be "Succeeded or Failed"
    Dec 14 09:09:26.931: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821747ms
    Dec 14 09:09:28.936: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007388295s
    Dec 14 09:09:30.938: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009270783s
    STEP: Saw pod success 12/14/22 09:09:30.938
    Dec 14 09:09:30.938: INFO: Pod "pod-85768896-6d2d-4586-8743-17a30055b35e" satisfied condition "Succeeded or Failed"
    Dec 14 09:09:30.941: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-85768896-6d2d-4586-8743-17a30055b35e container test-container: <nil>
    STEP: delete the pod 12/14/22 09:09:30.95
    Dec 14 09:09:30.958: INFO: Waiting for pod pod-85768896-6d2d-4586-8743-17a30055b35e to disappear
    Dec 14 09:09:30.960: INFO: Pod pod-85768896-6d2d-4586-8743-17a30055b35e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:09:30.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8017" for this suite. 12/14/22 09:09:30.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:30.97
Dec 14 09:09:30.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 09:09:30.971
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:30.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:30.983
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 09:09:30.986
Dec 14 09:09:30.995: INFO: Waiting up to 5m0s for pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba" in namespace "emptydir-7809" to be "Succeeded or Failed"
Dec 14 09:09:30.998: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070388ms
Dec 14 09:09:33.003: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba": Phase="Running", Reason="", readiness=false. Elapsed: 2.00864187s
Dec 14 09:09:35.002: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007575955s
STEP: Saw pod success 12/14/22 09:09:35.002
Dec 14 09:09:35.003: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba" satisfied condition "Succeeded or Failed"
Dec 14 09:09:35.006: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba container test-container: <nil>
STEP: delete the pod 12/14/22 09:09:35.015
Dec 14 09:09:35.022: INFO: Waiting for pod pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba to disappear
Dec 14 09:09:35.028: INFO: Pod pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 09:09:35.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7809" for this suite. 12/14/22 09:09:35.032
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4528,"failed":0}
------------------------------
• [4.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:30.97
    Dec 14 09:09:30.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 09:09:30.971
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:30.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:30.983
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 09:09:30.986
    Dec 14 09:09:30.995: INFO: Waiting up to 5m0s for pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba" in namespace "emptydir-7809" to be "Succeeded or Failed"
    Dec 14 09:09:30.998: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070388ms
    Dec 14 09:09:33.003: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba": Phase="Running", Reason="", readiness=false. Elapsed: 2.00864187s
    Dec 14 09:09:35.002: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007575955s
    STEP: Saw pod success 12/14/22 09:09:35.002
    Dec 14 09:09:35.003: INFO: Pod "pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba" satisfied condition "Succeeded or Failed"
    Dec 14 09:09:35.006: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba container test-container: <nil>
    STEP: delete the pod 12/14/22 09:09:35.015
    Dec 14 09:09:35.022: INFO: Waiting for pod pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba to disappear
    Dec 14 09:09:35.028: INFO: Pod pod-b5640413-e0e9-4c43-a13d-27190ef5e4ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:09:35.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7809" for this suite. 12/14/22 09:09:35.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:09:35.037
Dec 14 09:09:35.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch 12/14/22 09:09:35.038
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:35.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:35.05
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Dec 14 09:09:35.054: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR  12/14/22 09:09:37.621
Dec 14 09:09:37.626: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:37Z]] name:name1 resourceVersion:35184 uid:4e82dacd-e453-4977-896c-d41a45759cc2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 12/14/22 09:09:47.628
Dec 14 09:09:47.634: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:47Z]] name:name2 resourceVersion:35238 uid:01e27038-a9d6-4d9f-965d-dd16416e5a88] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 12/14/22 09:09:57.635
Dec 14 09:09:57.641: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:57Z]] name:name1 resourceVersion:35290 uid:4e82dacd-e453-4977-896c-d41a45759cc2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 12/14/22 09:10:07.641
Dec 14 09:10:07.647: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:10:07Z]] name:name2 resourceVersion:35343 uid:01e27038-a9d6-4d9f-965d-dd16416e5a88] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 12/14/22 09:10:17.649
Dec 14 09:10:17.655: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:57Z]] name:name1 resourceVersion:35415 uid:4e82dacd-e453-4977-896c-d41a45759cc2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 12/14/22 09:10:27.655
Dec 14 09:10:27.662: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:10:07Z]] name:name2 resourceVersion:35467 uid:01e27038-a9d6-4d9f-965d-dd16416e5a88] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:10:38.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5139" for this suite. 12/14/22 09:10:38.181
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":234,"skipped":4559,"failed":0}
------------------------------
• [63.152 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:09:35.037
    Dec 14 09:09:35.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-watch 12/14/22 09:09:35.038
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:09:35.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:09:35.05
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Dec 14 09:09:35.054: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating first CR  12/14/22 09:09:37.621
    Dec 14 09:09:37.626: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:37Z]] name:name1 resourceVersion:35184 uid:4e82dacd-e453-4977-896c-d41a45759cc2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 12/14/22 09:09:47.628
    Dec 14 09:09:47.634: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:47Z]] name:name2 resourceVersion:35238 uid:01e27038-a9d6-4d9f-965d-dd16416e5a88] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 12/14/22 09:09:57.635
    Dec 14 09:09:57.641: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:57Z]] name:name1 resourceVersion:35290 uid:4e82dacd-e453-4977-896c-d41a45759cc2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 12/14/22 09:10:07.641
    Dec 14 09:10:07.647: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:10:07Z]] name:name2 resourceVersion:35343 uid:01e27038-a9d6-4d9f-965d-dd16416e5a88] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 12/14/22 09:10:17.649
    Dec 14 09:10:17.655: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:09:57Z]] name:name1 resourceVersion:35415 uid:4e82dacd-e453-4977-896c-d41a45759cc2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 12/14/22 09:10:27.655
    Dec 14 09:10:27.662: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T09:09:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T09:10:07Z]] name:name2 resourceVersion:35467 uid:01e27038-a9d6-4d9f-965d-dd16416e5a88] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:10:38.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-5139" for this suite. 12/14/22 09:10:38.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:10:38.189
Dec 14 09:10:38.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:10:38.19
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:10:38.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:10:38.205
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 12/14/22 09:10:38.21
Dec 14 09:10:38.210: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6891 cluster-info'
Dec 14 09:10:38.273: INFO: stderr: ""
Dec 14 09:10:38.273: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:10:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6891" for this suite. 12/14/22 09:10:38.278
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":235,"skipped":4564,"failed":0}
------------------------------
• [0.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:10:38.189
    Dec 14 09:10:38.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:10:38.19
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:10:38.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:10:38.205
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 12/14/22 09:10:38.21
    Dec 14 09:10:38.210: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6891 cluster-info'
    Dec 14 09:10:38.273: INFO: stderr: ""
    Dec 14 09:10:38.273: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:10:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6891" for this suite. 12/14/22 09:10:38.278
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:10:38.283
Dec 14 09:10:38.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 12/14/22 09:10:38.284
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:10:38.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:10:38.299
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3942 12/14/22 09:10:38.304
STEP: creating a selector 12/14/22 09:10:38.304
STEP: Creating the service pods in kubernetes 12/14/22 09:10:38.304
Dec 14 09:10:38.304: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 09:10:38.340: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3942" to be "running and ready"
Dec 14 09:10:38.344: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221158ms
Dec 14 09:10:38.344: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:10:40.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010118577s
Dec 14 09:10:40.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:10:42.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010841123s
Dec 14 09:10:42.351: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:10:44.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009946219s
Dec 14 09:10:44.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:10:46.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010777491s
Dec 14 09:10:46.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:10:48.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010428804s
Dec 14 09:10:48.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:10:50.349: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009844664s
Dec 14 09:10:50.350: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 09:10:50.350: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 09:10:50.354: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3942" to be "running and ready"
Dec 14 09:10:50.358: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.448001ms
Dec 14 09:10:50.358: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 09:10:50.358: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 09:10:50.361
Dec 14 09:10:50.378: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3942" to be "running"
Dec 14 09:10:50.381: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014766ms
Dec 14 09:10:52.386: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008535038s
Dec 14 09:10:52.386: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 09:10:52.390: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3942" to be "running"
Dec 14 09:10:52.394: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.340862ms
Dec 14 09:10:52.394: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 14 09:10:52.398: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 14 09:10:52.398: INFO: Going to poll 100.64.0.108 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Dec 14 09:10:52.401: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.108:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3942 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:10:52.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:10:52.402: INFO: ExecWithOptions: Clientset creation
Dec 14 09:10:52.402: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3942/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.0.108%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 09:10:52.695: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 09:10:52.695: INFO: Going to poll 100.64.1.60 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Dec 14 09:10:52.700: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.60:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3942 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:10:52.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:10:52.700: INFO: ExecWithOptions: Clientset creation
Dec 14 09:10:52.700: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3942/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.1.60%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 09:10:52.910: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 14 09:10:52.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3942" for this suite. 12/14/22 09:10:52.917
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":236,"skipped":4565,"failed":0}
------------------------------
• [14.638 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:10:38.283
    Dec 14 09:10:38.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 09:10:38.284
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:10:38.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:10:38.299
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3942 12/14/22 09:10:38.304
    STEP: creating a selector 12/14/22 09:10:38.304
    STEP: Creating the service pods in kubernetes 12/14/22 09:10:38.304
    Dec 14 09:10:38.304: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 09:10:38.340: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3942" to be "running and ready"
    Dec 14 09:10:38.344: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221158ms
    Dec 14 09:10:38.344: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:10:40.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010118577s
    Dec 14 09:10:40.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:10:42.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010841123s
    Dec 14 09:10:42.351: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:10:44.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009946219s
    Dec 14 09:10:44.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:10:46.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010777491s
    Dec 14 09:10:46.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:10:48.350: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010428804s
    Dec 14 09:10:48.350: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:10:50.349: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009844664s
    Dec 14 09:10:50.350: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 09:10:50.350: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 09:10:50.354: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3942" to be "running and ready"
    Dec 14 09:10:50.358: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.448001ms
    Dec 14 09:10:50.358: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 09:10:50.358: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 09:10:50.361
    Dec 14 09:10:50.378: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3942" to be "running"
    Dec 14 09:10:50.381: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014766ms
    Dec 14 09:10:52.386: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008535038s
    Dec 14 09:10:52.386: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 09:10:52.390: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3942" to be "running"
    Dec 14 09:10:52.394: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.340862ms
    Dec 14 09:10:52.394: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 14 09:10:52.398: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec 14 09:10:52.398: INFO: Going to poll 100.64.0.108 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Dec 14 09:10:52.401: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.108:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3942 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:10:52.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:10:52.402: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:10:52.402: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3942/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.0.108%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 09:10:52.695: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 14 09:10:52.695: INFO: Going to poll 100.64.1.60 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Dec 14 09:10:52.700: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.60:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3942 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:10:52.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:10:52.700: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:10:52.700: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3942/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.1.60%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 09:10:52.910: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 14 09:10:52.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3942" for this suite. 12/14/22 09:10:52.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:10:52.922
Dec 14 09:10:52.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 12/14/22 09:10:52.922
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:10:52.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:10:52.939
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 12/14/22 09:10:52.944
STEP: creating a watch on configmaps with label B 12/14/22 09:10:52.947
STEP: creating a watch on configmaps with label A or B 12/14/22 09:10:52.949
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/14/22 09:10:52.951
Dec 14 09:10:52.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35637 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:10:52.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35637 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/14/22 09:10:52.956
Dec 14 09:10:52.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35638 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:10:52.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35638 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/14/22 09:10:52.964
Dec 14 09:10:52.972: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35639 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:10:52.972: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35639 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/14/22 09:10:52.972
Dec 14 09:10:52.977: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35640 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:10:52.977: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35640 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/14/22 09:10:52.977
Dec 14 09:10:52.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35641 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:10:52.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35641 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/14/22 09:11:02.981
Dec 14 09:11:02.987: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35711 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:11:02.987: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35711 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 14 09:11:12.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3766" for this suite. 12/14/22 09:11:12.996
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":237,"skipped":4571,"failed":0}
------------------------------
• [20.079 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:10:52.922
    Dec 14 09:10:52.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 12/14/22 09:10:52.922
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:10:52.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:10:52.939
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 12/14/22 09:10:52.944
    STEP: creating a watch on configmaps with label B 12/14/22 09:10:52.947
    STEP: creating a watch on configmaps with label A or B 12/14/22 09:10:52.949
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/14/22 09:10:52.951
    Dec 14 09:10:52.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35637 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:10:52.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35637 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/14/22 09:10:52.956
    Dec 14 09:10:52.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35638 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:10:52.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35638 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/14/22 09:10:52.964
    Dec 14 09:10:52.972: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35639 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:10:52.972: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35639 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/14/22 09:10:52.972
    Dec 14 09:10:52.977: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35640 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:10:52.977: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3766  cb8a1650-dde6-410c-98d9-31addef7ef0a 35640 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/14/22 09:10:52.977
    Dec 14 09:10:52.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35641 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:10:52.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35641 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/14/22 09:11:02.981
    Dec 14 09:11:02.987: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35711 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:11:02.987: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3766  8c5978c1-5acd-4367-970d-22747f42e91a 35711 0 2022-12-14 09:10:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 09:10:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 14 09:11:12.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3766" for this suite. 12/14/22 09:11:12.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:11:13.002
Dec 14 09:11:13.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 09:11:13.002
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:13.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:13.018
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 12/14/22 09:11:13.028
STEP: create the rc2 12/14/22 09:11:13.032
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/14/22 09:11:18.041
STEP: delete the rc simpletest-rc-to-be-deleted 12/14/22 09:11:18.351
STEP: wait for the rc to be deleted 12/14/22 09:11:18.355
Dec 14 09:11:23.372: INFO: 70 pods remaining
Dec 14 09:11:23.372: INFO: 70 pods has nil DeletionTimestamp
Dec 14 09:11:23.372: INFO: 
STEP: Gathering metrics 12/14/22 09:11:28.372
W1214 09:11:28.383059    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 09:11:28.383: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 09:11:28.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-2957b" in namespace "gc-6467"
Dec 14 09:11:28.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bj6r" in namespace "gc-6467"
Dec 14 09:11:28.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-42b5c" in namespace "gc-6467"
Dec 14 09:11:28.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-46fcd" in namespace "gc-6467"
Dec 14 09:11:28.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-48gz6" in namespace "gc-6467"
Dec 14 09:11:28.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kvp9" in namespace "gc-6467"
Dec 14 09:11:28.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-59qhr" in namespace "gc-6467"
Dec 14 09:11:28.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jfz9" in namespace "gc-6467"
Dec 14 09:11:28.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nbc6" in namespace "gc-6467"
Dec 14 09:11:28.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cvft" in namespace "gc-6467"
Dec 14 09:11:28.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h449" in namespace "gc-6467"
Dec 14 09:11:28.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lr4x" in namespace "gc-6467"
Dec 14 09:11:28.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vr2f" in namespace "gc-6467"
Dec 14 09:11:28.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z79v" in namespace "gc-6467"
Dec 14 09:11:28.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dsrt" in namespace "gc-6467"
Dec 14 09:11:28.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ggjj" in namespace "gc-6467"
Dec 14 09:11:28.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jmx2" in namespace "gc-6467"
Dec 14 09:11:28.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xgpm" in namespace "gc-6467"
Dec 14 09:11:28.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gbr9" in namespace "gc-6467"
Dec 14 09:11:28.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ghbg" in namespace "gc-6467"
Dec 14 09:11:28.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h2lw" in namespace "gc-6467"
Dec 14 09:11:28.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vxzl" in namespace "gc-6467"
Dec 14 09:11:28.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zmd5" in namespace "gc-6467"
Dec 14 09:11:28.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-9529w" in namespace "gc-6467"
Dec 14 09:11:28.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-96t9n" in namespace "gc-6467"
Dec 14 09:11:28.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-9866j" in namespace "gc-6467"
Dec 14 09:11:28.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h2ft" in namespace "gc-6467"
Dec 14 09:11:28.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h5w9" in namespace "gc-6467"
Dec 14 09:11:28.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kjfj" in namespace "gc-6467"
Dec 14 09:11:28.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-9njlb" in namespace "gc-6467"
Dec 14 09:11:28.606: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tfds" in namespace "gc-6467"
Dec 14 09:11:28.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xn88" in namespace "gc-6467"
Dec 14 09:11:28.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7jh5" in namespace "gc-6467"
Dec 14 09:11:28.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcdbd" in namespace "gc-6467"
Dec 14 09:11:28.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfd6g" in namespace "gc-6467"
Dec 14 09:11:28.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr7mt" in namespace "gc-6467"
Dec 14 09:11:28.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxx42" in namespace "gc-6467"
Dec 14 09:11:28.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkltn" in namespace "gc-6467"
Dec 14 09:11:28.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt7m4" in namespace "gc-6467"
Dec 14 09:11:28.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtm99" in namespace "gc-6467"
Dec 14 09:11:28.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdg5j" in namespace "gc-6467"
Dec 14 09:11:28.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjhwz" in namespace "gc-6467"
Dec 14 09:11:28.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjp58" in namespace "gc-6467"
Dec 14 09:11:28.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-grsds" in namespace "gc-6467"
Dec 14 09:11:28.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-gz27x" in namespace "gc-6467"
Dec 14 09:11:28.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcgm7" in namespace "gc-6467"
Dec 14 09:11:28.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-hq2gp" in namespace "gc-6467"
Dec 14 09:11:28.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-httcd" in namespace "gc-6467"
Dec 14 09:11:28.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv5wq" in namespace "gc-6467"
Dec 14 09:11:28.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmkjp" in namespace "gc-6467"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 09:11:28.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6467" for this suite. 12/14/22 09:11:28.745
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":238,"skipped":4593,"failed":0}
------------------------------
• [15.747 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:11:13.002
    Dec 14 09:11:13.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 09:11:13.002
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:13.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:13.018
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 12/14/22 09:11:13.028
    STEP: create the rc2 12/14/22 09:11:13.032
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/14/22 09:11:18.041
    STEP: delete the rc simpletest-rc-to-be-deleted 12/14/22 09:11:18.351
    STEP: wait for the rc to be deleted 12/14/22 09:11:18.355
    Dec 14 09:11:23.372: INFO: 70 pods remaining
    Dec 14 09:11:23.372: INFO: 70 pods has nil DeletionTimestamp
    Dec 14 09:11:23.372: INFO: 
    STEP: Gathering metrics 12/14/22 09:11:28.372
    W1214 09:11:28.383059    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 14 09:11:28.383: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 14 09:11:28.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-2957b" in namespace "gc-6467"
    Dec 14 09:11:28.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bj6r" in namespace "gc-6467"
    Dec 14 09:11:28.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-42b5c" in namespace "gc-6467"
    Dec 14 09:11:28.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-46fcd" in namespace "gc-6467"
    Dec 14 09:11:28.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-48gz6" in namespace "gc-6467"
    Dec 14 09:11:28.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kvp9" in namespace "gc-6467"
    Dec 14 09:11:28.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-59qhr" in namespace "gc-6467"
    Dec 14 09:11:28.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jfz9" in namespace "gc-6467"
    Dec 14 09:11:28.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nbc6" in namespace "gc-6467"
    Dec 14 09:11:28.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cvft" in namespace "gc-6467"
    Dec 14 09:11:28.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h449" in namespace "gc-6467"
    Dec 14 09:11:28.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lr4x" in namespace "gc-6467"
    Dec 14 09:11:28.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vr2f" in namespace "gc-6467"
    Dec 14 09:11:28.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z79v" in namespace "gc-6467"
    Dec 14 09:11:28.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dsrt" in namespace "gc-6467"
    Dec 14 09:11:28.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ggjj" in namespace "gc-6467"
    Dec 14 09:11:28.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jmx2" in namespace "gc-6467"
    Dec 14 09:11:28.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xgpm" in namespace "gc-6467"
    Dec 14 09:11:28.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gbr9" in namespace "gc-6467"
    Dec 14 09:11:28.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ghbg" in namespace "gc-6467"
    Dec 14 09:11:28.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h2lw" in namespace "gc-6467"
    Dec 14 09:11:28.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vxzl" in namespace "gc-6467"
    Dec 14 09:11:28.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zmd5" in namespace "gc-6467"
    Dec 14 09:11:28.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-9529w" in namespace "gc-6467"
    Dec 14 09:11:28.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-96t9n" in namespace "gc-6467"
    Dec 14 09:11:28.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-9866j" in namespace "gc-6467"
    Dec 14 09:11:28.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h2ft" in namespace "gc-6467"
    Dec 14 09:11:28.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h5w9" in namespace "gc-6467"
    Dec 14 09:11:28.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kjfj" in namespace "gc-6467"
    Dec 14 09:11:28.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-9njlb" in namespace "gc-6467"
    Dec 14 09:11:28.606: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tfds" in namespace "gc-6467"
    Dec 14 09:11:28.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xn88" in namespace "gc-6467"
    Dec 14 09:11:28.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7jh5" in namespace "gc-6467"
    Dec 14 09:11:28.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcdbd" in namespace "gc-6467"
    Dec 14 09:11:28.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfd6g" in namespace "gc-6467"
    Dec 14 09:11:28.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr7mt" in namespace "gc-6467"
    Dec 14 09:11:28.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxx42" in namespace "gc-6467"
    Dec 14 09:11:28.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkltn" in namespace "gc-6467"
    Dec 14 09:11:28.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt7m4" in namespace "gc-6467"
    Dec 14 09:11:28.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtm99" in namespace "gc-6467"
    Dec 14 09:11:28.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdg5j" in namespace "gc-6467"
    Dec 14 09:11:28.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjhwz" in namespace "gc-6467"
    Dec 14 09:11:28.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjp58" in namespace "gc-6467"
    Dec 14 09:11:28.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-grsds" in namespace "gc-6467"
    Dec 14 09:11:28.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-gz27x" in namespace "gc-6467"
    Dec 14 09:11:28.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcgm7" in namespace "gc-6467"
    Dec 14 09:11:28.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-hq2gp" in namespace "gc-6467"
    Dec 14 09:11:28.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-httcd" in namespace "gc-6467"
    Dec 14 09:11:28.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv5wq" in namespace "gc-6467"
    Dec 14 09:11:28.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmkjp" in namespace "gc-6467"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 09:11:28.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6467" for this suite. 12/14/22 09:11:28.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:11:28.751
Dec 14 09:11:28.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 09:11:28.751
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:28.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:28.766
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4648 12/14/22 09:11:28.771
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-4648 12/14/22 09:11:28.778
Dec 14 09:11:28.785: INFO: Found 0 stateful pods, waiting for 1
Dec 14 09:11:38.791: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 12/14/22 09:11:38.798
STEP: Getting /status 12/14/22 09:11:38.807
Dec 14 09:11:38.811: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 12/14/22 09:11:38.811
Dec 14 09:11:38.818: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 12/14/22 09:11:38.819
Dec 14 09:11:38.821: INFO: Observed &StatefulSet event: ADDED
Dec 14 09:11:38.821: INFO: Found Statefulset ss in namespace statefulset-4648 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 09:11:38.821: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 12/14/22 09:11:38.821
Dec 14 09:11:38.821: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 09:11:38.826: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 12/14/22 09:11:38.826
Dec 14 09:11:38.829: INFO: Observed &StatefulSet event: ADDED
Dec 14 09:11:38.829: INFO: Observed Statefulset ss in namespace statefulset-4648 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 09:11:38.829: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 09:11:38.830: INFO: Deleting all statefulset in ns statefulset-4648
Dec 14 09:11:38.833: INFO: Scaling statefulset ss to 0
Dec 14 09:11:48.849: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 09:11:48.853: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 09:11:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4648" for this suite. 12/14/22 09:11:48.87
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":239,"skipped":4647,"failed":0}
------------------------------
• [20.125 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:11:28.751
    Dec 14 09:11:28.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 09:11:28.751
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:28.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:28.766
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4648 12/14/22 09:11:28.771
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-4648 12/14/22 09:11:28.778
    Dec 14 09:11:28.785: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 09:11:38.791: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 12/14/22 09:11:38.798
    STEP: Getting /status 12/14/22 09:11:38.807
    Dec 14 09:11:38.811: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 12/14/22 09:11:38.811
    Dec 14 09:11:38.818: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 12/14/22 09:11:38.819
    Dec 14 09:11:38.821: INFO: Observed &StatefulSet event: ADDED
    Dec 14 09:11:38.821: INFO: Found Statefulset ss in namespace statefulset-4648 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 09:11:38.821: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 12/14/22 09:11:38.821
    Dec 14 09:11:38.821: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 14 09:11:38.826: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 12/14/22 09:11:38.826
    Dec 14 09:11:38.829: INFO: Observed &StatefulSet event: ADDED
    Dec 14 09:11:38.829: INFO: Observed Statefulset ss in namespace statefulset-4648 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 09:11:38.829: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 09:11:38.830: INFO: Deleting all statefulset in ns statefulset-4648
    Dec 14 09:11:38.833: INFO: Scaling statefulset ss to 0
    Dec 14 09:11:48.849: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 09:11:48.853: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 09:11:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4648" for this suite. 12/14/22 09:11:48.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:11:48.876
Dec 14 09:11:48.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:11:48.877
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:48.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:48.891
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 12/14/22 09:11:48.896
STEP: Getting a ResourceQuota 12/14/22 09:11:48.899
STEP: Listing all ResourceQuotas with LabelSelector 12/14/22 09:11:48.902
STEP: Patching the ResourceQuota 12/14/22 09:11:48.905
STEP: Deleting a Collection of ResourceQuotas 12/14/22 09:11:48.911
STEP: Verifying the deleted ResourceQuota 12/14/22 09:11:48.916
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:11:48.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6416" for this suite. 12/14/22 09:11:48.923
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":240,"skipped":4660,"failed":0}
------------------------------
• [0.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:11:48.876
    Dec 14 09:11:48.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:11:48.877
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:48.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:48.891
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 12/14/22 09:11:48.896
    STEP: Getting a ResourceQuota 12/14/22 09:11:48.899
    STEP: Listing all ResourceQuotas with LabelSelector 12/14/22 09:11:48.902
    STEP: Patching the ResourceQuota 12/14/22 09:11:48.905
    STEP: Deleting a Collection of ResourceQuotas 12/14/22 09:11:48.911
    STEP: Verifying the deleted ResourceQuota 12/14/22 09:11:48.916
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:11:48.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6416" for this suite. 12/14/22 09:11:48.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:11:48.927
Dec 14 09:11:48.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 09:11:48.928
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:48.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:48.942
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 12/14/22 09:11:48.967
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 09:11:48.971
Dec 14 09:11:48.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 09:11:48.981: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 09:11:49.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 09:11:49.990: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 09:11:50.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 09:11:50.992: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 12/14/22 09:11:50.996
Dec 14 09:11:50.999: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 12/14/22 09:11:50.999
Dec 14 09:11:51.009: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 12/14/22 09:11:51.009
Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: ADDED
Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.012: INFO: Found daemon set daemon-set in namespace daemonsets-2346 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 09:11:51.012: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 12/14/22 09:11:51.012
STEP: watching for the daemon set status to be patched 12/14/22 09:11:51.017
Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: ADDED
Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.020: INFO: Observed daemon set daemon-set in namespace daemonsets-2346 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 09:11:51.021: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 09:11:51.021: INFO: Found daemon set daemon-set in namespace daemonsets-2346 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec 14 09:11:51.021: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/14/22 09:11:51.024
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2346, will wait for the garbage collector to delete the pods 12/14/22 09:11:51.024
Dec 14 09:11:51.084: INFO: Deleting DaemonSet.extensions daemon-set took: 6.136739ms
Dec 14 09:11:51.184: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165161ms
Dec 14 09:11:53.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 09:11:53.489: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 09:11:53.492: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37008"},"items":null}

Dec 14 09:11:53.496: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37008"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:11:53.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2346" for this suite. 12/14/22 09:11:53.512
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":241,"skipped":4666,"failed":0}
------------------------------
• [4.590 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:11:48.927
    Dec 14 09:11:48.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 09:11:48.928
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:48.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:48.942
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 12/14/22 09:11:48.967
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 09:11:48.971
    Dec 14 09:11:48.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 09:11:48.981: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 09:11:49.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 09:11:49.990: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 09:11:50.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 09:11:50.992: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 12/14/22 09:11:50.996
    Dec 14 09:11:50.999: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 12/14/22 09:11:50.999
    Dec 14 09:11:51.009: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 12/14/22 09:11:51.009
    Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: ADDED
    Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.012: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.012: INFO: Found daemon set daemon-set in namespace daemonsets-2346 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 09:11:51.012: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 12/14/22 09:11:51.012
    STEP: watching for the daemon set status to be patched 12/14/22 09:11:51.017
    Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: ADDED
    Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.020: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.020: INFO: Observed daemon set daemon-set in namespace daemonsets-2346 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 09:11:51.021: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 09:11:51.021: INFO: Found daemon set daemon-set in namespace daemonsets-2346 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Dec 14 09:11:51.021: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 09:11:51.024
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2346, will wait for the garbage collector to delete the pods 12/14/22 09:11:51.024
    Dec 14 09:11:51.084: INFO: Deleting DaemonSet.extensions daemon-set took: 6.136739ms
    Dec 14 09:11:51.184: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165161ms
    Dec 14 09:11:53.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 09:11:53.489: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 09:11:53.492: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37008"},"items":null}

    Dec 14 09:11:53.496: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37008"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:11:53.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2346" for this suite. 12/14/22 09:11:53.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:11:53.517
Dec 14 09:11:53.518: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:11:53.518
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:53.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:53.532
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Dec 14 09:11:53.537: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 create -f -'
Dec 14 09:11:54.451: INFO: stderr: ""
Dec 14 09:11:54.451: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 14 09:11:54.451: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 create -f -'
Dec 14 09:11:55.310: INFO: stderr: ""
Dec 14 09:11:55.310: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/14/22 09:11:55.31
Dec 14 09:11:56.315: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:11:56.315: INFO: Found 0 / 1
Dec 14 09:11:57.314: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:11:57.314: INFO: Found 0 / 1
Dec 14 09:11:58.315: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:11:58.315: INFO: Found 0 / 1
Dec 14 09:11:59.315: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:11:59.315: INFO: Found 0 / 1
Dec 14 09:12:00.315: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:12:00.315: INFO: Found 0 / 1
Dec 14 09:12:01.316: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:12:01.316: INFO: Found 1 / 1
Dec 14 09:12:01.316: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 09:12:01.319: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:12:01.319: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 09:12:01.319: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe pod agnhost-primary-p2xq6'
Dec 14 09:12:01.392: INFO: stderr: ""
Dec 14 09:12:01.392: INFO: stdout: "Name:             agnhost-primary-p2xq6\nNamespace:        kubectl-6767\nPriority:         0\nService Account:  default\nNode:             shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb/10.250.3.210\nStart Time:       Wed, 14 Dec 2022 09:11:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: d74188638427fc6ebe4f926cd544e08d80ba813ee0e39d1ad4453faf3ca1eed4\n                  cni.projectcalico.org/podIP: 100.64.1.116/32\n                  cni.projectcalico.org/podIPs: 100.64.1.116/32\nStatus:           Running\nIP:               100.64.1.116\nIPs:\n  IP:           100.64.1.116\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2b7ec5984c26eebc418e01e195af0602ccc7f043d9d0ece7822ae164b398440c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Dec 2022 09:12:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tm5on-jne.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c4bnt (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-c4bnt:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  6s    default-scheduler  Successfully assigned kubectl-6767/agnhost-primary-p2xq6 to shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec 14 09:12:01.392: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe rc agnhost-primary'
Dec 14 09:12:01.466: INFO: stderr: ""
Dec 14 09:12:01.466: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6767\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: agnhost-primary-p2xq6\n"
Dec 14 09:12:01.466: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe service agnhost-primary'
Dec 14 09:12:01.535: INFO: stderr: ""
Dec 14 09:12:01.535: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6767\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.105.244.24\nIPs:               100.105.244.24\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.116:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 14 09:12:01.541: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f'
Dec 14 09:12:01.656: INFO: stderr: ""
Dec 14 09:12:01.656: INFO: stdout: "Name:               shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g_c2_m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-nl-1\n                    failure-domain.beta.kubernetes.io/zone=eu-nl-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=true\n                    node.kubernetes.io/instance-type=g_c2_m4\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu-nl-1a\n                    topology.kubernetes.io/region=eu-nl-1\n                    topology.kubernetes.io/zone=eu-nl-1a\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.25.4\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 696dd4b5b08fea19b9bbf5d88b5067501f1d48e6cfce71761742a772b01789d5\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"477adc92-7308-4e38-a5b8-dda6e42db33b\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"true\",\"no...\n                    projectcalico.org/IPv4Address: 10.250.3.58/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Dec 2022 07:51:53 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 14 Dec 2022 09:11:57 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  ClusterNetworkProblem         False   Wed, 14 Dec 2022 09:09:53 +0000   Wed, 14 Dec 2022 07:53:24 +0000   NoNetworkProblems               no cluster network problems\n  HostNetworkProblem            False   Wed, 14 Dec 2022 09:11:28 +0000   Wed, 14 Dec 2022 07:55:16 +0000   NoNetworkProblems               no host network problems\n  FrequentDockerRestart         False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  CorruptDockerOverlay2         False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  KernelDeadlock                False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  NetworkUnavailable            False   Wed, 14 Dec 2022 07:52:30 +0000   Wed, 14 Dec 2022 07:52:30 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:51:53 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:51:53 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:51:53 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:52:13 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.3.58\n  Hostname:    shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\nCapacity:\n  cpu:                2\n  ephemeral-storage:  63282228Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4008160Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  61560951351\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2857184Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 76e68de91895467ca369bdc8595a0c9f\n  System UUID:                7d320142-2694-795d-3295-f036e5ade948\n  Boot ID:                    963139d2-3c7a-460d-8ebc-24ae3d4161a8\n  Kernel Version:             5.15.77-gardenlinux-cloud-amd64\n  OS Image:                   Garden Linux 934.1\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      100.64.0.0/24\nPodCIDRs:                     100.64.0.0/24\nProviderID:                   openstack:///477adc92-7308-4e38-a5b8-dda6e42db33b\nNon-terminated Pods:          (21 in total)\n  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits    Age\n  ---------                   ----                                                               ------------  ----------  ---------------  -------------    ---\n  kube-system                 addons-nginx-ingress-controller-7fbf48c6b5-rq6ld                   100m (5%)     0 (0%)      128Mi (4%)       4Gi (146%)       68m\n  kube-system                 addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg    0 (0%)        0 (0%)      0 (0%)           0 (0%)           81m\n  kube-system                 apiserver-proxy-2dcbg                                              40m (2%)      0 (0%)      40Mi (1%)        1114Mi (39%)     80m\n  kube-system                 calico-node-9kgrw                                                  250m (13%)    0 (0%)      100Mi (3%)       2800Mi (100%)    80m\n  kube-system                 calico-node-vertical-autoscaler-6597dd8998-xkg8r                   10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       81m\n  kube-system                 calico-typha-deploy-65c54d4db6-mvhhf                               320m (16%)    0 (0%)      262144k (8%)     4194304k (143%)  79m\n  kube-system                 calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c                10m (0%)      0 (0%)      50Mi (1%)        100Mi (3%)       81m\n  kube-system                 calico-typha-vertical-autoscaler-84df655c88-9gbdh                  10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       81m\n  kube-system                 coredns-7869797f4c-ffpdz                                           50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     81m\n  kube-system                 coredns-7869797f4c-ftxgn                                           50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     81m\n  kube-system                 csi-driver-node-gnhxw                                              37m (1%)      0 (0%)      106Mi (3%)       3272Mi (117%)    80m\n  kube-system                 egress-filter-applier-zzlmq                                        50m (2%)      0 (0%)      64Mi (2%)        256Mi (9%)       80m\n  kube-system                 kube-proxy-worker-1-v1.25.4-v2k9w                                  34m (1%)      0 (0%)      47753748 (1%)    2Gi (73%)        41m\n  kube-system                 network-problem-detector-host-5sqjh                                10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        80m\n  kube-system                 network-problem-detector-pod-cqd4k                                 10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        80m\n  kube-system                 node-exporter-vpls6                                                50m (2%)      0 (0%)      50Mi (1%)        250Mi (8%)       80m\n  kube-system                 node-local-dns-dhx6j                                               11m (0%)      0 (0%)      36253748 (1%)    145014992 (4%)   61m\n  kube-system                 node-problem-detector-jh8v5                                        11m (0%)      0 (0%)      36253748 (1%)    120Mi (4%)       54m\n  kube-system                 vpn-shoot-5b855f7f89-mxn54                                         100m (5%)     0 (0%)      100Mi (3%)       100Mi (3%)       81m\n  kubernetes-dashboard        dashboard-metrics-scraper-6d54964d4b-cgnx2                         0 (0%)        0 (0%)      0 (0%)           0 (0%)           81m\n  kubernetes-dashboard        kubernetes-dashboard-7b56c57b5d-hqhsz                              50m (2%)      0 (0%)      50Mi (1%)        256Mi (9%)       81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                1203m (62%)       100m (5%)\n  memory             1307249276 (44%)  23003971792 (786%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type    Reason    Age   From        Message\n  ----    ------    ----  ----        -------\n  Normal  Starting  41m   kube-proxy  \n"
Dec 14 09:12:01.657: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe namespace kubectl-6767'
Dec 14 09:12:01.725: INFO: stderr: ""
Dec 14 09:12:01.725: INFO: stdout: "Name:         kubectl-6767\nLabels:       e2e-framework=kubectl\n              e2e-run=3532263e-47c3-4cea-870e-3d3ec39ba8c0\n              kubernetes.io/metadata.name=kubectl-6767\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:12:01.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6767" for this suite. 12/14/22 09:12:01.731
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":242,"skipped":4671,"failed":0}
------------------------------
• [8.237 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:11:53.517
    Dec 14 09:11:53.518: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:11:53.518
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:11:53.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:11:53.532
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Dec 14 09:11:53.537: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 create -f -'
    Dec 14 09:11:54.451: INFO: stderr: ""
    Dec 14 09:11:54.451: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Dec 14 09:11:54.451: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 create -f -'
    Dec 14 09:11:55.310: INFO: stderr: ""
    Dec 14 09:11:55.310: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/14/22 09:11:55.31
    Dec 14 09:11:56.315: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:11:56.315: INFO: Found 0 / 1
    Dec 14 09:11:57.314: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:11:57.314: INFO: Found 0 / 1
    Dec 14 09:11:58.315: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:11:58.315: INFO: Found 0 / 1
    Dec 14 09:11:59.315: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:11:59.315: INFO: Found 0 / 1
    Dec 14 09:12:00.315: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:12:00.315: INFO: Found 0 / 1
    Dec 14 09:12:01.316: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:12:01.316: INFO: Found 1 / 1
    Dec 14 09:12:01.316: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 14 09:12:01.319: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:12:01.319: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 14 09:12:01.319: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe pod agnhost-primary-p2xq6'
    Dec 14 09:12:01.392: INFO: stderr: ""
    Dec 14 09:12:01.392: INFO: stdout: "Name:             agnhost-primary-p2xq6\nNamespace:        kubectl-6767\nPriority:         0\nService Account:  default\nNode:             shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb/10.250.3.210\nStart Time:       Wed, 14 Dec 2022 09:11:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: d74188638427fc6ebe4f926cd544e08d80ba813ee0e39d1ad4453faf3ca1eed4\n                  cni.projectcalico.org/podIP: 100.64.1.116/32\n                  cni.projectcalico.org/podIPs: 100.64.1.116/32\nStatus:           Running\nIP:               100.64.1.116\nIPs:\n  IP:           100.64.1.116\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2b7ec5984c26eebc418e01e195af0602ccc7f043d9d0ece7822ae164b398440c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Dec 2022 09:12:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tm5on-jne.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c4bnt (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-c4bnt:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  6s    default-scheduler  Successfully assigned kubectl-6767/agnhost-primary-p2xq6 to shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Dec 14 09:12:01.392: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe rc agnhost-primary'
    Dec 14 09:12:01.466: INFO: stderr: ""
    Dec 14 09:12:01.466: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6767\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: agnhost-primary-p2xq6\n"
    Dec 14 09:12:01.466: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe service agnhost-primary'
    Dec 14 09:12:01.535: INFO: stderr: ""
    Dec 14 09:12:01.535: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6767\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.105.244.24\nIPs:               100.105.244.24\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.116:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Dec 14 09:12:01.541: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f'
    Dec 14 09:12:01.656: INFO: stderr: ""
    Dec 14 09:12:01.656: INFO: stdout: "Name:               shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g_c2_m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-nl-1\n                    failure-domain.beta.kubernetes.io/zone=eu-nl-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=true\n                    node.kubernetes.io/instance-type=g_c2_m4\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu-nl-1a\n                    topology.kubernetes.io/region=eu-nl-1\n                    topology.kubernetes.io/zone=eu-nl-1a\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.25.4\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 696dd4b5b08fea19b9bbf5d88b5067501f1d48e6cfce71761742a772b01789d5\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"477adc92-7308-4e38-a5b8-dda6e42db33b\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"true\",\"no...\n                    projectcalico.org/IPv4Address: 10.250.3.58/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Dec 2022 07:51:53 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 14 Dec 2022 09:11:57 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  ClusterNetworkProblem         False   Wed, 14 Dec 2022 09:09:53 +0000   Wed, 14 Dec 2022 07:53:24 +0000   NoNetworkProblems               no cluster network problems\n  HostNetworkProblem            False   Wed, 14 Dec 2022 09:11:28 +0000   Wed, 14 Dec 2022 07:55:16 +0000   NoNetworkProblems               no host network problems\n  FrequentDockerRestart         False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  CorruptDockerOverlay2         False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  KernelDeadlock                False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Wed, 14 Dec 2022 09:07:15 +0000   Wed, 14 Dec 2022 08:17:10 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  NetworkUnavailable            False   Wed, 14 Dec 2022 07:52:30 +0000   Wed, 14 Dec 2022 07:52:30 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:51:53 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:51:53 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:51:53 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Wed, 14 Dec 2022 09:11:56 +0000   Wed, 14 Dec 2022 07:52:13 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.3.58\n  Hostname:    shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f\nCapacity:\n  cpu:                2\n  ephemeral-storage:  63282228Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4008160Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  61560951351\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2857184Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 76e68de91895467ca369bdc8595a0c9f\n  System UUID:                7d320142-2694-795d-3295-f036e5ade948\n  Boot ID:                    963139d2-3c7a-460d-8ebc-24ae3d4161a8\n  Kernel Version:             5.15.77-gardenlinux-cloud-amd64\n  OS Image:                   Garden Linux 934.1\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      100.64.0.0/24\nPodCIDRs:                     100.64.0.0/24\nProviderID:                   openstack:///477adc92-7308-4e38-a5b8-dda6e42db33b\nNon-terminated Pods:          (21 in total)\n  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits    Age\n  ---------                   ----                                                               ------------  ----------  ---------------  -------------    ---\n  kube-system                 addons-nginx-ingress-controller-7fbf48c6b5-rq6ld                   100m (5%)     0 (0%)      128Mi (4%)       4Gi (146%)       68m\n  kube-system                 addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg    0 (0%)        0 (0%)      0 (0%)           0 (0%)           81m\n  kube-system                 apiserver-proxy-2dcbg                                              40m (2%)      0 (0%)      40Mi (1%)        1114Mi (39%)     80m\n  kube-system                 calico-node-9kgrw                                                  250m (13%)    0 (0%)      100Mi (3%)       2800Mi (100%)    80m\n  kube-system                 calico-node-vertical-autoscaler-6597dd8998-xkg8r                   10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       81m\n  kube-system                 calico-typha-deploy-65c54d4db6-mvhhf                               320m (16%)    0 (0%)      262144k (8%)     4194304k (143%)  79m\n  kube-system                 calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c                10m (0%)      0 (0%)      50Mi (1%)        100Mi (3%)       81m\n  kube-system                 calico-typha-vertical-autoscaler-84df655c88-9gbdh                  10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       81m\n  kube-system                 coredns-7869797f4c-ffpdz                                           50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     81m\n  kube-system                 coredns-7869797f4c-ftxgn                                           50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     81m\n  kube-system                 csi-driver-node-gnhxw                                              37m (1%)      0 (0%)      106Mi (3%)       3272Mi (117%)    80m\n  kube-system                 egress-filter-applier-zzlmq                                        50m (2%)      0 (0%)      64Mi (2%)        256Mi (9%)       80m\n  kube-system                 kube-proxy-worker-1-v1.25.4-v2k9w                                  34m (1%)      0 (0%)      47753748 (1%)    2Gi (73%)        41m\n  kube-system                 network-problem-detector-host-5sqjh                                10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        80m\n  kube-system                 network-problem-detector-pod-cqd4k                                 10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        80m\n  kube-system                 node-exporter-vpls6                                                50m (2%)      0 (0%)      50Mi (1%)        250Mi (8%)       80m\n  kube-system                 node-local-dns-dhx6j                                               11m (0%)      0 (0%)      36253748 (1%)    145014992 (4%)   61m\n  kube-system                 node-problem-detector-jh8v5                                        11m (0%)      0 (0%)      36253748 (1%)    120Mi (4%)       54m\n  kube-system                 vpn-shoot-5b855f7f89-mxn54                                         100m (5%)     0 (0%)      100Mi (3%)       100Mi (3%)       81m\n  kubernetes-dashboard        dashboard-metrics-scraper-6d54964d4b-cgnx2                         0 (0%)        0 (0%)      0 (0%)           0 (0%)           81m\n  kubernetes-dashboard        kubernetes-dashboard-7b56c57b5d-hqhsz                              50m (2%)      0 (0%)      50Mi (1%)        256Mi (9%)       81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                1203m (62%)       100m (5%)\n  memory             1307249276 (44%)  23003971792 (786%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type    Reason    Age   From        Message\n  ----    ------    ----  ----        -------\n  Normal  Starting  41m   kube-proxy  \n"
    Dec 14 09:12:01.657: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6767 describe namespace kubectl-6767'
    Dec 14 09:12:01.725: INFO: stderr: ""
    Dec 14 09:12:01.725: INFO: stdout: "Name:         kubectl-6767\nLabels:       e2e-framework=kubectl\n              e2e-run=3532263e-47c3-4cea-870e-3d3ec39ba8c0\n              kubernetes.io/metadata.name=kubectl-6767\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:12:01.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6767" for this suite. 12/14/22 09:12:01.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:12:01.755
Dec 14 09:12:01.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 12/14/22 09:12:01.756
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:12:01.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:12:01.771
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 12/14/22 09:12:01.776
STEP: Ensuring active pods == parallelism 12/14/22 09:12:01.781
STEP: Orphaning one of the Job's Pods 12/14/22 09:12:07.786
Dec 14 09:12:08.301: INFO: Successfully updated pod "adopt-release-jgd4l"
STEP: Checking that the Job readopts the Pod 12/14/22 09:12:08.301
Dec 14 09:12:08.301: INFO: Waiting up to 15m0s for pod "adopt-release-jgd4l" in namespace "job-9386" to be "adopted"
Dec 14 09:12:08.304: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 3.230095ms
Dec 14 09:12:10.309: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.007979403s
Dec 14 09:12:10.309: INFO: Pod "adopt-release-jgd4l" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 12/14/22 09:12:10.309
Dec 14 09:12:10.819: INFO: Successfully updated pod "adopt-release-jgd4l"
STEP: Checking that the Job releases the Pod 12/14/22 09:12:10.819
Dec 14 09:12:10.819: INFO: Waiting up to 15m0s for pod "adopt-release-jgd4l" in namespace "job-9386" to be "released"
Dec 14 09:12:10.822: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 3.242369ms
Dec 14 09:12:12.828: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.008765463s
Dec 14 09:12:12.828: INFO: Pod "adopt-release-jgd4l" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 14 09:12:12.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9386" for this suite. 12/14/22 09:12:12.833
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":243,"skipped":4700,"failed":0}
------------------------------
• [11.082 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:12:01.755
    Dec 14 09:12:01.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 12/14/22 09:12:01.756
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:12:01.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:12:01.771
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 12/14/22 09:12:01.776
    STEP: Ensuring active pods == parallelism 12/14/22 09:12:01.781
    STEP: Orphaning one of the Job's Pods 12/14/22 09:12:07.786
    Dec 14 09:12:08.301: INFO: Successfully updated pod "adopt-release-jgd4l"
    STEP: Checking that the Job readopts the Pod 12/14/22 09:12:08.301
    Dec 14 09:12:08.301: INFO: Waiting up to 15m0s for pod "adopt-release-jgd4l" in namespace "job-9386" to be "adopted"
    Dec 14 09:12:08.304: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 3.230095ms
    Dec 14 09:12:10.309: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.007979403s
    Dec 14 09:12:10.309: INFO: Pod "adopt-release-jgd4l" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 12/14/22 09:12:10.309
    Dec 14 09:12:10.819: INFO: Successfully updated pod "adopt-release-jgd4l"
    STEP: Checking that the Job releases the Pod 12/14/22 09:12:10.819
    Dec 14 09:12:10.819: INFO: Waiting up to 15m0s for pod "adopt-release-jgd4l" in namespace "job-9386" to be "released"
    Dec 14 09:12:10.822: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 3.242369ms
    Dec 14 09:12:12.828: INFO: Pod "adopt-release-jgd4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.008765463s
    Dec 14 09:12:12.828: INFO: Pod "adopt-release-jgd4l" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 14 09:12:12.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9386" for this suite. 12/14/22 09:12:12.833
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:12:12.837
Dec 14 09:12:12.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency 12/14/22 09:12:12.838
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:12:12.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:12:12.854
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Dec 14 09:12:12.859: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6492 12/14/22 09:12:12.859
I1214 09:12:12.863577    4635 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6492, replica count: 1
I1214 09:12:13.914108    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 09:12:14.914511    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 09:12:15.914976    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 09:12:16.915117    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 09:12:17.915347    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 09:12:18.916358    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:12:19.026: INFO: Created: latency-svc-rnttb
Dec 14 09:12:19.029: INFO: Got endpoints: latency-svc-rnttb [12.44351ms]
Dec 14 09:12:19.038: INFO: Created: latency-svc-5g5nl
Dec 14 09:12:19.040: INFO: Got endpoints: latency-svc-5g5nl [11.381647ms]
Dec 14 09:12:19.041: INFO: Created: latency-svc-hfl9k
Dec 14 09:12:19.043: INFO: Got endpoints: latency-svc-hfl9k [14.366897ms]
Dec 14 09:12:19.044: INFO: Created: latency-svc-msq26
Dec 14 09:12:19.047: INFO: Got endpoints: latency-svc-msq26 [18.338746ms]
Dec 14 09:12:19.048: INFO: Created: latency-svc-vn2v8
Dec 14 09:12:19.052: INFO: Got endpoints: latency-svc-vn2v8 [22.911663ms]
Dec 14 09:12:19.102: INFO: Created: latency-svc-jc5sh
Dec 14 09:12:19.102: INFO: Created: latency-svc-njh6r
Dec 14 09:12:19.102: INFO: Created: latency-svc-pkdtj
Dec 14 09:12:19.102: INFO: Created: latency-svc-hkcbd
Dec 14 09:12:19.102: INFO: Created: latency-svc-hw6ck
Dec 14 09:12:19.102: INFO: Created: latency-svc-46v7b
Dec 14 09:12:19.103: INFO: Created: latency-svc-4z6sx
Dec 14 09:12:19.103: INFO: Created: latency-svc-xkwxf
Dec 14 09:12:19.103: INFO: Created: latency-svc-4p7qw
Dec 14 09:12:19.103: INFO: Created: latency-svc-xlvwf
Dec 14 09:12:19.104: INFO: Created: latency-svc-ldst5
Dec 14 09:12:19.104: INFO: Created: latency-svc-nt7s7
Dec 14 09:12:19.104: INFO: Created: latency-svc-tkft8
Dec 14 09:12:19.104: INFO: Got endpoints: latency-svc-njh6r [74.657059ms]
Dec 14 09:12:19.104: INFO: Created: latency-svc-mmbbj
Dec 14 09:12:19.104: INFO: Got endpoints: latency-svc-mmbbj [75.251006ms]
Dec 14 09:12:19.105: INFO: Created: latency-svc-hzt67
Dec 14 09:12:19.105: INFO: Got endpoints: latency-svc-hzt67 [76.031909ms]
Dec 14 09:12:19.105: INFO: Got endpoints: latency-svc-jc5sh [61.947621ms]
Dec 14 09:12:19.110: INFO: Got endpoints: latency-svc-xkwxf [80.412374ms]
Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-nt7s7 [83.28587ms]
Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-ldst5 [72.838519ms]
Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-hw6ck [83.765745ms]
Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-4z6sx [66.137619ms]
Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-pkdtj [84.431284ms]
Dec 14 09:12:19.114: INFO: Created: latency-svc-m9v7c
Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-4p7qw [84.406292ms]
Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-46v7b [61.731717ms]
Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-xlvwf [84.698309ms]
Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-hkcbd [84.471427ms]
Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-tkft8 [84.555539ms]
Dec 14 09:12:19.118: INFO: Got endpoints: latency-svc-m9v7c [13.852565ms]
Dec 14 09:12:19.118: INFO: Created: latency-svc-dq2fp
Dec 14 09:12:19.120: INFO: Got endpoints: latency-svc-dq2fp [15.498263ms]
Dec 14 09:12:19.123: INFO: Created: latency-svc-nnt98
Dec 14 09:12:19.125: INFO: Got endpoints: latency-svc-nnt98 [19.837596ms]
Dec 14 09:12:19.128: INFO: Created: latency-svc-wcrpm
Dec 14 09:12:19.129: INFO: Got endpoints: latency-svc-wcrpm [23.982768ms]
Dec 14 09:12:19.133: INFO: Created: latency-svc-g6gdd
Dec 14 09:12:19.149: INFO: Got endpoints: latency-svc-g6gdd [39.465549ms]
Dec 14 09:12:19.150: INFO: Created: latency-svc-kcvck
Dec 14 09:12:19.155: INFO: Got endpoints: latency-svc-kcvck [41.962138ms]
Dec 14 09:12:19.157: INFO: Created: latency-svc-2zqtp
Dec 14 09:12:19.161: INFO: Got endpoints: latency-svc-2zqtp [47.599845ms]
Dec 14 09:12:19.163: INFO: Created: latency-svc-7psvm
Dec 14 09:12:19.167: INFO: Got endpoints: latency-svc-7psvm [53.493248ms]
Dec 14 09:12:19.167: INFO: Created: latency-svc-5z2nt
Dec 14 09:12:19.170: INFO: Got endpoints: latency-svc-5z2nt [56.369042ms]
Dec 14 09:12:19.172: INFO: Created: latency-svc-dzn82
Dec 14 09:12:19.176: INFO: Got endpoints: latency-svc-dzn82 [62.05117ms]
Dec 14 09:12:19.177: INFO: Created: latency-svc-gzjbs
Dec 14 09:12:19.182: INFO: Got endpoints: latency-svc-gzjbs [68.552155ms]
Dec 14 09:12:19.184: INFO: Created: latency-svc-gtt7h
Dec 14 09:12:19.188: INFO: Got endpoints: latency-svc-gtt7h [73.968495ms]
Dec 14 09:12:19.188: INFO: Created: latency-svc-95qmm
Dec 14 09:12:19.198: INFO: Got endpoints: latency-svc-95qmm [83.894026ms]
Dec 14 09:12:19.198: INFO: Created: latency-svc-7nfg2
Dec 14 09:12:19.203: INFO: Created: latency-svc-mhvhj
Dec 14 09:12:19.206: INFO: Created: latency-svc-lb2qw
Dec 14 09:12:19.210: INFO: Created: latency-svc-5smx9
Dec 14 09:12:19.213: INFO: Created: latency-svc-5vg49
Dec 14 09:12:19.216: INFO: Created: latency-svc-s2cvw
Dec 14 09:12:19.221: INFO: Created: latency-svc-vzh99
Dec 14 09:12:19.225: INFO: Created: latency-svc-lcd9k
Dec 14 09:12:19.228: INFO: Got endpoints: latency-svc-7nfg2 [113.926959ms]
Dec 14 09:12:19.229: INFO: Created: latency-svc-k2bdb
Dec 14 09:12:19.232: INFO: Created: latency-svc-qrllq
Dec 14 09:12:19.236: INFO: Created: latency-svc-bxhz8
Dec 14 09:12:19.240: INFO: Created: latency-svc-wsbj5
Dec 14 09:12:19.243: INFO: Created: latency-svc-jc4lp
Dec 14 09:12:19.246: INFO: Created: latency-svc-6hk4z
Dec 14 09:12:19.260: INFO: Created: latency-svc-b4tpf
Dec 14 09:12:19.264: INFO: Created: latency-svc-wxs7q
Dec 14 09:12:19.280: INFO: Got endpoints: latency-svc-mhvhj [166.301759ms]
Dec 14 09:12:19.287: INFO: Created: latency-svc-jjdd9
Dec 14 09:12:19.335: INFO: Got endpoints: latency-svc-lb2qw [216.993937ms]
Dec 14 09:12:19.343: INFO: Created: latency-svc-868qg
Dec 14 09:12:19.379: INFO: Got endpoints: latency-svc-5smx9 [258.852933ms]
Dec 14 09:12:19.387: INFO: Created: latency-svc-7zrbb
Dec 14 09:12:19.429: INFO: Got endpoints: latency-svc-5vg49 [304.390531ms]
Dec 14 09:12:19.437: INFO: Created: latency-svc-c9zgm
Dec 14 09:12:19.484: INFO: Got endpoints: latency-svc-s2cvw [354.996127ms]
Dec 14 09:12:19.524: INFO: Created: latency-svc-w2vlh
Dec 14 09:12:19.529: INFO: Got endpoints: latency-svc-vzh99 [379.953178ms]
Dec 14 09:12:19.554: INFO: Created: latency-svc-8nwj2
Dec 14 09:12:19.579: INFO: Got endpoints: latency-svc-lcd9k [424.311531ms]
Dec 14 09:12:19.591: INFO: Created: latency-svc-22w9m
Dec 14 09:12:19.629: INFO: Got endpoints: latency-svc-k2bdb [468.096155ms]
Dec 14 09:12:19.638: INFO: Created: latency-svc-d6kn7
Dec 14 09:12:19.683: INFO: Got endpoints: latency-svc-qrllq [516.59395ms]
Dec 14 09:12:19.693: INFO: Created: latency-svc-4rctm
Dec 14 09:12:19.730: INFO: Got endpoints: latency-svc-bxhz8 [559.836583ms]
Dec 14 09:12:19.739: INFO: Created: latency-svc-swxxf
Dec 14 09:12:19.779: INFO: Got endpoints: latency-svc-wsbj5 [603.722605ms]
Dec 14 09:12:19.788: INFO: Created: latency-svc-2lpzp
Dec 14 09:12:19.829: INFO: Got endpoints: latency-svc-jc4lp [646.965568ms]
Dec 14 09:12:19.919: INFO: Got endpoints: latency-svc-6hk4z [731.447519ms]
Dec 14 09:12:19.920: INFO: Created: latency-svc-wq6bw
Dec 14 09:12:19.929: INFO: Got endpoints: latency-svc-b4tpf [731.175881ms]
Dec 14 09:12:19.929: INFO: Created: latency-svc-ndtt4
Dec 14 09:12:19.938: INFO: Created: latency-svc-dlzl4
Dec 14 09:12:19.980: INFO: Got endpoints: latency-svc-wxs7q [752.075208ms]
Dec 14 09:12:19.988: INFO: Created: latency-svc-2z46h
Dec 14 09:12:20.029: INFO: Got endpoints: latency-svc-jjdd9 [749.098121ms]
Dec 14 09:12:20.037: INFO: Created: latency-svc-pmvx4
Dec 14 09:12:20.079: INFO: Got endpoints: latency-svc-868qg [744.408947ms]
Dec 14 09:12:20.088: INFO: Created: latency-svc-q8mnb
Dec 14 09:12:20.130: INFO: Got endpoints: latency-svc-7zrbb [751.002961ms]
Dec 14 09:12:20.138: INFO: Created: latency-svc-v2rsp
Dec 14 09:12:20.178: INFO: Got endpoints: latency-svc-c9zgm [748.910719ms]
Dec 14 09:12:20.188: INFO: Created: latency-svc-fpkvt
Dec 14 09:12:20.231: INFO: Got endpoints: latency-svc-w2vlh [746.373702ms]
Dec 14 09:12:20.240: INFO: Created: latency-svc-mj94l
Dec 14 09:12:20.279: INFO: Got endpoints: latency-svc-8nwj2 [750.065101ms]
Dec 14 09:12:20.288: INFO: Created: latency-svc-z6vvd
Dec 14 09:12:20.329: INFO: Got endpoints: latency-svc-22w9m [749.69384ms]
Dec 14 09:12:20.344: INFO: Created: latency-svc-879hq
Dec 14 09:12:20.380: INFO: Got endpoints: latency-svc-d6kn7 [751.166243ms]
Dec 14 09:12:20.388: INFO: Created: latency-svc-4fgnx
Dec 14 09:12:20.429: INFO: Got endpoints: latency-svc-4rctm [745.691612ms]
Dec 14 09:12:20.437: INFO: Created: latency-svc-wx9tw
Dec 14 09:12:20.480: INFO: Got endpoints: latency-svc-swxxf [750.013283ms]
Dec 14 09:12:20.489: INFO: Created: latency-svc-k952z
Dec 14 09:12:20.530: INFO: Got endpoints: latency-svc-2lpzp [750.431039ms]
Dec 14 09:12:20.539: INFO: Created: latency-svc-42lrl
Dec 14 09:12:20.579: INFO: Got endpoints: latency-svc-wq6bw [749.756599ms]
Dec 14 09:12:20.587: INFO: Created: latency-svc-c5s9h
Dec 14 09:12:20.629: INFO: Got endpoints: latency-svc-ndtt4 [709.898874ms]
Dec 14 09:12:20.655: INFO: Created: latency-svc-49r7q
Dec 14 09:12:20.679: INFO: Got endpoints: latency-svc-dlzl4 [750.030358ms]
Dec 14 09:12:20.687: INFO: Created: latency-svc-2jq2r
Dec 14 09:12:20.729: INFO: Got endpoints: latency-svc-2z46h [749.512218ms]
Dec 14 09:12:20.762: INFO: Created: latency-svc-55w9v
Dec 14 09:12:20.779: INFO: Got endpoints: latency-svc-pmvx4 [749.984757ms]
Dec 14 09:12:20.786: INFO: Created: latency-svc-6nqmw
Dec 14 09:12:20.829: INFO: Got endpoints: latency-svc-q8mnb [749.752086ms]
Dec 14 09:12:20.837: INFO: Created: latency-svc-q797x
Dec 14 09:12:20.879: INFO: Got endpoints: latency-svc-v2rsp [748.760064ms]
Dec 14 09:12:20.886: INFO: Created: latency-svc-gmbj6
Dec 14 09:12:20.928: INFO: Got endpoints: latency-svc-fpkvt [750.144294ms]
Dec 14 09:12:20.936: INFO: Created: latency-svc-z2dpg
Dec 14 09:12:20.979: INFO: Got endpoints: latency-svc-mj94l [748.310357ms]
Dec 14 09:12:21.027: INFO: Created: latency-svc-77vgt
Dec 14 09:12:21.035: INFO: Got endpoints: latency-svc-z6vvd [755.723872ms]
Dec 14 09:12:21.043: INFO: Created: latency-svc-w72d8
Dec 14 09:12:21.132: INFO: Got endpoints: latency-svc-4fgnx [752.267505ms]
Dec 14 09:12:21.132: INFO: Got endpoints: latency-svc-879hq [803.570102ms]
Dec 14 09:12:21.145: INFO: Created: latency-svc-w96z6
Dec 14 09:12:21.149: INFO: Created: latency-svc-h629h
Dec 14 09:12:21.179: INFO: Got endpoints: latency-svc-wx9tw [749.956868ms]
Dec 14 09:12:21.187: INFO: Created: latency-svc-cltvz
Dec 14 09:12:21.229: INFO: Got endpoints: latency-svc-k952z [749.007949ms]
Dec 14 09:12:21.239: INFO: Created: latency-svc-vsdws
Dec 14 09:12:21.280: INFO: Got endpoints: latency-svc-42lrl [750.352621ms]
Dec 14 09:12:21.288: INFO: Created: latency-svc-psshw
Dec 14 09:12:21.328: INFO: Got endpoints: latency-svc-c5s9h [749.296937ms]
Dec 14 09:12:21.345: INFO: Created: latency-svc-96q8q
Dec 14 09:12:21.379: INFO: Got endpoints: latency-svc-49r7q [749.695694ms]
Dec 14 09:12:21.388: INFO: Created: latency-svc-v8bxq
Dec 14 09:12:21.429: INFO: Got endpoints: latency-svc-2jq2r [749.872348ms]
Dec 14 09:12:21.439: INFO: Created: latency-svc-4vcnp
Dec 14 09:12:21.479: INFO: Got endpoints: latency-svc-55w9v [749.842018ms]
Dec 14 09:12:21.488: INFO: Created: latency-svc-ksmdz
Dec 14 09:12:21.529: INFO: Got endpoints: latency-svc-6nqmw [750.296004ms]
Dec 14 09:12:21.539: INFO: Created: latency-svc-vfgvn
Dec 14 09:12:21.579: INFO: Got endpoints: latency-svc-q797x [749.785708ms]
Dec 14 09:12:21.587: INFO: Created: latency-svc-jvgdx
Dec 14 09:12:21.630: INFO: Got endpoints: latency-svc-gmbj6 [751.592188ms]
Dec 14 09:12:21.642: INFO: Created: latency-svc-s58n7
Dec 14 09:12:21.679: INFO: Got endpoints: latency-svc-z2dpg [750.481608ms]
Dec 14 09:12:21.692: INFO: Created: latency-svc-plbjj
Dec 14 09:12:21.729: INFO: Got endpoints: latency-svc-77vgt [749.841287ms]
Dec 14 09:12:21.737: INFO: Created: latency-svc-7bt2p
Dec 14 09:12:21.779: INFO: Got endpoints: latency-svc-w72d8 [744.315323ms]
Dec 14 09:12:21.795: INFO: Created: latency-svc-5dmdt
Dec 14 09:12:21.829: INFO: Got endpoints: latency-svc-w96z6 [696.811992ms]
Dec 14 09:12:21.838: INFO: Created: latency-svc-b6bwv
Dec 14 09:12:21.879: INFO: Got endpoints: latency-svc-h629h [747.154243ms]
Dec 14 09:12:21.890: INFO: Created: latency-svc-kncws
Dec 14 09:12:21.929: INFO: Got endpoints: latency-svc-cltvz [750.106433ms]
Dec 14 09:12:21.937: INFO: Created: latency-svc-dm8cj
Dec 14 09:12:21.980: INFO: Got endpoints: latency-svc-vsdws [751.058603ms]
Dec 14 09:12:21.991: INFO: Created: latency-svc-lkqpw
Dec 14 09:12:22.030: INFO: Got endpoints: latency-svc-psshw [749.454209ms]
Dec 14 09:12:22.039: INFO: Created: latency-svc-svhvr
Dec 14 09:12:22.080: INFO: Got endpoints: latency-svc-96q8q [751.709379ms]
Dec 14 09:12:22.089: INFO: Created: latency-svc-zgs5t
Dec 14 09:12:22.130: INFO: Got endpoints: latency-svc-v8bxq [750.569121ms]
Dec 14 09:12:22.139: INFO: Created: latency-svc-bntsl
Dec 14 09:12:22.179: INFO: Got endpoints: latency-svc-4vcnp [750.241841ms]
Dec 14 09:12:22.187: INFO: Created: latency-svc-nxftp
Dec 14 09:12:22.229: INFO: Got endpoints: latency-svc-ksmdz [749.374879ms]
Dec 14 09:12:22.237: INFO: Created: latency-svc-zxbwx
Dec 14 09:12:22.281: INFO: Got endpoints: latency-svc-vfgvn [751.504869ms]
Dec 14 09:12:22.291: INFO: Created: latency-svc-9l44c
Dec 14 09:12:22.340: INFO: Got endpoints: latency-svc-jvgdx [760.911708ms]
Dec 14 09:12:22.350: INFO: Created: latency-svc-vvxfk
Dec 14 09:12:22.380: INFO: Got endpoints: latency-svc-s58n7 [749.376035ms]
Dec 14 09:12:22.390: INFO: Created: latency-svc-r7q4r
Dec 14 09:12:22.429: INFO: Got endpoints: latency-svc-plbjj [750.281715ms]
Dec 14 09:12:22.438: INFO: Created: latency-svc-r6sx2
Dec 14 09:12:22.479: INFO: Got endpoints: latency-svc-7bt2p [750.118421ms]
Dec 14 09:12:22.500: INFO: Created: latency-svc-w7wwz
Dec 14 09:12:22.530: INFO: Got endpoints: latency-svc-5dmdt [750.736812ms]
Dec 14 09:12:22.538: INFO: Created: latency-svc-lgcds
Dec 14 09:12:22.579: INFO: Got endpoints: latency-svc-b6bwv [749.713834ms]
Dec 14 09:12:22.604: INFO: Created: latency-svc-pq5d6
Dec 14 09:12:22.629: INFO: Got endpoints: latency-svc-kncws [749.420056ms]
Dec 14 09:12:22.655: INFO: Created: latency-svc-9cfhd
Dec 14 09:12:22.680: INFO: Got endpoints: latency-svc-dm8cj [751.071639ms]
Dec 14 09:12:22.690: INFO: Created: latency-svc-zpd7s
Dec 14 09:12:22.729: INFO: Got endpoints: latency-svc-lkqpw [748.997359ms]
Dec 14 09:12:22.757: INFO: Created: latency-svc-vr86r
Dec 14 09:12:22.779: INFO: Got endpoints: latency-svc-svhvr [748.762692ms]
Dec 14 09:12:22.786: INFO: Created: latency-svc-ftctw
Dec 14 09:12:22.830: INFO: Got endpoints: latency-svc-zgs5t [749.812972ms]
Dec 14 09:12:22.837: INFO: Created: latency-svc-6z75z
Dec 14 09:12:22.880: INFO: Got endpoints: latency-svc-bntsl [750.244184ms]
Dec 14 09:12:22.888: INFO: Created: latency-svc-5mb6h
Dec 14 09:12:22.931: INFO: Got endpoints: latency-svc-nxftp [751.437437ms]
Dec 14 09:12:22.940: INFO: Created: latency-svc-tk4b4
Dec 14 09:12:22.980: INFO: Got endpoints: latency-svc-zxbwx [750.730153ms]
Dec 14 09:12:22.988: INFO: Created: latency-svc-7sbwq
Dec 14 09:12:23.029: INFO: Got endpoints: latency-svc-9l44c [747.706543ms]
Dec 14 09:12:23.037: INFO: Created: latency-svc-9skb8
Dec 14 09:12:23.078: INFO: Got endpoints: latency-svc-vvxfk [737.924556ms]
Dec 14 09:12:23.085: INFO: Created: latency-svc-zds7s
Dec 14 09:12:23.128: INFO: Got endpoints: latency-svc-r7q4r [748.639593ms]
Dec 14 09:12:23.137: INFO: Created: latency-svc-ffkcn
Dec 14 09:12:23.180: INFO: Got endpoints: latency-svc-r6sx2 [750.300227ms]
Dec 14 09:12:23.189: INFO: Created: latency-svc-z48j4
Dec 14 09:12:23.232: INFO: Got endpoints: latency-svc-w7wwz [752.444649ms]
Dec 14 09:12:23.239: INFO: Created: latency-svc-9kl2r
Dec 14 09:12:23.279: INFO: Got endpoints: latency-svc-lgcds [748.917771ms]
Dec 14 09:12:23.287: INFO: Created: latency-svc-pkbxd
Dec 14 09:12:23.329: INFO: Got endpoints: latency-svc-pq5d6 [749.985859ms]
Dec 14 09:12:23.338: INFO: Created: latency-svc-5rpjg
Dec 14 09:12:23.380: INFO: Got endpoints: latency-svc-9cfhd [751.0893ms]
Dec 14 09:12:23.390: INFO: Created: latency-svc-mxlqk
Dec 14 09:12:23.430: INFO: Got endpoints: latency-svc-zpd7s [749.663659ms]
Dec 14 09:12:23.438: INFO: Created: latency-svc-ph5td
Dec 14 09:12:23.479: INFO: Got endpoints: latency-svc-vr86r [749.587811ms]
Dec 14 09:12:23.487: INFO: Created: latency-svc-ktcfb
Dec 14 09:12:23.530: INFO: Got endpoints: latency-svc-ftctw [750.995424ms]
Dec 14 09:12:23.540: INFO: Created: latency-svc-xw8jd
Dec 14 09:12:23.579: INFO: Got endpoints: latency-svc-6z75z [749.272664ms]
Dec 14 09:12:23.587: INFO: Created: latency-svc-2bn4g
Dec 14 09:12:23.630: INFO: Got endpoints: latency-svc-5mb6h [749.637928ms]
Dec 14 09:12:23.641: INFO: Created: latency-svc-g5rjl
Dec 14 09:12:23.679: INFO: Got endpoints: latency-svc-tk4b4 [748.452918ms]
Dec 14 09:12:23.689: INFO: Created: latency-svc-ks4q5
Dec 14 09:12:23.729: INFO: Got endpoints: latency-svc-7sbwq [749.263496ms]
Dec 14 09:12:23.739: INFO: Created: latency-svc-gw4j5
Dec 14 09:12:23.779: INFO: Got endpoints: latency-svc-9skb8 [749.906967ms]
Dec 14 09:12:23.786: INFO: Created: latency-svc-dlszr
Dec 14 09:12:23.829: INFO: Got endpoints: latency-svc-zds7s [751.350896ms]
Dec 14 09:12:23.838: INFO: Created: latency-svc-zlrhr
Dec 14 09:12:23.878: INFO: Got endpoints: latency-svc-ffkcn [749.862804ms]
Dec 14 09:12:23.887: INFO: Created: latency-svc-5sdqn
Dec 14 09:12:23.930: INFO: Got endpoints: latency-svc-z48j4 [750.59049ms]
Dec 14 09:12:23.941: INFO: Created: latency-svc-xd9mj
Dec 14 09:12:23.983: INFO: Got endpoints: latency-svc-9kl2r [751.715298ms]
Dec 14 09:12:23.995: INFO: Created: latency-svc-stbjn
Dec 14 09:12:24.029: INFO: Got endpoints: latency-svc-pkbxd [750.062786ms]
Dec 14 09:12:24.041: INFO: Created: latency-svc-xmztx
Dec 14 09:12:24.079: INFO: Got endpoints: latency-svc-5rpjg [750.316449ms]
Dec 14 09:12:24.088: INFO: Created: latency-svc-lrqlq
Dec 14 09:12:24.131: INFO: Got endpoints: latency-svc-mxlqk [750.773054ms]
Dec 14 09:12:24.142: INFO: Created: latency-svc-8bvg4
Dec 14 09:12:24.179: INFO: Got endpoints: latency-svc-ph5td [748.881434ms]
Dec 14 09:12:24.187: INFO: Created: latency-svc-8bvgm
Dec 14 09:12:24.230: INFO: Got endpoints: latency-svc-ktcfb [750.978615ms]
Dec 14 09:12:24.238: INFO: Created: latency-svc-qwrlj
Dec 14 09:12:24.279: INFO: Got endpoints: latency-svc-xw8jd [749.498551ms]
Dec 14 09:12:24.287: INFO: Created: latency-svc-x5hx2
Dec 14 09:12:24.329: INFO: Got endpoints: latency-svc-2bn4g [749.797168ms]
Dec 14 09:12:24.337: INFO: Created: latency-svc-gbsq5
Dec 14 09:12:24.379: INFO: Got endpoints: latency-svc-g5rjl [749.716774ms]
Dec 14 09:12:24.387: INFO: Created: latency-svc-hnrzs
Dec 14 09:12:24.430: INFO: Got endpoints: latency-svc-ks4q5 [750.404337ms]
Dec 14 09:12:24.438: INFO: Created: latency-svc-qjnv5
Dec 14 09:12:24.479: INFO: Got endpoints: latency-svc-gw4j5 [750.010897ms]
Dec 14 09:12:24.488: INFO: Created: latency-svc-f2bpj
Dec 14 09:12:24.530: INFO: Got endpoints: latency-svc-dlszr [751.197717ms]
Dec 14 09:12:24.537: INFO: Created: latency-svc-fdbws
Dec 14 09:12:24.579: INFO: Got endpoints: latency-svc-zlrhr [749.696569ms]
Dec 14 09:12:24.586: INFO: Created: latency-svc-7gmlp
Dec 14 09:12:24.629: INFO: Got endpoints: latency-svc-5sdqn [750.522647ms]
Dec 14 09:12:24.637: INFO: Created: latency-svc-6g8n8
Dec 14 09:12:24.679: INFO: Got endpoints: latency-svc-xd9mj [747.785293ms]
Dec 14 09:12:24.687: INFO: Created: latency-svc-wkgsk
Dec 14 09:12:24.729: INFO: Got endpoints: latency-svc-stbjn [745.660481ms]
Dec 14 09:12:24.738: INFO: Created: latency-svc-xl58s
Dec 14 09:12:24.779: INFO: Got endpoints: latency-svc-xmztx [749.623016ms]
Dec 14 09:12:24.787: INFO: Created: latency-svc-zdsck
Dec 14 09:12:24.829: INFO: Got endpoints: latency-svc-lrqlq [750.219471ms]
Dec 14 09:12:24.837: INFO: Created: latency-svc-nwdw9
Dec 14 09:12:24.879: INFO: Got endpoints: latency-svc-8bvg4 [748.197441ms]
Dec 14 09:12:24.886: INFO: Created: latency-svc-6gdqc
Dec 14 09:12:24.929: INFO: Got endpoints: latency-svc-8bvgm [750.371564ms]
Dec 14 09:12:24.939: INFO: Created: latency-svc-vwq4j
Dec 14 09:12:24.979: INFO: Got endpoints: latency-svc-qwrlj [749.219797ms]
Dec 14 09:12:24.986: INFO: Created: latency-svc-24lcg
Dec 14 09:12:25.029: INFO: Got endpoints: latency-svc-x5hx2 [749.398376ms]
Dec 14 09:12:25.036: INFO: Created: latency-svc-56qlj
Dec 14 09:12:25.081: INFO: Got endpoints: latency-svc-gbsq5 [752.231237ms]
Dec 14 09:12:25.092: INFO: Created: latency-svc-4vwrv
Dec 14 09:12:25.130: INFO: Got endpoints: latency-svc-hnrzs [751.089961ms]
Dec 14 09:12:25.139: INFO: Created: latency-svc-trgr5
Dec 14 09:12:25.179: INFO: Got endpoints: latency-svc-qjnv5 [749.345173ms]
Dec 14 09:12:25.188: INFO: Created: latency-svc-pk8k8
Dec 14 09:12:25.229: INFO: Got endpoints: latency-svc-f2bpj [750.413238ms]
Dec 14 09:12:25.238: INFO: Created: latency-svc-vglq8
Dec 14 09:12:25.279: INFO: Got endpoints: latency-svc-fdbws [749.462205ms]
Dec 14 09:12:25.291: INFO: Created: latency-svc-mqjhm
Dec 14 09:12:25.329: INFO: Got endpoints: latency-svc-7gmlp [750.47331ms]
Dec 14 09:12:25.337: INFO: Created: latency-svc-zxm8s
Dec 14 09:12:25.378: INFO: Got endpoints: latency-svc-6g8n8 [749.509301ms]
Dec 14 09:12:25.386: INFO: Created: latency-svc-59gp5
Dec 14 09:12:25.429: INFO: Got endpoints: latency-svc-wkgsk [749.767219ms]
Dec 14 09:12:25.437: INFO: Created: latency-svc-2zhs5
Dec 14 09:12:25.478: INFO: Got endpoints: latency-svc-xl58s [749.283385ms]
Dec 14 09:12:25.486: INFO: Created: latency-svc-g6tt5
Dec 14 09:12:25.529: INFO: Got endpoints: latency-svc-zdsck [750.306035ms]
Dec 14 09:12:25.538: INFO: Created: latency-svc-8w975
Dec 14 09:12:25.579: INFO: Got endpoints: latency-svc-nwdw9 [749.739094ms]
Dec 14 09:12:25.587: INFO: Created: latency-svc-vln2t
Dec 14 09:12:25.629: INFO: Got endpoints: latency-svc-6gdqc [750.011647ms]
Dec 14 09:12:25.638: INFO: Created: latency-svc-7nd6w
Dec 14 09:12:25.679: INFO: Got endpoints: latency-svc-vwq4j [749.399869ms]
Dec 14 09:12:25.688: INFO: Created: latency-svc-7fl4v
Dec 14 09:12:25.729: INFO: Got endpoints: latency-svc-24lcg [750.234495ms]
Dec 14 09:12:25.738: INFO: Created: latency-svc-4smzl
Dec 14 09:12:25.780: INFO: Got endpoints: latency-svc-56qlj [751.593931ms]
Dec 14 09:12:25.789: INFO: Created: latency-svc-8p5ch
Dec 14 09:12:25.829: INFO: Got endpoints: latency-svc-4vwrv [747.842781ms]
Dec 14 09:12:25.838: INFO: Created: latency-svc-cgf9t
Dec 14 09:12:25.880: INFO: Got endpoints: latency-svc-trgr5 [749.465883ms]
Dec 14 09:12:25.888: INFO: Created: latency-svc-6rkgw
Dec 14 09:12:25.929: INFO: Got endpoints: latency-svc-pk8k8 [750.162485ms]
Dec 14 09:12:25.938: INFO: Created: latency-svc-2gdgx
Dec 14 09:12:25.980: INFO: Got endpoints: latency-svc-vglq8 [750.097485ms]
Dec 14 09:12:25.988: INFO: Created: latency-svc-qdzwc
Dec 14 09:12:26.030: INFO: Got endpoints: latency-svc-mqjhm [750.269095ms]
Dec 14 09:12:26.038: INFO: Created: latency-svc-vpsf7
Dec 14 09:12:26.079: INFO: Got endpoints: latency-svc-zxm8s [749.790334ms]
Dec 14 09:12:26.087: INFO: Created: latency-svc-8dvmr
Dec 14 09:12:26.129: INFO: Got endpoints: latency-svc-59gp5 [750.726241ms]
Dec 14 09:12:26.137: INFO: Created: latency-svc-5rv6j
Dec 14 09:12:26.179: INFO: Got endpoints: latency-svc-2zhs5 [750.473382ms]
Dec 14 09:12:26.187: INFO: Created: latency-svc-kjmkq
Dec 14 09:12:26.230: INFO: Got endpoints: latency-svc-g6tt5 [751.734898ms]
Dec 14 09:12:26.240: INFO: Created: latency-svc-72qzc
Dec 14 09:12:26.280: INFO: Got endpoints: latency-svc-8w975 [751.197212ms]
Dec 14 09:12:26.291: INFO: Created: latency-svc-zdktk
Dec 14 09:12:26.329: INFO: Got endpoints: latency-svc-vln2t [750.122746ms]
Dec 14 09:12:26.337: INFO: Created: latency-svc-s5jxf
Dec 14 09:12:26.379: INFO: Got endpoints: latency-svc-7nd6w [750.156168ms]
Dec 14 09:12:26.388: INFO: Created: latency-svc-54jpx
Dec 14 09:12:26.429: INFO: Got endpoints: latency-svc-7fl4v [749.908318ms]
Dec 14 09:12:26.439: INFO: Created: latency-svc-c9mdp
Dec 14 09:12:26.478: INFO: Got endpoints: latency-svc-4smzl [748.988585ms]
Dec 14 09:12:26.487: INFO: Created: latency-svc-nld9w
Dec 14 09:12:26.528: INFO: Got endpoints: latency-svc-8p5ch [748.06693ms]
Dec 14 09:12:26.543: INFO: Created: latency-svc-4zfdf
Dec 14 09:12:26.580: INFO: Got endpoints: latency-svc-cgf9t [750.25981ms]
Dec 14 09:12:26.593: INFO: Created: latency-svc-m28tk
Dec 14 09:12:26.630: INFO: Got endpoints: latency-svc-6rkgw [749.686507ms]
Dec 14 09:12:26.638: INFO: Created: latency-svc-2dkfb
Dec 14 09:12:26.680: INFO: Got endpoints: latency-svc-2gdgx [750.523966ms]
Dec 14 09:12:26.688: INFO: Created: latency-svc-p25cl
Dec 14 09:12:26.729: INFO: Got endpoints: latency-svc-qdzwc [749.513742ms]
Dec 14 09:12:26.738: INFO: Created: latency-svc-v6mn9
Dec 14 09:12:26.780: INFO: Got endpoints: latency-svc-vpsf7 [749.823191ms]
Dec 14 09:12:26.787: INFO: Created: latency-svc-7trw5
Dec 14 09:12:26.829: INFO: Got endpoints: latency-svc-8dvmr [750.187094ms]
Dec 14 09:12:26.839: INFO: Created: latency-svc-hnjkd
Dec 14 09:12:26.879: INFO: Got endpoints: latency-svc-5rv6j [749.927139ms]
Dec 14 09:12:26.929: INFO: Got endpoints: latency-svc-kjmkq [749.744131ms]
Dec 14 09:12:26.980: INFO: Got endpoints: latency-svc-72qzc [749.3964ms]
Dec 14 09:12:27.029: INFO: Got endpoints: latency-svc-zdktk [748.764948ms]
Dec 14 09:12:27.079: INFO: Got endpoints: latency-svc-s5jxf [749.226294ms]
Dec 14 09:12:27.129: INFO: Got endpoints: latency-svc-54jpx [749.35004ms]
Dec 14 09:12:27.179: INFO: Got endpoints: latency-svc-c9mdp [750.163005ms]
Dec 14 09:12:27.230: INFO: Got endpoints: latency-svc-nld9w [752.100443ms]
Dec 14 09:12:27.279: INFO: Got endpoints: latency-svc-4zfdf [750.524941ms]
Dec 14 09:12:27.341: INFO: Got endpoints: latency-svc-m28tk [761.695969ms]
Dec 14 09:12:27.379: INFO: Got endpoints: latency-svc-2dkfb [749.640462ms]
Dec 14 09:12:27.429: INFO: Got endpoints: latency-svc-p25cl [749.531893ms]
Dec 14 09:12:27.480: INFO: Got endpoints: latency-svc-v6mn9 [750.703145ms]
Dec 14 09:12:27.529: INFO: Got endpoints: latency-svc-7trw5 [749.541933ms]
Dec 14 09:12:27.581: INFO: Got endpoints: latency-svc-hnjkd [751.94486ms]
Dec 14 09:12:27.581: INFO: Latencies: [11.381647ms 13.852565ms 14.366897ms 15.498263ms 18.338746ms 19.837596ms 22.911663ms 23.982768ms 39.465549ms 41.962138ms 47.599845ms 53.493248ms 56.369042ms 61.731717ms 61.947621ms 62.05117ms 66.137619ms 68.552155ms 72.838519ms 73.968495ms 74.657059ms 75.251006ms 76.031909ms 80.412374ms 83.28587ms 83.765745ms 83.894026ms 84.406292ms 84.431284ms 84.471427ms 84.555539ms 84.698309ms 113.926959ms 166.301759ms 216.993937ms 258.852933ms 304.390531ms 354.996127ms 379.953178ms 424.311531ms 468.096155ms 516.59395ms 559.836583ms 603.722605ms 646.965568ms 696.811992ms 709.898874ms 731.175881ms 731.447519ms 737.924556ms 744.315323ms 744.408947ms 745.660481ms 745.691612ms 746.373702ms 747.154243ms 747.706543ms 747.785293ms 747.842781ms 748.06693ms 748.197441ms 748.310357ms 748.452918ms 748.639593ms 748.760064ms 748.762692ms 748.764948ms 748.881434ms 748.910719ms 748.917771ms 748.988585ms 748.997359ms 749.007949ms 749.098121ms 749.219797ms 749.226294ms 749.263496ms 749.272664ms 749.283385ms 749.296937ms 749.345173ms 749.35004ms 749.374879ms 749.376035ms 749.3964ms 749.398376ms 749.399869ms 749.420056ms 749.454209ms 749.462205ms 749.465883ms 749.498551ms 749.509301ms 749.512218ms 749.513742ms 749.531893ms 749.541933ms 749.587811ms 749.623016ms 749.637928ms 749.640462ms 749.663659ms 749.686507ms 749.69384ms 749.695694ms 749.696569ms 749.713834ms 749.716774ms 749.739094ms 749.744131ms 749.752086ms 749.756599ms 749.767219ms 749.785708ms 749.790334ms 749.797168ms 749.812972ms 749.823191ms 749.841287ms 749.842018ms 749.862804ms 749.872348ms 749.906967ms 749.908318ms 749.927139ms 749.956868ms 749.984757ms 749.985859ms 750.010897ms 750.011647ms 750.013283ms 750.030358ms 750.062786ms 750.065101ms 750.097485ms 750.106433ms 750.118421ms 750.122746ms 750.144294ms 750.156168ms 750.162485ms 750.163005ms 750.187094ms 750.219471ms 750.234495ms 750.241841ms 750.244184ms 750.25981ms 750.269095ms 750.281715ms 750.296004ms 750.300227ms 750.306035ms 750.316449ms 750.352621ms 750.371564ms 750.404337ms 750.413238ms 750.431039ms 750.47331ms 750.473382ms 750.481608ms 750.522647ms 750.523966ms 750.524941ms 750.569121ms 750.59049ms 750.703145ms 750.726241ms 750.730153ms 750.736812ms 750.773054ms 750.978615ms 750.995424ms 751.002961ms 751.058603ms 751.071639ms 751.0893ms 751.089961ms 751.166243ms 751.197212ms 751.197717ms 751.350896ms 751.437437ms 751.504869ms 751.592188ms 751.593931ms 751.709379ms 751.715298ms 751.734898ms 751.94486ms 752.075208ms 752.100443ms 752.231237ms 752.267505ms 752.444649ms 755.723872ms 760.911708ms 761.695969ms 803.570102ms]
Dec 14 09:12:27.582: INFO: 50 %ile: 749.640462ms
Dec 14 09:12:27.582: INFO: 90 %ile: 751.197212ms
Dec 14 09:12:27.582: INFO: 99 %ile: 761.695969ms
Dec 14 09:12:27.582: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Dec 14 09:12:27.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6492" for this suite. 12/14/22 09:12:27.588
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":244,"skipped":4704,"failed":0}
------------------------------
• [14.755 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:12:12.837
    Dec 14 09:12:12.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svc-latency 12/14/22 09:12:12.838
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:12:12.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:12:12.854
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Dec 14 09:12:12.859: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6492 12/14/22 09:12:12.859
    I1214 09:12:12.863577    4635 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6492, replica count: 1
    I1214 09:12:13.914108    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 09:12:14.914511    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 09:12:15.914976    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 09:12:16.915117    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 09:12:17.915347    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 09:12:18.916358    4635 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:12:19.026: INFO: Created: latency-svc-rnttb
    Dec 14 09:12:19.029: INFO: Got endpoints: latency-svc-rnttb [12.44351ms]
    Dec 14 09:12:19.038: INFO: Created: latency-svc-5g5nl
    Dec 14 09:12:19.040: INFO: Got endpoints: latency-svc-5g5nl [11.381647ms]
    Dec 14 09:12:19.041: INFO: Created: latency-svc-hfl9k
    Dec 14 09:12:19.043: INFO: Got endpoints: latency-svc-hfl9k [14.366897ms]
    Dec 14 09:12:19.044: INFO: Created: latency-svc-msq26
    Dec 14 09:12:19.047: INFO: Got endpoints: latency-svc-msq26 [18.338746ms]
    Dec 14 09:12:19.048: INFO: Created: latency-svc-vn2v8
    Dec 14 09:12:19.052: INFO: Got endpoints: latency-svc-vn2v8 [22.911663ms]
    Dec 14 09:12:19.102: INFO: Created: latency-svc-jc5sh
    Dec 14 09:12:19.102: INFO: Created: latency-svc-njh6r
    Dec 14 09:12:19.102: INFO: Created: latency-svc-pkdtj
    Dec 14 09:12:19.102: INFO: Created: latency-svc-hkcbd
    Dec 14 09:12:19.102: INFO: Created: latency-svc-hw6ck
    Dec 14 09:12:19.102: INFO: Created: latency-svc-46v7b
    Dec 14 09:12:19.103: INFO: Created: latency-svc-4z6sx
    Dec 14 09:12:19.103: INFO: Created: latency-svc-xkwxf
    Dec 14 09:12:19.103: INFO: Created: latency-svc-4p7qw
    Dec 14 09:12:19.103: INFO: Created: latency-svc-xlvwf
    Dec 14 09:12:19.104: INFO: Created: latency-svc-ldst5
    Dec 14 09:12:19.104: INFO: Created: latency-svc-nt7s7
    Dec 14 09:12:19.104: INFO: Created: latency-svc-tkft8
    Dec 14 09:12:19.104: INFO: Got endpoints: latency-svc-njh6r [74.657059ms]
    Dec 14 09:12:19.104: INFO: Created: latency-svc-mmbbj
    Dec 14 09:12:19.104: INFO: Got endpoints: latency-svc-mmbbj [75.251006ms]
    Dec 14 09:12:19.105: INFO: Created: latency-svc-hzt67
    Dec 14 09:12:19.105: INFO: Got endpoints: latency-svc-hzt67 [76.031909ms]
    Dec 14 09:12:19.105: INFO: Got endpoints: latency-svc-jc5sh [61.947621ms]
    Dec 14 09:12:19.110: INFO: Got endpoints: latency-svc-xkwxf [80.412374ms]
    Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-nt7s7 [83.28587ms]
    Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-ldst5 [72.838519ms]
    Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-hw6ck [83.765745ms]
    Dec 14 09:12:19.113: INFO: Got endpoints: latency-svc-4z6sx [66.137619ms]
    Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-pkdtj [84.431284ms]
    Dec 14 09:12:19.114: INFO: Created: latency-svc-m9v7c
    Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-4p7qw [84.406292ms]
    Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-46v7b [61.731717ms]
    Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-xlvwf [84.698309ms]
    Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-hkcbd [84.471427ms]
    Dec 14 09:12:19.114: INFO: Got endpoints: latency-svc-tkft8 [84.555539ms]
    Dec 14 09:12:19.118: INFO: Got endpoints: latency-svc-m9v7c [13.852565ms]
    Dec 14 09:12:19.118: INFO: Created: latency-svc-dq2fp
    Dec 14 09:12:19.120: INFO: Got endpoints: latency-svc-dq2fp [15.498263ms]
    Dec 14 09:12:19.123: INFO: Created: latency-svc-nnt98
    Dec 14 09:12:19.125: INFO: Got endpoints: latency-svc-nnt98 [19.837596ms]
    Dec 14 09:12:19.128: INFO: Created: latency-svc-wcrpm
    Dec 14 09:12:19.129: INFO: Got endpoints: latency-svc-wcrpm [23.982768ms]
    Dec 14 09:12:19.133: INFO: Created: latency-svc-g6gdd
    Dec 14 09:12:19.149: INFO: Got endpoints: latency-svc-g6gdd [39.465549ms]
    Dec 14 09:12:19.150: INFO: Created: latency-svc-kcvck
    Dec 14 09:12:19.155: INFO: Got endpoints: latency-svc-kcvck [41.962138ms]
    Dec 14 09:12:19.157: INFO: Created: latency-svc-2zqtp
    Dec 14 09:12:19.161: INFO: Got endpoints: latency-svc-2zqtp [47.599845ms]
    Dec 14 09:12:19.163: INFO: Created: latency-svc-7psvm
    Dec 14 09:12:19.167: INFO: Got endpoints: latency-svc-7psvm [53.493248ms]
    Dec 14 09:12:19.167: INFO: Created: latency-svc-5z2nt
    Dec 14 09:12:19.170: INFO: Got endpoints: latency-svc-5z2nt [56.369042ms]
    Dec 14 09:12:19.172: INFO: Created: latency-svc-dzn82
    Dec 14 09:12:19.176: INFO: Got endpoints: latency-svc-dzn82 [62.05117ms]
    Dec 14 09:12:19.177: INFO: Created: latency-svc-gzjbs
    Dec 14 09:12:19.182: INFO: Got endpoints: latency-svc-gzjbs [68.552155ms]
    Dec 14 09:12:19.184: INFO: Created: latency-svc-gtt7h
    Dec 14 09:12:19.188: INFO: Got endpoints: latency-svc-gtt7h [73.968495ms]
    Dec 14 09:12:19.188: INFO: Created: latency-svc-95qmm
    Dec 14 09:12:19.198: INFO: Got endpoints: latency-svc-95qmm [83.894026ms]
    Dec 14 09:12:19.198: INFO: Created: latency-svc-7nfg2
    Dec 14 09:12:19.203: INFO: Created: latency-svc-mhvhj
    Dec 14 09:12:19.206: INFO: Created: latency-svc-lb2qw
    Dec 14 09:12:19.210: INFO: Created: latency-svc-5smx9
    Dec 14 09:12:19.213: INFO: Created: latency-svc-5vg49
    Dec 14 09:12:19.216: INFO: Created: latency-svc-s2cvw
    Dec 14 09:12:19.221: INFO: Created: latency-svc-vzh99
    Dec 14 09:12:19.225: INFO: Created: latency-svc-lcd9k
    Dec 14 09:12:19.228: INFO: Got endpoints: latency-svc-7nfg2 [113.926959ms]
    Dec 14 09:12:19.229: INFO: Created: latency-svc-k2bdb
    Dec 14 09:12:19.232: INFO: Created: latency-svc-qrllq
    Dec 14 09:12:19.236: INFO: Created: latency-svc-bxhz8
    Dec 14 09:12:19.240: INFO: Created: latency-svc-wsbj5
    Dec 14 09:12:19.243: INFO: Created: latency-svc-jc4lp
    Dec 14 09:12:19.246: INFO: Created: latency-svc-6hk4z
    Dec 14 09:12:19.260: INFO: Created: latency-svc-b4tpf
    Dec 14 09:12:19.264: INFO: Created: latency-svc-wxs7q
    Dec 14 09:12:19.280: INFO: Got endpoints: latency-svc-mhvhj [166.301759ms]
    Dec 14 09:12:19.287: INFO: Created: latency-svc-jjdd9
    Dec 14 09:12:19.335: INFO: Got endpoints: latency-svc-lb2qw [216.993937ms]
    Dec 14 09:12:19.343: INFO: Created: latency-svc-868qg
    Dec 14 09:12:19.379: INFO: Got endpoints: latency-svc-5smx9 [258.852933ms]
    Dec 14 09:12:19.387: INFO: Created: latency-svc-7zrbb
    Dec 14 09:12:19.429: INFO: Got endpoints: latency-svc-5vg49 [304.390531ms]
    Dec 14 09:12:19.437: INFO: Created: latency-svc-c9zgm
    Dec 14 09:12:19.484: INFO: Got endpoints: latency-svc-s2cvw [354.996127ms]
    Dec 14 09:12:19.524: INFO: Created: latency-svc-w2vlh
    Dec 14 09:12:19.529: INFO: Got endpoints: latency-svc-vzh99 [379.953178ms]
    Dec 14 09:12:19.554: INFO: Created: latency-svc-8nwj2
    Dec 14 09:12:19.579: INFO: Got endpoints: latency-svc-lcd9k [424.311531ms]
    Dec 14 09:12:19.591: INFO: Created: latency-svc-22w9m
    Dec 14 09:12:19.629: INFO: Got endpoints: latency-svc-k2bdb [468.096155ms]
    Dec 14 09:12:19.638: INFO: Created: latency-svc-d6kn7
    Dec 14 09:12:19.683: INFO: Got endpoints: latency-svc-qrllq [516.59395ms]
    Dec 14 09:12:19.693: INFO: Created: latency-svc-4rctm
    Dec 14 09:12:19.730: INFO: Got endpoints: latency-svc-bxhz8 [559.836583ms]
    Dec 14 09:12:19.739: INFO: Created: latency-svc-swxxf
    Dec 14 09:12:19.779: INFO: Got endpoints: latency-svc-wsbj5 [603.722605ms]
    Dec 14 09:12:19.788: INFO: Created: latency-svc-2lpzp
    Dec 14 09:12:19.829: INFO: Got endpoints: latency-svc-jc4lp [646.965568ms]
    Dec 14 09:12:19.919: INFO: Got endpoints: latency-svc-6hk4z [731.447519ms]
    Dec 14 09:12:19.920: INFO: Created: latency-svc-wq6bw
    Dec 14 09:12:19.929: INFO: Got endpoints: latency-svc-b4tpf [731.175881ms]
    Dec 14 09:12:19.929: INFO: Created: latency-svc-ndtt4
    Dec 14 09:12:19.938: INFO: Created: latency-svc-dlzl4
    Dec 14 09:12:19.980: INFO: Got endpoints: latency-svc-wxs7q [752.075208ms]
    Dec 14 09:12:19.988: INFO: Created: latency-svc-2z46h
    Dec 14 09:12:20.029: INFO: Got endpoints: latency-svc-jjdd9 [749.098121ms]
    Dec 14 09:12:20.037: INFO: Created: latency-svc-pmvx4
    Dec 14 09:12:20.079: INFO: Got endpoints: latency-svc-868qg [744.408947ms]
    Dec 14 09:12:20.088: INFO: Created: latency-svc-q8mnb
    Dec 14 09:12:20.130: INFO: Got endpoints: latency-svc-7zrbb [751.002961ms]
    Dec 14 09:12:20.138: INFO: Created: latency-svc-v2rsp
    Dec 14 09:12:20.178: INFO: Got endpoints: latency-svc-c9zgm [748.910719ms]
    Dec 14 09:12:20.188: INFO: Created: latency-svc-fpkvt
    Dec 14 09:12:20.231: INFO: Got endpoints: latency-svc-w2vlh [746.373702ms]
    Dec 14 09:12:20.240: INFO: Created: latency-svc-mj94l
    Dec 14 09:12:20.279: INFO: Got endpoints: latency-svc-8nwj2 [750.065101ms]
    Dec 14 09:12:20.288: INFO: Created: latency-svc-z6vvd
    Dec 14 09:12:20.329: INFO: Got endpoints: latency-svc-22w9m [749.69384ms]
    Dec 14 09:12:20.344: INFO: Created: latency-svc-879hq
    Dec 14 09:12:20.380: INFO: Got endpoints: latency-svc-d6kn7 [751.166243ms]
    Dec 14 09:12:20.388: INFO: Created: latency-svc-4fgnx
    Dec 14 09:12:20.429: INFO: Got endpoints: latency-svc-4rctm [745.691612ms]
    Dec 14 09:12:20.437: INFO: Created: latency-svc-wx9tw
    Dec 14 09:12:20.480: INFO: Got endpoints: latency-svc-swxxf [750.013283ms]
    Dec 14 09:12:20.489: INFO: Created: latency-svc-k952z
    Dec 14 09:12:20.530: INFO: Got endpoints: latency-svc-2lpzp [750.431039ms]
    Dec 14 09:12:20.539: INFO: Created: latency-svc-42lrl
    Dec 14 09:12:20.579: INFO: Got endpoints: latency-svc-wq6bw [749.756599ms]
    Dec 14 09:12:20.587: INFO: Created: latency-svc-c5s9h
    Dec 14 09:12:20.629: INFO: Got endpoints: latency-svc-ndtt4 [709.898874ms]
    Dec 14 09:12:20.655: INFO: Created: latency-svc-49r7q
    Dec 14 09:12:20.679: INFO: Got endpoints: latency-svc-dlzl4 [750.030358ms]
    Dec 14 09:12:20.687: INFO: Created: latency-svc-2jq2r
    Dec 14 09:12:20.729: INFO: Got endpoints: latency-svc-2z46h [749.512218ms]
    Dec 14 09:12:20.762: INFO: Created: latency-svc-55w9v
    Dec 14 09:12:20.779: INFO: Got endpoints: latency-svc-pmvx4 [749.984757ms]
    Dec 14 09:12:20.786: INFO: Created: latency-svc-6nqmw
    Dec 14 09:12:20.829: INFO: Got endpoints: latency-svc-q8mnb [749.752086ms]
    Dec 14 09:12:20.837: INFO: Created: latency-svc-q797x
    Dec 14 09:12:20.879: INFO: Got endpoints: latency-svc-v2rsp [748.760064ms]
    Dec 14 09:12:20.886: INFO: Created: latency-svc-gmbj6
    Dec 14 09:12:20.928: INFO: Got endpoints: latency-svc-fpkvt [750.144294ms]
    Dec 14 09:12:20.936: INFO: Created: latency-svc-z2dpg
    Dec 14 09:12:20.979: INFO: Got endpoints: latency-svc-mj94l [748.310357ms]
    Dec 14 09:12:21.027: INFO: Created: latency-svc-77vgt
    Dec 14 09:12:21.035: INFO: Got endpoints: latency-svc-z6vvd [755.723872ms]
    Dec 14 09:12:21.043: INFO: Created: latency-svc-w72d8
    Dec 14 09:12:21.132: INFO: Got endpoints: latency-svc-4fgnx [752.267505ms]
    Dec 14 09:12:21.132: INFO: Got endpoints: latency-svc-879hq [803.570102ms]
    Dec 14 09:12:21.145: INFO: Created: latency-svc-w96z6
    Dec 14 09:12:21.149: INFO: Created: latency-svc-h629h
    Dec 14 09:12:21.179: INFO: Got endpoints: latency-svc-wx9tw [749.956868ms]
    Dec 14 09:12:21.187: INFO: Created: latency-svc-cltvz
    Dec 14 09:12:21.229: INFO: Got endpoints: latency-svc-k952z [749.007949ms]
    Dec 14 09:12:21.239: INFO: Created: latency-svc-vsdws
    Dec 14 09:12:21.280: INFO: Got endpoints: latency-svc-42lrl [750.352621ms]
    Dec 14 09:12:21.288: INFO: Created: latency-svc-psshw
    Dec 14 09:12:21.328: INFO: Got endpoints: latency-svc-c5s9h [749.296937ms]
    Dec 14 09:12:21.345: INFO: Created: latency-svc-96q8q
    Dec 14 09:12:21.379: INFO: Got endpoints: latency-svc-49r7q [749.695694ms]
    Dec 14 09:12:21.388: INFO: Created: latency-svc-v8bxq
    Dec 14 09:12:21.429: INFO: Got endpoints: latency-svc-2jq2r [749.872348ms]
    Dec 14 09:12:21.439: INFO: Created: latency-svc-4vcnp
    Dec 14 09:12:21.479: INFO: Got endpoints: latency-svc-55w9v [749.842018ms]
    Dec 14 09:12:21.488: INFO: Created: latency-svc-ksmdz
    Dec 14 09:12:21.529: INFO: Got endpoints: latency-svc-6nqmw [750.296004ms]
    Dec 14 09:12:21.539: INFO: Created: latency-svc-vfgvn
    Dec 14 09:12:21.579: INFO: Got endpoints: latency-svc-q797x [749.785708ms]
    Dec 14 09:12:21.587: INFO: Created: latency-svc-jvgdx
    Dec 14 09:12:21.630: INFO: Got endpoints: latency-svc-gmbj6 [751.592188ms]
    Dec 14 09:12:21.642: INFO: Created: latency-svc-s58n7
    Dec 14 09:12:21.679: INFO: Got endpoints: latency-svc-z2dpg [750.481608ms]
    Dec 14 09:12:21.692: INFO: Created: latency-svc-plbjj
    Dec 14 09:12:21.729: INFO: Got endpoints: latency-svc-77vgt [749.841287ms]
    Dec 14 09:12:21.737: INFO: Created: latency-svc-7bt2p
    Dec 14 09:12:21.779: INFO: Got endpoints: latency-svc-w72d8 [744.315323ms]
    Dec 14 09:12:21.795: INFO: Created: latency-svc-5dmdt
    Dec 14 09:12:21.829: INFO: Got endpoints: latency-svc-w96z6 [696.811992ms]
    Dec 14 09:12:21.838: INFO: Created: latency-svc-b6bwv
    Dec 14 09:12:21.879: INFO: Got endpoints: latency-svc-h629h [747.154243ms]
    Dec 14 09:12:21.890: INFO: Created: latency-svc-kncws
    Dec 14 09:12:21.929: INFO: Got endpoints: latency-svc-cltvz [750.106433ms]
    Dec 14 09:12:21.937: INFO: Created: latency-svc-dm8cj
    Dec 14 09:12:21.980: INFO: Got endpoints: latency-svc-vsdws [751.058603ms]
    Dec 14 09:12:21.991: INFO: Created: latency-svc-lkqpw
    Dec 14 09:12:22.030: INFO: Got endpoints: latency-svc-psshw [749.454209ms]
    Dec 14 09:12:22.039: INFO: Created: latency-svc-svhvr
    Dec 14 09:12:22.080: INFO: Got endpoints: latency-svc-96q8q [751.709379ms]
    Dec 14 09:12:22.089: INFO: Created: latency-svc-zgs5t
    Dec 14 09:12:22.130: INFO: Got endpoints: latency-svc-v8bxq [750.569121ms]
    Dec 14 09:12:22.139: INFO: Created: latency-svc-bntsl
    Dec 14 09:12:22.179: INFO: Got endpoints: latency-svc-4vcnp [750.241841ms]
    Dec 14 09:12:22.187: INFO: Created: latency-svc-nxftp
    Dec 14 09:12:22.229: INFO: Got endpoints: latency-svc-ksmdz [749.374879ms]
    Dec 14 09:12:22.237: INFO: Created: latency-svc-zxbwx
    Dec 14 09:12:22.281: INFO: Got endpoints: latency-svc-vfgvn [751.504869ms]
    Dec 14 09:12:22.291: INFO: Created: latency-svc-9l44c
    Dec 14 09:12:22.340: INFO: Got endpoints: latency-svc-jvgdx [760.911708ms]
    Dec 14 09:12:22.350: INFO: Created: latency-svc-vvxfk
    Dec 14 09:12:22.380: INFO: Got endpoints: latency-svc-s58n7 [749.376035ms]
    Dec 14 09:12:22.390: INFO: Created: latency-svc-r7q4r
    Dec 14 09:12:22.429: INFO: Got endpoints: latency-svc-plbjj [750.281715ms]
    Dec 14 09:12:22.438: INFO: Created: latency-svc-r6sx2
    Dec 14 09:12:22.479: INFO: Got endpoints: latency-svc-7bt2p [750.118421ms]
    Dec 14 09:12:22.500: INFO: Created: latency-svc-w7wwz
    Dec 14 09:12:22.530: INFO: Got endpoints: latency-svc-5dmdt [750.736812ms]
    Dec 14 09:12:22.538: INFO: Created: latency-svc-lgcds
    Dec 14 09:12:22.579: INFO: Got endpoints: latency-svc-b6bwv [749.713834ms]
    Dec 14 09:12:22.604: INFO: Created: latency-svc-pq5d6
    Dec 14 09:12:22.629: INFO: Got endpoints: latency-svc-kncws [749.420056ms]
    Dec 14 09:12:22.655: INFO: Created: latency-svc-9cfhd
    Dec 14 09:12:22.680: INFO: Got endpoints: latency-svc-dm8cj [751.071639ms]
    Dec 14 09:12:22.690: INFO: Created: latency-svc-zpd7s
    Dec 14 09:12:22.729: INFO: Got endpoints: latency-svc-lkqpw [748.997359ms]
    Dec 14 09:12:22.757: INFO: Created: latency-svc-vr86r
    Dec 14 09:12:22.779: INFO: Got endpoints: latency-svc-svhvr [748.762692ms]
    Dec 14 09:12:22.786: INFO: Created: latency-svc-ftctw
    Dec 14 09:12:22.830: INFO: Got endpoints: latency-svc-zgs5t [749.812972ms]
    Dec 14 09:12:22.837: INFO: Created: latency-svc-6z75z
    Dec 14 09:12:22.880: INFO: Got endpoints: latency-svc-bntsl [750.244184ms]
    Dec 14 09:12:22.888: INFO: Created: latency-svc-5mb6h
    Dec 14 09:12:22.931: INFO: Got endpoints: latency-svc-nxftp [751.437437ms]
    Dec 14 09:12:22.940: INFO: Created: latency-svc-tk4b4
    Dec 14 09:12:22.980: INFO: Got endpoints: latency-svc-zxbwx [750.730153ms]
    Dec 14 09:12:22.988: INFO: Created: latency-svc-7sbwq
    Dec 14 09:12:23.029: INFO: Got endpoints: latency-svc-9l44c [747.706543ms]
    Dec 14 09:12:23.037: INFO: Created: latency-svc-9skb8
    Dec 14 09:12:23.078: INFO: Got endpoints: latency-svc-vvxfk [737.924556ms]
    Dec 14 09:12:23.085: INFO: Created: latency-svc-zds7s
    Dec 14 09:12:23.128: INFO: Got endpoints: latency-svc-r7q4r [748.639593ms]
    Dec 14 09:12:23.137: INFO: Created: latency-svc-ffkcn
    Dec 14 09:12:23.180: INFO: Got endpoints: latency-svc-r6sx2 [750.300227ms]
    Dec 14 09:12:23.189: INFO: Created: latency-svc-z48j4
    Dec 14 09:12:23.232: INFO: Got endpoints: latency-svc-w7wwz [752.444649ms]
    Dec 14 09:12:23.239: INFO: Created: latency-svc-9kl2r
    Dec 14 09:12:23.279: INFO: Got endpoints: latency-svc-lgcds [748.917771ms]
    Dec 14 09:12:23.287: INFO: Created: latency-svc-pkbxd
    Dec 14 09:12:23.329: INFO: Got endpoints: latency-svc-pq5d6 [749.985859ms]
    Dec 14 09:12:23.338: INFO: Created: latency-svc-5rpjg
    Dec 14 09:12:23.380: INFO: Got endpoints: latency-svc-9cfhd [751.0893ms]
    Dec 14 09:12:23.390: INFO: Created: latency-svc-mxlqk
    Dec 14 09:12:23.430: INFO: Got endpoints: latency-svc-zpd7s [749.663659ms]
    Dec 14 09:12:23.438: INFO: Created: latency-svc-ph5td
    Dec 14 09:12:23.479: INFO: Got endpoints: latency-svc-vr86r [749.587811ms]
    Dec 14 09:12:23.487: INFO: Created: latency-svc-ktcfb
    Dec 14 09:12:23.530: INFO: Got endpoints: latency-svc-ftctw [750.995424ms]
    Dec 14 09:12:23.540: INFO: Created: latency-svc-xw8jd
    Dec 14 09:12:23.579: INFO: Got endpoints: latency-svc-6z75z [749.272664ms]
    Dec 14 09:12:23.587: INFO: Created: latency-svc-2bn4g
    Dec 14 09:12:23.630: INFO: Got endpoints: latency-svc-5mb6h [749.637928ms]
    Dec 14 09:12:23.641: INFO: Created: latency-svc-g5rjl
    Dec 14 09:12:23.679: INFO: Got endpoints: latency-svc-tk4b4 [748.452918ms]
    Dec 14 09:12:23.689: INFO: Created: latency-svc-ks4q5
    Dec 14 09:12:23.729: INFO: Got endpoints: latency-svc-7sbwq [749.263496ms]
    Dec 14 09:12:23.739: INFO: Created: latency-svc-gw4j5
    Dec 14 09:12:23.779: INFO: Got endpoints: latency-svc-9skb8 [749.906967ms]
    Dec 14 09:12:23.786: INFO: Created: latency-svc-dlszr
    Dec 14 09:12:23.829: INFO: Got endpoints: latency-svc-zds7s [751.350896ms]
    Dec 14 09:12:23.838: INFO: Created: latency-svc-zlrhr
    Dec 14 09:12:23.878: INFO: Got endpoints: latency-svc-ffkcn [749.862804ms]
    Dec 14 09:12:23.887: INFO: Created: latency-svc-5sdqn
    Dec 14 09:12:23.930: INFO: Got endpoints: latency-svc-z48j4 [750.59049ms]
    Dec 14 09:12:23.941: INFO: Created: latency-svc-xd9mj
    Dec 14 09:12:23.983: INFO: Got endpoints: latency-svc-9kl2r [751.715298ms]
    Dec 14 09:12:23.995: INFO: Created: latency-svc-stbjn
    Dec 14 09:12:24.029: INFO: Got endpoints: latency-svc-pkbxd [750.062786ms]
    Dec 14 09:12:24.041: INFO: Created: latency-svc-xmztx
    Dec 14 09:12:24.079: INFO: Got endpoints: latency-svc-5rpjg [750.316449ms]
    Dec 14 09:12:24.088: INFO: Created: latency-svc-lrqlq
    Dec 14 09:12:24.131: INFO: Got endpoints: latency-svc-mxlqk [750.773054ms]
    Dec 14 09:12:24.142: INFO: Created: latency-svc-8bvg4
    Dec 14 09:12:24.179: INFO: Got endpoints: latency-svc-ph5td [748.881434ms]
    Dec 14 09:12:24.187: INFO: Created: latency-svc-8bvgm
    Dec 14 09:12:24.230: INFO: Got endpoints: latency-svc-ktcfb [750.978615ms]
    Dec 14 09:12:24.238: INFO: Created: latency-svc-qwrlj
    Dec 14 09:12:24.279: INFO: Got endpoints: latency-svc-xw8jd [749.498551ms]
    Dec 14 09:12:24.287: INFO: Created: latency-svc-x5hx2
    Dec 14 09:12:24.329: INFO: Got endpoints: latency-svc-2bn4g [749.797168ms]
    Dec 14 09:12:24.337: INFO: Created: latency-svc-gbsq5
    Dec 14 09:12:24.379: INFO: Got endpoints: latency-svc-g5rjl [749.716774ms]
    Dec 14 09:12:24.387: INFO: Created: latency-svc-hnrzs
    Dec 14 09:12:24.430: INFO: Got endpoints: latency-svc-ks4q5 [750.404337ms]
    Dec 14 09:12:24.438: INFO: Created: latency-svc-qjnv5
    Dec 14 09:12:24.479: INFO: Got endpoints: latency-svc-gw4j5 [750.010897ms]
    Dec 14 09:12:24.488: INFO: Created: latency-svc-f2bpj
    Dec 14 09:12:24.530: INFO: Got endpoints: latency-svc-dlszr [751.197717ms]
    Dec 14 09:12:24.537: INFO: Created: latency-svc-fdbws
    Dec 14 09:12:24.579: INFO: Got endpoints: latency-svc-zlrhr [749.696569ms]
    Dec 14 09:12:24.586: INFO: Created: latency-svc-7gmlp
    Dec 14 09:12:24.629: INFO: Got endpoints: latency-svc-5sdqn [750.522647ms]
    Dec 14 09:12:24.637: INFO: Created: latency-svc-6g8n8
    Dec 14 09:12:24.679: INFO: Got endpoints: latency-svc-xd9mj [747.785293ms]
    Dec 14 09:12:24.687: INFO: Created: latency-svc-wkgsk
    Dec 14 09:12:24.729: INFO: Got endpoints: latency-svc-stbjn [745.660481ms]
    Dec 14 09:12:24.738: INFO: Created: latency-svc-xl58s
    Dec 14 09:12:24.779: INFO: Got endpoints: latency-svc-xmztx [749.623016ms]
    Dec 14 09:12:24.787: INFO: Created: latency-svc-zdsck
    Dec 14 09:12:24.829: INFO: Got endpoints: latency-svc-lrqlq [750.219471ms]
    Dec 14 09:12:24.837: INFO: Created: latency-svc-nwdw9
    Dec 14 09:12:24.879: INFO: Got endpoints: latency-svc-8bvg4 [748.197441ms]
    Dec 14 09:12:24.886: INFO: Created: latency-svc-6gdqc
    Dec 14 09:12:24.929: INFO: Got endpoints: latency-svc-8bvgm [750.371564ms]
    Dec 14 09:12:24.939: INFO: Created: latency-svc-vwq4j
    Dec 14 09:12:24.979: INFO: Got endpoints: latency-svc-qwrlj [749.219797ms]
    Dec 14 09:12:24.986: INFO: Created: latency-svc-24lcg
    Dec 14 09:12:25.029: INFO: Got endpoints: latency-svc-x5hx2 [749.398376ms]
    Dec 14 09:12:25.036: INFO: Created: latency-svc-56qlj
    Dec 14 09:12:25.081: INFO: Got endpoints: latency-svc-gbsq5 [752.231237ms]
    Dec 14 09:12:25.092: INFO: Created: latency-svc-4vwrv
    Dec 14 09:12:25.130: INFO: Got endpoints: latency-svc-hnrzs [751.089961ms]
    Dec 14 09:12:25.139: INFO: Created: latency-svc-trgr5
    Dec 14 09:12:25.179: INFO: Got endpoints: latency-svc-qjnv5 [749.345173ms]
    Dec 14 09:12:25.188: INFO: Created: latency-svc-pk8k8
    Dec 14 09:12:25.229: INFO: Got endpoints: latency-svc-f2bpj [750.413238ms]
    Dec 14 09:12:25.238: INFO: Created: latency-svc-vglq8
    Dec 14 09:12:25.279: INFO: Got endpoints: latency-svc-fdbws [749.462205ms]
    Dec 14 09:12:25.291: INFO: Created: latency-svc-mqjhm
    Dec 14 09:12:25.329: INFO: Got endpoints: latency-svc-7gmlp [750.47331ms]
    Dec 14 09:12:25.337: INFO: Created: latency-svc-zxm8s
    Dec 14 09:12:25.378: INFO: Got endpoints: latency-svc-6g8n8 [749.509301ms]
    Dec 14 09:12:25.386: INFO: Created: latency-svc-59gp5
    Dec 14 09:12:25.429: INFO: Got endpoints: latency-svc-wkgsk [749.767219ms]
    Dec 14 09:12:25.437: INFO: Created: latency-svc-2zhs5
    Dec 14 09:12:25.478: INFO: Got endpoints: latency-svc-xl58s [749.283385ms]
    Dec 14 09:12:25.486: INFO: Created: latency-svc-g6tt5
    Dec 14 09:12:25.529: INFO: Got endpoints: latency-svc-zdsck [750.306035ms]
    Dec 14 09:12:25.538: INFO: Created: latency-svc-8w975
    Dec 14 09:12:25.579: INFO: Got endpoints: latency-svc-nwdw9 [749.739094ms]
    Dec 14 09:12:25.587: INFO: Created: latency-svc-vln2t
    Dec 14 09:12:25.629: INFO: Got endpoints: latency-svc-6gdqc [750.011647ms]
    Dec 14 09:12:25.638: INFO: Created: latency-svc-7nd6w
    Dec 14 09:12:25.679: INFO: Got endpoints: latency-svc-vwq4j [749.399869ms]
    Dec 14 09:12:25.688: INFO: Created: latency-svc-7fl4v
    Dec 14 09:12:25.729: INFO: Got endpoints: latency-svc-24lcg [750.234495ms]
    Dec 14 09:12:25.738: INFO: Created: latency-svc-4smzl
    Dec 14 09:12:25.780: INFO: Got endpoints: latency-svc-56qlj [751.593931ms]
    Dec 14 09:12:25.789: INFO: Created: latency-svc-8p5ch
    Dec 14 09:12:25.829: INFO: Got endpoints: latency-svc-4vwrv [747.842781ms]
    Dec 14 09:12:25.838: INFO: Created: latency-svc-cgf9t
    Dec 14 09:12:25.880: INFO: Got endpoints: latency-svc-trgr5 [749.465883ms]
    Dec 14 09:12:25.888: INFO: Created: latency-svc-6rkgw
    Dec 14 09:12:25.929: INFO: Got endpoints: latency-svc-pk8k8 [750.162485ms]
    Dec 14 09:12:25.938: INFO: Created: latency-svc-2gdgx
    Dec 14 09:12:25.980: INFO: Got endpoints: latency-svc-vglq8 [750.097485ms]
    Dec 14 09:12:25.988: INFO: Created: latency-svc-qdzwc
    Dec 14 09:12:26.030: INFO: Got endpoints: latency-svc-mqjhm [750.269095ms]
    Dec 14 09:12:26.038: INFO: Created: latency-svc-vpsf7
    Dec 14 09:12:26.079: INFO: Got endpoints: latency-svc-zxm8s [749.790334ms]
    Dec 14 09:12:26.087: INFO: Created: latency-svc-8dvmr
    Dec 14 09:12:26.129: INFO: Got endpoints: latency-svc-59gp5 [750.726241ms]
    Dec 14 09:12:26.137: INFO: Created: latency-svc-5rv6j
    Dec 14 09:12:26.179: INFO: Got endpoints: latency-svc-2zhs5 [750.473382ms]
    Dec 14 09:12:26.187: INFO: Created: latency-svc-kjmkq
    Dec 14 09:12:26.230: INFO: Got endpoints: latency-svc-g6tt5 [751.734898ms]
    Dec 14 09:12:26.240: INFO: Created: latency-svc-72qzc
    Dec 14 09:12:26.280: INFO: Got endpoints: latency-svc-8w975 [751.197212ms]
    Dec 14 09:12:26.291: INFO: Created: latency-svc-zdktk
    Dec 14 09:12:26.329: INFO: Got endpoints: latency-svc-vln2t [750.122746ms]
    Dec 14 09:12:26.337: INFO: Created: latency-svc-s5jxf
    Dec 14 09:12:26.379: INFO: Got endpoints: latency-svc-7nd6w [750.156168ms]
    Dec 14 09:12:26.388: INFO: Created: latency-svc-54jpx
    Dec 14 09:12:26.429: INFO: Got endpoints: latency-svc-7fl4v [749.908318ms]
    Dec 14 09:12:26.439: INFO: Created: latency-svc-c9mdp
    Dec 14 09:12:26.478: INFO: Got endpoints: latency-svc-4smzl [748.988585ms]
    Dec 14 09:12:26.487: INFO: Created: latency-svc-nld9w
    Dec 14 09:12:26.528: INFO: Got endpoints: latency-svc-8p5ch [748.06693ms]
    Dec 14 09:12:26.543: INFO: Created: latency-svc-4zfdf
    Dec 14 09:12:26.580: INFO: Got endpoints: latency-svc-cgf9t [750.25981ms]
    Dec 14 09:12:26.593: INFO: Created: latency-svc-m28tk
    Dec 14 09:12:26.630: INFO: Got endpoints: latency-svc-6rkgw [749.686507ms]
    Dec 14 09:12:26.638: INFO: Created: latency-svc-2dkfb
    Dec 14 09:12:26.680: INFO: Got endpoints: latency-svc-2gdgx [750.523966ms]
    Dec 14 09:12:26.688: INFO: Created: latency-svc-p25cl
    Dec 14 09:12:26.729: INFO: Got endpoints: latency-svc-qdzwc [749.513742ms]
    Dec 14 09:12:26.738: INFO: Created: latency-svc-v6mn9
    Dec 14 09:12:26.780: INFO: Got endpoints: latency-svc-vpsf7 [749.823191ms]
    Dec 14 09:12:26.787: INFO: Created: latency-svc-7trw5
    Dec 14 09:12:26.829: INFO: Got endpoints: latency-svc-8dvmr [750.187094ms]
    Dec 14 09:12:26.839: INFO: Created: latency-svc-hnjkd
    Dec 14 09:12:26.879: INFO: Got endpoints: latency-svc-5rv6j [749.927139ms]
    Dec 14 09:12:26.929: INFO: Got endpoints: latency-svc-kjmkq [749.744131ms]
    Dec 14 09:12:26.980: INFO: Got endpoints: latency-svc-72qzc [749.3964ms]
    Dec 14 09:12:27.029: INFO: Got endpoints: latency-svc-zdktk [748.764948ms]
    Dec 14 09:12:27.079: INFO: Got endpoints: latency-svc-s5jxf [749.226294ms]
    Dec 14 09:12:27.129: INFO: Got endpoints: latency-svc-54jpx [749.35004ms]
    Dec 14 09:12:27.179: INFO: Got endpoints: latency-svc-c9mdp [750.163005ms]
    Dec 14 09:12:27.230: INFO: Got endpoints: latency-svc-nld9w [752.100443ms]
    Dec 14 09:12:27.279: INFO: Got endpoints: latency-svc-4zfdf [750.524941ms]
    Dec 14 09:12:27.341: INFO: Got endpoints: latency-svc-m28tk [761.695969ms]
    Dec 14 09:12:27.379: INFO: Got endpoints: latency-svc-2dkfb [749.640462ms]
    Dec 14 09:12:27.429: INFO: Got endpoints: latency-svc-p25cl [749.531893ms]
    Dec 14 09:12:27.480: INFO: Got endpoints: latency-svc-v6mn9 [750.703145ms]
    Dec 14 09:12:27.529: INFO: Got endpoints: latency-svc-7trw5 [749.541933ms]
    Dec 14 09:12:27.581: INFO: Got endpoints: latency-svc-hnjkd [751.94486ms]
    Dec 14 09:12:27.581: INFO: Latencies: [11.381647ms 13.852565ms 14.366897ms 15.498263ms 18.338746ms 19.837596ms 22.911663ms 23.982768ms 39.465549ms 41.962138ms 47.599845ms 53.493248ms 56.369042ms 61.731717ms 61.947621ms 62.05117ms 66.137619ms 68.552155ms 72.838519ms 73.968495ms 74.657059ms 75.251006ms 76.031909ms 80.412374ms 83.28587ms 83.765745ms 83.894026ms 84.406292ms 84.431284ms 84.471427ms 84.555539ms 84.698309ms 113.926959ms 166.301759ms 216.993937ms 258.852933ms 304.390531ms 354.996127ms 379.953178ms 424.311531ms 468.096155ms 516.59395ms 559.836583ms 603.722605ms 646.965568ms 696.811992ms 709.898874ms 731.175881ms 731.447519ms 737.924556ms 744.315323ms 744.408947ms 745.660481ms 745.691612ms 746.373702ms 747.154243ms 747.706543ms 747.785293ms 747.842781ms 748.06693ms 748.197441ms 748.310357ms 748.452918ms 748.639593ms 748.760064ms 748.762692ms 748.764948ms 748.881434ms 748.910719ms 748.917771ms 748.988585ms 748.997359ms 749.007949ms 749.098121ms 749.219797ms 749.226294ms 749.263496ms 749.272664ms 749.283385ms 749.296937ms 749.345173ms 749.35004ms 749.374879ms 749.376035ms 749.3964ms 749.398376ms 749.399869ms 749.420056ms 749.454209ms 749.462205ms 749.465883ms 749.498551ms 749.509301ms 749.512218ms 749.513742ms 749.531893ms 749.541933ms 749.587811ms 749.623016ms 749.637928ms 749.640462ms 749.663659ms 749.686507ms 749.69384ms 749.695694ms 749.696569ms 749.713834ms 749.716774ms 749.739094ms 749.744131ms 749.752086ms 749.756599ms 749.767219ms 749.785708ms 749.790334ms 749.797168ms 749.812972ms 749.823191ms 749.841287ms 749.842018ms 749.862804ms 749.872348ms 749.906967ms 749.908318ms 749.927139ms 749.956868ms 749.984757ms 749.985859ms 750.010897ms 750.011647ms 750.013283ms 750.030358ms 750.062786ms 750.065101ms 750.097485ms 750.106433ms 750.118421ms 750.122746ms 750.144294ms 750.156168ms 750.162485ms 750.163005ms 750.187094ms 750.219471ms 750.234495ms 750.241841ms 750.244184ms 750.25981ms 750.269095ms 750.281715ms 750.296004ms 750.300227ms 750.306035ms 750.316449ms 750.352621ms 750.371564ms 750.404337ms 750.413238ms 750.431039ms 750.47331ms 750.473382ms 750.481608ms 750.522647ms 750.523966ms 750.524941ms 750.569121ms 750.59049ms 750.703145ms 750.726241ms 750.730153ms 750.736812ms 750.773054ms 750.978615ms 750.995424ms 751.002961ms 751.058603ms 751.071639ms 751.0893ms 751.089961ms 751.166243ms 751.197212ms 751.197717ms 751.350896ms 751.437437ms 751.504869ms 751.592188ms 751.593931ms 751.709379ms 751.715298ms 751.734898ms 751.94486ms 752.075208ms 752.100443ms 752.231237ms 752.267505ms 752.444649ms 755.723872ms 760.911708ms 761.695969ms 803.570102ms]
    Dec 14 09:12:27.582: INFO: 50 %ile: 749.640462ms
    Dec 14 09:12:27.582: INFO: 90 %ile: 751.197212ms
    Dec 14 09:12:27.582: INFO: 99 %ile: 761.695969ms
    Dec 14 09:12:27.582: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Dec 14 09:12:27.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-6492" for this suite. 12/14/22 09:12:27.588
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:12:27.593
Dec 14 09:12:27.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/14/22 09:12:27.594
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:12:27.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:12:27.629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 09:12:27.633: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:12:27.641: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:12:27.645: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
Dec 14 09:12:27.656: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 14 09:12:27.656: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec 14 09:12:27.656: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container proxy ready: true, restart count 0
Dec 14 09:12:27.656: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 09:12:27.656: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 09:12:27.656: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:12:27.656: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container calico-typha ready: true, restart count 0
Dec 14 09:12:27.656: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:12:27.656: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:12:27.656: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:12:27.656: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:12:27.656: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 09:12:27.656: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 09:12:27.656: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 09:12:27.656: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 09:12:27.656: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 09:12:27.656: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 09:12:27.656: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 09:12:27.656: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 09:12:27.656: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 09:12:27.656: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 09:12:27.656: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 09:12:27.656: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec 14 09:12:27.656: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 14 09:12:27.656: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.656: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:12:27.656: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
Dec 14 09:12:27.668: INFO: adopt-release-jgd4l from job-9386 started at 2022-12-14 09:12:01 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container c ready: true, restart count 0
Dec 14 09:12:27.668: INFO: adopt-release-tkdbg from job-9386 started at 2022-12-14 09:12:01 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container c ready: true, restart count 0
Dec 14 09:12:27.668: INFO: adopt-release-vfwj8 from job-9386 started at 2022-12-14 09:12:11 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container c ready: true, restart count 0
Dec 14 09:12:27.668: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container proxy ready: true, restart count 0
Dec 14 09:12:27.668: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 09:12:27.668: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 09:12:27.668: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 09:12:27.668: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 09:12:27.668: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 09:12:27.668: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 09:12:27.668: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 09:12:27.668: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 09:12:27.668: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 09:12:27.668: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 09:12:27.668: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container metrics-server ready: true, restart count 2
Dec 14 09:12:27.668: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container metrics-server ready: true, restart count 1
Dec 14 09:12:27.668: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 09:12:27.668: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 09:12:27.668: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 09:12:27.668: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 09:12:27.668: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 09:12:27.668: INFO: svc-latency-rc-qj672 from svc-latency-6492 started at 2022-12-14 09:12:12 +0000 UTC (1 container statuses recorded)
Dec 14 09:12:27.668: INFO: 	Container svc-latency-rc ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 09:12:27.668
Dec 14 09:12:27.678: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1752" to be "running"
Dec 14 09:12:27.682: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.199206ms
Dec 14 09:12:29.687: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008264076s
Dec 14 09:12:31.688: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009850977s
Dec 14 09:12:33.687: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 6.008051659s
Dec 14 09:12:33.687: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 09:12:33.69
STEP: Trying to apply a random label on the found node. 12/14/22 09:12:33.702
STEP: verifying the node has the label kubernetes.io/e2e-13171ae5-1aa1-43d7-a9fa-0f995724fb72 95 12/14/22 09:12:33.712
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/14/22 09:12:33.715
Dec 14 09:12:33.725: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1752" to be "not pending"
Dec 14 09:12:33.728: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260784ms
Dec 14 09:12:35.732: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007899282s
Dec 14 09:12:35.733: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.3.210 on the node which pod4 resides and expect not scheduled 12/14/22 09:12:35.733
Dec 14 09:12:35.753: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1752" to be "not pending"
Dec 14 09:12:35.756: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.362807ms
Dec 14 09:12:37.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007496393s
Dec 14 09:12:39.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007940022s
Dec 14 09:12:41.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009816292s
Dec 14 09:12:43.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008855525s
Dec 14 09:12:45.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008119729s
Dec 14 09:12:47.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009167863s
Dec 14 09:12:49.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008575591s
Dec 14 09:12:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008904167s
Dec 14 09:12:53.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008892432s
Dec 14 09:12:55.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007844837s
Dec 14 09:12:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008703564s
Dec 14 09:12:59.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007879989s
Dec 14 09:13:01.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008068512s
Dec 14 09:13:03.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009535295s
Dec 14 09:13:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008109794s
Dec 14 09:13:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008546085s
Dec 14 09:13:09.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008286498s
Dec 14 09:13:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00904983s
Dec 14 09:13:13.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009145359s
Dec 14 09:13:15.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009102454s
Dec 14 09:13:17.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009746263s
Dec 14 09:13:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008291508s
Dec 14 09:13:21.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009833235s
Dec 14 09:13:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009028793s
Dec 14 09:13:25.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008049184s
Dec 14 09:13:27.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00842514s
Dec 14 09:13:29.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008146204s
Dec 14 09:13:31.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009029783s
Dec 14 09:13:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008957796s
Dec 14 09:13:35.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008252123s
Dec 14 09:13:37.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009563063s
Dec 14 09:13:39.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007598118s
Dec 14 09:13:41.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008884631s
Dec 14 09:13:43.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008265001s
Dec 14 09:13:45.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009185865s
Dec 14 09:13:47.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009242466s
Dec 14 09:13:49.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00813498s
Dec 14 09:13:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009387431s
Dec 14 09:13:53.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008218766s
Dec 14 09:13:55.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008535723s
Dec 14 09:13:57.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008408816s
Dec 14 09:13:59.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008437548s
Dec 14 09:14:01.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009179617s
Dec 14 09:14:03.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008786147s
Dec 14 09:14:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007631336s
Dec 14 09:14:07.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009722182s
Dec 14 09:14:09.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008797347s
Dec 14 09:14:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008689371s
Dec 14 09:14:13.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008144651s
Dec 14 09:14:15.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00781666s
Dec 14 09:14:17.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009024229s
Dec 14 09:14:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008392169s
Dec 14 09:14:21.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008800545s
Dec 14 09:14:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008621082s
Dec 14 09:14:25.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008526302s
Dec 14 09:14:27.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00906388s
Dec 14 09:14:29.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008641938s
Dec 14 09:14:31.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008238002s
Dec 14 09:14:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009055995s
Dec 14 09:14:35.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007791689s
Dec 14 09:14:37.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009343014s
Dec 14 09:14:39.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008171837s
Dec 14 09:14:41.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008691176s
Dec 14 09:14:43.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009281067s
Dec 14 09:14:45.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008254437s
Dec 14 09:14:47.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009480306s
Dec 14 09:14:49.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008744017s
Dec 14 09:14:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009357587s
Dec 14 09:14:53.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007727756s
Dec 14 09:14:55.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008597211s
Dec 14 09:14:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008932842s
Dec 14 09:14:59.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009595936s
Dec 14 09:15:01.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008809721s
Dec 14 09:15:03.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008121485s
Dec 14 09:15:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00847659s
Dec 14 09:15:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009165201s
Dec 14 09:15:09.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008545592s
Dec 14 09:15:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009275783s
Dec 14 09:15:13.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009570953s
Dec 14 09:15:15.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007972734s
Dec 14 09:15:17.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008965997s
Dec 14 09:15:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.008156435s
Dec 14 09:15:21.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009076369s
Dec 14 09:15:23.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008304584s
Dec 14 09:15:25.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.007534751s
Dec 14 09:15:27.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007904062s
Dec 14 09:15:29.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008264463s
Dec 14 09:15:31.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009318269s
Dec 14 09:15:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008935394s
Dec 14 09:15:35.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008857193s
Dec 14 09:15:37.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008723884s
Dec 14 09:15:39.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008759653s
Dec 14 09:15:41.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009642931s
Dec 14 09:15:43.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009313708s
Dec 14 09:15:45.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008484975s
Dec 14 09:15:47.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009766503s
Dec 14 09:15:49.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008146766s
Dec 14 09:15:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009185011s
Dec 14 09:15:53.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008981679s
Dec 14 09:15:55.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009298771s
Dec 14 09:15:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008600748s
Dec 14 09:15:59.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008606107s
Dec 14 09:16:01.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009160858s
Dec 14 09:16:03.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009747731s
Dec 14 09:16:05.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008708083s
Dec 14 09:16:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008633906s
Dec 14 09:16:09.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008686348s
Dec 14 09:16:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009066419s
Dec 14 09:16:13.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009260372s
Dec 14 09:16:15.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00861014s
Dec 14 09:16:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010881143s
Dec 14 09:16:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008477933s
Dec 14 09:16:21.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010374664s
Dec 14 09:16:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009149593s
Dec 14 09:16:25.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.0087457s
Dec 14 09:16:27.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00929106s
Dec 14 09:16:29.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008652177s
Dec 14 09:16:31.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009821588s
Dec 14 09:16:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008570429s
Dec 14 09:16:35.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007589486s
Dec 14 09:16:37.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008469904s
Dec 14 09:16:39.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009209417s
Dec 14 09:16:41.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009222668s
Dec 14 09:16:43.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009724522s
Dec 14 09:16:45.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008019586s
Dec 14 09:16:47.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009007663s
Dec 14 09:16:49.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009520447s
Dec 14 09:16:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009187474s
Dec 14 09:16:53.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008788542s
Dec 14 09:16:55.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008300776s
Dec 14 09:16:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008847605s
Dec 14 09:16:59.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009131695s
Dec 14 09:17:01.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010037252s
Dec 14 09:17:03.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.00904867s
Dec 14 09:17:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008253005s
Dec 14 09:17:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008583852s
Dec 14 09:17:09.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008300181s
Dec 14 09:17:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008836128s
Dec 14 09:17:13.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008554339s
Dec 14 09:17:15.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008869916s
Dec 14 09:17:17.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009108612s
Dec 14 09:17:19.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009147508s
Dec 14 09:17:21.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008599896s
Dec 14 09:17:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009135739s
Dec 14 09:17:25.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009150472s
Dec 14 09:17:27.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009220627s
Dec 14 09:17:29.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008250657s
Dec 14 09:17:31.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009029624s
Dec 14 09:17:33.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009493141s
Dec 14 09:17:35.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008708903s
Dec 14 09:17:35.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012920411s
STEP: removing the label kubernetes.io/e2e-13171ae5-1aa1-43d7-a9fa-0f995724fb72 off the node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 09:17:35.766
STEP: verifying the node doesn't have the label kubernetes.io/e2e-13171ae5-1aa1-43d7-a9fa-0f995724fb72 12/14/22 09:17:35.783
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:17:35.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1752" for this suite. 12/14/22 09:17:35.791
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":245,"skipped":4704,"failed":0}
------------------------------
• [SLOW TEST] [308.203 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:12:27.593
    Dec 14 09:12:27.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/14/22 09:12:27.594
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:12:27.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:12:27.629
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 14 09:12:27.633: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 09:12:27.641: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 09:12:27.645: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
    Dec 14 09:12:27.656: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container calico-typha ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 09:12:27.656: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.656: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec 14 09:12:27.656: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
    Dec 14 09:12:27.668: INFO: adopt-release-jgd4l from job-9386 started at 2022-12-14 09:12:01 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container c ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: adopt-release-tkdbg from job-9386 started at 2022-12-14 09:12:01 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container c ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: adopt-release-vfwj8 from job-9386 started at 2022-12-14 09:12:11 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container c ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 09:12:27.668: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container metrics-server ready: true, restart count 2
    Dec 14 09:12:27.668: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container metrics-server ready: true, restart count 1
    Dec 14 09:12:27.668: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 09:12:27.668: INFO: svc-latency-rc-qj672 from svc-latency-6492 started at 2022-12-14 09:12:12 +0000 UTC (1 container statuses recorded)
    Dec 14 09:12:27.668: INFO: 	Container svc-latency-rc ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 09:12:27.668
    Dec 14 09:12:27.678: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1752" to be "running"
    Dec 14 09:12:27.682: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.199206ms
    Dec 14 09:12:29.687: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008264076s
    Dec 14 09:12:31.688: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009850977s
    Dec 14 09:12:33.687: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 6.008051659s
    Dec 14 09:12:33.687: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 09:12:33.69
    STEP: Trying to apply a random label on the found node. 12/14/22 09:12:33.702
    STEP: verifying the node has the label kubernetes.io/e2e-13171ae5-1aa1-43d7-a9fa-0f995724fb72 95 12/14/22 09:12:33.712
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/14/22 09:12:33.715
    Dec 14 09:12:33.725: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1752" to be "not pending"
    Dec 14 09:12:33.728: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260784ms
    Dec 14 09:12:35.732: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007899282s
    Dec 14 09:12:35.733: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.3.210 on the node which pod4 resides and expect not scheduled 12/14/22 09:12:35.733
    Dec 14 09:12:35.753: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1752" to be "not pending"
    Dec 14 09:12:35.756: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.362807ms
    Dec 14 09:12:37.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007496393s
    Dec 14 09:12:39.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007940022s
    Dec 14 09:12:41.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009816292s
    Dec 14 09:12:43.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008855525s
    Dec 14 09:12:45.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008119729s
    Dec 14 09:12:47.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009167863s
    Dec 14 09:12:49.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008575591s
    Dec 14 09:12:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008904167s
    Dec 14 09:12:53.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008892432s
    Dec 14 09:12:55.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007844837s
    Dec 14 09:12:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008703564s
    Dec 14 09:12:59.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007879989s
    Dec 14 09:13:01.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008068512s
    Dec 14 09:13:03.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009535295s
    Dec 14 09:13:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008109794s
    Dec 14 09:13:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008546085s
    Dec 14 09:13:09.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008286498s
    Dec 14 09:13:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00904983s
    Dec 14 09:13:13.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009145359s
    Dec 14 09:13:15.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009102454s
    Dec 14 09:13:17.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009746263s
    Dec 14 09:13:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008291508s
    Dec 14 09:13:21.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009833235s
    Dec 14 09:13:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009028793s
    Dec 14 09:13:25.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008049184s
    Dec 14 09:13:27.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00842514s
    Dec 14 09:13:29.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008146204s
    Dec 14 09:13:31.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009029783s
    Dec 14 09:13:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008957796s
    Dec 14 09:13:35.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008252123s
    Dec 14 09:13:37.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009563063s
    Dec 14 09:13:39.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007598118s
    Dec 14 09:13:41.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008884631s
    Dec 14 09:13:43.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008265001s
    Dec 14 09:13:45.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009185865s
    Dec 14 09:13:47.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009242466s
    Dec 14 09:13:49.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00813498s
    Dec 14 09:13:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009387431s
    Dec 14 09:13:53.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008218766s
    Dec 14 09:13:55.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008535723s
    Dec 14 09:13:57.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008408816s
    Dec 14 09:13:59.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008437548s
    Dec 14 09:14:01.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009179617s
    Dec 14 09:14:03.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008786147s
    Dec 14 09:14:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007631336s
    Dec 14 09:14:07.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009722182s
    Dec 14 09:14:09.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008797347s
    Dec 14 09:14:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008689371s
    Dec 14 09:14:13.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008144651s
    Dec 14 09:14:15.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00781666s
    Dec 14 09:14:17.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009024229s
    Dec 14 09:14:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008392169s
    Dec 14 09:14:21.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008800545s
    Dec 14 09:14:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008621082s
    Dec 14 09:14:25.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008526302s
    Dec 14 09:14:27.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00906388s
    Dec 14 09:14:29.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008641938s
    Dec 14 09:14:31.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008238002s
    Dec 14 09:14:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009055995s
    Dec 14 09:14:35.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007791689s
    Dec 14 09:14:37.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009343014s
    Dec 14 09:14:39.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008171837s
    Dec 14 09:14:41.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008691176s
    Dec 14 09:14:43.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009281067s
    Dec 14 09:14:45.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008254437s
    Dec 14 09:14:47.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009480306s
    Dec 14 09:14:49.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008744017s
    Dec 14 09:14:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009357587s
    Dec 14 09:14:53.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007727756s
    Dec 14 09:14:55.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008597211s
    Dec 14 09:14:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008932842s
    Dec 14 09:14:59.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009595936s
    Dec 14 09:15:01.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008809721s
    Dec 14 09:15:03.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008121485s
    Dec 14 09:15:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00847659s
    Dec 14 09:15:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009165201s
    Dec 14 09:15:09.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008545592s
    Dec 14 09:15:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009275783s
    Dec 14 09:15:13.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009570953s
    Dec 14 09:15:15.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007972734s
    Dec 14 09:15:17.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008965997s
    Dec 14 09:15:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.008156435s
    Dec 14 09:15:21.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009076369s
    Dec 14 09:15:23.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008304584s
    Dec 14 09:15:25.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.007534751s
    Dec 14 09:15:27.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007904062s
    Dec 14 09:15:29.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008264463s
    Dec 14 09:15:31.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009318269s
    Dec 14 09:15:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008935394s
    Dec 14 09:15:35.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008857193s
    Dec 14 09:15:37.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008723884s
    Dec 14 09:15:39.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008759653s
    Dec 14 09:15:41.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009642931s
    Dec 14 09:15:43.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009313708s
    Dec 14 09:15:45.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008484975s
    Dec 14 09:15:47.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009766503s
    Dec 14 09:15:49.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008146766s
    Dec 14 09:15:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009185011s
    Dec 14 09:15:53.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008981679s
    Dec 14 09:15:55.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009298771s
    Dec 14 09:15:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008600748s
    Dec 14 09:15:59.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008606107s
    Dec 14 09:16:01.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009160858s
    Dec 14 09:16:03.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009747731s
    Dec 14 09:16:05.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008708083s
    Dec 14 09:16:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008633906s
    Dec 14 09:16:09.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008686348s
    Dec 14 09:16:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009066419s
    Dec 14 09:16:13.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009260372s
    Dec 14 09:16:15.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00861014s
    Dec 14 09:16:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010881143s
    Dec 14 09:16:19.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008477933s
    Dec 14 09:16:21.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010374664s
    Dec 14 09:16:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009149593s
    Dec 14 09:16:25.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.0087457s
    Dec 14 09:16:27.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00929106s
    Dec 14 09:16:29.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008652177s
    Dec 14 09:16:31.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009821588s
    Dec 14 09:16:33.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008570429s
    Dec 14 09:16:35.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007589486s
    Dec 14 09:16:37.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008469904s
    Dec 14 09:16:39.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009209417s
    Dec 14 09:16:41.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009222668s
    Dec 14 09:16:43.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009724522s
    Dec 14 09:16:45.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008019586s
    Dec 14 09:16:47.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009007663s
    Dec 14 09:16:49.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009520447s
    Dec 14 09:16:51.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009187474s
    Dec 14 09:16:53.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008788542s
    Dec 14 09:16:55.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008300776s
    Dec 14 09:16:57.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008847605s
    Dec 14 09:16:59.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009131695s
    Dec 14 09:17:01.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010037252s
    Dec 14 09:17:03.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.00904867s
    Dec 14 09:17:05.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008253005s
    Dec 14 09:17:07.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008583852s
    Dec 14 09:17:09.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008300181s
    Dec 14 09:17:11.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008836128s
    Dec 14 09:17:13.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008554339s
    Dec 14 09:17:15.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008869916s
    Dec 14 09:17:17.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009108612s
    Dec 14 09:17:19.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009147508s
    Dec 14 09:17:21.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008599896s
    Dec 14 09:17:23.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009135739s
    Dec 14 09:17:25.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009150472s
    Dec 14 09:17:27.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009220627s
    Dec 14 09:17:29.761: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008250657s
    Dec 14 09:17:31.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009029624s
    Dec 14 09:17:33.763: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009493141s
    Dec 14 09:17:35.762: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008708903s
    Dec 14 09:17:35.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012920411s
    STEP: removing the label kubernetes.io/e2e-13171ae5-1aa1-43d7-a9fa-0f995724fb72 off the node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 09:17:35.766
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-13171ae5-1aa1-43d7-a9fa-0f995724fb72 12/14/22 09:17:35.783
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:17:35.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1752" for this suite. 12/14/22 09:17:35.791
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:17:35.796
Dec 14 09:17:35.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 12/14/22 09:17:35.797
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:17:35.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:17:35.813
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 12/14/22 09:17:35.819
STEP: Ensuring job reaches completions 12/14/22 09:17:35.824
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 14 09:17:45.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1371" for this suite. 12/14/22 09:17:45.835
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":246,"skipped":4722,"failed":0}
------------------------------
• [10.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:17:35.796
    Dec 14 09:17:35.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 12/14/22 09:17:35.797
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:17:35.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:17:35.813
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 12/14/22 09:17:35.819
    STEP: Ensuring job reaches completions 12/14/22 09:17:35.824
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 14 09:17:45.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1371" for this suite. 12/14/22 09:17:45.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:17:45.841
Dec 14 09:17:45.841: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 12/14/22 09:17:45.841
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:17:45.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:17:45.858
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1757 12/14/22 09:17:45.862
STEP: creating a selector 12/14/22 09:17:45.862
STEP: Creating the service pods in kubernetes 12/14/22 09:17:45.862
Dec 14 09:17:45.862: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 09:17:45.890: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1757" to be "running and ready"
Dec 14 09:17:45.894: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.852514ms
Dec 14 09:17:45.894: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:17:47.899: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009313261s
Dec 14 09:17:47.899: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:17:49.899: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009563993s
Dec 14 09:17:49.899: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:17:51.898: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008464822s
Dec 14 09:17:51.898: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:17:53.898: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008332952s
Dec 14 09:17:53.898: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:17:55.898: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008712524s
Dec 14 09:17:55.898: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:17:57.900: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010281702s
Dec 14 09:17:57.900: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 09:17:57.900: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 09:17:57.904: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1757" to be "running and ready"
Dec 14 09:17:57.907: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.20677ms
Dec 14 09:17:57.907: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 09:17:57.907: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 09:17:57.91
Dec 14 09:17:57.925: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1757" to be "running"
Dec 14 09:17:57.928: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.382002ms
Dec 14 09:17:59.938: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01291576s
Dec 14 09:17:59.938: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 09:17:59.942: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1757" to be "running"
Dec 14 09:17:59.949: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.133337ms
Dec 14 09:17:59.949: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 14 09:17:59.953: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 14 09:17:59.953: INFO: Going to poll 100.64.0.159 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec 14 09:17:59.956: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1757 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:17:59.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:17:59.956: INFO: ExecWithOptions: Clientset creation
Dec 14 09:17:59.956: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1757/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.0.159+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 09:18:01.270: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 09:18:01.270: INFO: Going to poll 100.64.1.127 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec 14 09:18:01.274: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.127 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1757 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:18:01.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:18:01.275: INFO: ExecWithOptions: Clientset creation
Dec 14 09:18:01.275: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1757/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.1.127+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 09:18:02.594: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 14 09:18:02.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1757" for this suite. 12/14/22 09:18:02.601
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4749,"failed":0}
------------------------------
• [16.774 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:17:45.841
    Dec 14 09:17:45.841: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 09:17:45.841
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:17:45.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:17:45.858
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1757 12/14/22 09:17:45.862
    STEP: creating a selector 12/14/22 09:17:45.862
    STEP: Creating the service pods in kubernetes 12/14/22 09:17:45.862
    Dec 14 09:17:45.862: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 09:17:45.890: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1757" to be "running and ready"
    Dec 14 09:17:45.894: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.852514ms
    Dec 14 09:17:45.894: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:17:47.899: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009313261s
    Dec 14 09:17:47.899: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:17:49.899: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009563993s
    Dec 14 09:17:49.899: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:17:51.898: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008464822s
    Dec 14 09:17:51.898: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:17:53.898: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008332952s
    Dec 14 09:17:53.898: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:17:55.898: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008712524s
    Dec 14 09:17:55.898: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:17:57.900: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010281702s
    Dec 14 09:17:57.900: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 09:17:57.900: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 09:17:57.904: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1757" to be "running and ready"
    Dec 14 09:17:57.907: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.20677ms
    Dec 14 09:17:57.907: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 09:17:57.907: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 09:17:57.91
    Dec 14 09:17:57.925: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1757" to be "running"
    Dec 14 09:17:57.928: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.382002ms
    Dec 14 09:17:59.938: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01291576s
    Dec 14 09:17:59.938: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 09:17:59.942: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1757" to be "running"
    Dec 14 09:17:59.949: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.133337ms
    Dec 14 09:17:59.949: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 14 09:17:59.953: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec 14 09:17:59.953: INFO: Going to poll 100.64.0.159 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Dec 14 09:17:59.956: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1757 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:17:59.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:17:59.956: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:17:59.956: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1757/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.0.159+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 09:18:01.270: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 14 09:18:01.270: INFO: Going to poll 100.64.1.127 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Dec 14 09:18:01.274: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.127 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1757 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:18:01.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:18:01.275: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:18:01.275: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1757/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.1.127+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 09:18:02.594: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 14 09:18:02.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1757" for this suite. 12/14/22 09:18:02.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:02.616
Dec 14 09:18:02.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 09:18:02.617
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:02.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:02.632
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Dec 14 09:18:02.645: INFO: Waiting up to 5m0s for pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250" in namespace "pods-9335" to be "running and ready"
Dec 14 09:18:02.649: INFO: Pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568453ms
Dec 14 09:18:02.649: INFO: The phase of Pod server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:18:04.654: INFO: Pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250": Phase="Running", Reason="", readiness=true. Elapsed: 2.008605115s
Dec 14 09:18:04.654: INFO: The phase of Pod server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250 is Running (Ready = true)
Dec 14 09:18:04.654: INFO: Pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250" satisfied condition "running and ready"
Dec 14 09:18:04.676: INFO: Waiting up to 5m0s for pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678" in namespace "pods-9335" to be "Succeeded or Failed"
Dec 14 09:18:04.679: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5639ms
Dec 14 09:18:06.685: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008873383s
Dec 14 09:18:08.685: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008774375s
STEP: Saw pod success 12/14/22 09:18:08.685
Dec 14 09:18:08.685: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678" satisfied condition "Succeeded or Failed"
Dec 14 09:18:08.688: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678 container env3cont: <nil>
STEP: delete the pod 12/14/22 09:18:08.701
Dec 14 09:18:08.711: INFO: Waiting for pod client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678 to disappear
Dec 14 09:18:08.715: INFO: Pod client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 09:18:08.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9335" for this suite. 12/14/22 09:18:08.721
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":248,"skipped":4763,"failed":0}
------------------------------
• [6.109 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:02.616
    Dec 14 09:18:02.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 09:18:02.617
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:02.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:02.632
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Dec 14 09:18:02.645: INFO: Waiting up to 5m0s for pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250" in namespace "pods-9335" to be "running and ready"
    Dec 14 09:18:02.649: INFO: Pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568453ms
    Dec 14 09:18:02.649: INFO: The phase of Pod server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:18:04.654: INFO: Pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250": Phase="Running", Reason="", readiness=true. Elapsed: 2.008605115s
    Dec 14 09:18:04.654: INFO: The phase of Pod server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250 is Running (Ready = true)
    Dec 14 09:18:04.654: INFO: Pod "server-envvars-887cfe1b-1c77-4b21-9355-f7800bcea250" satisfied condition "running and ready"
    Dec 14 09:18:04.676: INFO: Waiting up to 5m0s for pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678" in namespace "pods-9335" to be "Succeeded or Failed"
    Dec 14 09:18:04.679: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5639ms
    Dec 14 09:18:06.685: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008873383s
    Dec 14 09:18:08.685: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008774375s
    STEP: Saw pod success 12/14/22 09:18:08.685
    Dec 14 09:18:08.685: INFO: Pod "client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678" satisfied condition "Succeeded or Failed"
    Dec 14 09:18:08.688: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678 container env3cont: <nil>
    STEP: delete the pod 12/14/22 09:18:08.701
    Dec 14 09:18:08.711: INFO: Waiting for pod client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678 to disappear
    Dec 14 09:18:08.715: INFO: Pod client-envvars-23af6b01-6bdd-48f0-b4cc-894113067678 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 09:18:08.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9335" for this suite. 12/14/22 09:18:08.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:08.726
Dec 14 09:18:08.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 12/14/22 09:18:08.727
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:08.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:08.742
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 12/14/22 09:18:08.751
STEP: waiting for RC to be added 12/14/22 09:18:08.755
STEP: waiting for available Replicas 12/14/22 09:18:08.755
STEP: patching ReplicationController 12/14/22 09:18:10.366
STEP: waiting for RC to be modified 12/14/22 09:18:10.374
STEP: patching ReplicationController status 12/14/22 09:18:10.374
STEP: waiting for RC to be modified 12/14/22 09:18:10.379
STEP: waiting for available Replicas 12/14/22 09:18:10.379
STEP: fetching ReplicationController status 12/14/22 09:18:10.381
STEP: patching ReplicationController scale 12/14/22 09:18:10.385
STEP: waiting for RC to be modified 12/14/22 09:18:10.389
STEP: waiting for ReplicationController's scale to be the max amount 12/14/22 09:18:10.389
STEP: fetching ReplicationController; ensuring that it's patched 12/14/22 09:18:11.365
STEP: updating ReplicationController status 12/14/22 09:18:11.369
STEP: waiting for RC to be modified 12/14/22 09:18:11.373
STEP: listing all ReplicationControllers 12/14/22 09:18:11.374
STEP: checking that ReplicationController has expected values 12/14/22 09:18:11.377
STEP: deleting ReplicationControllers by collection 12/14/22 09:18:11.377
STEP: waiting for ReplicationController to have a DELETED watchEvent 12/14/22 09:18:11.383
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 14 09:18:11.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3449" for this suite. 12/14/22 09:18:11.43
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":249,"skipped":4782,"failed":0}
------------------------------
• [2.709 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:08.726
    Dec 14 09:18:08.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 12/14/22 09:18:08.727
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:08.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:08.742
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 12/14/22 09:18:08.751
    STEP: waiting for RC to be added 12/14/22 09:18:08.755
    STEP: waiting for available Replicas 12/14/22 09:18:08.755
    STEP: patching ReplicationController 12/14/22 09:18:10.366
    STEP: waiting for RC to be modified 12/14/22 09:18:10.374
    STEP: patching ReplicationController status 12/14/22 09:18:10.374
    STEP: waiting for RC to be modified 12/14/22 09:18:10.379
    STEP: waiting for available Replicas 12/14/22 09:18:10.379
    STEP: fetching ReplicationController status 12/14/22 09:18:10.381
    STEP: patching ReplicationController scale 12/14/22 09:18:10.385
    STEP: waiting for RC to be modified 12/14/22 09:18:10.389
    STEP: waiting for ReplicationController's scale to be the max amount 12/14/22 09:18:10.389
    STEP: fetching ReplicationController; ensuring that it's patched 12/14/22 09:18:11.365
    STEP: updating ReplicationController status 12/14/22 09:18:11.369
    STEP: waiting for RC to be modified 12/14/22 09:18:11.373
    STEP: listing all ReplicationControllers 12/14/22 09:18:11.374
    STEP: checking that ReplicationController has expected values 12/14/22 09:18:11.377
    STEP: deleting ReplicationControllers by collection 12/14/22 09:18:11.377
    STEP: waiting for ReplicationController to have a DELETED watchEvent 12/14/22 09:18:11.383
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 14 09:18:11.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3449" for this suite. 12/14/22 09:18:11.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:11.435
Dec 14 09:18:11.435: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange 12/14/22 09:18:11.436
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:11.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:11.45
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 12/14/22 09:18:11.455
STEP: Setting up watch 12/14/22 09:18:11.455
STEP: Submitting a LimitRange 12/14/22 09:18:11.561
STEP: Verifying LimitRange creation was observed 12/14/22 09:18:11.567
STEP: Fetching the LimitRange to ensure it has proper values 12/14/22 09:18:11.568
Dec 14 09:18:11.572: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 09:18:11.572: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 12/14/22 09:18:11.572
STEP: Ensuring Pod has resource requirements applied from LimitRange 12/14/22 09:18:11.581
Dec 14 09:18:11.585: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 09:18:11.585: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 12/14/22 09:18:11.585
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/14/22 09:18:11.592
Dec 14 09:18:11.596: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 14 09:18:11.596: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 12/14/22 09:18:11.596
STEP: Failing to create a Pod with more than max resources 12/14/22 09:18:11.601
STEP: Updating a LimitRange 12/14/22 09:18:11.608
STEP: Verifying LimitRange updating is effective 12/14/22 09:18:11.612
STEP: Creating a Pod with less than former min resources 12/14/22 09:18:13.617
STEP: Failing to create a Pod with more than max resources 12/14/22 09:18:13.626
STEP: Deleting a LimitRange 12/14/22 09:18:13.633
STEP: Verifying the LimitRange was deleted 12/14/22 09:18:13.637
Dec 14 09:18:18.642: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 12/14/22 09:18:18.642
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Dec 14 09:18:18.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1519" for this suite. 12/14/22 09:18:18.657
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":250,"skipped":4788,"failed":0}
------------------------------
• [7.226 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:11.435
    Dec 14 09:18:11.435: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename limitrange 12/14/22 09:18:11.436
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:11.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:11.45
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 12/14/22 09:18:11.455
    STEP: Setting up watch 12/14/22 09:18:11.455
    STEP: Submitting a LimitRange 12/14/22 09:18:11.561
    STEP: Verifying LimitRange creation was observed 12/14/22 09:18:11.567
    STEP: Fetching the LimitRange to ensure it has proper values 12/14/22 09:18:11.568
    Dec 14 09:18:11.572: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 14 09:18:11.572: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 12/14/22 09:18:11.572
    STEP: Ensuring Pod has resource requirements applied from LimitRange 12/14/22 09:18:11.581
    Dec 14 09:18:11.585: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 14 09:18:11.585: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 12/14/22 09:18:11.585
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/14/22 09:18:11.592
    Dec 14 09:18:11.596: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Dec 14 09:18:11.596: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 12/14/22 09:18:11.596
    STEP: Failing to create a Pod with more than max resources 12/14/22 09:18:11.601
    STEP: Updating a LimitRange 12/14/22 09:18:11.608
    STEP: Verifying LimitRange updating is effective 12/14/22 09:18:11.612
    STEP: Creating a Pod with less than former min resources 12/14/22 09:18:13.617
    STEP: Failing to create a Pod with more than max resources 12/14/22 09:18:13.626
    STEP: Deleting a LimitRange 12/14/22 09:18:13.633
    STEP: Verifying the LimitRange was deleted 12/14/22 09:18:13.637
    Dec 14 09:18:18.642: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 12/14/22 09:18:18.642
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Dec 14 09:18:18.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-1519" for this suite. 12/14/22 09:18:18.657
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:18.662
Dec 14 09:18:18.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:18:18.662
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:18.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:18.677
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-536af51d-9d14-40ac-bc73-e8a672d3b821 12/14/22 09:18:18.682
STEP: Creating a pod to test consume configMaps 12/14/22 09:18:18.686
Dec 14 09:18:18.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc" in namespace "configmap-4230" to be "Succeeded or Failed"
Dec 14 09:18:18.698: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498595ms
Dec 14 09:18:20.703: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008171175s
Dec 14 09:18:22.702: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007651209s
STEP: Saw pod success 12/14/22 09:18:22.702
Dec 14 09:18:22.702: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc" satisfied condition "Succeeded or Failed"
Dec 14 09:18:22.705: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:18:22.714
Dec 14 09:18:22.721: INFO: Waiting for pod pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc to disappear
Dec 14 09:18:22.724: INFO: Pod pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:18:22.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4230" for this suite. 12/14/22 09:18:22.729
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":4790,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:18.662
    Dec 14 09:18:18.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:18:18.662
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:18.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:18.677
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-536af51d-9d14-40ac-bc73-e8a672d3b821 12/14/22 09:18:18.682
    STEP: Creating a pod to test consume configMaps 12/14/22 09:18:18.686
    Dec 14 09:18:18.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc" in namespace "configmap-4230" to be "Succeeded or Failed"
    Dec 14 09:18:18.698: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498595ms
    Dec 14 09:18:20.703: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008171175s
    Dec 14 09:18:22.702: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007651209s
    STEP: Saw pod success 12/14/22 09:18:22.702
    Dec 14 09:18:22.702: INFO: Pod "pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc" satisfied condition "Succeeded or Failed"
    Dec 14 09:18:22.705: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:18:22.714
    Dec 14 09:18:22.721: INFO: Waiting for pod pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc to disappear
    Dec 14 09:18:22.724: INFO: Pod pod-configmaps-5bc7031e-986a-4b1d-a058-3570b7e4d5dc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:18:22.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4230" for this suite. 12/14/22 09:18:22.729
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:22.733
Dec 14 09:18:22.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 12/14/22 09:18:22.734
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:22.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:22.749
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 12/14/22 09:18:22.754
STEP: creating a new configmap 12/14/22 09:18:22.757
STEP: modifying the configmap once 12/14/22 09:18:22.761
STEP: closing the watch once it receives two notifications 12/14/22 09:18:22.769
Dec 14 09:18:22.769: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41136 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:18:22.769: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41137 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 12/14/22 09:18:22.769
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/14/22 09:18:22.776
STEP: deleting the configmap 12/14/22 09:18:22.778
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/14/22 09:18:22.786
Dec 14 09:18:22.786: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41138 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 09:18:22.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41139 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 14 09:18:22.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5197" for this suite. 12/14/22 09:18:22.791
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":252,"skipped":4791,"failed":0}
------------------------------
• [0.063 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:22.733
    Dec 14 09:18:22.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 12/14/22 09:18:22.734
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:22.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:22.749
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 12/14/22 09:18:22.754
    STEP: creating a new configmap 12/14/22 09:18:22.757
    STEP: modifying the configmap once 12/14/22 09:18:22.761
    STEP: closing the watch once it receives two notifications 12/14/22 09:18:22.769
    Dec 14 09:18:22.769: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41136 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:18:22.769: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41137 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 12/14/22 09:18:22.769
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/14/22 09:18:22.776
    STEP: deleting the configmap 12/14/22 09:18:22.778
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/14/22 09:18:22.786
    Dec 14 09:18:22.786: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41138 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 09:18:22.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5197  ca48735c-105e-4730-b2a3-bf4e375c55b3 41139 0 2022-12-14 09:18:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 09:18:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 14 09:18:22.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5197" for this suite. 12/14/22 09:18:22.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:22.796
Dec 14 09:18:22.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 09:18:22.797
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:22.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:22.814
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Dec 14 09:18:22.818: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod 12/14/22 09:18:22.819
STEP: submitting the pod to kubernetes 12/14/22 09:18:22.819
Dec 14 09:18:22.828: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a" in namespace "pods-3970" to be "running and ready"
Dec 14 09:18:22.831: INFO: Pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279749ms
Dec 14 09:18:22.831: INFO: The phase of Pod pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:18:24.836: INFO: Pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007965766s
Dec 14 09:18:24.836: INFO: The phase of Pod pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a is Running (Ready = true)
Dec 14 09:18:24.836: INFO: Pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 09:18:24.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3970" for this suite. 12/14/22 09:18:24.988
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":253,"skipped":4803,"failed":0}
------------------------------
• [2.197 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:22.796
    Dec 14 09:18:22.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 09:18:22.797
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:22.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:22.814
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Dec 14 09:18:22.818: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating the pod 12/14/22 09:18:22.819
    STEP: submitting the pod to kubernetes 12/14/22 09:18:22.819
    Dec 14 09:18:22.828: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a" in namespace "pods-3970" to be "running and ready"
    Dec 14 09:18:22.831: INFO: Pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279749ms
    Dec 14 09:18:22.831: INFO: The phase of Pod pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:18:24.836: INFO: Pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007965766s
    Dec 14 09:18:24.836: INFO: The phase of Pod pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a is Running (Ready = true)
    Dec 14 09:18:24.836: INFO: Pod "pod-exec-websocket-7cb8c229-3cb8-4c92-b698-b9836f84002a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 09:18:24.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3970" for this suite. 12/14/22 09:18:24.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:24.994
Dec 14 09:18:24.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:18:24.995
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:25.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:25.011
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:18:25.015
Dec 14 09:18:25.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f" in namespace "downward-api-8464" to be "Succeeded or Failed"
Dec 14 09:18:25.029: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611464ms
Dec 14 09:18:27.034: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009554509s
Dec 14 09:18:29.036: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010844931s
STEP: Saw pod success 12/14/22 09:18:29.036
Dec 14 09:18:29.036: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f" satisfied condition "Succeeded or Failed"
Dec 14 09:18:29.039: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f container client-container: <nil>
STEP: delete the pod 12/14/22 09:18:29.05
Dec 14 09:18:29.058: INFO: Waiting for pod downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f to disappear
Dec 14 09:18:29.062: INFO: Pod downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 09:18:29.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8464" for this suite. 12/14/22 09:18:29.068
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":254,"skipped":4816,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:24.994
    Dec 14 09:18:24.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:18:24.995
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:25.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:25.011
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:18:25.015
    Dec 14 09:18:25.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f" in namespace "downward-api-8464" to be "Succeeded or Failed"
    Dec 14 09:18:25.029: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611464ms
    Dec 14 09:18:27.034: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009554509s
    Dec 14 09:18:29.036: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010844931s
    STEP: Saw pod success 12/14/22 09:18:29.036
    Dec 14 09:18:29.036: INFO: Pod "downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f" satisfied condition "Succeeded or Failed"
    Dec 14 09:18:29.039: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f container client-container: <nil>
    STEP: delete the pod 12/14/22 09:18:29.05
    Dec 14 09:18:29.058: INFO: Waiting for pod downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f to disappear
    Dec 14 09:18:29.062: INFO: Pod downwardapi-volume-d910900b-81eb-4807-baa9-371f5a81250f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 09:18:29.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8464" for this suite. 12/14/22 09:18:29.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:29.072
Dec 14 09:18:29.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:18:29.073
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:29.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:29.089
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:18:29.102
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:18:29.516
STEP: Deploying the webhook pod 12/14/22 09:18:29.521
STEP: Wait for the deployment to be ready 12/14/22 09:18:29.532
Dec 14 09:18:29.539: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:18:31.551
STEP: Verifying the service has paired with the endpoint 12/14/22 09:18:31.561
Dec 14 09:18:32.562: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 12/14/22 09:18:32.626
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:18:32.679
STEP: Deleting the collection of validation webhooks 12/14/22 09:18:32.744
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:18:32.77
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:18:32.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1734" for this suite. 12/14/22 09:18:32.786
STEP: Destroying namespace "webhook-1734-markers" for this suite. 12/14/22 09:18:32.79
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":255,"skipped":4839,"failed":0}
------------------------------
• [3.746 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:29.072
    Dec 14 09:18:29.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:18:29.073
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:29.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:29.089
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:18:29.102
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:18:29.516
    STEP: Deploying the webhook pod 12/14/22 09:18:29.521
    STEP: Wait for the deployment to be ready 12/14/22 09:18:29.532
    Dec 14 09:18:29.539: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:18:31.551
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:18:31.561
    Dec 14 09:18:32.562: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 12/14/22 09:18:32.626
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:18:32.679
    STEP: Deleting the collection of validation webhooks 12/14/22 09:18:32.744
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:18:32.77
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:18:32.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1734" for this suite. 12/14/22 09:18:32.786
    STEP: Destroying namespace "webhook-1734-markers" for this suite. 12/14/22 09:18:32.79
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:18:32.819
Dec 14 09:18:32.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:18:32.819
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:32.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:32.834
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 09:18:32.851: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 09:19:32.893: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 12/14/22 09:19:32.896
Dec 14 09:19:32.920: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 14 09:19:32.928: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 14 09:19:32.947: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 14 09:19:32.956: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/14/22 09:19:32.956
Dec 14 09:19:32.956: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5947" to be "running"
Dec 14 09:19:32.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544329ms
Dec 14 09:19:34.965: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009127192s
Dec 14 09:19:36.965: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0094202s
Dec 14 09:19:38.966: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.010115762s
Dec 14 09:19:38.966: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 14 09:19:38.966: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5947" to be "running"
Dec 14 09:19:38.970: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.631718ms
Dec 14 09:19:38.970: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 09:19:38.970: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5947" to be "running"
Dec 14 09:19:38.973: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.36692ms
Dec 14 09:19:38.973: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 09:19:38.973: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5947" to be "running"
Dec 14 09:19:38.977: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.462225ms
Dec 14 09:19:38.977: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 12/14/22 09:19:38.977
Dec 14 09:19:38.994: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Dec 14 09:19:38.998: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189183ms
Dec 14 09:19:41.004: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009263269s
Dec 14 09:19:43.005: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011186369s
Dec 14 09:19:43.005: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:19:43.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5947" for this suite. 12/14/22 09:19:43.036
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":256,"skipped":4839,"failed":0}
------------------------------
• [70.257 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:18:32.819
    Dec 14 09:18:32.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:18:32.819
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:18:32.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:18:32.834
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 14 09:18:32.851: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 09:19:32.893: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 12/14/22 09:19:32.896
    Dec 14 09:19:32.920: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 14 09:19:32.928: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 14 09:19:32.947: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 14 09:19:32.956: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/14/22 09:19:32.956
    Dec 14 09:19:32.956: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5947" to be "running"
    Dec 14 09:19:32.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544329ms
    Dec 14 09:19:34.965: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009127192s
    Dec 14 09:19:36.965: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0094202s
    Dec 14 09:19:38.966: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.010115762s
    Dec 14 09:19:38.966: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 14 09:19:38.966: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5947" to be "running"
    Dec 14 09:19:38.970: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.631718ms
    Dec 14 09:19:38.970: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 09:19:38.970: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5947" to be "running"
    Dec 14 09:19:38.973: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.36692ms
    Dec 14 09:19:38.973: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 09:19:38.973: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5947" to be "running"
    Dec 14 09:19:38.977: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.462225ms
    Dec 14 09:19:38.977: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 12/14/22 09:19:38.977
    Dec 14 09:19:38.994: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Dec 14 09:19:38.998: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189183ms
    Dec 14 09:19:41.004: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009263269s
    Dec 14 09:19:43.005: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011186369s
    Dec 14 09:19:43.005: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:19:43.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5947" for this suite. 12/14/22 09:19:43.036
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:19:43.076
Dec 14 09:19:43.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/14/22 09:19:43.076
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:43.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:19:43.1
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 12/14/22 09:19:43.105
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:43.118
STEP: Creating a service in the namespace 12/14/22 09:19:43.123
STEP: Deleting the namespace 12/14/22 09:19:43.132
STEP: Waiting for the namespace to be removed. 12/14/22 09:19:43.136
STEP: Recreating the namespace 12/14/22 09:19:49.14
STEP: Verifying there is no service in the namespace 12/14/22 09:19:49.15
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:19:49.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9651" for this suite. 12/14/22 09:19:49.159
STEP: Destroying namespace "nsdeletetest-3820" for this suite. 12/14/22 09:19:49.164
Dec 14 09:19:49.167: INFO: Namespace nsdeletetest-3820 was already deleted
STEP: Destroying namespace "nsdeletetest-3109" for this suite. 12/14/22 09:19:49.167
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":257,"skipped":4839,"failed":0}
------------------------------
• [6.096 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:19:43.076
    Dec 14 09:19:43.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/14/22 09:19:43.076
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:43.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:19:43.1
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 12/14/22 09:19:43.105
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:43.118
    STEP: Creating a service in the namespace 12/14/22 09:19:43.123
    STEP: Deleting the namespace 12/14/22 09:19:43.132
    STEP: Waiting for the namespace to be removed. 12/14/22 09:19:43.136
    STEP: Recreating the namespace 12/14/22 09:19:49.14
    STEP: Verifying there is no service in the namespace 12/14/22 09:19:49.15
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:19:49.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9651" for this suite. 12/14/22 09:19:49.159
    STEP: Destroying namespace "nsdeletetest-3820" for this suite. 12/14/22 09:19:49.164
    Dec 14 09:19:49.167: INFO: Namespace nsdeletetest-3820 was already deleted
    STEP: Destroying namespace "nsdeletetest-3109" for this suite. 12/14/22 09:19:49.167
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:19:49.172
Dec 14 09:19:49.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 09:19:49.172
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:49.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:19:49.196
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Dec 14 09:19:49.233: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e6cd53f8-265c-430d-b600-9a8dfed736cb", Controller:(*bool)(0xc00370aae6), BlockOwnerDeletion:(*bool)(0xc00370aae7)}}
Dec 14 09:19:49.238: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8fe1ec70-4884-4ad3-84ae-1a5604fd0654", Controller:(*bool)(0xc00540412a), BlockOwnerDeletion:(*bool)(0xc00540412b)}}
Dec 14 09:19:49.244: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6628a3d7-72ee-42f3-8a09-ea0782db39d5", Controller:(*bool)(0xc0054043de), BlockOwnerDeletion:(*bool)(0xc0054043df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 09:19:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2911" for this suite. 12/14/22 09:19:54.272
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":258,"skipped":4843,"failed":0}
------------------------------
• [5.105 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:19:49.172
    Dec 14 09:19:49.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 09:19:49.172
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:49.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:19:49.196
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Dec 14 09:19:49.233: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e6cd53f8-265c-430d-b600-9a8dfed736cb", Controller:(*bool)(0xc00370aae6), BlockOwnerDeletion:(*bool)(0xc00370aae7)}}
    Dec 14 09:19:49.238: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8fe1ec70-4884-4ad3-84ae-1a5604fd0654", Controller:(*bool)(0xc00540412a), BlockOwnerDeletion:(*bool)(0xc00540412b)}}
    Dec 14 09:19:49.244: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6628a3d7-72ee-42f3-8a09-ea0782db39d5", Controller:(*bool)(0xc0054043de), BlockOwnerDeletion:(*bool)(0xc0054043df)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 09:19:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2911" for this suite. 12/14/22 09:19:54.272
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:19:54.277
Dec 14 09:19:54.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 12/14/22 09:19:54.278
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:54.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:19:54.293
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 09:19:54.298
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-ggg9 12/14/22 09:19:54.305
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:19:54.305
Dec 14 09:19:54.314: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ggg9" in namespace "subpath-742" to be "Succeeded or Failed"
Dec 14 09:19:54.318: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257048ms
Dec 14 09:19:56.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008345515s
Dec 14 09:19:58.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 4.008085807s
Dec 14 09:20:00.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009001788s
Dec 14 09:20:02.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009290646s
Dec 14 09:20:04.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009466459s
Dec 14 09:20:06.398: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 12.083645206s
Dec 14 09:20:08.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 14.008405055s
Dec 14 09:20:10.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 16.009160647s
Dec 14 09:20:12.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 18.009236909s
Dec 14 09:20:14.325: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 20.01018588s
Dec 14 09:20:16.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=false. Elapsed: 22.009739404s
Dec 14 09:20:18.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008241097s
STEP: Saw pod success 12/14/22 09:20:18.323
Dec 14 09:20:18.323: INFO: Pod "pod-subpath-test-projected-ggg9" satisfied condition "Succeeded or Failed"
Dec 14 09:20:18.327: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-projected-ggg9 container test-container-subpath-projected-ggg9: <nil>
STEP: delete the pod 12/14/22 09:20:18.371
Dec 14 09:20:18.380: INFO: Waiting for pod pod-subpath-test-projected-ggg9 to disappear
Dec 14 09:20:18.384: INFO: Pod pod-subpath-test-projected-ggg9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-ggg9 12/14/22 09:20:18.384
Dec 14 09:20:18.384: INFO: Deleting pod "pod-subpath-test-projected-ggg9" in namespace "subpath-742"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 14 09:20:18.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-742" for this suite. 12/14/22 09:20:18.394
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":259,"skipped":4844,"failed":0}
------------------------------
• [24.122 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:19:54.277
    Dec 14 09:19:54.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 12/14/22 09:19:54.278
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:19:54.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:19:54.293
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 09:19:54.298
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-ggg9 12/14/22 09:19:54.305
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:19:54.305
    Dec 14 09:19:54.314: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ggg9" in namespace "subpath-742" to be "Succeeded or Failed"
    Dec 14 09:19:54.318: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257048ms
    Dec 14 09:19:56.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008345515s
    Dec 14 09:19:58.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 4.008085807s
    Dec 14 09:20:00.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009001788s
    Dec 14 09:20:02.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009290646s
    Dec 14 09:20:04.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009466459s
    Dec 14 09:20:06.398: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 12.083645206s
    Dec 14 09:20:08.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 14.008405055s
    Dec 14 09:20:10.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 16.009160647s
    Dec 14 09:20:12.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 18.009236909s
    Dec 14 09:20:14.325: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=true. Elapsed: 20.01018588s
    Dec 14 09:20:16.324: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Running", Reason="", readiness=false. Elapsed: 22.009739404s
    Dec 14 09:20:18.323: INFO: Pod "pod-subpath-test-projected-ggg9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008241097s
    STEP: Saw pod success 12/14/22 09:20:18.323
    Dec 14 09:20:18.323: INFO: Pod "pod-subpath-test-projected-ggg9" satisfied condition "Succeeded or Failed"
    Dec 14 09:20:18.327: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-projected-ggg9 container test-container-subpath-projected-ggg9: <nil>
    STEP: delete the pod 12/14/22 09:20:18.371
    Dec 14 09:20:18.380: INFO: Waiting for pod pod-subpath-test-projected-ggg9 to disappear
    Dec 14 09:20:18.384: INFO: Pod pod-subpath-test-projected-ggg9 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-ggg9 12/14/22 09:20:18.384
    Dec 14 09:20:18.384: INFO: Deleting pod "pod-subpath-test-projected-ggg9" in namespace "subpath-742"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 14 09:20:18.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-742" for this suite. 12/14/22 09:20:18.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:20:18.4
Dec 14 09:20:18.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:20:18.4
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:20:18.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:20:18.418
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:20:18.423
Dec 14 09:20:18.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608" in namespace "downward-api-2177" to be "Succeeded or Failed"
Dec 14 09:20:18.438: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15825ms
Dec 14 09:20:20.444: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010739055s
Dec 14 09:20:22.443: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010405728s
STEP: Saw pod success 12/14/22 09:20:22.443
Dec 14 09:20:22.443: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608" satisfied condition "Succeeded or Failed"
Dec 14 09:20:22.447: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608 container client-container: <nil>
STEP: delete the pod 12/14/22 09:20:22.457
Dec 14 09:20:22.465: INFO: Waiting for pod downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608 to disappear
Dec 14 09:20:22.468: INFO: Pod downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 09:20:22.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2177" for this suite. 12/14/22 09:20:22.474
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":260,"skipped":4867,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:20:18.4
    Dec 14 09:20:18.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:20:18.4
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:20:18.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:20:18.418
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:20:18.423
    Dec 14 09:20:18.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608" in namespace "downward-api-2177" to be "Succeeded or Failed"
    Dec 14 09:20:18.438: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15825ms
    Dec 14 09:20:20.444: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010739055s
    Dec 14 09:20:22.443: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010405728s
    STEP: Saw pod success 12/14/22 09:20:22.443
    Dec 14 09:20:22.443: INFO: Pod "downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608" satisfied condition "Succeeded or Failed"
    Dec 14 09:20:22.447: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:20:22.457
    Dec 14 09:20:22.465: INFO: Waiting for pod downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608 to disappear
    Dec 14 09:20:22.468: INFO: Pod downwardapi-volume-8c6e4011-df54-46bf-b04e-d915b56fe608 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 09:20:22.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2177" for this suite. 12/14/22 09:20:22.474
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:20:22.479
Dec 14 09:20:22.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 12/14/22 09:20:22.48
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:20:22.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:20:22.494
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 12/14/22 09:20:22.499
STEP: Ensuring a job is scheduled 12/14/22 09:20:22.504
STEP: Ensuring exactly one is scheduled 12/14/22 09:21:00.511
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 09:21:00.515
STEP: Ensuring the job is replaced with a new one 12/14/22 09:21:00.518
STEP: Removing cronjob 12/14/22 09:22:00.523
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 14 09:22:00.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8329" for this suite. 12/14/22 09:22:00.536
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":261,"skipped":4869,"failed":0}
------------------------------
• [98.063 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:20:22.479
    Dec 14 09:20:22.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 12/14/22 09:20:22.48
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:20:22.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:20:22.494
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 12/14/22 09:20:22.499
    STEP: Ensuring a job is scheduled 12/14/22 09:20:22.504
    STEP: Ensuring exactly one is scheduled 12/14/22 09:21:00.511
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 09:21:00.515
    STEP: Ensuring the job is replaced with a new one 12/14/22 09:21:00.518
    STEP: Removing cronjob 12/14/22 09:22:00.523
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 14 09:22:00.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8329" for this suite. 12/14/22 09:22:00.536
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:00.542
Dec 14 09:22:00.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:22:00.543
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:00.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:00.562
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:22:00.567
Dec 14 09:22:00.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe" in namespace "projected-848" to be "Succeeded or Failed"
Dec 14 09:22:00.582: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.924435ms
Dec 14 09:22:02.588: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010811489s
Dec 14 09:22:04.587: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009321967s
STEP: Saw pod success 12/14/22 09:22:04.587
Dec 14 09:22:04.587: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe" satisfied condition "Succeeded or Failed"
Dec 14 09:22:04.590: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe container client-container: <nil>
STEP: delete the pod 12/14/22 09:22:04.599
Dec 14 09:22:04.605: INFO: Waiting for pod downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe to disappear
Dec 14 09:22:04.608: INFO: Pod downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:22:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-848" for this suite. 12/14/22 09:22:04.619
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":262,"skipped":4869,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:00.542
    Dec 14 09:22:00.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:22:00.543
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:00.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:00.562
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:22:00.567
    Dec 14 09:22:00.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe" in namespace "projected-848" to be "Succeeded or Failed"
    Dec 14 09:22:00.582: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.924435ms
    Dec 14 09:22:02.588: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010811489s
    Dec 14 09:22:04.587: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009321967s
    STEP: Saw pod success 12/14/22 09:22:04.587
    Dec 14 09:22:04.587: INFO: Pod "downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe" satisfied condition "Succeeded or Failed"
    Dec 14 09:22:04.590: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe container client-container: <nil>
    STEP: delete the pod 12/14/22 09:22:04.599
    Dec 14 09:22:04.605: INFO: Waiting for pod downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe to disappear
    Dec 14 09:22:04.608: INFO: Pod downwardapi-volume-0d5c403b-1424-42e4-8d2f-add01fd8ebbe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:22:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-848" for this suite. 12/14/22 09:22:04.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:04.625
Dec 14 09:22:04.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook 12/14/22 09:22:04.626
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:04.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:04.642
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/14/22 09:22:04.647
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 09:22:04.956
STEP: Deploying the custom resource conversion webhook pod 12/14/22 09:22:04.962
STEP: Wait for the deployment to be ready 12/14/22 09:22:04.97
Dec 14 09:22:04.977: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:22:06.991
STEP: Verifying the service has paired with the endpoint 12/14/22 09:22:07.006
Dec 14 09:22:08.006: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Dec 14 09:22:08.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource 12/14/22 09:22:10.822
STEP: Create a v2 custom resource 12/14/22 09:22:10.836
STEP: List CRs in v1 12/14/22 09:22:10.891
STEP: List CRs in v2 12/14/22 09:22:10.9
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:22:11.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9541" for this suite. 12/14/22 09:22:11.47
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":263,"skipped":4875,"failed":0}
------------------------------
• [6.875 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:04.625
    Dec 14 09:22:04.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-webhook 12/14/22 09:22:04.626
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:04.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:04.642
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/14/22 09:22:04.647
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 09:22:04.956
    STEP: Deploying the custom resource conversion webhook pod 12/14/22 09:22:04.962
    STEP: Wait for the deployment to be ready 12/14/22 09:22:04.97
    Dec 14 09:22:04.977: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:22:06.991
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:22:07.006
    Dec 14 09:22:08.006: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Dec 14 09:22:08.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating a v1 custom resource 12/14/22 09:22:10.822
    STEP: Create a v2 custom resource 12/14/22 09:22:10.836
    STEP: List CRs in v1 12/14/22 09:22:10.891
    STEP: List CRs in v2 12/14/22 09:22:10.9
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:22:11.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-9541" for this suite. 12/14/22 09:22:11.47
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:11.501
Dec 14 09:22:11.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 12/14/22 09:22:11.502
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:11.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:11.517
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 12/14/22 09:22:11.522
Dec 14 09:22:11.532: INFO: Waiting up to 5m0s for pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd" in namespace "containers-5934" to be "Succeeded or Failed"
Dec 14 09:22:11.535: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708196ms
Dec 14 09:22:13.540: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008422157s
Dec 14 09:22:15.541: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008997814s
STEP: Saw pod success 12/14/22 09:22:15.541
Dec 14 09:22:15.541: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd" satisfied condition "Succeeded or Failed"
Dec 14 09:22:15.545: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:22:15.558
Dec 14 09:22:15.566: INFO: Waiting for pod client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd to disappear
Dec 14 09:22:15.569: INFO: Pod client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 14 09:22:15.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5934" for this suite. 12/14/22 09:22:15.575
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":264,"skipped":4910,"failed":0}
------------------------------
• [4.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:11.501
    Dec 14 09:22:11.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 12/14/22 09:22:11.502
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:11.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:11.517
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 12/14/22 09:22:11.522
    Dec 14 09:22:11.532: INFO: Waiting up to 5m0s for pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd" in namespace "containers-5934" to be "Succeeded or Failed"
    Dec 14 09:22:11.535: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708196ms
    Dec 14 09:22:13.540: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008422157s
    Dec 14 09:22:15.541: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008997814s
    STEP: Saw pod success 12/14/22 09:22:15.541
    Dec 14 09:22:15.541: INFO: Pod "client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd" satisfied condition "Succeeded or Failed"
    Dec 14 09:22:15.545: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:22:15.558
    Dec 14 09:22:15.566: INFO: Waiting for pod client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd to disappear
    Dec 14 09:22:15.569: INFO: Pod client-containers-637397bb-ef79-465c-84e0-444dd2d5c6fd no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 14 09:22:15.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5934" for this suite. 12/14/22 09:22:15.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:15.581
Dec 14 09:22:15.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress 12/14/22 09:22:15.582
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:15.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:15.597
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 12/14/22 09:22:15.602
STEP: getting /apis/networking.k8s.io 12/14/22 09:22:15.609
STEP: getting /apis/networking.k8s.iov1 12/14/22 09:22:15.611
STEP: creating 12/14/22 09:22:15.613
STEP: getting 12/14/22 09:22:15.626
STEP: listing 12/14/22 09:22:15.629
STEP: watching 12/14/22 09:22:15.633
Dec 14 09:22:15.633: INFO: starting watch
STEP: cluster-wide listing 12/14/22 09:22:15.635
STEP: cluster-wide watching 12/14/22 09:22:15.639
Dec 14 09:22:15.639: INFO: starting watch
STEP: patching 12/14/22 09:22:15.642
STEP: updating 12/14/22 09:22:15.646
Dec 14 09:22:15.654: INFO: waiting for watch events with expected annotations
Dec 14 09:22:15.655: INFO: saw patched and updated annotations
STEP: patching /status 12/14/22 09:22:15.655
STEP: updating /status 12/14/22 09:22:15.659
STEP: get /status 12/14/22 09:22:15.666
STEP: deleting 12/14/22 09:22:15.669
STEP: deleting a collection 12/14/22 09:22:15.678
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Dec 14 09:22:15.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8088" for this suite. 12/14/22 09:22:15.693
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":265,"skipped":4931,"failed":0}
------------------------------
• [0.116 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:15.581
    Dec 14 09:22:15.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ingress 12/14/22 09:22:15.582
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:15.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:15.597
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 12/14/22 09:22:15.602
    STEP: getting /apis/networking.k8s.io 12/14/22 09:22:15.609
    STEP: getting /apis/networking.k8s.iov1 12/14/22 09:22:15.611
    STEP: creating 12/14/22 09:22:15.613
    STEP: getting 12/14/22 09:22:15.626
    STEP: listing 12/14/22 09:22:15.629
    STEP: watching 12/14/22 09:22:15.633
    Dec 14 09:22:15.633: INFO: starting watch
    STEP: cluster-wide listing 12/14/22 09:22:15.635
    STEP: cluster-wide watching 12/14/22 09:22:15.639
    Dec 14 09:22:15.639: INFO: starting watch
    STEP: patching 12/14/22 09:22:15.642
    STEP: updating 12/14/22 09:22:15.646
    Dec 14 09:22:15.654: INFO: waiting for watch events with expected annotations
    Dec 14 09:22:15.655: INFO: saw patched and updated annotations
    STEP: patching /status 12/14/22 09:22:15.655
    STEP: updating /status 12/14/22 09:22:15.659
    STEP: get /status 12/14/22 09:22:15.666
    STEP: deleting 12/14/22 09:22:15.669
    STEP: deleting a collection 12/14/22 09:22:15.678
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Dec 14 09:22:15.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-8088" for this suite. 12/14/22 09:22:15.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:15.697
Dec 14 09:22:15.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:22:15.698
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:15.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:15.713
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:22:15.717
Dec 14 09:22:15.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c" in namespace "projected-9929" to be "Succeeded or Failed"
Dec 14 09:22:15.731: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917925ms
Dec 14 09:22:17.736: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009589505s
Dec 14 09:22:19.736: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008991175s
STEP: Saw pod success 12/14/22 09:22:19.736
Dec 14 09:22:19.736: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c" satisfied condition "Succeeded or Failed"
Dec 14 09:22:19.739: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c container client-container: <nil>
STEP: delete the pod 12/14/22 09:22:19.76
Dec 14 09:22:19.769: INFO: Waiting for pod downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c to disappear
Dec 14 09:22:19.771: INFO: Pod downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:22:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9929" for this suite. 12/14/22 09:22:19.777
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":266,"skipped":4952,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:15.697
    Dec 14 09:22:15.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:22:15.698
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:15.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:15.713
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:22:15.717
    Dec 14 09:22:15.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c" in namespace "projected-9929" to be "Succeeded or Failed"
    Dec 14 09:22:15.731: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917925ms
    Dec 14 09:22:17.736: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009589505s
    Dec 14 09:22:19.736: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008991175s
    STEP: Saw pod success 12/14/22 09:22:19.736
    Dec 14 09:22:19.736: INFO: Pod "downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c" satisfied condition "Succeeded or Failed"
    Dec 14 09:22:19.739: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c container client-container: <nil>
    STEP: delete the pod 12/14/22 09:22:19.76
    Dec 14 09:22:19.769: INFO: Waiting for pod downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c to disappear
    Dec 14 09:22:19.771: INFO: Pod downwardapi-volume-f456b394-4779-469d-bef2-bd26a4af846c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:22:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9929" for this suite. 12/14/22 09:22:19.777
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:19.781
Dec 14 09:22:19.781: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl 12/14/22 09:22:19.782
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:19.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:19.797
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/14/22 09:22:19.802
STEP: Watching for error events or started pod 12/14/22 09:22:19.811
STEP: Waiting for pod completion 12/14/22 09:22:21.816
Dec 14 09:22:21.816: INFO: Waiting up to 3m0s for pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d" in namespace "sysctl-6676" to be "completed"
Dec 14 09:22:21.820: INFO: Pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908545ms
Dec 14 09:22:23.824: INFO: Pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008719932s
Dec 14 09:22:23.824: INFO: Pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d" satisfied condition "completed"
STEP: Checking that the pod succeeded 12/14/22 09:22:23.828
STEP: Getting logs from the pod 12/14/22 09:22:23.828
STEP: Checking that the sysctl is actually updated 12/14/22 09:22:23.837
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 09:22:23.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6676" for this suite. 12/14/22 09:22:23.842
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":267,"skipped":4953,"failed":0}
------------------------------
• [4.066 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:19.781
    Dec 14 09:22:19.781: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sysctl 12/14/22 09:22:19.782
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:19.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:19.797
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/14/22 09:22:19.802
    STEP: Watching for error events or started pod 12/14/22 09:22:19.811
    STEP: Waiting for pod completion 12/14/22 09:22:21.816
    Dec 14 09:22:21.816: INFO: Waiting up to 3m0s for pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d" in namespace "sysctl-6676" to be "completed"
    Dec 14 09:22:21.820: INFO: Pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908545ms
    Dec 14 09:22:23.824: INFO: Pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008719932s
    Dec 14 09:22:23.824: INFO: Pod "sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d" satisfied condition "completed"
    STEP: Checking that the pod succeeded 12/14/22 09:22:23.828
    STEP: Getting logs from the pod 12/14/22 09:22:23.828
    STEP: Checking that the sysctl is actually updated 12/14/22 09:22:23.837
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 09:22:23.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-6676" for this suite. 12/14/22 09:22:23.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:23.848
Dec 14 09:22:23.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:22:23.848
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:23.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:23.863
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:22:23.868
Dec 14 09:22:23.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4" in namespace "downward-api-6995" to be "Succeeded or Failed"
Dec 14 09:22:23.880: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853988ms
Dec 14 09:22:25.886: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008321756s
Dec 14 09:22:27.886: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00830748s
STEP: Saw pod success 12/14/22 09:22:27.886
Dec 14 09:22:27.886: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4" satisfied condition "Succeeded or Failed"
Dec 14 09:22:27.890: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4 container client-container: <nil>
STEP: delete the pod 12/14/22 09:22:27.899
Dec 14 09:22:27.907: INFO: Waiting for pod downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4 to disappear
Dec 14 09:22:27.911: INFO: Pod downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 09:22:27.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6995" for this suite. 12/14/22 09:22:27.917
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":268,"skipped":4982,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:23.848
    Dec 14 09:22:23.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:22:23.848
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:23.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:23.863
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:22:23.868
    Dec 14 09:22:23.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4" in namespace "downward-api-6995" to be "Succeeded or Failed"
    Dec 14 09:22:23.880: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853988ms
    Dec 14 09:22:25.886: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008321756s
    Dec 14 09:22:27.886: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00830748s
    STEP: Saw pod success 12/14/22 09:22:27.886
    Dec 14 09:22:27.886: INFO: Pod "downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4" satisfied condition "Succeeded or Failed"
    Dec 14 09:22:27.890: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:22:27.899
    Dec 14 09:22:27.907: INFO: Waiting for pod downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4 to disappear
    Dec 14 09:22:27.911: INFO: Pod downwardapi-volume-161b5592-7d39-4282-91e5-e659c36557b4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 09:22:27.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6995" for this suite. 12/14/22 09:22:27.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:27.922
Dec 14 09:22:27.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/14/22 09:22:27.923
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:27.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:27.937
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 09:22:27.943: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:22:27.950: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:22:27.953: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
Dec 14 09:22:27.962: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 14 09:22:27.962: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec 14 09:22:27.962: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container proxy ready: true, restart count 0
Dec 14 09:22:27.962: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 09:22:27.962: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 09:22:27.962: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:22:27.962: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container calico-typha ready: true, restart count 0
Dec 14 09:22:27.962: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:22:27.962: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:22:27.962: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:22:27.962: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:22:27.962: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 09:22:27.962: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 09:22:27.962: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 09:22:27.962: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 09:22:27.962: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 09:22:27.962: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 09:22:27.962: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 09:22:27.962: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 09:22:27.962: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 09:22:27.962: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 09:22:27.962: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 09:22:27.962: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec 14 09:22:27.962: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 14 09:22:27.962: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.962: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:22:27.962: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
Dec 14 09:22:27.973: INFO: replace-27850161-6n877 from cronjob-8329 started at 2022-12-14 09:21:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container c ready: true, restart count 0
Dec 14 09:22:27.973: INFO: replace-27850162-zpgdj from cronjob-8329 started at 2022-12-14 09:22:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container c ready: true, restart count 0
Dec 14 09:22:27.973: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container proxy ready: true, restart count 0
Dec 14 09:22:27.973: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 09:22:27.973: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 09:22:27.973: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 09:22:27.973: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 09:22:27.973: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 09:22:27.973: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 09:22:27.973: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 09:22:27.973: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 09:22:27.973: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 09:22:27.973: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 09:22:27.973: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container metrics-server ready: true, restart count 2
Dec 14 09:22:27.973: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container metrics-server ready: true, restart count 1
Dec 14 09:22:27.973: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 09:22:27.973: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 09:22:27.973: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 09:22:27.973: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 09:22:27.973: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 09:22:27.973: INFO: sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d from sysctl-6676 started at 2022-12-14 09:22:19 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:27.973: INFO: 	Container test-container ready: false, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f 12/14/22 09:22:27.996
STEP: verifying the node has the label node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 09:22:28.011
Dec 14 09:22:28.026: INFO: Pod replace-27850161-6n877 requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod replace-27850162-zpgdj requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod addons-nginx-ingress-controller-7fbf48c6b5-rq6ld requesting resource cpu=100m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod apiserver-proxy-2dcbg requesting resource cpu=40m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod apiserver-proxy-7qqrm requesting resource cpu=40m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod blackbox-exporter-59447f4c55-6d48x requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod blackbox-exporter-59447f4c55-rmcv9 requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod calico-node-2vlmc requesting resource cpu=250m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod calico-node-9kgrw requesting resource cpu=250m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod calico-node-vertical-autoscaler-6597dd8998-xkg8r requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod calico-typha-deploy-65c54d4db6-mvhhf requesting resource cpu=320m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod calico-typha-vertical-autoscaler-84df655c88-9gbdh requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod coredns-7869797f4c-ffpdz requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod coredns-7869797f4c-ftxgn requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod csi-driver-node-gnhxw requesting resource cpu=37m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod csi-driver-node-vxc87 requesting resource cpu=37m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod egress-filter-applier-q4n7q requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod egress-filter-applier-zzlmq requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod kube-proxy-worker-1-v1.25.4-fgmr4 requesting resource cpu=34m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod kube-proxy-worker-1-v1.25.4-v2k9w requesting resource cpu=34m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod metrics-server-fd6cb96fd-b99cv requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod metrics-server-fd6cb96fd-dn96c requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod network-problem-detector-host-5sqjh requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod network-problem-detector-host-74zdg requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod network-problem-detector-pod-cqd4k requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod network-problem-detector-pod-mjtpq requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod node-exporter-5rbx7 requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod node-exporter-vpls6 requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod node-local-dns-92tld requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod node-local-dns-dhx6j requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod node-problem-detector-6xk8z requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.026: INFO: Pod node-problem-detector-jh8v5 requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod vpn-shoot-5b855f7f89-mxn54 requesting resource cpu=100m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod dashboard-metrics-scraper-6d54964d4b-cgnx2 requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.026: INFO: Pod kubernetes-dashboard-7b56c57b5d-hqhsz requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
STEP: Starting Pods to consume most of the cluster CPU. 12/14/22 09:22:28.026
Dec 14 09:22:28.026: INFO: Creating a pod which consumes cpu=501m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
Dec 14 09:22:28.036: INFO: Creating a pod which consumes cpu=907m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
Dec 14 09:22:28.042: INFO: Waiting up to 5m0s for pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2" in namespace "sched-pred-743" to be "running"
Dec 14 09:22:28.046: INFO: Pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250599ms
Dec 14 09:22:30.050: INFO: Pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007635673s
Dec 14 09:22:30.050: INFO: Pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2" satisfied condition "running"
Dec 14 09:22:30.050: INFO: Waiting up to 5m0s for pod "filler-pod-bcb64295-db65-4773-9643-c517669ac4a6" in namespace "sched-pred-743" to be "running"
Dec 14 09:22:30.053: INFO: Pod "filler-pod-bcb64295-db65-4773-9643-c517669ac4a6": Phase="Running", Reason="", readiness=true. Elapsed: 3.215412ms
Dec 14 09:22:30.053: INFO: Pod "filler-pod-bcb64295-db65-4773-9643-c517669ac4a6" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 12/14/22 09:22:30.053
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e832fbd91e0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-743/filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2 to shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f] 12/14/22 09:22:30.057
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e834dbbfb98], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e834e8d2dc3], Reason = [Created], Message = [Created container filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e8352accc5c], Reason = [Started], Message = [Started container filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e83300d4c4e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-743/filler-pod-bcb64295-db65-4773-9643-c517669ac4a6 to shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e834eee5401], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e835024eeb0], Reason = [Created], Message = [Created container filler-pod-bcb64295-db65-4773-9643-c517669ac4a6] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e8354417503], Reason = [Started], Message = [Started container filler-pod-bcb64295-db65-4773-9643-c517669ac4a6] 12/14/22 09:22:30.058
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17309e83a8b0bfbf], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 12/14/22 09:22:30.071
STEP: removing the label node off the node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f 12/14/22 09:22:31.074
STEP: verifying the node doesn't have the label node 12/14/22 09:22:31.09
STEP: removing the label node off the node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 09:22:31.094
STEP: verifying the node doesn't have the label node 12/14/22 09:22:31.107
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:22:31.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-743" for this suite. 12/14/22 09:22:31.114
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":269,"skipped":5002,"failed":0}
------------------------------
• [3.196 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:27.922
    Dec 14 09:22:27.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/14/22 09:22:27.923
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:27.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:27.937
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 14 09:22:27.943: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 09:22:27.950: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 09:22:27.953: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
    Dec 14 09:22:27.962: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container calico-typha ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 09:22:27.962: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.962: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec 14 09:22:27.962: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
    Dec 14 09:22:27.973: INFO: replace-27850161-6n877 from cronjob-8329 started at 2022-12-14 09:21:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container c ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: replace-27850162-zpgdj from cronjob-8329 started at 2022-12-14 09:22:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container c ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 09:22:27.973: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container metrics-server ready: true, restart count 2
    Dec 14 09:22:27.973: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container metrics-server ready: true, restart count 1
    Dec 14 09:22:27.973: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 09:22:27.973: INFO: sysctl-36efe8e7-6b50-42cd-832f-9b2d946dd59d from sysctl-6676 started at 2022-12-14 09:22:19 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:27.973: INFO: 	Container test-container ready: false, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f 12/14/22 09:22:27.996
    STEP: verifying the node has the label node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 09:22:28.011
    Dec 14 09:22:28.026: INFO: Pod replace-27850161-6n877 requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod replace-27850162-zpgdj requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod addons-nginx-ingress-controller-7fbf48c6b5-rq6ld requesting resource cpu=100m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod apiserver-proxy-2dcbg requesting resource cpu=40m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod apiserver-proxy-7qqrm requesting resource cpu=40m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod blackbox-exporter-59447f4c55-6d48x requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod blackbox-exporter-59447f4c55-rmcv9 requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod calico-node-2vlmc requesting resource cpu=250m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod calico-node-9kgrw requesting resource cpu=250m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod calico-node-vertical-autoscaler-6597dd8998-xkg8r requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod calico-typha-deploy-65c54d4db6-mvhhf requesting resource cpu=320m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod calico-typha-vertical-autoscaler-84df655c88-9gbdh requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod coredns-7869797f4c-ffpdz requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod coredns-7869797f4c-ftxgn requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod csi-driver-node-gnhxw requesting resource cpu=37m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod csi-driver-node-vxc87 requesting resource cpu=37m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod egress-filter-applier-q4n7q requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod egress-filter-applier-zzlmq requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod kube-proxy-worker-1-v1.25.4-fgmr4 requesting resource cpu=34m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod kube-proxy-worker-1-v1.25.4-v2k9w requesting resource cpu=34m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod metrics-server-fd6cb96fd-b99cv requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod metrics-server-fd6cb96fd-dn96c requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod network-problem-detector-host-5sqjh requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod network-problem-detector-host-74zdg requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod network-problem-detector-pod-cqd4k requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod network-problem-detector-pod-mjtpq requesting resource cpu=10m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod node-exporter-5rbx7 requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod node-exporter-vpls6 requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod node-local-dns-92tld requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod node-local-dns-dhx6j requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod node-problem-detector-6xk8z requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.026: INFO: Pod node-problem-detector-jh8v5 requesting resource cpu=11m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod vpn-shoot-5b855f7f89-mxn54 requesting resource cpu=100m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod dashboard-metrics-scraper-6d54964d4b-cgnx2 requesting resource cpu=0m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.026: INFO: Pod kubernetes-dashboard-7b56c57b5d-hqhsz requesting resource cpu=50m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    STEP: Starting Pods to consume most of the cluster CPU. 12/14/22 09:22:28.026
    Dec 14 09:22:28.026: INFO: Creating a pod which consumes cpu=501m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f
    Dec 14 09:22:28.036: INFO: Creating a pod which consumes cpu=907m on Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    Dec 14 09:22:28.042: INFO: Waiting up to 5m0s for pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2" in namespace "sched-pred-743" to be "running"
    Dec 14 09:22:28.046: INFO: Pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250599ms
    Dec 14 09:22:30.050: INFO: Pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007635673s
    Dec 14 09:22:30.050: INFO: Pod "filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2" satisfied condition "running"
    Dec 14 09:22:30.050: INFO: Waiting up to 5m0s for pod "filler-pod-bcb64295-db65-4773-9643-c517669ac4a6" in namespace "sched-pred-743" to be "running"
    Dec 14 09:22:30.053: INFO: Pod "filler-pod-bcb64295-db65-4773-9643-c517669ac4a6": Phase="Running", Reason="", readiness=true. Elapsed: 3.215412ms
    Dec 14 09:22:30.053: INFO: Pod "filler-pod-bcb64295-db65-4773-9643-c517669ac4a6" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 12/14/22 09:22:30.053
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e832fbd91e0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-743/filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2 to shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f] 12/14/22 09:22:30.057
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e834dbbfb98], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e834e8d2dc3], Reason = [Created], Message = [Created container filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2.17309e8352accc5c], Reason = [Started], Message = [Started container filler-pod-ab36add6-5cd6-4c01-81a4-5731c59b5ff2] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e83300d4c4e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-743/filler-pod-bcb64295-db65-4773-9643-c517669ac4a6 to shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e834eee5401], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e835024eeb0], Reason = [Created], Message = [Created container filler-pod-bcb64295-db65-4773-9643-c517669ac4a6] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bcb64295-db65-4773-9643-c517669ac4a6.17309e8354417503], Reason = [Started], Message = [Started container filler-pod-bcb64295-db65-4773-9643-c517669ac4a6] 12/14/22 09:22:30.058
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17309e83a8b0bfbf], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 12/14/22 09:22:30.071
    STEP: removing the label node off the node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f 12/14/22 09:22:31.074
    STEP: verifying the node doesn't have the label node 12/14/22 09:22:31.09
    STEP: removing the label node off the node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb 12/14/22 09:22:31.094
    STEP: verifying the node doesn't have the label node 12/14/22 09:22:31.107
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:22:31.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-743" for this suite. 12/14/22 09:22:31.114
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:31.119
Dec 14 09:22:31.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:22:31.12
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:31.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:31.135
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 12/14/22 09:22:31.14
STEP: listing secrets in all namespaces to ensure that there are more than zero 12/14/22 09:22:31.145
STEP: patching the secret 12/14/22 09:22:31.151
STEP: deleting the secret using a LabelSelector 12/14/22 09:22:31.159
STEP: listing secrets in all namespaces, searching for label name and value in patch 12/14/22 09:22:31.164
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:22:31.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8928" for this suite. 12/14/22 09:22:31.173
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":270,"skipped":5003,"failed":0}
------------------------------
• [0.059 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:31.119
    Dec 14 09:22:31.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:22:31.12
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:31.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:31.135
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 12/14/22 09:22:31.14
    STEP: listing secrets in all namespaces to ensure that there are more than zero 12/14/22 09:22:31.145
    STEP: patching the secret 12/14/22 09:22:31.151
    STEP: deleting the secret using a LabelSelector 12/14/22 09:22:31.159
    STEP: listing secrets in all namespaces, searching for label name and value in patch 12/14/22 09:22:31.164
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:22:31.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8928" for this suite. 12/14/22 09:22:31.173
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:31.178
Dec 14 09:22:31.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop 12/14/22 09:22:31.178
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:31.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:31.193
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4352 12/14/22 09:22:31.197
STEP: Waiting for pods to come up. 12/14/22 09:22:31.205
Dec 14 09:22:31.205: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4352" to be "running"
Dec 14 09:22:31.208: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773562ms
Dec 14 09:22:33.214: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008880292s
Dec 14 09:22:33.214: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4352 12/14/22 09:22:33.218
Dec 14 09:22:33.225: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4352" to be "running"
Dec 14 09:22:33.229: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.505446ms
Dec 14 09:22:35.234: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175256s
Dec 14 09:22:35.234: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 12/14/22 09:22:35.234
Dec 14 09:22:40.352: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 12/14/22 09:22:40.352
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Dec 14 09:22:40.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4352" for this suite. 12/14/22 09:22:40.367
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":271,"skipped":5005,"failed":0}
------------------------------
• [9.194 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:31.178
    Dec 14 09:22:31.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename prestop 12/14/22 09:22:31.178
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:31.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:31.193
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4352 12/14/22 09:22:31.197
    STEP: Waiting for pods to come up. 12/14/22 09:22:31.205
    Dec 14 09:22:31.205: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4352" to be "running"
    Dec 14 09:22:31.208: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773562ms
    Dec 14 09:22:33.214: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008880292s
    Dec 14 09:22:33.214: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4352 12/14/22 09:22:33.218
    Dec 14 09:22:33.225: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4352" to be "running"
    Dec 14 09:22:33.229: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.505446ms
    Dec 14 09:22:35.234: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175256s
    Dec 14 09:22:35.234: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 12/14/22 09:22:35.234
    Dec 14 09:22:40.352: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 12/14/22 09:22:40.352
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Dec 14 09:22:40.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-4352" for this suite. 12/14/22 09:22:40.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:40.373
Dec 14 09:22:40.373: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 12/14/22 09:22:40.374
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:40.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:40.389
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 14 09:22:40.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8736" for this suite. 12/14/22 09:22:40.405
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":272,"skipped":5060,"failed":0}
------------------------------
• [0.037 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:40.373
    Dec 14 09:22:40.373: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 09:22:40.374
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:40.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:40.389
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 14 09:22:40.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8736" for this suite. 12/14/22 09:22:40.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:40.41
Dec 14 09:22:40.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 09:22:40.411
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:40.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:40.426
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Dec 14 09:22:40.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:22:46.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4236" for this suite. 12/14/22 09:22:46.679
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":273,"skipped":5078,"failed":0}
------------------------------
• [6.286 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:40.41
    Dec 14 09:22:40.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 09:22:40.411
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:40.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:40.426
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Dec 14 09:22:40.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:22:46.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4236" for this suite. 12/14/22 09:22:46.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:46.697
Dec 14 09:22:46.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 12/14/22 09:22:46.698
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:46.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:46.713
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 09:22:46.718: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 09:22:46.726: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 09:22:46.730: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
Dec 14 09:22:46.739: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 14 09:22:46.739: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec 14 09:22:46.739: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container proxy ready: true, restart count 0
Dec 14 09:22:46.739: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 09:22:46.739: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 09:22:46.739: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:22:46.739: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container calico-typha ready: true, restart count 0
Dec 14 09:22:46.739: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:22:46.739: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container autoscaler ready: true, restart count 0
Dec 14 09:22:46.739: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:22:46.739: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container coredns ready: true, restart count 0
Dec 14 09:22:46.739: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 09:22:46.739: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 09:22:46.739: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 09:22:46.739: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 09:22:46.739: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 09:22:46.739: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 09:22:46.739: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 09:22:46.739: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 09:22:46.739: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 09:22:46.739: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 09:22:46.739: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 09:22:46.739: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec 14 09:22:46.739: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec 14 09:22:46.739: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.739: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 14 09:22:46.739: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
Dec 14 09:22:46.750: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container proxy ready: true, restart count 0
Dec 14 09:22:46.750: INFO: 	Container sidecar ready: true, restart count 0
Dec 14 09:22:46.750: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 09:22:46.750: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec 14 09:22:46.750: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container calico-node ready: true, restart count 0
Dec 14 09:22:46.750: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 09:22:46.750: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 09:22:46.750: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 09:22:46.750: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container egress-filter-applier ready: true, restart count 1
Dec 14 09:22:46.750: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container conntrack-fix ready: true, restart count 0
Dec 14 09:22:46.750: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 09:22:46.750: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container metrics-server ready: true, restart count 2
Dec 14 09:22:46.750: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container metrics-server ready: true, restart count 1
Dec 14 09:22:46.750: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Dec 14 09:22:46.750: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Dec 14 09:22:46.750: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 09:22:46.750: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container node-cache ready: true, restart count 0
Dec 14 09:22:46.750: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec 14 09:22:46.750: INFO: tester from prestop-4352 started at 2022-12-14 09:22:33 +0000 UTC (1 container statuses recorded)
Dec 14 09:22:46.750: INFO: 	Container tester ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 12/14/22 09:22:46.75
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17309e878c91965d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 12/14/22 09:22:46.779
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:22:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2442" for this suite. 12/14/22 09:22:47.788
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":274,"skipped":5111,"failed":0}
------------------------------
• [1.096 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:46.697
    Dec 14 09:22:46.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 12/14/22 09:22:46.698
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:46.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:46.713
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 14 09:22:46.718: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 09:22:46.726: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 09:22:46.730: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f before test
    Dec 14 09:22:46.739: INFO: addons-nginx-ingress-controller-7fbf48c6b5-rq6ld from kube-system started at 2022-12-14 08:03:32 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-8668c9bb59-j7tfg from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: apiserver-proxy-2dcbg from kube-system started at 2022-12-14 07:51:55 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: calico-node-9kgrw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: calico-node-vertical-autoscaler-6597dd8998-xkg8r from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: calico-typha-deploy-65c54d4db6-mvhhf from kube-system started at 2022-12-14 07:52:44 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container calico-typha ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: calico-typha-horizontal-autoscaler-6bb4bc55bc-n548c from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: calico-typha-vertical-autoscaler-84df655c88-9gbdh from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: coredns-7869797f4c-ffpdz from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: coredns-7869797f4c-ftxgn from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: csi-driver-node-gnhxw from kube-system started at 2022-12-14 07:51:55 +0000 UTC (3 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: egress-filter-applier-zzlmq from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 09:22:46.739: INFO: kube-proxy-worker-1-v1.25.4-v2k9w from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: network-problem-detector-host-5sqjh from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: network-problem-detector-pod-cqd4k from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: node-exporter-vpls6 from kube-system started at 2022-12-14 07:51:55 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: node-local-dns-dhx6j from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: node-problem-detector-jh8v5 from kube-system started at 2022-12-14 08:17:09 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: vpn-shoot-5b855f7f89-mxn54 from kube-system started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container vpn-shoot ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: dashboard-metrics-scraper-6d54964d4b-cgnx2 from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: kubernetes-dashboard-7b56c57b5d-hqhsz from kubernetes-dashboard started at 2022-12-14 07:52:14 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.739: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Dec 14 09:22:46.739: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb before test
    Dec 14 09:22:46.750: INFO: apiserver-proxy-7qqrm from kube-system started at 2022-12-14 07:52:00 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container proxy ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: 	Container sidecar ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: blackbox-exporter-59447f4c55-6d48x from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: blackbox-exporter-59447f4c55-rmcv9 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: calico-node-2vlmc from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container calico-node ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: csi-driver-node-vxc87 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (3 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container csi-driver ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: egress-filter-applier-q4n7q from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container egress-filter-applier ready: true, restart count 1
    Dec 14 09:22:46.750: INFO: kube-proxy-worker-1-v1.25.4-fgmr4 from kube-system started at 2022-12-14 08:30:10 +0000 UTC (2 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container conntrack-fix ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: metrics-server-fd6cb96fd-b99cv from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container metrics-server ready: true, restart count 2
    Dec 14 09:22:46.750: INFO: metrics-server-fd6cb96fd-dn96c from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container metrics-server ready: true, restart count 1
    Dec 14 09:22:46.750: INFO: network-problem-detector-host-74zdg from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: network-problem-detector-pod-mjtpq from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: node-exporter-5rbx7 from kube-system started at 2022-12-14 07:52:00 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container node-exporter ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: node-local-dns-92tld from kube-system started at 2022-12-14 08:10:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container node-cache ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: node-problem-detector-6xk8z from kube-system started at 2022-12-14 08:17:10 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container node-problem-detector ready: true, restart count 0
    Dec 14 09:22:46.750: INFO: tester from prestop-4352 started at 2022-12-14 09:22:33 +0000 UTC (1 container statuses recorded)
    Dec 14 09:22:46.750: INFO: 	Container tester ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 12/14/22 09:22:46.75
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17309e878c91965d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 12/14/22 09:22:46.779
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:22:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2442" for this suite. 12/14/22 09:22:47.788
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:47.794
Dec 14 09:22:47.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:22:47.795
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:47.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:47.81
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:22:47.823
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:22:48.038
STEP: Deploying the webhook pod 12/14/22 09:22:48.043
STEP: Wait for the deployment to be ready 12/14/22 09:22:48.053
Dec 14 09:22:48.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:22:50.07
STEP: Verifying the service has paired with the endpoint 12/14/22 09:22:50.087
Dec 14 09:22:51.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/14/22 09:22:51.093
STEP: create a namespace for the webhook 12/14/22 09:22:51.169
STEP: create a configmap should be unconditionally rejected by the webhook 12/14/22 09:22:51.175
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:22:51.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8974" for this suite. 12/14/22 09:22:51.25
STEP: Destroying namespace "webhook-8974-markers" for this suite. 12/14/22 09:22:51.256
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":275,"skipped":5162,"failed":0}
------------------------------
• [3.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:47.794
    Dec 14 09:22:47.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:22:47.795
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:47.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:47.81
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:22:47.823
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:22:48.038
    STEP: Deploying the webhook pod 12/14/22 09:22:48.043
    STEP: Wait for the deployment to be ready 12/14/22 09:22:48.053
    Dec 14 09:22:48.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:22:50.07
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:22:50.087
    Dec 14 09:22:51.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/14/22 09:22:51.093
    STEP: create a namespace for the webhook 12/14/22 09:22:51.169
    STEP: create a configmap should be unconditionally rejected by the webhook 12/14/22 09:22:51.175
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:22:51.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8974" for this suite. 12/14/22 09:22:51.25
    STEP: Destroying namespace "webhook-8974-markers" for this suite. 12/14/22 09:22:51.256
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:22:51.283
Dec 14 09:22:51.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:22:51.284
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:51.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:51.298
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:22:51.312
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:22:51.925
STEP: Deploying the webhook pod 12/14/22 09:22:51.93
STEP: Wait for the deployment to be ready 12/14/22 09:22:51.94
Dec 14 09:22:51.947: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:22:53.959
STEP: Verifying the service has paired with the endpoint 12/14/22 09:22:53.967
Dec 14 09:22:54.968: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/14/22 09:22:54.972
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:22:54.972
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/14/22 09:22:55.004
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/14/22 09:22:56.013
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:22:56.013
STEP: Having no error when timeout is longer than webhook latency 12/14/22 09:22:57.049
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:22:57.049
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/14/22 09:23:02.233
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:23:02.233
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:23:07.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3353" for this suite. 12/14/22 09:23:07.334
STEP: Destroying namespace "webhook-3353-markers" for this suite. 12/14/22 09:23:07.339
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":276,"skipped":5172,"failed":0}
------------------------------
• [16.088 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:22:51.283
    Dec 14 09:22:51.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:22:51.284
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:22:51.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:22:51.298
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:22:51.312
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:22:51.925
    STEP: Deploying the webhook pod 12/14/22 09:22:51.93
    STEP: Wait for the deployment to be ready 12/14/22 09:22:51.94
    Dec 14 09:22:51.947: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:22:53.959
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:22:53.967
    Dec 14 09:22:54.968: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/14/22 09:22:54.972
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:22:54.972
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/14/22 09:22:55.004
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/14/22 09:22:56.013
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:22:56.013
    STEP: Having no error when timeout is longer than webhook latency 12/14/22 09:22:57.049
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:22:57.049
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/14/22 09:23:02.233
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 09:23:02.233
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:23:07.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3353" for this suite. 12/14/22 09:23:07.334
    STEP: Destroying namespace "webhook-3353-markers" for this suite. 12/14/22 09:23:07.339
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:07.371
Dec 14 09:23:07.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:23:07.372
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:07.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:07.389
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-0376851e-dcba-4d5b-9ba6-51f095a2d242 12/14/22 09:23:07.394
STEP: Creating a pod to test consume configMaps 12/14/22 09:23:07.398
Dec 14 09:23:07.407: INFO: Waiting up to 5m0s for pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a" in namespace "configmap-6807" to be "Succeeded or Failed"
Dec 14 09:23:07.411: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88415ms
Dec 14 09:23:09.416: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008776428s
Dec 14 09:23:11.417: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009591146s
STEP: Saw pod success 12/14/22 09:23:11.417
Dec 14 09:23:11.417: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a" satisfied condition "Succeeded or Failed"
Dec 14 09:23:11.421: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a container configmap-volume-test: <nil>
STEP: delete the pod 12/14/22 09:23:11.432
Dec 14 09:23:11.439: INFO: Waiting for pod pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a to disappear
Dec 14 09:23:11.441: INFO: Pod pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:23:11.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6807" for this suite. 12/14/22 09:23:11.447
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":277,"skipped":5178,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:07.371
    Dec 14 09:23:07.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:23:07.372
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:07.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:07.389
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-0376851e-dcba-4d5b-9ba6-51f095a2d242 12/14/22 09:23:07.394
    STEP: Creating a pod to test consume configMaps 12/14/22 09:23:07.398
    Dec 14 09:23:07.407: INFO: Waiting up to 5m0s for pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a" in namespace "configmap-6807" to be "Succeeded or Failed"
    Dec 14 09:23:07.411: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88415ms
    Dec 14 09:23:09.416: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008776428s
    Dec 14 09:23:11.417: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009591146s
    STEP: Saw pod success 12/14/22 09:23:11.417
    Dec 14 09:23:11.417: INFO: Pod "pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a" satisfied condition "Succeeded or Failed"
    Dec 14 09:23:11.421: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a container configmap-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:23:11.432
    Dec 14 09:23:11.439: INFO: Waiting for pod pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a to disappear
    Dec 14 09:23:11.441: INFO: Pod pod-configmaps-62fd51c2-21f0-4928-83d1-19fa2e4deb2a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:23:11.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6807" for this suite. 12/14/22 09:23:11.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:11.452
Dec 14 09:23:11.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:23:11.453
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:11.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:11.467
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-1f163f3c-5168-44ea-8e0e-d59e4d8a0701 12/14/22 09:23:11.475
STEP: Creating secret with name s-test-opt-upd-50d86092-92ab-45fb-abe6-54bf88afe593 12/14/22 09:23:11.478
STEP: Creating the pod 12/14/22 09:23:11.482
Dec 14 09:23:11.494: INFO: Waiting up to 5m0s for pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3" in namespace "secrets-3533" to be "running and ready"
Dec 14 09:23:11.497: INFO: Pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707642ms
Dec 14 09:23:11.497: INFO: The phase of Pod pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:23:13.503: INFO: Pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008953944s
Dec 14 09:23:13.503: INFO: The phase of Pod pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3 is Running (Ready = true)
Dec 14 09:23:13.503: INFO: Pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-1f163f3c-5168-44ea-8e0e-d59e4d8a0701 12/14/22 09:23:13.698
STEP: Updating secret s-test-opt-upd-50d86092-92ab-45fb-abe6-54bf88afe593 12/14/22 09:23:13.703
STEP: Creating secret with name s-test-opt-create-b13d58de-5d72-47eb-9219-9824bb1aae3e 12/14/22 09:23:13.707
STEP: waiting to observe update in volume 12/14/22 09:23:13.71
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:23:17.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3533" for this suite. 12/14/22 09:23:18.002
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":278,"skipped":5200,"failed":0}
------------------------------
• [6.554 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:11.452
    Dec 14 09:23:11.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:23:11.453
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:11.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:11.467
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-1f163f3c-5168-44ea-8e0e-d59e4d8a0701 12/14/22 09:23:11.475
    STEP: Creating secret with name s-test-opt-upd-50d86092-92ab-45fb-abe6-54bf88afe593 12/14/22 09:23:11.478
    STEP: Creating the pod 12/14/22 09:23:11.482
    Dec 14 09:23:11.494: INFO: Waiting up to 5m0s for pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3" in namespace "secrets-3533" to be "running and ready"
    Dec 14 09:23:11.497: INFO: Pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707642ms
    Dec 14 09:23:11.497: INFO: The phase of Pod pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:23:13.503: INFO: Pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008953944s
    Dec 14 09:23:13.503: INFO: The phase of Pod pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3 is Running (Ready = true)
    Dec 14 09:23:13.503: INFO: Pod "pod-secrets-d6531a19-cf3f-499f-92ae-68deeaec1ef3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-1f163f3c-5168-44ea-8e0e-d59e4d8a0701 12/14/22 09:23:13.698
    STEP: Updating secret s-test-opt-upd-50d86092-92ab-45fb-abe6-54bf88afe593 12/14/22 09:23:13.703
    STEP: Creating secret with name s-test-opt-create-b13d58de-5d72-47eb-9219-9824bb1aae3e 12/14/22 09:23:13.707
    STEP: waiting to observe update in volume 12/14/22 09:23:13.71
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:23:17.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3533" for this suite. 12/14/22 09:23:18.002
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:18.006
Dec 14 09:23:18.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 12/14/22 09:23:18.007
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:18.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:18.022
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 12/14/22 09:23:18.03
STEP: Patching the Job 12/14/22 09:23:18.034
STEP: Watching for Job to be patched 12/14/22 09:23:18.045
Dec 14 09:23:18.048: INFO: Event ADDED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 14 09:23:18.048: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 14 09:23:18.048: INFO: Event MODIFIED found for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 12/14/22 09:23:18.048
STEP: Watching for Job to be updated 12/14/22 09:23:18.055
Dec 14 09:23:18.058: INFO: Event MODIFIED found for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 09:23:18.058: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 12/14/22 09:23:18.058
Dec 14 09:23:18.060: INFO: Job: e2e-7gs5q as labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q]
STEP: Waiting for job to complete 12/14/22 09:23:18.06
STEP: Delete a job collection with a labelselector 12/14/22 09:23:26.066
STEP: Watching for Job to be deleted 12/14/22 09:23:26.072
Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 09:23:26.075: INFO: Event DELETED found for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 12/14/22 09:23:26.075
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 14 09:23:26.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8905" for this suite. 12/14/22 09:23:26.082
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":279,"skipped":5202,"failed":0}
------------------------------
• [8.081 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:18.006
    Dec 14 09:23:18.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 12/14/22 09:23:18.007
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:18.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:18.022
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 12/14/22 09:23:18.03
    STEP: Patching the Job 12/14/22 09:23:18.034
    STEP: Watching for Job to be patched 12/14/22 09:23:18.045
    Dec 14 09:23:18.048: INFO: Event ADDED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 14 09:23:18.048: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 14 09:23:18.048: INFO: Event MODIFIED found for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 12/14/22 09:23:18.048
    STEP: Watching for Job to be updated 12/14/22 09:23:18.055
    Dec 14 09:23:18.058: INFO: Event MODIFIED found for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 09:23:18.058: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 12/14/22 09:23:18.058
    Dec 14 09:23:18.060: INFO: Job: e2e-7gs5q as labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q]
    STEP: Waiting for job to complete 12/14/22 09:23:18.06
    STEP: Delete a job collection with a labelselector 12/14/22 09:23:26.066
    STEP: Watching for Job to be deleted 12/14/22 09:23:26.072
    Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 09:23:26.075: INFO: Event MODIFIED observed for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 09:23:26.075: INFO: Event DELETED found for Job e2e-7gs5q in namespace job-8905 with labels: map[e2e-7gs5q:patched e2e-job-label:e2e-7gs5q] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 12/14/22 09:23:26.075
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 14 09:23:26.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8905" for this suite. 12/14/22 09:23:26.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:26.088
Dec 14 09:23:26.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 12/14/22 09:23:26.088
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:26.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:26.105
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Dec 14 09:23:26.110: INFO: Creating pod...
Dec 14 09:23:26.119: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8136" to be "running"
Dec 14 09:23:26.122: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.577144ms
Dec 14 09:23:28.127: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008468636s
Dec 14 09:23:28.127: INFO: Pod "agnhost" satisfied condition "running"
Dec 14 09:23:28.127: INFO: Creating service...
Dec 14 09:23:28.136: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/DELETE
Dec 14 09:23:28.251: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 09:23:28.251: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/GET
Dec 14 09:23:28.294: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 09:23:28.294: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/HEAD
Dec 14 09:23:28.301: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 09:23:28.301: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/OPTIONS
Dec 14 09:23:28.307: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 09:23:28.307: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/PATCH
Dec 14 09:23:28.313: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 09:23:28.313: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/POST
Dec 14 09:23:28.320: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 09:23:28.320: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/PUT
Dec 14 09:23:28.327: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 09:23:28.327: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/DELETE
Dec 14 09:23:28.334: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 09:23:28.334: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/GET
Dec 14 09:23:28.341: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 09:23:28.341: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/HEAD
Dec 14 09:23:28.348: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 09:23:28.348: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/OPTIONS
Dec 14 09:23:28.355: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 09:23:28.355: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/PATCH
Dec 14 09:23:28.362: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 09:23:28.362: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/POST
Dec 14 09:23:28.369: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 09:23:28.369: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/PUT
Dec 14 09:23:28.380: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Dec 14 09:23:28.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8136" for this suite. 12/14/22 09:23:28.386
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":280,"skipped":5210,"failed":0}
------------------------------
• [2.303 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:26.088
    Dec 14 09:23:26.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 12/14/22 09:23:26.088
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:26.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:26.105
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Dec 14 09:23:26.110: INFO: Creating pod...
    Dec 14 09:23:26.119: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8136" to be "running"
    Dec 14 09:23:26.122: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.577144ms
    Dec 14 09:23:28.127: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008468636s
    Dec 14 09:23:28.127: INFO: Pod "agnhost" satisfied condition "running"
    Dec 14 09:23:28.127: INFO: Creating service...
    Dec 14 09:23:28.136: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/DELETE
    Dec 14 09:23:28.251: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 09:23:28.251: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/GET
    Dec 14 09:23:28.294: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 14 09:23:28.294: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/HEAD
    Dec 14 09:23:28.301: INFO: http.Client request:HEAD | StatusCode:200
    Dec 14 09:23:28.301: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/OPTIONS
    Dec 14 09:23:28.307: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 09:23:28.307: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/PATCH
    Dec 14 09:23:28.313: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 09:23:28.313: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/POST
    Dec 14 09:23:28.320: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 09:23:28.320: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/pods/agnhost/proxy/some/path/with/PUT
    Dec 14 09:23:28.327: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 14 09:23:28.327: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/DELETE
    Dec 14 09:23:28.334: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 09:23:28.334: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/GET
    Dec 14 09:23:28.341: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 14 09:23:28.341: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/HEAD
    Dec 14 09:23:28.348: INFO: http.Client request:HEAD | StatusCode:200
    Dec 14 09:23:28.348: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/OPTIONS
    Dec 14 09:23:28.355: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 09:23:28.355: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/PATCH
    Dec 14 09:23:28.362: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 09:23:28.362: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/POST
    Dec 14 09:23:28.369: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 09:23:28.369: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-8136/services/test-service/proxy/some/path/with/PUT
    Dec 14 09:23:28.380: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Dec 14 09:23:28.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8136" for this suite. 12/14/22 09:23:28.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:28.391
Dec 14 09:23:28.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:23:28.392
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:28.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:28.406
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-a6dbe4bf-c7e2-4de6-9761-691d5e9cef7f 12/14/22 09:23:28.411
STEP: Creating a pod to test consume secrets 12/14/22 09:23:28.415
Dec 14 09:23:28.424: INFO: Waiting up to 5m0s for pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e" in namespace "secrets-7276" to be "Succeeded or Failed"
Dec 14 09:23:28.428: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.932514ms
Dec 14 09:23:30.433: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008622933s
Dec 14 09:23:32.432: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008405456s
STEP: Saw pod success 12/14/22 09:23:32.432
Dec 14 09:23:32.433: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e" satisfied condition "Succeeded or Failed"
Dec 14 09:23:32.436: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:23:32.446
Dec 14 09:23:32.454: INFO: Waiting for pod pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e to disappear
Dec 14 09:23:32.457: INFO: Pod pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:23:32.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7276" for this suite. 12/14/22 09:23:32.462
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5231,"failed":0}
------------------------------
• [4.075 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:28.391
    Dec 14 09:23:28.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:23:28.392
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:28.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:28.406
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-a6dbe4bf-c7e2-4de6-9761-691d5e9cef7f 12/14/22 09:23:28.411
    STEP: Creating a pod to test consume secrets 12/14/22 09:23:28.415
    Dec 14 09:23:28.424: INFO: Waiting up to 5m0s for pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e" in namespace "secrets-7276" to be "Succeeded or Failed"
    Dec 14 09:23:28.428: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.932514ms
    Dec 14 09:23:30.433: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008622933s
    Dec 14 09:23:32.432: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008405456s
    STEP: Saw pod success 12/14/22 09:23:32.432
    Dec 14 09:23:32.433: INFO: Pod "pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e" satisfied condition "Succeeded or Failed"
    Dec 14 09:23:32.436: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:23:32.446
    Dec 14 09:23:32.454: INFO: Waiting for pod pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e to disappear
    Dec 14 09:23:32.457: INFO: Pod pod-secrets-50e1a773-c327-4bcb-b543-fb3b9b9fc11e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:23:32.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7276" for this suite. 12/14/22 09:23:32.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:32.467
Dec 14 09:23:32.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:23:32.467
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:32.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:32.483
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:23:32.487
Dec 14 09:23:32.497: INFO: Waiting up to 5m0s for pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712" in namespace "projected-6063" to be "Succeeded or Failed"
Dec 14 09:23:32.501: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712": Phase="Pending", Reason="", readiness=false. Elapsed: 4.290175ms
Dec 14 09:23:34.506: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712": Phase="Running", Reason="", readiness=false. Elapsed: 2.009021975s
Dec 14 09:23:36.506: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00876725s
STEP: Saw pod success 12/14/22 09:23:36.506
Dec 14 09:23:36.506: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712" satisfied condition "Succeeded or Failed"
Dec 14 09:23:36.509: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712 container client-container: <nil>
STEP: delete the pod 12/14/22 09:23:36.548
Dec 14 09:23:36.566: INFO: Waiting for pod downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712 to disappear
Dec 14 09:23:36.569: INFO: Pod downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:23:36.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6063" for this suite. 12/14/22 09:23:36.575
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":282,"skipped":5239,"failed":0}
------------------------------
• [4.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:32.467
    Dec 14 09:23:32.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:23:32.467
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:32.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:32.483
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:23:32.487
    Dec 14 09:23:32.497: INFO: Waiting up to 5m0s for pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712" in namespace "projected-6063" to be "Succeeded or Failed"
    Dec 14 09:23:32.501: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712": Phase="Pending", Reason="", readiness=false. Elapsed: 4.290175ms
    Dec 14 09:23:34.506: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712": Phase="Running", Reason="", readiness=false. Elapsed: 2.009021975s
    Dec 14 09:23:36.506: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00876725s
    STEP: Saw pod success 12/14/22 09:23:36.506
    Dec 14 09:23:36.506: INFO: Pod "downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712" satisfied condition "Succeeded or Failed"
    Dec 14 09:23:36.509: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:23:36.548
    Dec 14 09:23:36.566: INFO: Waiting for pod downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712 to disappear
    Dec 14 09:23:36.569: INFO: Pod downwardapi-volume-597bcec5-9fad-49f5-979e-d7e75388d712 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:23:36.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6063" for this suite. 12/14/22 09:23:36.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:36.58
Dec 14 09:23:36.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 12/14/22 09:23:36.58
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:36.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:36.596
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 12/14/22 09:23:36.6
Dec 14 09:23:36.600: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 09:23:42.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9706" for this suite. 12/14/22 09:23:42.153
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":283,"skipped":5264,"failed":0}
------------------------------
• [5.578 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:36.58
    Dec 14 09:23:36.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 12/14/22 09:23:36.58
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:36.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:36.596
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 12/14/22 09:23:36.6
    Dec 14 09:23:36.600: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 09:23:42.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9706" for this suite. 12/14/22 09:23:42.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:42.158
Dec 14 09:23:42.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:23:42.159
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:42.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:42.175
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-2596bc07-4c3e-47ad-9e15-a421358c063a 12/14/22 09:23:42.179
STEP: Creating a pod to test consume configMaps 12/14/22 09:23:42.183
Dec 14 09:23:42.191: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30" in namespace "projected-9965" to be "Succeeded or Failed"
Dec 14 09:23:42.195: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13199ms
Dec 14 09:23:44.200: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008891443s
Dec 14 09:23:46.200: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008401354s
STEP: Saw pod success 12/14/22 09:23:46.2
Dec 14 09:23:46.200: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30" satisfied condition "Succeeded or Failed"
Dec 14 09:23:46.204: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:23:46.213
Dec 14 09:23:46.221: INFO: Waiting for pod pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30 to disappear
Dec 14 09:23:46.225: INFO: Pod pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 09:23:46.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9965" for this suite. 12/14/22 09:23:46.231
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":284,"skipped":5270,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:42.158
    Dec 14 09:23:42.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:23:42.159
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:42.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:42.175
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-2596bc07-4c3e-47ad-9e15-a421358c063a 12/14/22 09:23:42.179
    STEP: Creating a pod to test consume configMaps 12/14/22 09:23:42.183
    Dec 14 09:23:42.191: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30" in namespace "projected-9965" to be "Succeeded or Failed"
    Dec 14 09:23:42.195: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13199ms
    Dec 14 09:23:44.200: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008891443s
    Dec 14 09:23:46.200: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008401354s
    STEP: Saw pod success 12/14/22 09:23:46.2
    Dec 14 09:23:46.200: INFO: Pod "pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30" satisfied condition "Succeeded or Failed"
    Dec 14 09:23:46.204: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:23:46.213
    Dec 14 09:23:46.221: INFO: Waiting for pod pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30 to disappear
    Dec 14 09:23:46.225: INFO: Pod pod-projected-configmaps-5eaed4f3-c379-47d6-aa15-4079eb8a1a30 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 09:23:46.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9965" for this suite. 12/14/22 09:23:46.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:23:46.247
Dec 14 09:23:46.247: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 12/14/22 09:23:46.248
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:46.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:46.265
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5931 12/14/22 09:23:46.269
STEP: creating a selector 12/14/22 09:23:46.269
STEP: Creating the service pods in kubernetes 12/14/22 09:23:46.27
Dec 14 09:23:46.270: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 09:23:46.294: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5931" to be "running and ready"
Dec 14 09:23:46.298: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481563ms
Dec 14 09:23:46.298: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:23:48.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008471344s
Dec 14 09:23:48.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:23:50.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008860632s
Dec 14 09:23:50.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:23:52.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00857758s
Dec 14 09:23:52.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:23:54.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00831352s
Dec 14 09:23:54.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:23:56.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008154921s
Dec 14 09:23:56.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 09:23:58.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008560723s
Dec 14 09:23:58.303: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 09:23:58.303: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 09:23:58.307: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5931" to be "running and ready"
Dec 14 09:23:58.310: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.117053ms
Dec 14 09:23:58.310: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 09:23:58.310: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 09:23:58.313
Dec 14 09:23:58.321: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5931" to be "running"
Dec 14 09:23:58.325: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262438ms
Dec 14 09:24:00.330: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008339144s
Dec 14 09:24:00.330: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 09:24:00.334: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 14 09:24:00.334: INFO: Breadth first check of 100.64.0.164 on host 10.250.3.58...
Dec 14 09:24:00.337: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.167:9080/dial?request=hostname&protocol=http&host=100.64.0.164&port=8083&tries=1'] Namespace:pod-network-test-5931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:24:00.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:24:00.337: INFO: ExecWithOptions: Clientset creation
Dec 14 09:24:00.337: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-5931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.0.164%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 09:24:00.744: INFO: Waiting for responses: map[]
Dec 14 09:24:00.744: INFO: reached 100.64.0.164 after 0/1 tries
Dec 14 09:24:00.744: INFO: Breadth first check of 100.64.1.166 on host 10.250.3.210...
Dec 14 09:24:00.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.167:9080/dial?request=hostname&protocol=http&host=100.64.1.166&port=8083&tries=1'] Namespace:pod-network-test-5931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:24:00.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:24:00.762: INFO: ExecWithOptions: Clientset creation
Dec 14 09:24:00.762: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-5931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.1.166%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 09:24:01.216: INFO: Waiting for responses: map[]
Dec 14 09:24:01.216: INFO: reached 100.64.1.166 after 0/1 tries
Dec 14 09:24:01.216: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 14 09:24:01.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5931" for this suite. 12/14/22 09:24:01.223
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":285,"skipped":5282,"failed":0}
------------------------------
• [14.980 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:23:46.247
    Dec 14 09:23:46.247: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 09:23:46.248
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:23:46.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:23:46.265
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5931 12/14/22 09:23:46.269
    STEP: creating a selector 12/14/22 09:23:46.269
    STEP: Creating the service pods in kubernetes 12/14/22 09:23:46.27
    Dec 14 09:23:46.270: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 09:23:46.294: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5931" to be "running and ready"
    Dec 14 09:23:46.298: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481563ms
    Dec 14 09:23:46.298: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:23:48.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008471344s
    Dec 14 09:23:48.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:23:50.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008860632s
    Dec 14 09:23:50.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:23:52.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00857758s
    Dec 14 09:23:52.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:23:54.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00831352s
    Dec 14 09:23:54.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:23:56.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008154921s
    Dec 14 09:23:56.303: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 09:23:58.303: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008560723s
    Dec 14 09:23:58.303: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 09:23:58.303: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 09:23:58.307: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5931" to be "running and ready"
    Dec 14 09:23:58.310: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.117053ms
    Dec 14 09:23:58.310: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 09:23:58.310: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 09:23:58.313
    Dec 14 09:23:58.321: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5931" to be "running"
    Dec 14 09:23:58.325: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262438ms
    Dec 14 09:24:00.330: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008339144s
    Dec 14 09:24:00.330: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 09:24:00.334: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec 14 09:24:00.334: INFO: Breadth first check of 100.64.0.164 on host 10.250.3.58...
    Dec 14 09:24:00.337: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.167:9080/dial?request=hostname&protocol=http&host=100.64.0.164&port=8083&tries=1'] Namespace:pod-network-test-5931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:24:00.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:24:00.337: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:24:00.337: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-5931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.0.164%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 09:24:00.744: INFO: Waiting for responses: map[]
    Dec 14 09:24:00.744: INFO: reached 100.64.0.164 after 0/1 tries
    Dec 14 09:24:00.744: INFO: Breadth first check of 100.64.1.166 on host 10.250.3.210...
    Dec 14 09:24:00.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.167:9080/dial?request=hostname&protocol=http&host=100.64.1.166&port=8083&tries=1'] Namespace:pod-network-test-5931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:24:00.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:24:00.762: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:24:00.762: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-5931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.1.166%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 09:24:01.216: INFO: Waiting for responses: map[]
    Dec 14 09:24:01.216: INFO: reached 100.64.1.166 after 0/1 tries
    Dec 14 09:24:01.216: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 14 09:24:01.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5931" for this suite. 12/14/22 09:24:01.223
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:01.228
Dec 14 09:24:01.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:24:01.228
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:01.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:01.256
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  12/14/22 09:24:01.26
Dec 14 09:24:01.269: INFO: Waiting up to 5m0s for pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4" in namespace "svcaccounts-9373" to be "Succeeded or Failed"
Dec 14 09:24:01.272: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316209ms
Dec 14 09:24:03.277: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008161765s
Dec 14 09:24:05.278: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008504738s
STEP: Saw pod success 12/14/22 09:24:05.278
Dec 14 09:24:05.278: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4" satisfied condition "Succeeded or Failed"
Dec 14 09:24:05.281: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:24:05.29
Dec 14 09:24:05.298: INFO: Waiting for pod test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4 to disappear
Dec 14 09:24:05.301: INFO: Pod test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 14 09:24:05.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9373" for this suite. 12/14/22 09:24:05.306
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":286,"skipped":5282,"failed":0}
------------------------------
• [4.082 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:01.228
    Dec 14 09:24:01.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:24:01.228
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:01.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:01.256
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  12/14/22 09:24:01.26
    Dec 14 09:24:01.269: INFO: Waiting up to 5m0s for pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4" in namespace "svcaccounts-9373" to be "Succeeded or Failed"
    Dec 14 09:24:01.272: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316209ms
    Dec 14 09:24:03.277: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008161765s
    Dec 14 09:24:05.278: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008504738s
    STEP: Saw pod success 12/14/22 09:24:05.278
    Dec 14 09:24:05.278: INFO: Pod "test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4" satisfied condition "Succeeded or Failed"
    Dec 14 09:24:05.281: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:24:05.29
    Dec 14 09:24:05.298: INFO: Waiting for pod test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4 to disappear
    Dec 14 09:24:05.301: INFO: Pod test-pod-cd4f1bf6-e5d4-4350-b121-48771e4952e4 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 14 09:24:05.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9373" for this suite. 12/14/22 09:24:05.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:05.314
Dec 14 09:24:05.314: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:24:05.315
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:05.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:05.329
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 12/14/22 09:24:05.333
Dec 14 09:24:05.342: INFO: Waiting up to 5m0s for pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e" in namespace "projected-6697" to be "running and ready"
Dec 14 09:24:05.346: INFO: Pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154676ms
Dec 14 09:24:05.346: INFO: The phase of Pod annotationupdateaf316c92-5855-4919-adc7-306620a2a76e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:24:07.350: INFO: Pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008818614s
Dec 14 09:24:07.350: INFO: The phase of Pod annotationupdateaf316c92-5855-4919-adc7-306620a2a76e is Running (Ready = true)
Dec 14 09:24:07.350: INFO: Pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e" satisfied condition "running and ready"
Dec 14 09:24:07.874: INFO: Successfully updated pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:24:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6697" for this suite. 12/14/22 09:24:11.913
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":287,"skipped":5309,"failed":0}
------------------------------
• [6.603 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:05.314
    Dec 14 09:24:05.314: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:24:05.315
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:05.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:05.329
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 12/14/22 09:24:05.333
    Dec 14 09:24:05.342: INFO: Waiting up to 5m0s for pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e" in namespace "projected-6697" to be "running and ready"
    Dec 14 09:24:05.346: INFO: Pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154676ms
    Dec 14 09:24:05.346: INFO: The phase of Pod annotationupdateaf316c92-5855-4919-adc7-306620a2a76e is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:24:07.350: INFO: Pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008818614s
    Dec 14 09:24:07.350: INFO: The phase of Pod annotationupdateaf316c92-5855-4919-adc7-306620a2a76e is Running (Ready = true)
    Dec 14 09:24:07.350: INFO: Pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e" satisfied condition "running and ready"
    Dec 14 09:24:07.874: INFO: Successfully updated pod "annotationupdateaf316c92-5855-4919-adc7-306620a2a76e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:24:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6697" for this suite. 12/14/22 09:24:11.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:11.918
Dec 14 09:24:11.918: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 12/14/22 09:24:11.919
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:11.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:11.938
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 12/14/22 09:24:11.942
Dec 14 09:24:11.951: INFO: Waiting up to 5m0s for pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17" in namespace "containers-1691" to be "Succeeded or Failed"
Dec 14 09:24:11.954: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592701ms
Dec 14 09:24:13.958: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006765311s
Dec 14 09:24:15.959: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00813251s
STEP: Saw pod success 12/14/22 09:24:15.959
Dec 14 09:24:15.960: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17" satisfied condition "Succeeded or Failed"
Dec 14 09:24:15.963: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-containers-7262760b-6ae0-4537-add8-7318d3cbee17 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:24:15.972
Dec 14 09:24:15.978: INFO: Waiting for pod client-containers-7262760b-6ae0-4537-add8-7318d3cbee17 to disappear
Dec 14 09:24:15.981: INFO: Pod client-containers-7262760b-6ae0-4537-add8-7318d3cbee17 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 14 09:24:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1691" for this suite. 12/14/22 09:24:15.987
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":288,"skipped":5320,"failed":0}
------------------------------
• [4.073 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:11.918
    Dec 14 09:24:11.918: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 12/14/22 09:24:11.919
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:11.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:11.938
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 12/14/22 09:24:11.942
    Dec 14 09:24:11.951: INFO: Waiting up to 5m0s for pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17" in namespace "containers-1691" to be "Succeeded or Failed"
    Dec 14 09:24:11.954: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592701ms
    Dec 14 09:24:13.958: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006765311s
    Dec 14 09:24:15.959: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00813251s
    STEP: Saw pod success 12/14/22 09:24:15.959
    Dec 14 09:24:15.960: INFO: Pod "client-containers-7262760b-6ae0-4537-add8-7318d3cbee17" satisfied condition "Succeeded or Failed"
    Dec 14 09:24:15.963: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod client-containers-7262760b-6ae0-4537-add8-7318d3cbee17 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:24:15.972
    Dec 14 09:24:15.978: INFO: Waiting for pod client-containers-7262760b-6ae0-4537-add8-7318d3cbee17 to disappear
    Dec 14 09:24:15.981: INFO: Pod client-containers-7262760b-6ae0-4537-add8-7318d3cbee17 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 14 09:24:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1691" for this suite. 12/14/22 09:24:15.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:15.991
Dec 14 09:24:15.991: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 12/14/22 09:24:15.992
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:16.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:16.007
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 12/14/22 09:24:16.012
STEP: Ensuring active pods == parallelism 12/14/22 09:24:16.018
STEP: delete a job 12/14/22 09:24:18.024
STEP: deleting Job.batch foo in namespace job-7428, will wait for the garbage collector to delete the pods 12/14/22 09:24:18.024
Dec 14 09:24:18.085: INFO: Deleting Job.batch foo took: 5.896206ms
Dec 14 09:24:18.185: INFO: Terminating Job.batch foo pods took: 100.258358ms
STEP: Ensuring job was deleted 12/14/22 09:24:50.386
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 14 09:24:50.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7428" for this suite. 12/14/22 09:24:50.397
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":289,"skipped":5328,"failed":0}
------------------------------
• [34.412 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:15.991
    Dec 14 09:24:15.991: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 12/14/22 09:24:15.992
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:16.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:16.007
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 12/14/22 09:24:16.012
    STEP: Ensuring active pods == parallelism 12/14/22 09:24:16.018
    STEP: delete a job 12/14/22 09:24:18.024
    STEP: deleting Job.batch foo in namespace job-7428, will wait for the garbage collector to delete the pods 12/14/22 09:24:18.024
    Dec 14 09:24:18.085: INFO: Deleting Job.batch foo took: 5.896206ms
    Dec 14 09:24:18.185: INFO: Terminating Job.batch foo pods took: 100.258358ms
    STEP: Ensuring job was deleted 12/14/22 09:24:50.386
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 14 09:24:50.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7428" for this suite. 12/14/22 09:24:50.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:50.403
Dec 14 09:24:50.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 12/14/22 09:24:50.404
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:50.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:50.419
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 12/14/22 09:24:50.424
STEP: Ensuring job reaches completions 12/14/22 09:24:50.428
STEP: Ensuring pods with index for job exist 12/14/22 09:24:58.434
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 14 09:24:58.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2183" for this suite. 12/14/22 09:24:58.445
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":290,"skipped":5340,"failed":0}
------------------------------
• [8.046 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:50.403
    Dec 14 09:24:50.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 12/14/22 09:24:50.404
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:50.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:50.419
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 12/14/22 09:24:50.424
    STEP: Ensuring job reaches completions 12/14/22 09:24:50.428
    STEP: Ensuring pods with index for job exist 12/14/22 09:24:58.434
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 14 09:24:58.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2183" for this suite. 12/14/22 09:24:58.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:58.45
Dec 14 09:24:58.450: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 12/14/22 09:24:58.45
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:58.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:58.465
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 12/14/22 09:24:58.47
STEP: listing all events in all namespaces 12/14/22 09:24:58.474
STEP: patching the test event 12/14/22 09:24:58.478
STEP: fetching the test event 12/14/22 09:24:58.483
STEP: updating the test event 12/14/22 09:24:58.486
STEP: getting the test event 12/14/22 09:24:58.493
STEP: deleting the test event 12/14/22 09:24:58.496
STEP: listing all events in all namespaces 12/14/22 09:24:58.5
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Dec 14 09:24:58.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9386" for this suite. 12/14/22 09:24:58.508
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":291,"skipped":5359,"failed":0}
------------------------------
• [0.062 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:58.45
    Dec 14 09:24:58.450: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 12/14/22 09:24:58.45
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:58.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:58.465
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 12/14/22 09:24:58.47
    STEP: listing all events in all namespaces 12/14/22 09:24:58.474
    STEP: patching the test event 12/14/22 09:24:58.478
    STEP: fetching the test event 12/14/22 09:24:58.483
    STEP: updating the test event 12/14/22 09:24:58.486
    STEP: getting the test event 12/14/22 09:24:58.493
    STEP: deleting the test event 12/14/22 09:24:58.496
    STEP: listing all events in all namespaces 12/14/22 09:24:58.5
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Dec 14 09:24:58.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9386" for this suite. 12/14/22 09:24:58.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:24:58.512
Dec 14 09:24:58.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:24:58.513
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:58.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:58.529
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:24:58.542
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:24:58.986
STEP: Deploying the webhook pod 12/14/22 09:24:58.99
STEP: Wait for the deployment to be ready 12/14/22 09:24:58.998
Dec 14 09:24:59.007: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:25:01.02
STEP: Verifying the service has paired with the endpoint 12/14/22 09:25:01.031
Dec 14 09:25:02.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 12/14/22 09:25:02.036
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:25:02.113
STEP: Updating a validating webhook configuration's rules to not include the create operation 12/14/22 09:25:02.222
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:25:02.231
STEP: Patching a validating webhook configuration's rules to include the create operation 12/14/22 09:25:02.239
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:25:02.245
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:25:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1149" for this suite. 12/14/22 09:25:02.373
STEP: Destroying namespace "webhook-1149-markers" for this suite. 12/14/22 09:25:02.377
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":292,"skipped":5371,"failed":0}
------------------------------
• [3.892 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:24:58.512
    Dec 14 09:24:58.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:24:58.513
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:24:58.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:24:58.529
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:24:58.542
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:24:58.986
    STEP: Deploying the webhook pod 12/14/22 09:24:58.99
    STEP: Wait for the deployment to be ready 12/14/22 09:24:58.998
    Dec 14 09:24:59.007: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:25:01.02
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:25:01.031
    Dec 14 09:25:02.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 12/14/22 09:25:02.036
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:25:02.113
    STEP: Updating a validating webhook configuration's rules to not include the create operation 12/14/22 09:25:02.222
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:25:02.231
    STEP: Patching a validating webhook configuration's rules to include the create operation 12/14/22 09:25:02.239
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 09:25:02.245
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:25:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1149" for this suite. 12/14/22 09:25:02.373
    STEP: Destroying namespace "webhook-1149-markers" for this suite. 12/14/22 09:25:02.377
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:25:02.405
Dec 14 09:25:02.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:25:02.405
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:25:02.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:25:02.42
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7cf3f61d-fa56-4961-932e-f5a59966aa91 12/14/22 09:25:02.427
STEP: Creating the pod 12/14/22 09:25:02.434
Dec 14 09:25:02.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1" in namespace "projected-4028" to be "running and ready"
Dec 14 09:25:02.444: INFO: Pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.892507ms
Dec 14 09:25:02.444: INFO: The phase of Pod pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:25:04.449: INFO: Pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007346121s
Dec 14 09:25:04.449: INFO: The phase of Pod pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1 is Running (Ready = true)
Dec 14 09:25:04.449: INFO: Pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-7cf3f61d-fa56-4961-932e-f5a59966aa91 12/14/22 09:25:04.51
STEP: waiting to observe update in volume 12/14/22 09:25:04.515
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 09:26:10.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4028" for this suite. 12/14/22 09:26:10.983
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":293,"skipped":5397,"failed":0}
------------------------------
• [68.583 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:25:02.405
    Dec 14 09:25:02.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:25:02.405
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:25:02.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:25:02.42
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-7cf3f61d-fa56-4961-932e-f5a59966aa91 12/14/22 09:25:02.427
    STEP: Creating the pod 12/14/22 09:25:02.434
    Dec 14 09:25:02.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1" in namespace "projected-4028" to be "running and ready"
    Dec 14 09:25:02.444: INFO: Pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.892507ms
    Dec 14 09:25:02.444: INFO: The phase of Pod pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:25:04.449: INFO: Pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007346121s
    Dec 14 09:25:04.449: INFO: The phase of Pod pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1 is Running (Ready = true)
    Dec 14 09:25:04.449: INFO: Pod "pod-projected-configmaps-0e154b88-4475-419f-aa5a-00198eb34ea1" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-7cf3f61d-fa56-4961-932e-f5a59966aa91 12/14/22 09:25:04.51
    STEP: waiting to observe update in volume 12/14/22 09:25:04.515
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 09:26:10.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4028" for this suite. 12/14/22 09:26:10.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:26:10.99
Dec 14 09:26:10.990: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:26:10.991
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:26:11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:26:11.004
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-82f5b282-bc8a-4c55-b0be-f52a22d43815 12/14/22 09:26:11.008
STEP: Creating a pod to test consume configMaps 12/14/22 09:26:11.012
Dec 14 09:26:11.023: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6" in namespace "projected-291" to be "Succeeded or Failed"
Dec 14 09:26:11.027: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007959ms
Dec 14 09:26:13.032: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008545262s
Dec 14 09:26:15.032: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009345667s
STEP: Saw pod success 12/14/22 09:26:15.032
Dec 14 09:26:15.033: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6" satisfied condition "Succeeded or Failed"
Dec 14 09:26:15.036: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:26:15.047
Dec 14 09:26:15.056: INFO: Waiting for pod pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6 to disappear
Dec 14 09:26:15.059: INFO: Pod pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 14 09:26:15.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-291" for this suite. 12/14/22 09:26:15.064
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":294,"skipped":5463,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:26:10.99
    Dec 14 09:26:10.990: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:26:10.991
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:26:11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:26:11.004
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-82f5b282-bc8a-4c55-b0be-f52a22d43815 12/14/22 09:26:11.008
    STEP: Creating a pod to test consume configMaps 12/14/22 09:26:11.012
    Dec 14 09:26:11.023: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6" in namespace "projected-291" to be "Succeeded or Failed"
    Dec 14 09:26:11.027: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007959ms
    Dec 14 09:26:13.032: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008545262s
    Dec 14 09:26:15.032: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009345667s
    STEP: Saw pod success 12/14/22 09:26:15.032
    Dec 14 09:26:15.033: INFO: Pod "pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6" satisfied condition "Succeeded or Failed"
    Dec 14 09:26:15.036: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:26:15.047
    Dec 14 09:26:15.056: INFO: Waiting for pod pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6 to disappear
    Dec 14 09:26:15.059: INFO: Pod pod-projected-configmaps-3a7882ce-bf87-4623-9577-49aebe90f4a6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 14 09:26:15.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-291" for this suite. 12/14/22 09:26:15.064
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:26:15.07
Dec 14 09:26:15.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 12/14/22 09:26:15.07
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:26:44.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:26:44.634
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Dec 14 09:26:44.646: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 14 09:26:49.650: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 09:26:49.65
Dec 14 09:26:49.650: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 14 09:26:51.655: INFO: Creating deployment "test-rollover-deployment"
Dec 14 09:26:51.662: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 14 09:26:53.669: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 14 09:26:53.675: INFO: Ensure that both replica sets have 1 created replica
Dec 14 09:26:53.680: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 14 09:26:53.697: INFO: Updating deployment test-rollover-deployment
Dec 14 09:26:53.697: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 14 09:26:55.703: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 14 09:26:55.710: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 14 09:26:55.714: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:26:55.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:26:57.722: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:26:57.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:26:59.723: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:26:59.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:27:01.724: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:27:01.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:27:03.722: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 09:27:03.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 09:27:05.722: INFO: 
Dec 14 09:27:05.722: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 09:27:05.730: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7138  17fb0303-aabc-4239-bf8e-79d28a870db4 45220 2 2022-12-14 09:26:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c08eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 09:26:51 +0000 UTC,LastTransitionTime:2022-12-14 09:26:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-12-14 09:27:04 +0000 UTC,LastTransitionTime:2022-12-14 09:26:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 09:27:05.733: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7138  2c84b572-cff0-45a4-acef-1fec2f08aec0 45213 2 2022-12-14 09:26:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 17fb0303-aabc-4239-bf8e-79d28a870db4 0xc003c094b7 0xc003c094b8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17fb0303-aabc-4239-bf8e-79d28a870db4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c09568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:27:05.733: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 14 09:27:05.733: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7138  087c203e-4560-464d-9cb4-57d7e9645311 45219 2 2022-12-14 09:26:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 17fb0303-aabc-4239-bf8e-79d28a870db4 0xc003c09267 0xc003c09268}] [] [{e2e.test Update apps/v1 2022-12-14 09:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17fb0303-aabc-4239-bf8e-79d28a870db4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c09328 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:27:05.733: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7138  33350e4a-3071-46c0-9d50-04189a6d6526 45152 2 2022-12-14 09:26:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 17fb0303-aabc-4239-bf8e-79d28a870db4 0xc003c09397 0xc003c09398}] [] [{kube-controller-manager Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17fb0303-aabc-4239-bf8e-79d28a870db4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c09448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 09:27:05.737: INFO: Pod "test-rollover-deployment-6d45fd857b-45hj8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-45hj8 test-rollover-deployment-6d45fd857b- deployment-7138  a4b263b3-0315-4906-915d-746c9eb01078 45162 0 2022-12-14 09:26:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:d19dbd82b3da16ff91bed12a055cbfd5eb06082e238d405c837fd1eeb17579ca cni.projectcalico.org/podIP:100.64.1.182/32 cni.projectcalico.org/podIPs:100.64.1.182/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 2c84b572-cff0-45a4-acef-1fec2f08aec0 0xc003c09b57 0xc003c09b58}] [] [{kube-controller-manager Update v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c84b572-cff0-45a4-acef-1fec2f08aec0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 09:26:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 09:26:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxj4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxj4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.182,StartTime:2022-12-14 09:26:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 09:26:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1835707fe061ba5132c45fa7197805edba08fe07aca697b69ffaf2f03ff53de2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 14 09:27:05.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7138" for this suite. 12/14/22 09:27:05.741
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":295,"skipped":5465,"failed":0}
------------------------------
• [50.675 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:26:15.07
    Dec 14 09:26:15.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 12/14/22 09:26:15.07
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:26:44.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:26:44.634
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Dec 14 09:26:44.646: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Dec 14 09:26:49.650: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 09:26:49.65
    Dec 14 09:26:49.650: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Dec 14 09:26:51.655: INFO: Creating deployment "test-rollover-deployment"
    Dec 14 09:26:51.662: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Dec 14 09:26:53.669: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Dec 14 09:26:53.675: INFO: Ensure that both replica sets have 1 created replica
    Dec 14 09:26:53.680: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Dec 14 09:26:53.697: INFO: Updating deployment test-rollover-deployment
    Dec 14 09:26:53.697: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Dec 14 09:26:55.703: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Dec 14 09:26:55.710: INFO: Make sure deployment "test-rollover-deployment" is complete
    Dec 14 09:26:55.714: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 09:26:55.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 09:26:57.722: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 09:26:57.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 09:26:59.723: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 09:26:59.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 09:27:01.724: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 09:27:01.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 09:27:03.722: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 09:27:03.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 9, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 9, 26, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 09:27:05.722: INFO: 
    Dec 14 09:27:05.722: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 09:27:05.730: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7138  17fb0303-aabc-4239-bf8e-79d28a870db4 45220 2 2022-12-14 09:26:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c08eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 09:26:51 +0000 UTC,LastTransitionTime:2022-12-14 09:26:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-12-14 09:27:04 +0000 UTC,LastTransitionTime:2022-12-14 09:26:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 09:27:05.733: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7138  2c84b572-cff0-45a4-acef-1fec2f08aec0 45213 2 2022-12-14 09:26:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 17fb0303-aabc-4239-bf8e-79d28a870db4 0xc003c094b7 0xc003c094b8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17fb0303-aabc-4239-bf8e-79d28a870db4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c09568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 09:27:05.733: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Dec 14 09:27:05.733: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7138  087c203e-4560-464d-9cb4-57d7e9645311 45219 2 2022-12-14 09:26:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 17fb0303-aabc-4239-bf8e-79d28a870db4 0xc003c09267 0xc003c09268}] [] [{e2e.test Update apps/v1 2022-12-14 09:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17fb0303-aabc-4239-bf8e-79d28a870db4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:27:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c09328 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 09:27:05.733: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7138  33350e4a-3071-46c0-9d50-04189a6d6526 45152 2 2022-12-14 09:26:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 17fb0303-aabc-4239-bf8e-79d28a870db4 0xc003c09397 0xc003c09398}] [] [{kube-controller-manager Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17fb0303-aabc-4239-bf8e-79d28a870db4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c09448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 09:27:05.737: INFO: Pod "test-rollover-deployment-6d45fd857b-45hj8" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-45hj8 test-rollover-deployment-6d45fd857b- deployment-7138  a4b263b3-0315-4906-915d-746c9eb01078 45162 0 2022-12-14 09:26:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:d19dbd82b3da16ff91bed12a055cbfd5eb06082e238d405c837fd1eeb17579ca cni.projectcalico.org/podIP:100.64.1.182/32 cni.projectcalico.org/podIPs:100.64.1.182/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 2c84b572-cff0-45a4-acef-1fec2f08aec0 0xc003c09b57 0xc003c09b58}] [] [{kube-controller-manager Update v1 2022-12-14 09:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c84b572-cff0-45a4-acef-1fec2f08aec0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-14 09:26:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-14 09:26:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxj4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxj4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 09:26:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.210,PodIP:100.64.1.182,StartTime:2022-12-14 09:26:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 09:26:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1835707fe061ba5132c45fa7197805edba08fe07aca697b69ffaf2f03ff53de2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 14 09:27:05.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7138" for this suite. 12/14/22 09:27:05.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:27:05.745
Dec 14 09:27:05.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 09:27:05.745
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:05.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:05.757
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Dec 14 09:27:05.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod 12/14/22 09:27:05.761
STEP: submitting the pod to kubernetes 12/14/22 09:27:05.761
Dec 14 09:27:05.770: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a" in namespace "pods-7526" to be "running and ready"
Dec 14 09:27:05.773: INFO: Pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637287ms
Dec 14 09:27:05.773: INFO: The phase of Pod pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:27:07.777: INFO: Pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006477316s
Dec 14 09:27:07.777: INFO: The phase of Pod pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a is Running (Ready = true)
Dec 14 09:27:07.777: INFO: Pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 09:27:07.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7526" for this suite. 12/14/22 09:27:07.801
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":296,"skipped":5471,"failed":0}
------------------------------
• [2.060 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:27:05.745
    Dec 14 09:27:05.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 09:27:05.745
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:05.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:05.757
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Dec 14 09:27:05.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating the pod 12/14/22 09:27:05.761
    STEP: submitting the pod to kubernetes 12/14/22 09:27:05.761
    Dec 14 09:27:05.770: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a" in namespace "pods-7526" to be "running and ready"
    Dec 14 09:27:05.773: INFO: Pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637287ms
    Dec 14 09:27:05.773: INFO: The phase of Pod pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:27:07.777: INFO: Pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006477316s
    Dec 14 09:27:07.777: INFO: The phase of Pod pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a is Running (Ready = true)
    Dec 14 09:27:07.777: INFO: Pod "pod-logs-websocket-12e548a3-a169-47fa-a020-a0b833a27a8a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 09:27:07.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7526" for this suite. 12/14/22 09:27:07.801
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:27:07.805
Dec 14 09:27:07.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 12/14/22 09:27:07.806
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:07.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:07.817
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Dec 14 09:27:07.821: INFO: Creating pod...
Dec 14 09:27:07.829: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3525" to be "running"
Dec 14 09:27:07.831: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586835ms
Dec 14 09:27:09.836: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007411591s
Dec 14 09:27:09.836: INFO: Pod "agnhost" satisfied condition "running"
Dec 14 09:27:09.836: INFO: Creating service...
Dec 14 09:27:09.844: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=DELETE
Dec 14 09:27:09.945: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 09:27:09.945: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=OPTIONS
Dec 14 09:27:09.990: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 09:27:09.990: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=PATCH
Dec 14 09:27:09.997: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 09:27:09.997: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=POST
Dec 14 09:27:10.004: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 09:27:10.004: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=PUT
Dec 14 09:27:10.010: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 09:27:10.010: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=DELETE
Dec 14 09:27:10.018: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 09:27:10.018: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=OPTIONS
Dec 14 09:27:10.025: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 09:27:10.026: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=PATCH
Dec 14 09:27:10.032: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 09:27:10.032: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=POST
Dec 14 09:27:10.039: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 09:27:10.040: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=PUT
Dec 14 09:27:10.047: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 09:27:10.047: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=GET
Dec 14 09:27:10.050: INFO: http.Client request:GET StatusCode:301
Dec 14 09:27:10.050: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=GET
Dec 14 09:27:10.053: INFO: http.Client request:GET StatusCode:301
Dec 14 09:27:10.053: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=HEAD
Dec 14 09:27:10.056: INFO: http.Client request:HEAD StatusCode:301
Dec 14 09:27:10.056: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=HEAD
Dec 14 09:27:10.059: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Dec 14 09:27:10.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3525" for this suite. 12/14/22 09:27:10.064
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":297,"skipped":5471,"failed":0}
------------------------------
• [2.263 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:27:07.805
    Dec 14 09:27:07.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 12/14/22 09:27:07.806
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:07.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:07.817
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Dec 14 09:27:07.821: INFO: Creating pod...
    Dec 14 09:27:07.829: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3525" to be "running"
    Dec 14 09:27:07.831: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586835ms
    Dec 14 09:27:09.836: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007411591s
    Dec 14 09:27:09.836: INFO: Pod "agnhost" satisfied condition "running"
    Dec 14 09:27:09.836: INFO: Creating service...
    Dec 14 09:27:09.844: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=DELETE
    Dec 14 09:27:09.945: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 09:27:09.945: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=OPTIONS
    Dec 14 09:27:09.990: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 09:27:09.990: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=PATCH
    Dec 14 09:27:09.997: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 09:27:09.997: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=POST
    Dec 14 09:27:10.004: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 09:27:10.004: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=PUT
    Dec 14 09:27:10.010: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 14 09:27:10.010: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=DELETE
    Dec 14 09:27:10.018: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 09:27:10.018: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Dec 14 09:27:10.025: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 09:27:10.026: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=PATCH
    Dec 14 09:27:10.032: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 09:27:10.032: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=POST
    Dec 14 09:27:10.039: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 09:27:10.040: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=PUT
    Dec 14 09:27:10.047: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 14 09:27:10.047: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=GET
    Dec 14 09:27:10.050: INFO: http.Client request:GET StatusCode:301
    Dec 14 09:27:10.050: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=GET
    Dec 14 09:27:10.053: INFO: http.Client request:GET StatusCode:301
    Dec 14 09:27:10.053: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/pods/agnhost/proxy?method=HEAD
    Dec 14 09:27:10.056: INFO: http.Client request:HEAD StatusCode:301
    Dec 14 09:27:10.056: INFO: Starting http.Client for https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3525/services/e2e-proxy-test-service/proxy?method=HEAD
    Dec 14 09:27:10.059: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Dec 14 09:27:10.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3525" for this suite. 12/14/22 09:27:10.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:27:10.07
Dec 14 09:27:10.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:27:10.071
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:10.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:10.084
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Dec 14 09:27:10.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 09:27:12.298
Dec 14 09:27:12.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 create -f -'
Dec 14 09:27:12.939: INFO: stderr: ""
Dec 14 09:27:12.939: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 09:27:12.939: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 delete e2e-test-crd-publish-openapi-5860-crds test-cr'
Dec 14 09:27:13.003: INFO: stderr: ""
Dec 14 09:27:13.003: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 14 09:27:13.003: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 apply -f -'
Dec 14 09:27:13.201: INFO: stderr: ""
Dec 14 09:27:13.201: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 09:27:13.201: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 delete e2e-test-crd-publish-openapi-5860-crds test-cr'
Dec 14 09:27:13.264: INFO: stderr: ""
Dec 14 09:27:13.264: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/14/22 09:27:13.264
Dec 14 09:27:13.264: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 explain e2e-test-crd-publish-openapi-5860-crds'
Dec 14 09:27:13.443: INFO: stderr: ""
Dec 14 09:27:13.443: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5860-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:27:15.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5947" for this suite. 12/14/22 09:27:15.707
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":298,"skipped":5516,"failed":0}
------------------------------
• [5.641 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:27:10.07
    Dec 14 09:27:10.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:27:10.071
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:10.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:10.084
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Dec 14 09:27:10.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 09:27:12.298
    Dec 14 09:27:12.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 create -f -'
    Dec 14 09:27:12.939: INFO: stderr: ""
    Dec 14 09:27:12.939: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 14 09:27:12.939: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 delete e2e-test-crd-publish-openapi-5860-crds test-cr'
    Dec 14 09:27:13.003: INFO: stderr: ""
    Dec 14 09:27:13.003: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Dec 14 09:27:13.003: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 apply -f -'
    Dec 14 09:27:13.201: INFO: stderr: ""
    Dec 14 09:27:13.201: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 14 09:27:13.201: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 --namespace=crd-publish-openapi-5947 delete e2e-test-crd-publish-openapi-5860-crds test-cr'
    Dec 14 09:27:13.264: INFO: stderr: ""
    Dec 14 09:27:13.264: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/14/22 09:27:13.264
    Dec 14 09:27:13.264: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5947 explain e2e-test-crd-publish-openapi-5860-crds'
    Dec 14 09:27:13.443: INFO: stderr: ""
    Dec 14 09:27:13.443: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5860-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:27:15.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5947" for this suite. 12/14/22 09:27:15.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:27:15.712
Dec 14 09:27:15.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 12/14/22 09:27:15.712
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:15.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:15.723
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/14/22 09:27:15.734
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/14/22 09:27:34.855
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/14/22 09:27:34.857
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/14/22 09:27:34.862
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/14/22 09:27:34.863
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/14/22 09:27:34.878
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/14/22 09:27:36.889
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/14/22 09:27:38.899
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/14/22 09:27:38.904
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/14/22 09:27:38.904
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/14/22 09:27:38.919
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/14/22 09:27:39.926
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/14/22 09:27:41.937
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/14/22 09:27:41.942
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/14/22 09:27:41.942
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 14 09:27:41.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2491" for this suite. 12/14/22 09:27:41.991
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":299,"skipped":5545,"failed":0}
------------------------------
• [26.283 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:27:15.712
    Dec 14 09:27:15.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 12/14/22 09:27:15.712
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:15.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:15.723
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/14/22 09:27:15.734
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/14/22 09:27:34.855
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/14/22 09:27:34.857
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/14/22 09:27:34.862
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/14/22 09:27:34.863
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/14/22 09:27:34.878
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/14/22 09:27:36.889
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/14/22 09:27:38.899
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/14/22 09:27:38.904
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/14/22 09:27:38.904
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/14/22 09:27:38.919
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/14/22 09:27:39.926
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/14/22 09:27:41.937
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/14/22 09:27:41.942
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/14/22 09:27:41.942
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 14 09:27:41.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2491" for this suite. 12/14/22 09:27:41.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:27:41.997
Dec 14 09:27:41.997: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:27:41.997
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:42.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:42.009
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-2df696ba-6185-4b72-b48b-9e48746351e6 12/14/22 09:27:42.011
STEP: Creating a pod to test consume secrets 12/14/22 09:27:42.014
Dec 14 09:27:42.022: INFO: Waiting up to 5m0s for pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9" in namespace "secrets-5787" to be "Succeeded or Failed"
Dec 14 09:27:42.025: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421613ms
Dec 14 09:27:44.029: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006947159s
Dec 14 09:27:46.029: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006634711s
STEP: Saw pod success 12/14/22 09:27:46.029
Dec 14 09:27:46.029: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9" satisfied condition "Succeeded or Failed"
Dec 14 09:27:46.031: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:27:46.081
Dec 14 09:27:46.087: INFO: Waiting for pod pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9 to disappear
Dec 14 09:27:46.090: INFO: Pod pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:27:46.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5787" for this suite. 12/14/22 09:27:46.094
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":300,"skipped":5639,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:27:41.997
    Dec 14 09:27:41.997: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:27:41.997
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:42.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:42.009
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-2df696ba-6185-4b72-b48b-9e48746351e6 12/14/22 09:27:42.011
    STEP: Creating a pod to test consume secrets 12/14/22 09:27:42.014
    Dec 14 09:27:42.022: INFO: Waiting up to 5m0s for pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9" in namespace "secrets-5787" to be "Succeeded or Failed"
    Dec 14 09:27:42.025: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421613ms
    Dec 14 09:27:44.029: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006947159s
    Dec 14 09:27:46.029: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006634711s
    STEP: Saw pod success 12/14/22 09:27:46.029
    Dec 14 09:27:46.029: INFO: Pod "pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9" satisfied condition "Succeeded or Failed"
    Dec 14 09:27:46.031: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:27:46.081
    Dec 14 09:27:46.087: INFO: Waiting for pod pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9 to disappear
    Dec 14 09:27:46.090: INFO: Pod pod-secrets-2f5183fe-870d-401d-bc3a-c7d6f4a520a9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:27:46.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5787" for this suite. 12/14/22 09:27:46.094
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:27:46.098
Dec 14 09:27:46.098: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:27:46.099
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:46.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:46.109
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Dec 14 09:27:46.121: INFO: created pod
Dec 14 09:27:46.121: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6103" to be "Succeeded or Failed"
Dec 14 09:27:46.124: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386498ms
Dec 14 09:27:48.127: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.00582296s
Dec 14 09:27:50.128: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006211412s
STEP: Saw pod success 12/14/22 09:27:50.128
Dec 14 09:27:50.128: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec 14 09:28:20.129: INFO: polling logs
W1214 09:28:51.541396    4635 reflector.go:347] test/e2e/node/taints.go:147: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1214 09:28:51.541463    4635 reflector.go:347] test/e2e/node/taints.go:147: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
Dec 14 09:28:51.541: INFO: Error pulling logs: Get "https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/svcaccounts-6103/pods/oidc-discovery-validator/log?container=oidc-discovery-validator&previous=false": http2: client connection lost
Dec 14 09:29:20.128: INFO: polling logs
Dec 14 09:29:20.301: INFO: Pod logs: 
I1214 09:27:46.714752       1 log.go:195] OK: Got token
I1214 09:27:46.714888       1 log.go:195] validating with in-cluster discovery
I1214 09:27:46.715557       1 log.go:195] OK: got issuer https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com
I1214 09:27:46.715645       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-6103:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671010666, NotBefore:1671010066, IssuedAt:1671010066, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6103", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fa3374f-3d44-4357-a137-56e53fe41d78"}}}
I1214 09:27:46.729581       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com
I1214 09:27:46.734084       1 log.go:195] OK: Validated signature on JWT
I1214 09:27:46.734247       1 log.go:195] OK: Got valid claims from token!
I1214 09:27:46.734302       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-6103:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671010666, NotBefore:1671010066, IssuedAt:1671010066, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6103", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fa3374f-3d44-4357-a137-56e53fe41d78"}}}

Dec 14 09:29:20.301: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 14 09:29:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6103" for this suite. 12/14/22 09:29:20.314
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":301,"skipped":5643,"failed":0}
------------------------------
• [94.222 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:27:46.098
    Dec 14 09:27:46.098: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:27:46.099
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:27:46.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:27:46.109
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Dec 14 09:27:46.121: INFO: created pod
    Dec 14 09:27:46.121: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6103" to be "Succeeded or Failed"
    Dec 14 09:27:46.124: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386498ms
    Dec 14 09:27:48.127: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.00582296s
    Dec 14 09:27:50.128: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006211412s
    STEP: Saw pod success 12/14/22 09:27:50.128
    Dec 14 09:27:50.128: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Dec 14 09:28:20.129: INFO: polling logs
    W1214 09:28:51.541396    4635 reflector.go:347] test/e2e/node/taints.go:147: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
    W1214 09:28:51.541463    4635 reflector.go:347] test/e2e/node/taints.go:147: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
    Dec 14 09:28:51.541: INFO: Error pulling logs: Get "https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/svcaccounts-6103/pods/oidc-discovery-validator/log?container=oidc-discovery-validator&previous=false": http2: client connection lost
    Dec 14 09:29:20.128: INFO: polling logs
    Dec 14 09:29:20.301: INFO: Pod logs: 
    I1214 09:27:46.714752       1 log.go:195] OK: Got token
    I1214 09:27:46.714888       1 log.go:195] validating with in-cluster discovery
    I1214 09:27:46.715557       1 log.go:195] OK: got issuer https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com
    I1214 09:27:46.715645       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-6103:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671010666, NotBefore:1671010066, IssuedAt:1671010066, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6103", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fa3374f-3d44-4357-a137-56e53fe41d78"}}}
    I1214 09:27:46.729581       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com
    I1214 09:27:46.734084       1 log.go:195] OK: Validated signature on JWT
    I1214 09:27:46.734247       1 log.go:195] OK: Got valid claims from token!
    I1214 09:27:46.734302       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tm5on-jne.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-6103:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671010666, NotBefore:1671010066, IssuedAt:1671010066, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6103", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fa3374f-3d44-4357-a137-56e53fe41d78"}}}

    Dec 14 09:29:20.301: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 14 09:29:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6103" for this suite. 12/14/22 09:29:20.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:29:20.321
Dec 14 09:29:20.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 12/14/22 09:29:20.322
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:20.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:20.343
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 12/14/22 09:29:20.348
STEP: Replace a pod template 12/14/22 09:29:20.353
Dec 14 09:29:20.364: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Dec 14 09:29:20.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5266" for this suite. 12/14/22 09:29:20.37
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":302,"skipped":5655,"failed":0}
------------------------------
• [0.055 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:29:20.321
    Dec 14 09:29:20.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 12/14/22 09:29:20.322
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:20.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:20.343
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 12/14/22 09:29:20.348
    STEP: Replace a pod template 12/14/22 09:29:20.353
    Dec 14 09:29:20.364: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Dec 14 09:29:20.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5266" for this suite. 12/14/22 09:29:20.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:29:20.376
Dec 14 09:29:20.376: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:29:20.377
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:20.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:20.395
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 12/14/22 09:29:20.401
Dec 14 09:29:20.401: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-541 proxy --unix-socket=/tmp/kubectl-proxy-unix4216787740/test'
STEP: retrieving proxy /api/ output 12/14/22 09:29:20.447
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:29:20.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-541" for this suite. 12/14/22 09:29:20.454
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":303,"skipped":5674,"failed":0}
------------------------------
• [0.086 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:29:20.376
    Dec 14 09:29:20.376: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:29:20.377
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:20.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:20.395
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 12/14/22 09:29:20.401
    Dec 14 09:29:20.401: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-541 proxy --unix-socket=/tmp/kubectl-proxy-unix4216787740/test'
    STEP: retrieving proxy /api/ output 12/14/22 09:29:20.447
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:29:20.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-541" for this suite. 12/14/22 09:29:20.454
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:29:20.462
Dec 14 09:29:20.462: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:29:20.463
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:20.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:20.481
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:29:20.486
Dec 14 09:29:20.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7044 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 14 09:29:20.580: INFO: stderr: ""
Dec 14 09:29:20.580: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 12/14/22 09:29:20.58
Dec 14 09:29:20.580: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7044 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Dec 14 09:29:21.556: INFO: stderr: ""
Dec 14 09:29:21.556: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:29:21.556
Dec 14 09:29:21.570: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7044 delete pods e2e-test-httpd-pod'
Dec 14 09:29:53.019: INFO: stderr: ""
Dec 14 09:29:53.019: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:29:53.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7044" for this suite. 12/14/22 09:29:53.025
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":304,"skipped":5676,"failed":0}
------------------------------
• [32.568 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:29:20.462
    Dec 14 09:29:20.462: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:29:20.463
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:20.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:20.481
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:29:20.486
    Dec 14 09:29:20.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7044 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 14 09:29:20.580: INFO: stderr: ""
    Dec 14 09:29:20.580: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 12/14/22 09:29:20.58
    Dec 14 09:29:20.580: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7044 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Dec 14 09:29:21.556: INFO: stderr: ""
    Dec 14 09:29:21.556: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:29:21.556
    Dec 14 09:29:21.570: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7044 delete pods e2e-test-httpd-pod'
    Dec 14 09:29:53.019: INFO: stderr: ""
    Dec 14 09:29:53.019: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:29:53.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7044" for this suite. 12/14/22 09:29:53.025
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:29:53.03
Dec 14 09:29:53.030: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/14/22 09:29:53.031
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:53.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:53.047
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 12/14/22 09:29:53.053
STEP: Creating hostNetwork=false pod 12/14/22 09:29:53.053
Dec 14 09:29:53.064: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8727" to be "running and ready"
Dec 14 09:29:53.079: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.814085ms
Dec 14 09:29:53.079: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:29:55.086: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02210695s
Dec 14 09:29:55.086: INFO: The phase of Pod test-pod is Running (Ready = true)
Dec 14 09:29:55.086: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 12/14/22 09:29:55.093
Dec 14 09:29:55.106: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8727" to be "running and ready"
Dec 14 09:29:55.112: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.607387ms
Dec 14 09:29:55.112: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:29:57.118: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012298242s
Dec 14 09:29:57.118: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Dec 14 09:29:57.118: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 12/14/22 09:29:57.122
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/14/22 09:29:57.122
Dec 14 09:29:57.122: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:57.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:57.123: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:57.123: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 09:29:57.440: INFO: Exec stderr: ""
Dec 14 09:29:57.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:57.440: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:57.441: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:57.441: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 09:29:57.837: INFO: Exec stderr: ""
Dec 14 09:29:57.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:57.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:57.837: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:57.838: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 09:29:58.161: INFO: Exec stderr: ""
Dec 14 09:29:58.161: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:58.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:58.162: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:58.162: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 09:29:58.608: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/14/22 09:29:58.608
Dec 14 09:29:58.608: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:58.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:58.609: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:58.609: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 14 09:29:59.097: INFO: Exec stderr: ""
Dec 14 09:29:59.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:59.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:59.097: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:59.097: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 14 09:29:59.493: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/14/22 09:29:59.493
Dec 14 09:29:59.493: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:59.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:59.494: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:59.494: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 09:29:59.945: INFO: Exec stderr: ""
Dec 14 09:29:59.945: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:29:59.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:29:59.945: INFO: ExecWithOptions: Clientset creation
Dec 14 09:29:59.945: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 09:30:00.406: INFO: Exec stderr: ""
Dec 14 09:30:00.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:30:00.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:30:00.407: INFO: ExecWithOptions: Clientset creation
Dec 14 09:30:00.407: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 09:30:00.820: INFO: Exec stderr: ""
Dec 14 09:30:00.820: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:30:00.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:30:00.821: INFO: ExecWithOptions: Clientset creation
Dec 14 09:30:00.821: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 09:30:01.288: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Dec 14 09:30:01.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8727" for this suite. 12/14/22 09:30:01.296
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":305,"skipped":5677,"failed":0}
------------------------------
• [8.271 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:29:53.03
    Dec 14 09:29:53.030: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/14/22 09:29:53.031
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:29:53.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:29:53.047
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 12/14/22 09:29:53.053
    STEP: Creating hostNetwork=false pod 12/14/22 09:29:53.053
    Dec 14 09:29:53.064: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8727" to be "running and ready"
    Dec 14 09:29:53.079: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.814085ms
    Dec 14 09:29:53.079: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:29:55.086: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02210695s
    Dec 14 09:29:55.086: INFO: The phase of Pod test-pod is Running (Ready = true)
    Dec 14 09:29:55.086: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 12/14/22 09:29:55.093
    Dec 14 09:29:55.106: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8727" to be "running and ready"
    Dec 14 09:29:55.112: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.607387ms
    Dec 14 09:29:55.112: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:29:57.118: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012298242s
    Dec 14 09:29:57.118: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Dec 14 09:29:57.118: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 12/14/22 09:29:57.122
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/14/22 09:29:57.122
    Dec 14 09:29:57.122: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:57.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:57.123: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:57.123: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 09:29:57.440: INFO: Exec stderr: ""
    Dec 14 09:29:57.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:57.440: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:57.441: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:57.441: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 09:29:57.837: INFO: Exec stderr: ""
    Dec 14 09:29:57.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:57.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:57.837: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:57.838: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 09:29:58.161: INFO: Exec stderr: ""
    Dec 14 09:29:58.161: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:58.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:58.162: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:58.162: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 09:29:58.608: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/14/22 09:29:58.608
    Dec 14 09:29:58.608: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:58.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:58.609: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:58.609: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 14 09:29:59.097: INFO: Exec stderr: ""
    Dec 14 09:29:59.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:59.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:59.097: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:59.097: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 14 09:29:59.493: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/14/22 09:29:59.493
    Dec 14 09:29:59.493: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:59.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:59.494: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:59.494: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 09:29:59.945: INFO: Exec stderr: ""
    Dec 14 09:29:59.945: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:29:59.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:29:59.945: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:29:59.945: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 09:30:00.406: INFO: Exec stderr: ""
    Dec 14 09:30:00.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:30:00.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:30:00.407: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:30:00.407: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 09:30:00.820: INFO: Exec stderr: ""
    Dec 14 09:30:00.820: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:30:00.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:30:00.821: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:30:00.821: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-8727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 09:30:01.288: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Dec 14 09:30:01.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8727" for this suite. 12/14/22 09:30:01.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:30:01.302
Dec 14 09:30:01.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 12/14/22 09:30:01.303
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:01.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:01.325
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 in namespace container-probe-9250 12/14/22 09:30:01.33
Dec 14 09:30:01.342: INFO: Waiting up to 5m0s for pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2" in namespace "container-probe-9250" to be "not pending"
Dec 14 09:30:01.347: INFO: Pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660486ms
Dec 14 09:30:03.353: INFO: Pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010460924s
Dec 14 09:30:03.353: INFO: Pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2" satisfied condition "not pending"
Dec 14 09:30:03.353: INFO: Started pod liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 in namespace container-probe-9250
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 09:30:03.353
Dec 14 09:30:03.357: INFO: Initial restart count of pod liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 is 0
Dec 14 09:30:23.786: INFO: Restart count of pod container-probe-9250/liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 is now 1 (20.428894267s elapsed)
STEP: deleting the pod 12/14/22 09:30:23.786
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 14 09:30:23.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9250" for this suite. 12/14/22 09:30:23.802
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":306,"skipped":5697,"failed":0}
------------------------------
• [22.504 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:30:01.302
    Dec 14 09:30:01.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 12/14/22 09:30:01.303
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:01.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:01.325
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 in namespace container-probe-9250 12/14/22 09:30:01.33
    Dec 14 09:30:01.342: INFO: Waiting up to 5m0s for pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2" in namespace "container-probe-9250" to be "not pending"
    Dec 14 09:30:01.347: INFO: Pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660486ms
    Dec 14 09:30:03.353: INFO: Pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010460924s
    Dec 14 09:30:03.353: INFO: Pod "liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2" satisfied condition "not pending"
    Dec 14 09:30:03.353: INFO: Started pod liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 in namespace container-probe-9250
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 09:30:03.353
    Dec 14 09:30:03.357: INFO: Initial restart count of pod liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 is 0
    Dec 14 09:30:23.786: INFO: Restart count of pod container-probe-9250/liveness-23bc5b36-bbe9-4014-8f1a-07baef4533f2 is now 1 (20.428894267s elapsed)
    STEP: deleting the pod 12/14/22 09:30:23.786
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 14 09:30:23.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9250" for this suite. 12/14/22 09:30:23.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:30:23.807
Dec 14 09:30:23.807: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:30:23.808
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:23.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:23.826
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 12/14/22 09:30:23.832
STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:30:23.838
STEP: Creating a ResourceQuota with not best effort scope 12/14/22 09:30:25.843
STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:30:25.847
STEP: Creating a best-effort pod 12/14/22 09:30:27.853
STEP: Ensuring resource quota with best effort scope captures the pod usage 12/14/22 09:30:27.868
STEP: Ensuring resource quota with not best effort ignored the pod usage 12/14/22 09:30:29.873
STEP: Deleting the pod 12/14/22 09:30:31.879
STEP: Ensuring resource quota status released the pod usage 12/14/22 09:30:31.885
STEP: Creating a not best-effort pod 12/14/22 09:30:33.89
STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/14/22 09:30:33.903
STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/14/22 09:30:35.909
STEP: Deleting the pod 12/14/22 09:30:37.914
STEP: Ensuring resource quota status released the pod usage 12/14/22 09:30:37.922
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:30:39.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-888" for this suite. 12/14/22 09:30:39.935
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":307,"skipped":5724,"failed":0}
------------------------------
• [16.135 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:30:23.807
    Dec 14 09:30:23.807: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:30:23.808
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:23.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:23.826
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 12/14/22 09:30:23.832
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:30:23.838
    STEP: Creating a ResourceQuota with not best effort scope 12/14/22 09:30:25.843
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:30:25.847
    STEP: Creating a best-effort pod 12/14/22 09:30:27.853
    STEP: Ensuring resource quota with best effort scope captures the pod usage 12/14/22 09:30:27.868
    STEP: Ensuring resource quota with not best effort ignored the pod usage 12/14/22 09:30:29.873
    STEP: Deleting the pod 12/14/22 09:30:31.879
    STEP: Ensuring resource quota status released the pod usage 12/14/22 09:30:31.885
    STEP: Creating a not best-effort pod 12/14/22 09:30:33.89
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/14/22 09:30:33.903
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/14/22 09:30:35.909
    STEP: Deleting the pod 12/14/22 09:30:37.914
    STEP: Ensuring resource quota status released the pod usage 12/14/22 09:30:37.922
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:30:39.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-888" for this suite. 12/14/22 09:30:39.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:30:39.942
Dec 14 09:30:39.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 12/14/22 09:30:39.943
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:39.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:39.958
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 12/14/22 09:30:39.963
STEP: wait for the container to reach Succeeded 12/14/22 09:30:39.973
STEP: get the container status 12/14/22 09:30:43.997
STEP: the container should be terminated 12/14/22 09:30:44.001
STEP: the termination message should be set 12/14/22 09:30:44.001
Dec 14 09:30:44.001: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 12/14/22 09:30:44.001
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 14 09:30:44.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6935" for this suite. 12/14/22 09:30:44.022
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":308,"skipped":5730,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:30:39.942
    Dec 14 09:30:39.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 12/14/22 09:30:39.943
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:39.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:39.958
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 12/14/22 09:30:39.963
    STEP: wait for the container to reach Succeeded 12/14/22 09:30:39.973
    STEP: get the container status 12/14/22 09:30:43.997
    STEP: the container should be terminated 12/14/22 09:30:44.001
    STEP: the termination message should be set 12/14/22 09:30:44.001
    Dec 14 09:30:44.001: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 12/14/22 09:30:44.001
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 14 09:30:44.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6935" for this suite. 12/14/22 09:30:44.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:30:44.029
Dec 14 09:30:44.029: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:30:44.03
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:44.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:44.046
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 12/14/22 09:30:44.052
Dec 14 09:30:44.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version 12/14/22 09:30:51.153
STEP: check the new version name is served 12/14/22 09:30:51.171
STEP: check the old version name is removed 12/14/22 09:30:54.278
STEP: check the other version is not changed 12/14/22 09:30:55.772
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:31:02.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1266" for this suite. 12/14/22 09:31:02.372
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":309,"skipped":5760,"failed":0}
------------------------------
• [18.348 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:30:44.029
    Dec 14 09:30:44.029: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 09:30:44.03
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:30:44.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:30:44.046
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 12/14/22 09:30:44.052
    Dec 14 09:30:44.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: rename a version 12/14/22 09:30:51.153
    STEP: check the new version name is served 12/14/22 09:30:51.171
    STEP: check the old version name is removed 12/14/22 09:30:54.278
    STEP: check the other version is not changed 12/14/22 09:30:55.772
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:31:02.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1266" for this suite. 12/14/22 09:31:02.372
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:31:02.377
Dec 14 09:31:02.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 12/14/22 09:31:02.377
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:02.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:02.395
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2261 12/14/22 09:31:02.399
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Dec 14 09:31:02.413: INFO: Found 0 stateful pods, waiting for 1
Dec 14 09:31:12.419: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 12/14/22 09:31:12.424
W1214 09:31:12.433076    4635 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 14 09:31:12.438: INFO: Found 1 stateful pods, waiting for 2
Dec 14 09:31:22.445: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 09:31:22.445: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 12/14/22 09:31:22.452
STEP: Delete all of the StatefulSets 12/14/22 09:31:22.455
STEP: Verify that StatefulSets have been deleted 12/14/22 09:31:22.46
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 09:31:22.463: INFO: Deleting all statefulset in ns statefulset-2261
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 14 09:31:22.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2261" for this suite. 12/14/22 09:31:22.474
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":310,"skipped":5761,"failed":0}
------------------------------
• [20.104 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:31:02.377
    Dec 14 09:31:02.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 12/14/22 09:31:02.377
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:02.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:02.395
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2261 12/14/22 09:31:02.399
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Dec 14 09:31:02.413: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 09:31:12.419: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 12/14/22 09:31:12.424
    W1214 09:31:12.433076    4635 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 14 09:31:12.438: INFO: Found 1 stateful pods, waiting for 2
    Dec 14 09:31:22.445: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 09:31:22.445: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 12/14/22 09:31:22.452
    STEP: Delete all of the StatefulSets 12/14/22 09:31:22.455
    STEP: Verify that StatefulSets have been deleted 12/14/22 09:31:22.46
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 14 09:31:22.463: INFO: Deleting all statefulset in ns statefulset-2261
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 14 09:31:22.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2261" for this suite. 12/14/22 09:31:22.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:31:22.481
Dec 14 09:31:22.481: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 12/14/22 09:31:22.481
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:22.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:22.5
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 12/14/22 09:31:22.504
Dec 14 09:31:22.514: INFO: Waiting up to 5m0s for pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856" in namespace "var-expansion-67" to be "Succeeded or Failed"
Dec 14 09:31:22.517: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905524ms
Dec 14 09:31:24.522: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008235325s
Dec 14 09:31:26.522: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007885569s
STEP: Saw pod success 12/14/22 09:31:26.522
Dec 14 09:31:26.522: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856" satisfied condition "Succeeded or Failed"
Dec 14 09:31:26.526: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856 container dapi-container: <nil>
STEP: delete the pod 12/14/22 09:31:26.689
Dec 14 09:31:26.698: INFO: Waiting for pod var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856 to disappear
Dec 14 09:31:26.700: INFO: Pod var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 14 09:31:26.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-67" for this suite. 12/14/22 09:31:26.705
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":311,"skipped":5767,"failed":0}
------------------------------
• [4.229 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:31:22.481
    Dec 14 09:31:22.481: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 12/14/22 09:31:22.481
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:22.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:22.5
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 12/14/22 09:31:22.504
    Dec 14 09:31:22.514: INFO: Waiting up to 5m0s for pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856" in namespace "var-expansion-67" to be "Succeeded or Failed"
    Dec 14 09:31:22.517: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905524ms
    Dec 14 09:31:24.522: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008235325s
    Dec 14 09:31:26.522: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007885569s
    STEP: Saw pod success 12/14/22 09:31:26.522
    Dec 14 09:31:26.522: INFO: Pod "var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856" satisfied condition "Succeeded or Failed"
    Dec 14 09:31:26.526: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 09:31:26.689
    Dec 14 09:31:26.698: INFO: Waiting for pod var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856 to disappear
    Dec 14 09:31:26.700: INFO: Pod var-expansion-58bbe34a-63a3-4997-9bff-d3baf176c856 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 14 09:31:26.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-67" for this suite. 12/14/22 09:31:26.705
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:31:26.71
Dec 14 09:31:26.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:31:26.71
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:26.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:26.724
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8906 12/14/22 09:31:26.727
STEP: changing the ExternalName service to type=NodePort 12/14/22 09:31:26.731
STEP: creating replication controller externalname-service in namespace services-8906 12/14/22 09:31:26.751
I1214 09:31:26.765808    4635 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8906, replica count: 2
I1214 09:31:29.816587    4635 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:31:29.816: INFO: Creating new exec pod
Dec 14 09:31:29.828: INFO: Waiting up to 5m0s for pod "execpodbg4wt" in namespace "services-8906" to be "running"
Dec 14 09:31:29.831: INFO: Pod "execpodbg4wt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133699ms
Dec 14 09:31:31.836: INFO: Pod "execpodbg4wt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008040203s
Dec 14 09:31:31.836: INFO: Pod "execpodbg4wt" satisfied condition "running"
Dec 14 09:31:32.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 09:31:33.221: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 09:31:33.221: INFO: stdout: "externalname-service-zcchf"
Dec 14 09:31:33.221: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.154.127 80'
Dec 14 09:31:33.638: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.154.127 80\nConnection to 100.104.154.127 80 port [tcp/http] succeeded!\n"
Dec 14 09:31:33.638: INFO: stdout: ""
Dec 14 09:31:34.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.154.127 80'
Dec 14 09:31:35.086: INFO: stderr: "+ nc -v -t -w 2 100.104.154.127 80\n+ Connection to 100.104.154.127 80 port [tcp/http] succeeded!\necho hostName\n"
Dec 14 09:31:35.086: INFO: stdout: "externalname-service-wtbjp"
Dec 14 09:31:35.086: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30724'
Dec 14 09:31:35.521: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30724\nConnection to 10.250.3.58 30724 port [tcp/*] succeeded!\n"
Dec 14 09:31:35.521: INFO: stdout: ""
Dec 14 09:31:36.521: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30724'
Dec 14 09:31:36.984: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30724\nConnection to 10.250.3.58 30724 port [tcp/*] succeeded!\n"
Dec 14 09:31:36.985: INFO: stdout: "externalname-service-zcchf"
Dec 14 09:31:36.985: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30724'
Dec 14 09:31:37.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30724\nConnection to 10.250.3.210 30724 port [tcp/*] succeeded!\n"
Dec 14 09:31:37.485: INFO: stdout: ""
Dec 14 09:31:38.485: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30724'
Dec 14 09:31:38.784: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30724\nConnection to 10.250.3.210 30724 port [tcp/*] succeeded!\n"
Dec 14 09:31:38.784: INFO: stdout: "externalname-service-wtbjp"
Dec 14 09:31:38.784: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:31:38.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8906" for this suite. 12/14/22 09:31:38.805
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":312,"skipped":5770,"failed":0}
------------------------------
• [12.100 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:31:26.71
    Dec 14 09:31:26.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:31:26.71
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:26.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:26.724
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8906 12/14/22 09:31:26.727
    STEP: changing the ExternalName service to type=NodePort 12/14/22 09:31:26.731
    STEP: creating replication controller externalname-service in namespace services-8906 12/14/22 09:31:26.751
    I1214 09:31:26.765808    4635 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8906, replica count: 2
    I1214 09:31:29.816587    4635 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:31:29.816: INFO: Creating new exec pod
    Dec 14 09:31:29.828: INFO: Waiting up to 5m0s for pod "execpodbg4wt" in namespace "services-8906" to be "running"
    Dec 14 09:31:29.831: INFO: Pod "execpodbg4wt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133699ms
    Dec 14 09:31:31.836: INFO: Pod "execpodbg4wt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008040203s
    Dec 14 09:31:31.836: INFO: Pod "execpodbg4wt" satisfied condition "running"
    Dec 14 09:31:32.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Dec 14 09:31:33.221: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 14 09:31:33.221: INFO: stdout: "externalname-service-zcchf"
    Dec 14 09:31:33.221: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.154.127 80'
    Dec 14 09:31:33.638: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.104.154.127 80\nConnection to 100.104.154.127 80 port [tcp/http] succeeded!\n"
    Dec 14 09:31:33.638: INFO: stdout: ""
    Dec 14 09:31:34.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.104.154.127 80'
    Dec 14 09:31:35.086: INFO: stderr: "+ nc -v -t -w 2 100.104.154.127 80\n+ Connection to 100.104.154.127 80 port [tcp/http] succeeded!\necho hostName\n"
    Dec 14 09:31:35.086: INFO: stdout: "externalname-service-wtbjp"
    Dec 14 09:31:35.086: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30724'
    Dec 14 09:31:35.521: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30724\nConnection to 10.250.3.58 30724 port [tcp/*] succeeded!\n"
    Dec 14 09:31:35.521: INFO: stdout: ""
    Dec 14 09:31:36.521: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 30724'
    Dec 14 09:31:36.984: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.58 30724\nConnection to 10.250.3.58 30724 port [tcp/*] succeeded!\n"
    Dec 14 09:31:36.985: INFO: stdout: "externalname-service-zcchf"
    Dec 14 09:31:36.985: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30724'
    Dec 14 09:31:37.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30724\nConnection to 10.250.3.210 30724 port [tcp/*] succeeded!\n"
    Dec 14 09:31:37.485: INFO: stdout: ""
    Dec 14 09:31:38.485: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8906 exec execpodbg4wt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 30724'
    Dec 14 09:31:38.784: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 30724\nConnection to 10.250.3.210 30724 port [tcp/*] succeeded!\n"
    Dec 14 09:31:38.784: INFO: stdout: "externalname-service-wtbjp"
    Dec 14 09:31:38.784: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:31:38.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8906" for this suite. 12/14/22 09:31:38.805
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:31:38.81
Dec 14 09:31:38.810: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:31:38.811
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:38.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:38.823
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 12/14/22 09:31:38.827
Dec 14 09:31:38.827: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1056 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 12/14/22 09:31:38.87
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:31:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1056" for this suite. 12/14/22 09:31:38.887
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":313,"skipped":5777,"failed":0}
------------------------------
• [0.081 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:31:38.81
    Dec 14 09:31:38.810: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:31:38.811
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:38.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:38.823
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 12/14/22 09:31:38.827
    Dec 14 09:31:38.827: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1056 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 12/14/22 09:31:38.87
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:31:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1056" for this suite. 12/14/22 09:31:38.887
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:31:38.891
Dec 14 09:31:38.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:31:38.892
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:38.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:38.905
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 09:31:38.921: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 09:32:38.971: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:32:38.975
Dec 14 09:32:38.975: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 09:32:38.975
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:32:38.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:32:38.989
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 12/14/22 09:32:38.993
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 09:32:38.993
Dec 14 09:32:39.004: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-133" to be "running"
Dec 14 09:32:39.007: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535843ms
Dec 14 09:32:41.013: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008462794s
Dec 14 09:32:41.013: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 09:32:41.016
Dec 14 09:32:41.023: INFO: found a healthy node: shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Dec 14 09:32:55.090: INFO: pods created so far: [1 1 1]
Dec 14 09:32:55.090: INFO: length of pods created so far: 3
Dec 14 09:32:57.104: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Dec 14 09:33:04.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-133" for this suite. 12/14/22 09:33:04.116
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:33:04.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7497" for this suite. 12/14/22 09:33:04.15
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":314,"skipped":5777,"failed":0}
------------------------------
• [85.321 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:31:38.891
    Dec 14 09:31:38.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:31:38.892
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:31:38.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:31:38.905
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 14 09:31:38.921: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 09:32:38.971: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:32:38.975
    Dec 14 09:32:38.975: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 09:32:38.975
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:32:38.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:32:38.989
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 12/14/22 09:32:38.993
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 09:32:38.993
    Dec 14 09:32:39.004: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-133" to be "running"
    Dec 14 09:32:39.007: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535843ms
    Dec 14 09:32:41.013: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008462794s
    Dec 14 09:32:41.013: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 09:32:41.016
    Dec 14 09:32:41.023: INFO: found a healthy node: shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Dec 14 09:32:55.090: INFO: pods created so far: [1 1 1]
    Dec 14 09:32:55.090: INFO: length of pods created so far: 3
    Dec 14 09:32:57.104: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Dec 14 09:33:04.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-133" for this suite. 12/14/22 09:33:04.116
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:33:04.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7497" for this suite. 12/14/22 09:33:04.15
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:04.212
Dec 14 09:33:04.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:33:04.213
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:04.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:04.227
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-91a15a47-b6ab-4f15-87c8-ab546e591451 12/14/22 09:33:04.23
STEP: Creating a pod to test consume secrets 12/14/22 09:33:04.234
Dec 14 09:33:04.256: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1" in namespace "projected-8200" to be "Succeeded or Failed"
Dec 14 09:33:04.260: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492677ms
Dec 14 09:33:06.265: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009306156s
Dec 14 09:33:08.266: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009705026s
STEP: Saw pod success 12/14/22 09:33:08.266
Dec 14 09:33:08.267: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1" satisfied condition "Succeeded or Failed"
Dec 14 09:33:08.270: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:33:08.433
Dec 14 09:33:08.441: INFO: Waiting for pod pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1 to disappear
Dec 14 09:33:08.444: INFO: Pod pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 09:33:08.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8200" for this suite. 12/14/22 09:33:08.449
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":315,"skipped":5778,"failed":0}
------------------------------
• [4.241 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:04.212
    Dec 14 09:33:04.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:33:04.213
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:04.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:04.227
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-91a15a47-b6ab-4f15-87c8-ab546e591451 12/14/22 09:33:04.23
    STEP: Creating a pod to test consume secrets 12/14/22 09:33:04.234
    Dec 14 09:33:04.256: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1" in namespace "projected-8200" to be "Succeeded or Failed"
    Dec 14 09:33:04.260: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492677ms
    Dec 14 09:33:06.265: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009306156s
    Dec 14 09:33:08.266: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009705026s
    STEP: Saw pod success 12/14/22 09:33:08.266
    Dec 14 09:33:08.267: INFO: Pod "pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1" satisfied condition "Succeeded or Failed"
    Dec 14 09:33:08.270: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:33:08.433
    Dec 14 09:33:08.441: INFO: Waiting for pod pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1 to disappear
    Dec 14 09:33:08.444: INFO: Pod pod-projected-secrets-64530672-671f-4d05-8ad9-4b27eba159f1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 09:33:08.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8200" for this suite. 12/14/22 09:33:08.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:08.453
Dec 14 09:33:08.453: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test 12/14/22 09:33:08.454
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:08.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:08.468
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Dec 14 09:33:08.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6370" for this suite. 12/14/22 09:33:08.522
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":316,"skipped":5787,"failed":0}
------------------------------
• [0.072 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:08.453
    Dec 14 09:33:08.453: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename lease-test 12/14/22 09:33:08.454
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:08.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:08.468
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Dec 14 09:33:08.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-6370" for this suite. 12/14/22 09:33:08.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:08.526
Dec 14 09:33:08.526: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:33:08.527
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:08.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:08.541
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 12/14/22 09:33:08.544
STEP: Creating a ResourceQuota 12/14/22 09:33:13.548
STEP: Ensuring resource quota status is calculated 12/14/22 09:33:13.552
STEP: Creating a Pod that fits quota 12/14/22 09:33:15.556
STEP: Ensuring ResourceQuota status captures the pod usage 12/14/22 09:33:15.57
STEP: Not allowing a pod to be created that exceeds remaining quota 12/14/22 09:33:17.575
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/14/22 09:33:17.581
STEP: Ensuring a pod cannot update its resource requirements 12/14/22 09:33:17.588
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/14/22 09:33:17.592
STEP: Deleting the pod 12/14/22 09:33:19.598
STEP: Ensuring resource quota status released the pod usage 12/14/22 09:33:19.605
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:33:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7890" for this suite. 12/14/22 09:33:21.616
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":317,"skipped":5805,"failed":0}
------------------------------
• [13.095 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:08.526
    Dec 14 09:33:08.526: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:33:08.527
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:08.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:08.541
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 12/14/22 09:33:08.544
    STEP: Creating a ResourceQuota 12/14/22 09:33:13.548
    STEP: Ensuring resource quota status is calculated 12/14/22 09:33:13.552
    STEP: Creating a Pod that fits quota 12/14/22 09:33:15.556
    STEP: Ensuring ResourceQuota status captures the pod usage 12/14/22 09:33:15.57
    STEP: Not allowing a pod to be created that exceeds remaining quota 12/14/22 09:33:17.575
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/14/22 09:33:17.581
    STEP: Ensuring a pod cannot update its resource requirements 12/14/22 09:33:17.588
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/14/22 09:33:17.592
    STEP: Deleting the pod 12/14/22 09:33:19.598
    STEP: Ensuring resource quota status released the pod usage 12/14/22 09:33:19.605
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:33:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7890" for this suite. 12/14/22 09:33:21.616
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:21.621
Dec 14 09:33:21.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:33:21.622
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:21.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:21.637
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-f6796f84-d673-4d60-bc71-b3acd62b865a 12/14/22 09:33:21.64
STEP: Creating a pod to test consume secrets 12/14/22 09:33:21.644
Dec 14 09:33:21.654: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5" in namespace "projected-222" to be "Succeeded or Failed"
Dec 14 09:33:21.657: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900033ms
Dec 14 09:33:23.662: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008178301s
Dec 14 09:33:25.661: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007652975s
STEP: Saw pod success 12/14/22 09:33:25.661
Dec 14 09:33:25.662: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5" satisfied condition "Succeeded or Failed"
Dec 14 09:33:25.666: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:33:25.675
Dec 14 09:33:25.683: INFO: Waiting for pod pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5 to disappear
Dec 14 09:33:25.686: INFO: Pod pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 09:33:25.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-222" for this suite. 12/14/22 09:33:25.69
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":5806,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:21.621
    Dec 14 09:33:21.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:33:21.622
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:21.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:21.637
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-f6796f84-d673-4d60-bc71-b3acd62b865a 12/14/22 09:33:21.64
    STEP: Creating a pod to test consume secrets 12/14/22 09:33:21.644
    Dec 14 09:33:21.654: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5" in namespace "projected-222" to be "Succeeded or Failed"
    Dec 14 09:33:21.657: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900033ms
    Dec 14 09:33:23.662: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008178301s
    Dec 14 09:33:25.661: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007652975s
    STEP: Saw pod success 12/14/22 09:33:25.661
    Dec 14 09:33:25.662: INFO: Pod "pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5" satisfied condition "Succeeded or Failed"
    Dec 14 09:33:25.666: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:33:25.675
    Dec 14 09:33:25.683: INFO: Waiting for pod pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5 to disappear
    Dec 14 09:33:25.686: INFO: Pod pod-projected-secrets-1e9c15ae-0fea-4bce-9d00-6c577996d5d5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 09:33:25.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-222" for this suite. 12/14/22 09:33:25.69
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:25.695
Dec 14 09:33:25.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 09:33:25.696
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:25.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:25.709
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 12/14/22 09:33:25.712
STEP: Creating RC which spawns configmap-volume pods 12/14/22 09:33:25.952
Dec 14 09:33:26.049: INFO: Pod name wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9: Found 1 pods out of 5
Dec 14 09:33:31.059: INFO: Pod name wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/14/22 09:33:31.059
Dec 14 09:33:31.059: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-8z9cz" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:31.063: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-8z9cz": Phase="Running", Reason="", readiness=true. Elapsed: 4.118504ms
Dec 14 09:33:31.063: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-8z9cz" satisfied condition "running"
Dec 14 09:33:31.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-dnbwh" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:31.067: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-dnbwh": Phase="Running", Reason="", readiness=true. Elapsed: 3.700097ms
Dec 14 09:33:31.067: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-dnbwh" satisfied condition "running"
Dec 14 09:33:31.067: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-rpj6h" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:31.071: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-rpj6h": Phase="Running", Reason="", readiness=true. Elapsed: 4.175416ms
Dec 14 09:33:31.071: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-rpj6h" satisfied condition "running"
Dec 14 09:33:31.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-s8vl4" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:31.074: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-s8vl4": Phase="Running", Reason="", readiness=true. Elapsed: 3.050077ms
Dec 14 09:33:31.074: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-s8vl4" satisfied condition "running"
Dec 14 09:33:31.074: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-tsg5j" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:31.077: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-tsg5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.980399ms
Dec 14 09:33:31.077: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-tsg5j" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9 in namespace emptydir-wrapper-3859, will wait for the garbage collector to delete the pods 12/14/22 09:33:31.077
Dec 14 09:33:31.136: INFO: Deleting ReplicationController wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9 took: 5.098008ms
Dec 14 09:33:31.237: INFO: Terminating ReplicationController wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9 pods took: 100.686002ms
STEP: Creating RC which spawns configmap-volume pods 12/14/22 09:33:32.142
Dec 14 09:33:32.152: INFO: Pod name wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5: Found 0 pods out of 5
Dec 14 09:33:37.163: INFO: Pod name wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/14/22 09:33:37.163
Dec 14 09:33:37.164: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-5d5q9" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:37.167: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-5d5q9": Phase="Running", Reason="", readiness=true. Elapsed: 3.663633ms
Dec 14 09:33:37.167: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-5d5q9" satisfied condition "running"
Dec 14 09:33:37.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-bclq6" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:37.171: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-bclq6": Phase="Running", Reason="", readiness=true. Elapsed: 3.327976ms
Dec 14 09:33:37.171: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-bclq6" satisfied condition "running"
Dec 14 09:33:37.171: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-f26jf" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:37.174: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-f26jf": Phase="Running", Reason="", readiness=true. Elapsed: 3.066635ms
Dec 14 09:33:37.174: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-f26jf" satisfied condition "running"
Dec 14 09:33:37.174: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-j8zcl" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:37.177: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-j8zcl": Phase="Running", Reason="", readiness=true. Elapsed: 2.915217ms
Dec 14 09:33:37.177: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-j8zcl" satisfied condition "running"
Dec 14 09:33:37.177: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-sbr7t" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:37.180: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-sbr7t": Phase="Running", Reason="", readiness=true. Elapsed: 3.197728ms
Dec 14 09:33:37.180: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-sbr7t" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5 in namespace emptydir-wrapper-3859, will wait for the garbage collector to delete the pods 12/14/22 09:33:37.18
Dec 14 09:33:37.241: INFO: Deleting ReplicationController wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5 took: 5.837511ms
Dec 14 09:33:37.342: INFO: Terminating ReplicationController wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5 pods took: 100.655074ms
STEP: Creating RC which spawns configmap-volume pods 12/14/22 09:33:39.047
Dec 14 09:33:39.058: INFO: Pod name wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875: Found 0 pods out of 5
Dec 14 09:33:44.071: INFO: Pod name wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/14/22 09:33:44.071
Dec 14 09:33:44.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-klwjp" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:44.075: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-klwjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.357625ms
Dec 14 09:33:44.075: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-klwjp" satisfied condition "running"
Dec 14 09:33:44.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kpwgk" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:44.079: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kpwgk": Phase="Running", Reason="", readiness=true. Elapsed: 3.78628ms
Dec 14 09:33:44.079: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kpwgk" satisfied condition "running"
Dec 14 09:33:44.079: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kxpnl" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:44.083: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kxpnl": Phase="Running", Reason="", readiness=true. Elapsed: 4.089241ms
Dec 14 09:33:44.083: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kxpnl" satisfied condition "running"
Dec 14 09:33:44.083: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zw8k5" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:44.087: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zw8k5": Phase="Running", Reason="", readiness=true. Elapsed: 3.794843ms
Dec 14 09:33:44.087: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zw8k5" satisfied condition "running"
Dec 14 09:33:44.087: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zz27z" in namespace "emptydir-wrapper-3859" to be "running"
Dec 14 09:33:44.091: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zz27z": Phase="Running", Reason="", readiness=true. Elapsed: 3.735064ms
Dec 14 09:33:44.091: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zz27z" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875 in namespace emptydir-wrapper-3859, will wait for the garbage collector to delete the pods 12/14/22 09:33:44.091
Dec 14 09:33:44.155: INFO: Deleting ReplicationController wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875 took: 7.116393ms
Dec 14 09:33:44.256: INFO: Terminating ReplicationController wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875 pods took: 100.849349ms
STEP: Cleaning up the configMaps 12/14/22 09:33:45.256
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Dec 14 09:33:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3859" for this suite. 12/14/22 09:33:45.434
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":319,"skipped":5809,"failed":0}
------------------------------
• [19.743 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:25.695
    Dec 14 09:33:25.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 09:33:25.696
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:25.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:25.709
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 12/14/22 09:33:25.712
    STEP: Creating RC which spawns configmap-volume pods 12/14/22 09:33:25.952
    Dec 14 09:33:26.049: INFO: Pod name wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9: Found 1 pods out of 5
    Dec 14 09:33:31.059: INFO: Pod name wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/14/22 09:33:31.059
    Dec 14 09:33:31.059: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-8z9cz" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:31.063: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-8z9cz": Phase="Running", Reason="", readiness=true. Elapsed: 4.118504ms
    Dec 14 09:33:31.063: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-8z9cz" satisfied condition "running"
    Dec 14 09:33:31.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-dnbwh" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:31.067: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-dnbwh": Phase="Running", Reason="", readiness=true. Elapsed: 3.700097ms
    Dec 14 09:33:31.067: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-dnbwh" satisfied condition "running"
    Dec 14 09:33:31.067: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-rpj6h" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:31.071: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-rpj6h": Phase="Running", Reason="", readiness=true. Elapsed: 4.175416ms
    Dec 14 09:33:31.071: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-rpj6h" satisfied condition "running"
    Dec 14 09:33:31.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-s8vl4" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:31.074: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-s8vl4": Phase="Running", Reason="", readiness=true. Elapsed: 3.050077ms
    Dec 14 09:33:31.074: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-s8vl4" satisfied condition "running"
    Dec 14 09:33:31.074: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-tsg5j" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:31.077: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-tsg5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.980399ms
    Dec 14 09:33:31.077: INFO: Pod "wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9-tsg5j" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9 in namespace emptydir-wrapper-3859, will wait for the garbage collector to delete the pods 12/14/22 09:33:31.077
    Dec 14 09:33:31.136: INFO: Deleting ReplicationController wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9 took: 5.098008ms
    Dec 14 09:33:31.237: INFO: Terminating ReplicationController wrapped-volume-race-45e5947e-e971-4871-8043-6e124409fcc9 pods took: 100.686002ms
    STEP: Creating RC which spawns configmap-volume pods 12/14/22 09:33:32.142
    Dec 14 09:33:32.152: INFO: Pod name wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5: Found 0 pods out of 5
    Dec 14 09:33:37.163: INFO: Pod name wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/14/22 09:33:37.163
    Dec 14 09:33:37.164: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-5d5q9" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:37.167: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-5d5q9": Phase="Running", Reason="", readiness=true. Elapsed: 3.663633ms
    Dec 14 09:33:37.167: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-5d5q9" satisfied condition "running"
    Dec 14 09:33:37.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-bclq6" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:37.171: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-bclq6": Phase="Running", Reason="", readiness=true. Elapsed: 3.327976ms
    Dec 14 09:33:37.171: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-bclq6" satisfied condition "running"
    Dec 14 09:33:37.171: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-f26jf" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:37.174: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-f26jf": Phase="Running", Reason="", readiness=true. Elapsed: 3.066635ms
    Dec 14 09:33:37.174: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-f26jf" satisfied condition "running"
    Dec 14 09:33:37.174: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-j8zcl" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:37.177: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-j8zcl": Phase="Running", Reason="", readiness=true. Elapsed: 2.915217ms
    Dec 14 09:33:37.177: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-j8zcl" satisfied condition "running"
    Dec 14 09:33:37.177: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-sbr7t" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:37.180: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-sbr7t": Phase="Running", Reason="", readiness=true. Elapsed: 3.197728ms
    Dec 14 09:33:37.180: INFO: Pod "wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5-sbr7t" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5 in namespace emptydir-wrapper-3859, will wait for the garbage collector to delete the pods 12/14/22 09:33:37.18
    Dec 14 09:33:37.241: INFO: Deleting ReplicationController wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5 took: 5.837511ms
    Dec 14 09:33:37.342: INFO: Terminating ReplicationController wrapped-volume-race-b0f473ed-33c4-4c4b-b8bc-47629e715ad5 pods took: 100.655074ms
    STEP: Creating RC which spawns configmap-volume pods 12/14/22 09:33:39.047
    Dec 14 09:33:39.058: INFO: Pod name wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875: Found 0 pods out of 5
    Dec 14 09:33:44.071: INFO: Pod name wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/14/22 09:33:44.071
    Dec 14 09:33:44.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-klwjp" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:44.075: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-klwjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.357625ms
    Dec 14 09:33:44.075: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-klwjp" satisfied condition "running"
    Dec 14 09:33:44.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kpwgk" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:44.079: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kpwgk": Phase="Running", Reason="", readiness=true. Elapsed: 3.78628ms
    Dec 14 09:33:44.079: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kpwgk" satisfied condition "running"
    Dec 14 09:33:44.079: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kxpnl" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:44.083: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kxpnl": Phase="Running", Reason="", readiness=true. Elapsed: 4.089241ms
    Dec 14 09:33:44.083: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-kxpnl" satisfied condition "running"
    Dec 14 09:33:44.083: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zw8k5" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:44.087: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zw8k5": Phase="Running", Reason="", readiness=true. Elapsed: 3.794843ms
    Dec 14 09:33:44.087: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zw8k5" satisfied condition "running"
    Dec 14 09:33:44.087: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zz27z" in namespace "emptydir-wrapper-3859" to be "running"
    Dec 14 09:33:44.091: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zz27z": Phase="Running", Reason="", readiness=true. Elapsed: 3.735064ms
    Dec 14 09:33:44.091: INFO: Pod "wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875-zz27z" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875 in namespace emptydir-wrapper-3859, will wait for the garbage collector to delete the pods 12/14/22 09:33:44.091
    Dec 14 09:33:44.155: INFO: Deleting ReplicationController wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875 took: 7.116393ms
    Dec 14 09:33:44.256: INFO: Terminating ReplicationController wrapped-volume-race-dcee3712-e67e-4cbe-a7eb-cd6954389875 pods took: 100.849349ms
    STEP: Cleaning up the configMaps 12/14/22 09:33:45.256
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:33:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3859" for this suite. 12/14/22 09:33:45.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:45.439
Dec 14 09:33:45.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:33:45.439
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:45.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:45.454
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-1947/configmap-test-915c420a-f4d8-47f3-bc3a-e5bd5d65cec8 12/14/22 09:33:45.457
STEP: Creating a pod to test consume configMaps 12/14/22 09:33:45.461
Dec 14 09:33:45.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54" in namespace "configmap-1947" to be "Succeeded or Failed"
Dec 14 09:33:45.476: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784304ms
Dec 14 09:33:47.482: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008389358s
Dec 14 09:33:49.482: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008575275s
STEP: Saw pod success 12/14/22 09:33:49.482
Dec 14 09:33:49.482: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54" satisfied condition "Succeeded or Failed"
Dec 14 09:33:49.486: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54 container env-test: <nil>
STEP: delete the pod 12/14/22 09:33:49.496
Dec 14 09:33:49.504: INFO: Waiting for pod pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54 to disappear
Dec 14 09:33:49.507: INFO: Pod pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:33:49.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1947" for this suite. 12/14/22 09:33:49.511
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":320,"skipped":5822,"failed":0}
------------------------------
• [4.076 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:45.439
    Dec 14 09:33:45.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:33:45.439
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:45.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:45.454
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-1947/configmap-test-915c420a-f4d8-47f3-bc3a-e5bd5d65cec8 12/14/22 09:33:45.457
    STEP: Creating a pod to test consume configMaps 12/14/22 09:33:45.461
    Dec 14 09:33:45.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54" in namespace "configmap-1947" to be "Succeeded or Failed"
    Dec 14 09:33:45.476: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784304ms
    Dec 14 09:33:47.482: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008389358s
    Dec 14 09:33:49.482: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008575275s
    STEP: Saw pod success 12/14/22 09:33:49.482
    Dec 14 09:33:49.482: INFO: Pod "pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54" satisfied condition "Succeeded or Failed"
    Dec 14 09:33:49.486: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54 container env-test: <nil>
    STEP: delete the pod 12/14/22 09:33:49.496
    Dec 14 09:33:49.504: INFO: Waiting for pod pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54 to disappear
    Dec 14 09:33:49.507: INFO: Pod pod-configmaps-9bb4fa83-cbd8-4e3d-a0f1-463d511baf54 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:33:49.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1947" for this suite. 12/14/22 09:33:49.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:49.515
Dec 14 09:33:49.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 09:33:49.516
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:49.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:49.53
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 12/14/22 09:33:49.536
Dec 14 09:33:49.545: INFO: created test-pod-1
Dec 14 09:33:49.553: INFO: created test-pod-2
Dec 14 09:33:49.570: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 12/14/22 09:33:49.57
Dec 14 09:33:49.570: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9911' to be running and ready
Dec 14 09:33:49.578: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 09:33:49.578: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 09:33:49.578: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 09:33:49.578: INFO: 0 / 3 pods in namespace 'pods-9911' are running and ready (0 seconds elapsed)
Dec 14 09:33:49.578: INFO: expected 0 pod replicas in namespace 'pods-9911', 0 are Running and Ready.
Dec 14 09:33:49.578: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
Dec 14 09:33:49.578: INFO: test-pod-1  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
Dec 14 09:33:49.579: INFO: test-pod-2  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
Dec 14 09:33:49.579: INFO: test-pod-3  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
Dec 14 09:33:49.579: INFO: 
Dec 14 09:33:51.589: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 09:33:51.589: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 09:33:51.589: INFO: 1 / 3 pods in namespace 'pods-9911' are running and ready (2 seconds elapsed)
Dec 14 09:33:51.589: INFO: expected 0 pod replicas in namespace 'pods-9911', 0 are Running and Ready.
Dec 14 09:33:51.589: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
Dec 14 09:33:51.589: INFO: test-pod-2  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
Dec 14 09:33:51.589: INFO: test-pod-3  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
Dec 14 09:33:51.589: INFO: 
Dec 14 09:33:53.591: INFO: 3 / 3 pods in namespace 'pods-9911' are running and ready (4 seconds elapsed)
Dec 14 09:33:53.591: INFO: expected 0 pod replicas in namespace 'pods-9911', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 12/14/22 09:33:53.604
Dec 14 09:33:53.607: INFO: Pod quantity 3 is different from expected quantity 0
Dec 14 09:33:54.612: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 09:33:55.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9911" for this suite. 12/14/22 09:33:55.617
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":321,"skipped":5831,"failed":0}
------------------------------
• [6.106 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:49.515
    Dec 14 09:33:49.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 09:33:49.516
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:49.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:49.53
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 12/14/22 09:33:49.536
    Dec 14 09:33:49.545: INFO: created test-pod-1
    Dec 14 09:33:49.553: INFO: created test-pod-2
    Dec 14 09:33:49.570: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 12/14/22 09:33:49.57
    Dec 14 09:33:49.570: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9911' to be running and ready
    Dec 14 09:33:49.578: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 09:33:49.578: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 09:33:49.578: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 09:33:49.578: INFO: 0 / 3 pods in namespace 'pods-9911' are running and ready (0 seconds elapsed)
    Dec 14 09:33:49.578: INFO: expected 0 pod replicas in namespace 'pods-9911', 0 are Running and Ready.
    Dec 14 09:33:49.578: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
    Dec 14 09:33:49.578: INFO: test-pod-1  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
    Dec 14 09:33:49.579: INFO: test-pod-2  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
    Dec 14 09:33:49.579: INFO: test-pod-3  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
    Dec 14 09:33:49.579: INFO: 
    Dec 14 09:33:51.589: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 09:33:51.589: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 09:33:51.589: INFO: 1 / 3 pods in namespace 'pods-9911' are running and ready (2 seconds elapsed)
    Dec 14 09:33:51.589: INFO: expected 0 pod replicas in namespace 'pods-9911', 0 are Running and Ready.
    Dec 14 09:33:51.589: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
    Dec 14 09:33:51.589: INFO: test-pod-2  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
    Dec 14 09:33:51.589: INFO: test-pod-3  shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 09:33:49 +0000 UTC  }]
    Dec 14 09:33:51.589: INFO: 
    Dec 14 09:33:53.591: INFO: 3 / 3 pods in namespace 'pods-9911' are running and ready (4 seconds elapsed)
    Dec 14 09:33:53.591: INFO: expected 0 pod replicas in namespace 'pods-9911', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 12/14/22 09:33:53.604
    Dec 14 09:33:53.607: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 14 09:33:54.612: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 09:33:55.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9911" for this suite. 12/14/22 09:33:55.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:55.623
Dec 14 09:33:55.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 09:33:55.624
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:55.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:55.639
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1571.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1571.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 12/14/22 09:33:55.643
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1571.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1571.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 12/14/22 09:33:55.643
STEP: creating a pod to probe /etc/hosts 12/14/22 09:33:55.643
STEP: submitting the pod to kubernetes 12/14/22 09:33:55.643
Dec 14 09:33:55.654: INFO: Waiting up to 15m0s for pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f" in namespace "dns-1571" to be "running"
Dec 14 09:33:55.658: INFO: Pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825885ms
Dec 14 09:33:57.662: INFO: Pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008224879s
Dec 14 09:33:57.663: INFO: Pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f" satisfied condition "running"
STEP: retrieving the pod 12/14/22 09:33:57.663
STEP: looking for the results for each expected name from probers 12/14/22 09:33:57.667
Dec 14 09:33:57.837: INFO: DNS probes using dns-1571/dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f succeeded

STEP: deleting the pod 12/14/22 09:33:57.837
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 09:33:57.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1571" for this suite. 12/14/22 09:33:57.851
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":322,"skipped":5884,"failed":0}
------------------------------
• [2.232 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:55.623
    Dec 14 09:33:55.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 09:33:55.624
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:55.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:55.639
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1571.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1571.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     12/14/22 09:33:55.643
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1571.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1571.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     12/14/22 09:33:55.643
    STEP: creating a pod to probe /etc/hosts 12/14/22 09:33:55.643
    STEP: submitting the pod to kubernetes 12/14/22 09:33:55.643
    Dec 14 09:33:55.654: INFO: Waiting up to 15m0s for pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f" in namespace "dns-1571" to be "running"
    Dec 14 09:33:55.658: INFO: Pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825885ms
    Dec 14 09:33:57.662: INFO: Pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008224879s
    Dec 14 09:33:57.663: INFO: Pod "dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 09:33:57.663
    STEP: looking for the results for each expected name from probers 12/14/22 09:33:57.667
    Dec 14 09:33:57.837: INFO: DNS probes using dns-1571/dns-test-d55746cf-dd35-4f6d-a23b-44c69f47c54f succeeded

    STEP: deleting the pod 12/14/22 09:33:57.837
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 09:33:57.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1571" for this suite. 12/14/22 09:33:57.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:57.856
Dec 14 09:33:57.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:33:57.856
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:57.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:57.873
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 12/14/22 09:33:57.877
STEP: watching for the ServiceAccount to be added 12/14/22 09:33:57.883
STEP: patching the ServiceAccount 12/14/22 09:33:57.885
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/14/22 09:33:57.888
STEP: deleting the ServiceAccount 12/14/22 09:33:57.907
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 14 09:33:57.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5728" for this suite. 12/14/22 09:33:57.919
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":323,"skipped":5904,"failed":0}
------------------------------
• [0.067 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:57.856
    Dec 14 09:33:57.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:33:57.856
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:57.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:57.873
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 12/14/22 09:33:57.877
    STEP: watching for the ServiceAccount to be added 12/14/22 09:33:57.883
    STEP: patching the ServiceAccount 12/14/22 09:33:57.885
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/14/22 09:33:57.888
    STEP: deleting the ServiceAccount 12/14/22 09:33:57.907
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 14 09:33:57.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5728" for this suite. 12/14/22 09:33:57.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:33:57.924
Dec 14 09:33:57.924: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 12/14/22 09:33:57.924
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:57.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:57.938
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 12/14/22 09:33:57.957
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 09:33:57.961
Dec 14 09:33:57.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 09:33:57.967: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
Dec 14 09:33:58.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 09:33:58.976: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 12/14/22 09:33:58.979
Dec 14 09:33:58.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 09:33:58.995: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 09:34:00.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 09:34:00.006: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 09:34:01.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 09:34:01.006: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 09:34:02.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 09:34:02.006: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
Dec 14 09:34:03.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 09:34:03.005: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/14/22 09:34:03.007
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2940, will wait for the garbage collector to delete the pods 12/14/22 09:34:03.007
Dec 14 09:34:03.066: INFO: Deleting DaemonSet.extensions daemon-set took: 5.202827ms
Dec 14 09:34:03.166: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.901482ms
Dec 14 09:34:05.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 09:34:05.771: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 09:34:05.774: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48555"},"items":null}

Dec 14 09:34:05.777: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48555"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:34:05.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2940" for this suite. 12/14/22 09:34:05.793
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":324,"skipped":5934,"failed":0}
------------------------------
• [7.873 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:33:57.924
    Dec 14 09:33:57.924: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 12/14/22 09:33:57.924
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:33:57.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:33:57.938
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 12/14/22 09:33:57.957
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 09:33:57.961
    Dec 14 09:33:57.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 09:33:57.967: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f is running 0 daemon pod, expected 1
    Dec 14 09:33:58.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 09:33:58.976: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 12/14/22 09:33:58.979
    Dec 14 09:33:58.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 09:33:58.995: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 09:34:00.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 09:34:00.006: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 09:34:01.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 09:34:01.006: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 09:34:02.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 09:34:02.006: INFO: Node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb is running 0 daemon pod, expected 1
    Dec 14 09:34:03.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 09:34:03.005: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 09:34:03.007
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2940, will wait for the garbage collector to delete the pods 12/14/22 09:34:03.007
    Dec 14 09:34:03.066: INFO: Deleting DaemonSet.extensions daemon-set took: 5.202827ms
    Dec 14 09:34:03.166: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.901482ms
    Dec 14 09:34:05.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 09:34:05.771: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 09:34:05.774: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48555"},"items":null}

    Dec 14 09:34:05.777: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48555"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:34:05.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2940" for this suite. 12/14/22 09:34:05.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:34:05.798
Dec 14 09:34:05.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 09:34:05.799
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:34:05.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:34:05.812
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 12/14/22 09:34:05.817
STEP: submitting the pod to kubernetes 12/14/22 09:34:05.817
STEP: verifying QOS class is set on the pod 12/14/22 09:34:05.826
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Dec 14 09:34:05.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1708" for this suite. 12/14/22 09:34:05.833
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":325,"skipped":5974,"failed":0}
------------------------------
• [0.039 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:34:05.798
    Dec 14 09:34:05.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 09:34:05.799
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:34:05.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:34:05.812
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 12/14/22 09:34:05.817
    STEP: submitting the pod to kubernetes 12/14/22 09:34:05.817
    STEP: verifying QOS class is set on the pod 12/14/22 09:34:05.826
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Dec 14 09:34:05.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1708" for this suite. 12/14/22 09:34:05.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:34:05.837
Dec 14 09:34:05.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:34:05.838
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:34:05.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:34:05.85
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 09:34:05.864: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 09:35:05.906: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 12/14/22 09:35:05.91
Dec 14 09:35:05.934: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 14 09:35:05.943: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 14 09:35:05.964: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 14 09:35:05.971: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/14/22 09:35:05.971
Dec 14 09:35:05.971: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1628" to be "running"
Dec 14 09:35:05.975: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68362ms
Dec 14 09:35:07.979: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008616453s
Dec 14 09:35:09.981: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009875143s
Dec 14 09:35:09.981: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 14 09:35:09.981: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1628" to be "running"
Dec 14 09:35:09.985: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.22153ms
Dec 14 09:35:09.985: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 09:35:09.985: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1628" to be "running"
Dec 14 09:35:09.989: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849235ms
Dec 14 09:35:11.994: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009414561s
Dec 14 09:35:13.995: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.010298517s
Dec 14 09:35:13.995: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 09:35:13.995: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1628" to be "running"
Dec 14 09:35:13.999: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.645374ms
Dec 14 09:35:13.999: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/14/22 09:35:13.999
Dec 14 09:35:14.008: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1628" to be "running"
Dec 14 09:35:14.012: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498718ms
Dec 14 09:35:16.018: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009072082s
Dec 14 09:35:18.018: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009877367s
Dec 14 09:35:18.018: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:35:18.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1628" for this suite. 12/14/22 09:35:18.041
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":326,"skipped":5985,"failed":0}
------------------------------
• [72.244 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:34:05.837
    Dec 14 09:34:05.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 09:34:05.838
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:34:05.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:34:05.85
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 14 09:34:05.864: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 09:35:05.906: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 12/14/22 09:35:05.91
    Dec 14 09:35:05.934: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 14 09:35:05.943: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 14 09:35:05.964: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 14 09:35:05.971: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/14/22 09:35:05.971
    Dec 14 09:35:05.971: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1628" to be "running"
    Dec 14 09:35:05.975: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68362ms
    Dec 14 09:35:07.979: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008616453s
    Dec 14 09:35:09.981: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009875143s
    Dec 14 09:35:09.981: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 14 09:35:09.981: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1628" to be "running"
    Dec 14 09:35:09.985: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.22153ms
    Dec 14 09:35:09.985: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 09:35:09.985: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1628" to be "running"
    Dec 14 09:35:09.989: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849235ms
    Dec 14 09:35:11.994: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009414561s
    Dec 14 09:35:13.995: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.010298517s
    Dec 14 09:35:13.995: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 09:35:13.995: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1628" to be "running"
    Dec 14 09:35:13.999: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.645374ms
    Dec 14 09:35:13.999: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/14/22 09:35:13.999
    Dec 14 09:35:14.008: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1628" to be "running"
    Dec 14 09:35:14.012: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498718ms
    Dec 14 09:35:16.018: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009072082s
    Dec 14 09:35:18.018: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009877367s
    Dec 14 09:35:18.018: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:35:18.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1628" for this suite. 12/14/22 09:35:18.041
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:35:18.082
Dec 14 09:35:18.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 12/14/22 09:35:18.082
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:35:18.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:35:18.1
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 12/14/22 09:35:18.105
STEP: Ensuring more than one job is running at a time 12/14/22 09:35:18.11
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/14/22 09:37:00.115
STEP: Removing cronjob 12/14/22 09:37:00.12
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 14 09:37:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2370" for this suite. 12/14/22 09:37:00.133
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":327,"skipped":5997,"failed":0}
------------------------------
• [102.058 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:35:18.082
    Dec 14 09:35:18.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 12/14/22 09:35:18.082
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:35:18.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:35:18.1
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 12/14/22 09:35:18.105
    STEP: Ensuring more than one job is running at a time 12/14/22 09:35:18.11
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/14/22 09:37:00.115
    STEP: Removing cronjob 12/14/22 09:37:00.12
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 14 09:37:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2370" for this suite. 12/14/22 09:37:00.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:00.14
Dec 14 09:37:00.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 12/14/22 09:37:00.141
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:00.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:00.156
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 12/14/22 09:37:00.161
Dec 14 09:37:00.166: INFO: created test-podtemplate-1
Dec 14 09:37:00.170: INFO: created test-podtemplate-2
Dec 14 09:37:00.174: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 12/14/22 09:37:00.174
STEP: delete collection of pod templates 12/14/22 09:37:00.178
Dec 14 09:37:00.178: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 12/14/22 09:37:00.191
Dec 14 09:37:00.191: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Dec 14 09:37:00.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5058" for this suite. 12/14/22 09:37:00.2
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":328,"skipped":6014,"failed":0}
------------------------------
• [0.066 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:00.14
    Dec 14 09:37:00.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 12/14/22 09:37:00.141
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:00.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:00.156
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 12/14/22 09:37:00.161
    Dec 14 09:37:00.166: INFO: created test-podtemplate-1
    Dec 14 09:37:00.170: INFO: created test-podtemplate-2
    Dec 14 09:37:00.174: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 12/14/22 09:37:00.174
    STEP: delete collection of pod templates 12/14/22 09:37:00.178
    Dec 14 09:37:00.178: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 12/14/22 09:37:00.191
    Dec 14 09:37:00.191: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Dec 14 09:37:00.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5058" for this suite. 12/14/22 09:37:00.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:00.206
Dec 14 09:37:00.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 12/14/22 09:37:00.207
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:00.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:00.225
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 09:37:00.23
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-fwm7 12/14/22 09:37:00.238
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:37:00.239
Dec 14 09:37:00.260: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fwm7" in namespace "subpath-9951" to be "Succeeded or Failed"
Dec 14 09:37:00.264: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194024ms
Dec 14 09:37:02.271: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01096317s
Dec 14 09:37:04.272: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 4.011991321s
Dec 14 09:37:06.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 6.01010266s
Dec 14 09:37:08.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 8.010897835s
Dec 14 09:37:10.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 10.009533385s
Dec 14 09:37:12.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 12.010457447s
Dec 14 09:37:14.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 14.00991434s
Dec 14 09:37:16.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 16.009152684s
Dec 14 09:37:18.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 18.010825905s
Dec 14 09:37:20.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 20.009939077s
Dec 14 09:37:22.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=false. Elapsed: 22.010333988s
Dec 14 09:37:24.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009721445s
STEP: Saw pod success 12/14/22 09:37:24.269
Dec 14 09:37:24.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7" satisfied condition "Succeeded or Failed"
Dec 14 09:37:24.273: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-downwardapi-fwm7 container test-container-subpath-downwardapi-fwm7: <nil>
STEP: delete the pod 12/14/22 09:37:24.398
Dec 14 09:37:24.407: INFO: Waiting for pod pod-subpath-test-downwardapi-fwm7 to disappear
Dec 14 09:37:24.411: INFO: Pod pod-subpath-test-downwardapi-fwm7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fwm7 12/14/22 09:37:24.411
Dec 14 09:37:24.411: INFO: Deleting pod "pod-subpath-test-downwardapi-fwm7" in namespace "subpath-9951"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 14 09:37:24.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9951" for this suite. 12/14/22 09:37:24.421
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":329,"skipped":6025,"failed":0}
------------------------------
• [24.221 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:00.206
    Dec 14 09:37:00.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 12/14/22 09:37:00.207
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:00.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:00.225
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 09:37:00.23
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-fwm7 12/14/22 09:37:00.238
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 09:37:00.239
    Dec 14 09:37:00.260: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fwm7" in namespace "subpath-9951" to be "Succeeded or Failed"
    Dec 14 09:37:00.264: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194024ms
    Dec 14 09:37:02.271: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01096317s
    Dec 14 09:37:04.272: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 4.011991321s
    Dec 14 09:37:06.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 6.01010266s
    Dec 14 09:37:08.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 8.010897835s
    Dec 14 09:37:10.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 10.009533385s
    Dec 14 09:37:12.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 12.010457447s
    Dec 14 09:37:14.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 14.00991434s
    Dec 14 09:37:16.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 16.009152684s
    Dec 14 09:37:18.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 18.010825905s
    Dec 14 09:37:20.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=true. Elapsed: 20.009939077s
    Dec 14 09:37:22.270: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Running", Reason="", readiness=false. Elapsed: 22.010333988s
    Dec 14 09:37:24.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009721445s
    STEP: Saw pod success 12/14/22 09:37:24.269
    Dec 14 09:37:24.269: INFO: Pod "pod-subpath-test-downwardapi-fwm7" satisfied condition "Succeeded or Failed"
    Dec 14 09:37:24.273: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-subpath-test-downwardapi-fwm7 container test-container-subpath-downwardapi-fwm7: <nil>
    STEP: delete the pod 12/14/22 09:37:24.398
    Dec 14 09:37:24.407: INFO: Waiting for pod pod-subpath-test-downwardapi-fwm7 to disappear
    Dec 14 09:37:24.411: INFO: Pod pod-subpath-test-downwardapi-fwm7 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-fwm7 12/14/22 09:37:24.411
    Dec 14 09:37:24.411: INFO: Deleting pod "pod-subpath-test-downwardapi-fwm7" in namespace "subpath-9951"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 14 09:37:24.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9951" for this suite. 12/14/22 09:37:24.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:24.429
Dec 14 09:37:24.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ephemeral-containers-test 12/14/22 09:37:24.429
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:24.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:24.446
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 12/14/22 09:37:24.451
Dec 14 09:37:24.461: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9565" to be "running and ready"
Dec 14 09:37:24.464: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.327893ms
Dec 14 09:37:24.464: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:37:26.470: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581978s
Dec 14 09:37:26.470: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Dec 14 09:37:26.470: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 12/14/22 09:37:26.474
Dec 14 09:37:26.483: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9565" to be "container debugger running"
Dec 14 09:37:26.485: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.817327ms
Dec 14 09:37:28.491: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007929883s
Dec 14 09:37:28.491: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 12/14/22 09:37:28.491
Dec 14 09:37:28.491: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9565 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:37:28.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:37:28.491: INFO: ExecWithOptions: Clientset creation
Dec 14 09:37:28.491: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/ephemeral-containers-test-9565/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Dec 14 09:37:28.899: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 14 09:37:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-9565" for this suite. 12/14/22 09:37:28.995
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":330,"skipped":6057,"failed":0}
------------------------------
• [4.571 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:24.429
    Dec 14 09:37:24.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ephemeral-containers-test 12/14/22 09:37:24.429
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:24.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:24.446
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 12/14/22 09:37:24.451
    Dec 14 09:37:24.461: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9565" to be "running and ready"
    Dec 14 09:37:24.464: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.327893ms
    Dec 14 09:37:24.464: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:37:26.470: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581978s
    Dec 14 09:37:26.470: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Dec 14 09:37:26.470: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 12/14/22 09:37:26.474
    Dec 14 09:37:26.483: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9565" to be "container debugger running"
    Dec 14 09:37:26.485: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.817327ms
    Dec 14 09:37:28.491: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007929883s
    Dec 14 09:37:28.491: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 12/14/22 09:37:28.491
    Dec 14 09:37:28.491: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9565 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:37:28.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:37:28.491: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:37:28.491: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/ephemeral-containers-test-9565/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Dec 14 09:37:28.899: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 14 09:37:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-9565" for this suite. 12/14/22 09:37:28.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:29
Dec 14 09:37:29.000: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:37:29.001
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:29.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:29.017
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:37:29.022
Dec 14 09:37:29.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480" in namespace "projected-3782" to be "Succeeded or Failed"
Dec 14 09:37:29.036: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480": Phase="Pending", Reason="", readiness=false. Elapsed: 3.237044ms
Dec 14 09:37:31.041: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008930092s
Dec 14 09:37:33.042: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009140034s
STEP: Saw pod success 12/14/22 09:37:33.042
Dec 14 09:37:33.042: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480" satisfied condition "Succeeded or Failed"
Dec 14 09:37:33.046: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480 container client-container: <nil>
STEP: delete the pod 12/14/22 09:37:33.056
Dec 14 09:37:33.067: INFO: Waiting for pod downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480 to disappear
Dec 14 09:37:33.071: INFO: Pod downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:37:33.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3782" for this suite. 12/14/22 09:37:33.076
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":331,"skipped":6069,"failed":0}
------------------------------
• [4.081 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:29
    Dec 14 09:37:29.000: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:37:29.001
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:29.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:29.017
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:37:29.022
    Dec 14 09:37:29.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480" in namespace "projected-3782" to be "Succeeded or Failed"
    Dec 14 09:37:29.036: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480": Phase="Pending", Reason="", readiness=false. Elapsed: 3.237044ms
    Dec 14 09:37:31.041: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008930092s
    Dec 14 09:37:33.042: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009140034s
    STEP: Saw pod success 12/14/22 09:37:33.042
    Dec 14 09:37:33.042: INFO: Pod "downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480" satisfied condition "Succeeded or Failed"
    Dec 14 09:37:33.046: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:37:33.056
    Dec 14 09:37:33.067: INFO: Waiting for pod downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480 to disappear
    Dec 14 09:37:33.071: INFO: Pod downwardapi-volume-470564ac-59c0-4a89-bd46-96646b77e480 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:37:33.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3782" for this suite. 12/14/22 09:37:33.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:33.082
Dec 14 09:37:33.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 12/14/22 09:37:33.083
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:33.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:33.099
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Dec 14 09:37:33.104: INFO: Creating ReplicaSet my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc
Dec 14 09:37:33.112: INFO: Pod name my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc: Found 0 pods out of 1
Dec 14 09:37:38.118: INFO: Pod name my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc: Found 1 pods out of 1
Dec 14 09:37:38.118: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc" is running
Dec 14 09:37:38.118: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29" in namespace "replicaset-2486" to be "running"
Dec 14 09:37:38.122: INFO: Pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29": Phase="Running", Reason="", readiness=true. Elapsed: 4.146297ms
Dec 14 09:37:38.122: INFO: Pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29" satisfied condition "running"
Dec 14 09:37:38.122: INFO: Pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:33 +0000 UTC Reason: Message:}])
Dec 14 09:37:38.122: INFO: Trying to dial the pod
Dec 14 09:37:43.232: INFO: Controller my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc: Got expected result from replica 1 [my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29]: "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 14 09:37:43.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2486" for this suite. 12/14/22 09:37:43.239
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":332,"skipped":6114,"failed":0}
------------------------------
• [10.162 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:33.082
    Dec 14 09:37:33.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 12/14/22 09:37:33.083
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:33.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:33.099
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Dec 14 09:37:33.104: INFO: Creating ReplicaSet my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc
    Dec 14 09:37:33.112: INFO: Pod name my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc: Found 0 pods out of 1
    Dec 14 09:37:38.118: INFO: Pod name my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc: Found 1 pods out of 1
    Dec 14 09:37:38.118: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc" is running
    Dec 14 09:37:38.118: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29" in namespace "replicaset-2486" to be "running"
    Dec 14 09:37:38.122: INFO: Pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29": Phase="Running", Reason="", readiness=true. Elapsed: 4.146297ms
    Dec 14 09:37:38.122: INFO: Pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29" satisfied condition "running"
    Dec 14 09:37:38.122: INFO: Pod "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 09:37:33 +0000 UTC Reason: Message:}])
    Dec 14 09:37:38.122: INFO: Trying to dial the pod
    Dec 14 09:37:43.232: INFO: Controller my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc: Got expected result from replica 1 [my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29]: "my-hostname-basic-c91856e1-7b36-40bd-b6f0-22278afe22cc-l9f29", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 14 09:37:43.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2486" for this suite. 12/14/22 09:37:43.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:43.245
Dec 14 09:37:43.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:37:43.246
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:43.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:43.262
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 12/14/22 09:37:43.267
Dec 14 09:37:43.278: INFO: Waiting up to 5m0s for pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26" in namespace "downward-api-830" to be "Succeeded or Failed"
Dec 14 09:37:43.282: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26": Phase="Pending", Reason="", readiness=false. Elapsed: 3.674295ms
Dec 14 09:37:45.289: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010459771s
Dec 14 09:37:47.288: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009209731s
STEP: Saw pod success 12/14/22 09:37:47.288
Dec 14 09:37:47.288: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26" satisfied condition "Succeeded or Failed"
Dec 14 09:37:47.291: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26 container dapi-container: <nil>
STEP: delete the pod 12/14/22 09:37:47.303
Dec 14 09:37:47.311: INFO: Waiting for pod downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26 to disappear
Dec 14 09:37:47.314: INFO: Pod downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 14 09:37:47.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-830" for this suite. 12/14/22 09:37:47.319
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":333,"skipped":6127,"failed":0}
------------------------------
• [4.080 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:43.245
    Dec 14 09:37:43.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:37:43.246
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:43.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:43.262
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 12/14/22 09:37:43.267
    Dec 14 09:37:43.278: INFO: Waiting up to 5m0s for pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26" in namespace "downward-api-830" to be "Succeeded or Failed"
    Dec 14 09:37:43.282: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26": Phase="Pending", Reason="", readiness=false. Elapsed: 3.674295ms
    Dec 14 09:37:45.289: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010459771s
    Dec 14 09:37:47.288: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009209731s
    STEP: Saw pod success 12/14/22 09:37:47.288
    Dec 14 09:37:47.288: INFO: Pod "downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26" satisfied condition "Succeeded or Failed"
    Dec 14 09:37:47.291: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 09:37:47.303
    Dec 14 09:37:47.311: INFO: Waiting for pod downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26 to disappear
    Dec 14 09:37:47.314: INFO: Pod downward-api-a93303ce-fa62-4e21-a2bc-77ed48babd26 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 14 09:37:47.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-830" for this suite. 12/14/22 09:37:47.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:47.325
Dec 14 09:37:47.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 12/14/22 09:37:47.326
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:47.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:47.342
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 09:37:47.347
Dec 14 09:37:47.357: INFO: Waiting up to 5m0s for pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215" in namespace "emptydir-9857" to be "Succeeded or Failed"
Dec 14 09:37:47.359: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91709ms
Dec 14 09:37:49.364: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753864s
Dec 14 09:37:51.367: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010122973s
STEP: Saw pod success 12/14/22 09:37:51.367
Dec 14 09:37:51.367: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215" satisfied condition "Succeeded or Failed"
Dec 14 09:37:51.370: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-e57b81e6-a56f-46f2-be27-4cf0543fe215 container test-container: <nil>
STEP: delete the pod 12/14/22 09:37:51.38
Dec 14 09:37:51.390: INFO: Waiting for pod pod-e57b81e6-a56f-46f2-be27-4cf0543fe215 to disappear
Dec 14 09:37:51.394: INFO: Pod pod-e57b81e6-a56f-46f2-be27-4cf0543fe215 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 14 09:37:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9857" for this suite. 12/14/22 09:37:51.4
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6145,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:47.325
    Dec 14 09:37:47.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 12/14/22 09:37:47.326
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:47.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:47.342
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 09:37:47.347
    Dec 14 09:37:47.357: INFO: Waiting up to 5m0s for pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215" in namespace "emptydir-9857" to be "Succeeded or Failed"
    Dec 14 09:37:47.359: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91709ms
    Dec 14 09:37:49.364: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753864s
    Dec 14 09:37:51.367: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010122973s
    STEP: Saw pod success 12/14/22 09:37:51.367
    Dec 14 09:37:51.367: INFO: Pod "pod-e57b81e6-a56f-46f2-be27-4cf0543fe215" satisfied condition "Succeeded or Failed"
    Dec 14 09:37:51.370: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-e57b81e6-a56f-46f2-be27-4cf0543fe215 container test-container: <nil>
    STEP: delete the pod 12/14/22 09:37:51.38
    Dec 14 09:37:51.390: INFO: Waiting for pod pod-e57b81e6-a56f-46f2-be27-4cf0543fe215 to disappear
    Dec 14 09:37:51.394: INFO: Pod pod-e57b81e6-a56f-46f2-be27-4cf0543fe215 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 14 09:37:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9857" for this suite. 12/14/22 09:37:51.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:51.407
Dec 14 09:37:51.407: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context 12/14/22 09:37:51.408
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:51.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:51.425
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 09:37:51.43
Dec 14 09:37:51.442: INFO: Waiting up to 5m0s for pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442" in namespace "security-context-8851" to be "Succeeded or Failed"
Dec 14 09:37:51.446: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534471ms
Dec 14 09:37:53.451: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008887962s
Dec 14 09:37:55.451: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008628106s
STEP: Saw pod success 12/14/22 09:37:55.451
Dec 14 09:37:55.451: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442" satisfied condition "Succeeded or Failed"
Dec 14 09:37:55.455: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod security-context-7f09db2b-9773-49f2-933d-1032ab9ac442 container test-container: <nil>
STEP: delete the pod 12/14/22 09:37:55.468
Dec 14 09:37:55.475: INFO: Waiting for pod security-context-7f09db2b-9773-49f2-933d-1032ab9ac442 to disappear
Dec 14 09:37:55.479: INFO: Pod security-context-7f09db2b-9773-49f2-933d-1032ab9ac442 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 14 09:37:55.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8851" for this suite. 12/14/22 09:37:55.486
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":335,"skipped":6193,"failed":0}
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:51.407
    Dec 14 09:37:51.407: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context 12/14/22 09:37:51.408
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:51.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:51.425
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 09:37:51.43
    Dec 14 09:37:51.442: INFO: Waiting up to 5m0s for pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442" in namespace "security-context-8851" to be "Succeeded or Failed"
    Dec 14 09:37:51.446: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534471ms
    Dec 14 09:37:53.451: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008887962s
    Dec 14 09:37:55.451: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008628106s
    STEP: Saw pod success 12/14/22 09:37:55.451
    Dec 14 09:37:55.451: INFO: Pod "security-context-7f09db2b-9773-49f2-933d-1032ab9ac442" satisfied condition "Succeeded or Failed"
    Dec 14 09:37:55.455: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod security-context-7f09db2b-9773-49f2-933d-1032ab9ac442 container test-container: <nil>
    STEP: delete the pod 12/14/22 09:37:55.468
    Dec 14 09:37:55.475: INFO: Waiting for pod security-context-7f09db2b-9773-49f2-933d-1032ab9ac442 to disappear
    Dec 14 09:37:55.479: INFO: Pod security-context-7f09db2b-9773-49f2-933d-1032ab9ac442 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 14 09:37:55.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8851" for this suite. 12/14/22 09:37:55.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:37:55.491
Dec 14 09:37:55.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 12/14/22 09:37:55.492
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:55.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:55.51
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 12/14/22 09:37:55.521
STEP: delete the rc 12/14/22 09:38:00.53
STEP: wait for the rc to be deleted 12/14/22 09:38:00.548
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/14/22 09:38:05.552
STEP: Gathering metrics 12/14/22 09:38:35.617
W1214 09:38:35.628982    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 09:38:35.629: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 09:38:35.629: INFO: Deleting pod "simpletest.rc-25b7d" in namespace "gc-4721"
Dec 14 09:38:35.635: INFO: Deleting pod "simpletest.rc-2dvf4" in namespace "gc-4721"
Dec 14 09:38:35.643: INFO: Deleting pod "simpletest.rc-2pg6z" in namespace "gc-4721"
Dec 14 09:38:35.649: INFO: Deleting pod "simpletest.rc-48t4s" in namespace "gc-4721"
Dec 14 09:38:35.655: INFO: Deleting pod "simpletest.rc-4g85j" in namespace "gc-4721"
Dec 14 09:38:35.660: INFO: Deleting pod "simpletest.rc-4tx27" in namespace "gc-4721"
Dec 14 09:38:35.667: INFO: Deleting pod "simpletest.rc-4xq65" in namespace "gc-4721"
Dec 14 09:38:35.673: INFO: Deleting pod "simpletest.rc-5kjq8" in namespace "gc-4721"
Dec 14 09:38:35.678: INFO: Deleting pod "simpletest.rc-5q64w" in namespace "gc-4721"
Dec 14 09:38:35.694: INFO: Deleting pod "simpletest.rc-5xzrk" in namespace "gc-4721"
Dec 14 09:38:35.701: INFO: Deleting pod "simpletest.rc-62q7t" in namespace "gc-4721"
Dec 14 09:38:35.707: INFO: Deleting pod "simpletest.rc-68jvl" in namespace "gc-4721"
Dec 14 09:38:35.713: INFO: Deleting pod "simpletest.rc-6wdkn" in namespace "gc-4721"
Dec 14 09:38:35.718: INFO: Deleting pod "simpletest.rc-78cp8" in namespace "gc-4721"
Dec 14 09:38:35.725: INFO: Deleting pod "simpletest.rc-8c77p" in namespace "gc-4721"
Dec 14 09:38:35.734: INFO: Deleting pod "simpletest.rc-8l8w5" in namespace "gc-4721"
Dec 14 09:38:35.741: INFO: Deleting pod "simpletest.rc-8lw7r" in namespace "gc-4721"
Dec 14 09:38:35.747: INFO: Deleting pod "simpletest.rc-98czw" in namespace "gc-4721"
Dec 14 09:38:35.754: INFO: Deleting pod "simpletest.rc-9cvv2" in namespace "gc-4721"
Dec 14 09:38:35.761: INFO: Deleting pod "simpletest.rc-bcq52" in namespace "gc-4721"
Dec 14 09:38:35.766: INFO: Deleting pod "simpletest.rc-bd74d" in namespace "gc-4721"
Dec 14 09:38:35.771: INFO: Deleting pod "simpletest.rc-bgrzd" in namespace "gc-4721"
Dec 14 09:38:35.778: INFO: Deleting pod "simpletest.rc-bqxmb" in namespace "gc-4721"
Dec 14 09:38:35.783: INFO: Deleting pod "simpletest.rc-bx58d" in namespace "gc-4721"
Dec 14 09:38:35.789: INFO: Deleting pod "simpletest.rc-cb6r4" in namespace "gc-4721"
Dec 14 09:38:35.795: INFO: Deleting pod "simpletest.rc-chjhp" in namespace "gc-4721"
Dec 14 09:38:35.801: INFO: Deleting pod "simpletest.rc-cqcmk" in namespace "gc-4721"
Dec 14 09:38:35.807: INFO: Deleting pod "simpletest.rc-cwhqs" in namespace "gc-4721"
Dec 14 09:38:35.815: INFO: Deleting pod "simpletest.rc-d42vn" in namespace "gc-4721"
Dec 14 09:38:35.822: INFO: Deleting pod "simpletest.rc-d4lqd" in namespace "gc-4721"
Dec 14 09:38:35.827: INFO: Deleting pod "simpletest.rc-dbtzt" in namespace "gc-4721"
Dec 14 09:38:35.834: INFO: Deleting pod "simpletest.rc-djzp9" in namespace "gc-4721"
Dec 14 09:38:35.840: INFO: Deleting pod "simpletest.rc-f4l8m" in namespace "gc-4721"
Dec 14 09:38:35.845: INFO: Deleting pod "simpletest.rc-fgf8w" in namespace "gc-4721"
Dec 14 09:38:35.850: INFO: Deleting pod "simpletest.rc-fhnzt" in namespace "gc-4721"
Dec 14 09:38:35.856: INFO: Deleting pod "simpletest.rc-ftjns" in namespace "gc-4721"
Dec 14 09:38:35.863: INFO: Deleting pod "simpletest.rc-g7pwm" in namespace "gc-4721"
Dec 14 09:38:35.869: INFO: Deleting pod "simpletest.rc-gb22z" in namespace "gc-4721"
Dec 14 09:38:35.874: INFO: Deleting pod "simpletest.rc-gp8xc" in namespace "gc-4721"
Dec 14 09:38:35.880: INFO: Deleting pod "simpletest.rc-grxns" in namespace "gc-4721"
Dec 14 09:38:35.887: INFO: Deleting pod "simpletest.rc-hq9m8" in namespace "gc-4721"
Dec 14 09:38:35.894: INFO: Deleting pod "simpletest.rc-hwrct" in namespace "gc-4721"
Dec 14 09:38:35.901: INFO: Deleting pod "simpletest.rc-j6x92" in namespace "gc-4721"
Dec 14 09:38:35.907: INFO: Deleting pod "simpletest.rc-j87wj" in namespace "gc-4721"
Dec 14 09:38:35.913: INFO: Deleting pod "simpletest.rc-jbscm" in namespace "gc-4721"
Dec 14 09:38:35.918: INFO: Deleting pod "simpletest.rc-jctlv" in namespace "gc-4721"
Dec 14 09:38:35.924: INFO: Deleting pod "simpletest.rc-k69sc" in namespace "gc-4721"
Dec 14 09:38:35.931: INFO: Deleting pod "simpletest.rc-k8zs7" in namespace "gc-4721"
Dec 14 09:38:35.938: INFO: Deleting pod "simpletest.rc-kpthn" in namespace "gc-4721"
Dec 14 09:38:35.946: INFO: Deleting pod "simpletest.rc-l2d5n" in namespace "gc-4721"
Dec 14 09:38:35.951: INFO: Deleting pod "simpletest.rc-l9p5p" in namespace "gc-4721"
Dec 14 09:38:35.956: INFO: Deleting pod "simpletest.rc-lfv2l" in namespace "gc-4721"
Dec 14 09:38:35.962: INFO: Deleting pod "simpletest.rc-lp2dw" in namespace "gc-4721"
Dec 14 09:38:35.969: INFO: Deleting pod "simpletest.rc-lstrs" in namespace "gc-4721"
Dec 14 09:38:35.975: INFO: Deleting pod "simpletest.rc-md5p9" in namespace "gc-4721"
Dec 14 09:38:35.980: INFO: Deleting pod "simpletest.rc-mkrmm" in namespace "gc-4721"
Dec 14 09:38:35.986: INFO: Deleting pod "simpletest.rc-mmgqt" in namespace "gc-4721"
Dec 14 09:38:36.026: INFO: Deleting pod "simpletest.rc-mr8pl" in namespace "gc-4721"
Dec 14 09:38:36.079: INFO: Deleting pod "simpletest.rc-mtn7r" in namespace "gc-4721"
Dec 14 09:38:36.127: INFO: Deleting pod "simpletest.rc-n74jd" in namespace "gc-4721"
Dec 14 09:38:36.180: INFO: Deleting pod "simpletest.rc-n8vds" in namespace "gc-4721"
Dec 14 09:38:36.226: INFO: Deleting pod "simpletest.rc-n9t56" in namespace "gc-4721"
Dec 14 09:38:36.276: INFO: Deleting pod "simpletest.rc-nb4zs" in namespace "gc-4721"
Dec 14 09:38:36.324: INFO: Deleting pod "simpletest.rc-nf7px" in namespace "gc-4721"
Dec 14 09:38:36.377: INFO: Deleting pod "simpletest.rc-nn4tg" in namespace "gc-4721"
Dec 14 09:38:36.427: INFO: Deleting pod "simpletest.rc-nnqbx" in namespace "gc-4721"
Dec 14 09:38:36.477: INFO: Deleting pod "simpletest.rc-nrzlm" in namespace "gc-4721"
Dec 14 09:38:36.526: INFO: Deleting pod "simpletest.rc-nsdb8" in namespace "gc-4721"
Dec 14 09:38:36.583: INFO: Deleting pod "simpletest.rc-nxlpf" in namespace "gc-4721"
Dec 14 09:38:36.626: INFO: Deleting pod "simpletest.rc-p9fpx" in namespace "gc-4721"
Dec 14 09:38:36.676: INFO: Deleting pod "simpletest.rc-pb58p" in namespace "gc-4721"
Dec 14 09:38:36.725: INFO: Deleting pod "simpletest.rc-phh9d" in namespace "gc-4721"
Dec 14 09:38:36.776: INFO: Deleting pod "simpletest.rc-pqqj5" in namespace "gc-4721"
Dec 14 09:38:36.826: INFO: Deleting pod "simpletest.rc-ptqsn" in namespace "gc-4721"
Dec 14 09:38:36.875: INFO: Deleting pod "simpletest.rc-pxh2d" in namespace "gc-4721"
Dec 14 09:38:36.927: INFO: Deleting pod "simpletest.rc-r69h9" in namespace "gc-4721"
Dec 14 09:38:36.977: INFO: Deleting pod "simpletest.rc-rbjmd" in namespace "gc-4721"
Dec 14 09:38:37.027: INFO: Deleting pod "simpletest.rc-rgz29" in namespace "gc-4721"
Dec 14 09:38:37.076: INFO: Deleting pod "simpletest.rc-rh8sd" in namespace "gc-4721"
Dec 14 09:38:37.127: INFO: Deleting pod "simpletest.rc-rl4p9" in namespace "gc-4721"
Dec 14 09:38:37.175: INFO: Deleting pod "simpletest.rc-rp2dn" in namespace "gc-4721"
Dec 14 09:38:37.225: INFO: Deleting pod "simpletest.rc-rp6wl" in namespace "gc-4721"
Dec 14 09:38:37.277: INFO: Deleting pod "simpletest.rc-s4dfp" in namespace "gc-4721"
Dec 14 09:38:37.326: INFO: Deleting pod "simpletest.rc-s4ts6" in namespace "gc-4721"
Dec 14 09:38:37.377: INFO: Deleting pod "simpletest.rc-sd5tm" in namespace "gc-4721"
Dec 14 09:38:37.425: INFO: Deleting pod "simpletest.rc-sdcvc" in namespace "gc-4721"
Dec 14 09:38:37.478: INFO: Deleting pod "simpletest.rc-sfl8n" in namespace "gc-4721"
Dec 14 09:38:37.526: INFO: Deleting pod "simpletest.rc-snrpf" in namespace "gc-4721"
Dec 14 09:38:37.580: INFO: Deleting pod "simpletest.rc-spj47" in namespace "gc-4721"
Dec 14 09:38:37.626: INFO: Deleting pod "simpletest.rc-t78th" in namespace "gc-4721"
Dec 14 09:38:37.676: INFO: Deleting pod "simpletest.rc-vmn8z" in namespace "gc-4721"
Dec 14 09:38:37.727: INFO: Deleting pod "simpletest.rc-vvzjx" in namespace "gc-4721"
Dec 14 09:38:37.778: INFO: Deleting pod "simpletest.rc-wbx8m" in namespace "gc-4721"
Dec 14 09:38:37.827: INFO: Deleting pod "simpletest.rc-wn86c" in namespace "gc-4721"
Dec 14 09:38:37.876: INFO: Deleting pod "simpletest.rc-xh4st" in namespace "gc-4721"
Dec 14 09:38:37.927: INFO: Deleting pod "simpletest.rc-xl96w" in namespace "gc-4721"
Dec 14 09:38:37.976: INFO: Deleting pod "simpletest.rc-xlcbd" in namespace "gc-4721"
Dec 14 09:38:38.025: INFO: Deleting pod "simpletest.rc-xwsdm" in namespace "gc-4721"
Dec 14 09:38:38.077: INFO: Deleting pod "simpletest.rc-z89fg" in namespace "gc-4721"
Dec 14 09:38:38.126: INFO: Deleting pod "simpletest.rc-zb9rd" in namespace "gc-4721"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 14 09:38:38.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4721" for this suite. 12/14/22 09:38:38.223
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":336,"skipped":6206,"failed":0}
------------------------------
• [42.783 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:37:55.491
    Dec 14 09:37:55.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 12/14/22 09:37:55.492
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:37:55.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:37:55.51
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 12/14/22 09:37:55.521
    STEP: delete the rc 12/14/22 09:38:00.53
    STEP: wait for the rc to be deleted 12/14/22 09:38:00.548
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/14/22 09:38:05.552
    STEP: Gathering metrics 12/14/22 09:38:35.617
    W1214 09:38:35.628982    4635 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 14 09:38:35.629: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 14 09:38:35.629: INFO: Deleting pod "simpletest.rc-25b7d" in namespace "gc-4721"
    Dec 14 09:38:35.635: INFO: Deleting pod "simpletest.rc-2dvf4" in namespace "gc-4721"
    Dec 14 09:38:35.643: INFO: Deleting pod "simpletest.rc-2pg6z" in namespace "gc-4721"
    Dec 14 09:38:35.649: INFO: Deleting pod "simpletest.rc-48t4s" in namespace "gc-4721"
    Dec 14 09:38:35.655: INFO: Deleting pod "simpletest.rc-4g85j" in namespace "gc-4721"
    Dec 14 09:38:35.660: INFO: Deleting pod "simpletest.rc-4tx27" in namespace "gc-4721"
    Dec 14 09:38:35.667: INFO: Deleting pod "simpletest.rc-4xq65" in namespace "gc-4721"
    Dec 14 09:38:35.673: INFO: Deleting pod "simpletest.rc-5kjq8" in namespace "gc-4721"
    Dec 14 09:38:35.678: INFO: Deleting pod "simpletest.rc-5q64w" in namespace "gc-4721"
    Dec 14 09:38:35.694: INFO: Deleting pod "simpletest.rc-5xzrk" in namespace "gc-4721"
    Dec 14 09:38:35.701: INFO: Deleting pod "simpletest.rc-62q7t" in namespace "gc-4721"
    Dec 14 09:38:35.707: INFO: Deleting pod "simpletest.rc-68jvl" in namespace "gc-4721"
    Dec 14 09:38:35.713: INFO: Deleting pod "simpletest.rc-6wdkn" in namespace "gc-4721"
    Dec 14 09:38:35.718: INFO: Deleting pod "simpletest.rc-78cp8" in namespace "gc-4721"
    Dec 14 09:38:35.725: INFO: Deleting pod "simpletest.rc-8c77p" in namespace "gc-4721"
    Dec 14 09:38:35.734: INFO: Deleting pod "simpletest.rc-8l8w5" in namespace "gc-4721"
    Dec 14 09:38:35.741: INFO: Deleting pod "simpletest.rc-8lw7r" in namespace "gc-4721"
    Dec 14 09:38:35.747: INFO: Deleting pod "simpletest.rc-98czw" in namespace "gc-4721"
    Dec 14 09:38:35.754: INFO: Deleting pod "simpletest.rc-9cvv2" in namespace "gc-4721"
    Dec 14 09:38:35.761: INFO: Deleting pod "simpletest.rc-bcq52" in namespace "gc-4721"
    Dec 14 09:38:35.766: INFO: Deleting pod "simpletest.rc-bd74d" in namespace "gc-4721"
    Dec 14 09:38:35.771: INFO: Deleting pod "simpletest.rc-bgrzd" in namespace "gc-4721"
    Dec 14 09:38:35.778: INFO: Deleting pod "simpletest.rc-bqxmb" in namespace "gc-4721"
    Dec 14 09:38:35.783: INFO: Deleting pod "simpletest.rc-bx58d" in namespace "gc-4721"
    Dec 14 09:38:35.789: INFO: Deleting pod "simpletest.rc-cb6r4" in namespace "gc-4721"
    Dec 14 09:38:35.795: INFO: Deleting pod "simpletest.rc-chjhp" in namespace "gc-4721"
    Dec 14 09:38:35.801: INFO: Deleting pod "simpletest.rc-cqcmk" in namespace "gc-4721"
    Dec 14 09:38:35.807: INFO: Deleting pod "simpletest.rc-cwhqs" in namespace "gc-4721"
    Dec 14 09:38:35.815: INFO: Deleting pod "simpletest.rc-d42vn" in namespace "gc-4721"
    Dec 14 09:38:35.822: INFO: Deleting pod "simpletest.rc-d4lqd" in namespace "gc-4721"
    Dec 14 09:38:35.827: INFO: Deleting pod "simpletest.rc-dbtzt" in namespace "gc-4721"
    Dec 14 09:38:35.834: INFO: Deleting pod "simpletest.rc-djzp9" in namespace "gc-4721"
    Dec 14 09:38:35.840: INFO: Deleting pod "simpletest.rc-f4l8m" in namespace "gc-4721"
    Dec 14 09:38:35.845: INFO: Deleting pod "simpletest.rc-fgf8w" in namespace "gc-4721"
    Dec 14 09:38:35.850: INFO: Deleting pod "simpletest.rc-fhnzt" in namespace "gc-4721"
    Dec 14 09:38:35.856: INFO: Deleting pod "simpletest.rc-ftjns" in namespace "gc-4721"
    Dec 14 09:38:35.863: INFO: Deleting pod "simpletest.rc-g7pwm" in namespace "gc-4721"
    Dec 14 09:38:35.869: INFO: Deleting pod "simpletest.rc-gb22z" in namespace "gc-4721"
    Dec 14 09:38:35.874: INFO: Deleting pod "simpletest.rc-gp8xc" in namespace "gc-4721"
    Dec 14 09:38:35.880: INFO: Deleting pod "simpletest.rc-grxns" in namespace "gc-4721"
    Dec 14 09:38:35.887: INFO: Deleting pod "simpletest.rc-hq9m8" in namespace "gc-4721"
    Dec 14 09:38:35.894: INFO: Deleting pod "simpletest.rc-hwrct" in namespace "gc-4721"
    Dec 14 09:38:35.901: INFO: Deleting pod "simpletest.rc-j6x92" in namespace "gc-4721"
    Dec 14 09:38:35.907: INFO: Deleting pod "simpletest.rc-j87wj" in namespace "gc-4721"
    Dec 14 09:38:35.913: INFO: Deleting pod "simpletest.rc-jbscm" in namespace "gc-4721"
    Dec 14 09:38:35.918: INFO: Deleting pod "simpletest.rc-jctlv" in namespace "gc-4721"
    Dec 14 09:38:35.924: INFO: Deleting pod "simpletest.rc-k69sc" in namespace "gc-4721"
    Dec 14 09:38:35.931: INFO: Deleting pod "simpletest.rc-k8zs7" in namespace "gc-4721"
    Dec 14 09:38:35.938: INFO: Deleting pod "simpletest.rc-kpthn" in namespace "gc-4721"
    Dec 14 09:38:35.946: INFO: Deleting pod "simpletest.rc-l2d5n" in namespace "gc-4721"
    Dec 14 09:38:35.951: INFO: Deleting pod "simpletest.rc-l9p5p" in namespace "gc-4721"
    Dec 14 09:38:35.956: INFO: Deleting pod "simpletest.rc-lfv2l" in namespace "gc-4721"
    Dec 14 09:38:35.962: INFO: Deleting pod "simpletest.rc-lp2dw" in namespace "gc-4721"
    Dec 14 09:38:35.969: INFO: Deleting pod "simpletest.rc-lstrs" in namespace "gc-4721"
    Dec 14 09:38:35.975: INFO: Deleting pod "simpletest.rc-md5p9" in namespace "gc-4721"
    Dec 14 09:38:35.980: INFO: Deleting pod "simpletest.rc-mkrmm" in namespace "gc-4721"
    Dec 14 09:38:35.986: INFO: Deleting pod "simpletest.rc-mmgqt" in namespace "gc-4721"
    Dec 14 09:38:36.026: INFO: Deleting pod "simpletest.rc-mr8pl" in namespace "gc-4721"
    Dec 14 09:38:36.079: INFO: Deleting pod "simpletest.rc-mtn7r" in namespace "gc-4721"
    Dec 14 09:38:36.127: INFO: Deleting pod "simpletest.rc-n74jd" in namespace "gc-4721"
    Dec 14 09:38:36.180: INFO: Deleting pod "simpletest.rc-n8vds" in namespace "gc-4721"
    Dec 14 09:38:36.226: INFO: Deleting pod "simpletest.rc-n9t56" in namespace "gc-4721"
    Dec 14 09:38:36.276: INFO: Deleting pod "simpletest.rc-nb4zs" in namespace "gc-4721"
    Dec 14 09:38:36.324: INFO: Deleting pod "simpletest.rc-nf7px" in namespace "gc-4721"
    Dec 14 09:38:36.377: INFO: Deleting pod "simpletest.rc-nn4tg" in namespace "gc-4721"
    Dec 14 09:38:36.427: INFO: Deleting pod "simpletest.rc-nnqbx" in namespace "gc-4721"
    Dec 14 09:38:36.477: INFO: Deleting pod "simpletest.rc-nrzlm" in namespace "gc-4721"
    Dec 14 09:38:36.526: INFO: Deleting pod "simpletest.rc-nsdb8" in namespace "gc-4721"
    Dec 14 09:38:36.583: INFO: Deleting pod "simpletest.rc-nxlpf" in namespace "gc-4721"
    Dec 14 09:38:36.626: INFO: Deleting pod "simpletest.rc-p9fpx" in namespace "gc-4721"
    Dec 14 09:38:36.676: INFO: Deleting pod "simpletest.rc-pb58p" in namespace "gc-4721"
    Dec 14 09:38:36.725: INFO: Deleting pod "simpletest.rc-phh9d" in namespace "gc-4721"
    Dec 14 09:38:36.776: INFO: Deleting pod "simpletest.rc-pqqj5" in namespace "gc-4721"
    Dec 14 09:38:36.826: INFO: Deleting pod "simpletest.rc-ptqsn" in namespace "gc-4721"
    Dec 14 09:38:36.875: INFO: Deleting pod "simpletest.rc-pxh2d" in namespace "gc-4721"
    Dec 14 09:38:36.927: INFO: Deleting pod "simpletest.rc-r69h9" in namespace "gc-4721"
    Dec 14 09:38:36.977: INFO: Deleting pod "simpletest.rc-rbjmd" in namespace "gc-4721"
    Dec 14 09:38:37.027: INFO: Deleting pod "simpletest.rc-rgz29" in namespace "gc-4721"
    Dec 14 09:38:37.076: INFO: Deleting pod "simpletest.rc-rh8sd" in namespace "gc-4721"
    Dec 14 09:38:37.127: INFO: Deleting pod "simpletest.rc-rl4p9" in namespace "gc-4721"
    Dec 14 09:38:37.175: INFO: Deleting pod "simpletest.rc-rp2dn" in namespace "gc-4721"
    Dec 14 09:38:37.225: INFO: Deleting pod "simpletest.rc-rp6wl" in namespace "gc-4721"
    Dec 14 09:38:37.277: INFO: Deleting pod "simpletest.rc-s4dfp" in namespace "gc-4721"
    Dec 14 09:38:37.326: INFO: Deleting pod "simpletest.rc-s4ts6" in namespace "gc-4721"
    Dec 14 09:38:37.377: INFO: Deleting pod "simpletest.rc-sd5tm" in namespace "gc-4721"
    Dec 14 09:38:37.425: INFO: Deleting pod "simpletest.rc-sdcvc" in namespace "gc-4721"
    Dec 14 09:38:37.478: INFO: Deleting pod "simpletest.rc-sfl8n" in namespace "gc-4721"
    Dec 14 09:38:37.526: INFO: Deleting pod "simpletest.rc-snrpf" in namespace "gc-4721"
    Dec 14 09:38:37.580: INFO: Deleting pod "simpletest.rc-spj47" in namespace "gc-4721"
    Dec 14 09:38:37.626: INFO: Deleting pod "simpletest.rc-t78th" in namespace "gc-4721"
    Dec 14 09:38:37.676: INFO: Deleting pod "simpletest.rc-vmn8z" in namespace "gc-4721"
    Dec 14 09:38:37.727: INFO: Deleting pod "simpletest.rc-vvzjx" in namespace "gc-4721"
    Dec 14 09:38:37.778: INFO: Deleting pod "simpletest.rc-wbx8m" in namespace "gc-4721"
    Dec 14 09:38:37.827: INFO: Deleting pod "simpletest.rc-wn86c" in namespace "gc-4721"
    Dec 14 09:38:37.876: INFO: Deleting pod "simpletest.rc-xh4st" in namespace "gc-4721"
    Dec 14 09:38:37.927: INFO: Deleting pod "simpletest.rc-xl96w" in namespace "gc-4721"
    Dec 14 09:38:37.976: INFO: Deleting pod "simpletest.rc-xlcbd" in namespace "gc-4721"
    Dec 14 09:38:38.025: INFO: Deleting pod "simpletest.rc-xwsdm" in namespace "gc-4721"
    Dec 14 09:38:38.077: INFO: Deleting pod "simpletest.rc-z89fg" in namespace "gc-4721"
    Dec 14 09:38:38.126: INFO: Deleting pod "simpletest.rc-zb9rd" in namespace "gc-4721"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 14 09:38:38.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4721" for this suite. 12/14/22 09:38:38.223
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:38:38.274
Dec 14 09:38:38.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 12/14/22 09:38:38.275
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:38.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:38.292
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/14/22 09:38:38.306
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:38:39.035
STEP: Deploying the webhook pod 12/14/22 09:38:39.043
STEP: Wait for the deployment to be ready 12/14/22 09:38:39.072
Dec 14 09:38:39.082: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 09:38:41.094
STEP: Verifying the service has paired with the endpoint 12/14/22 09:38:41.103
Dec 14 09:38:42.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 09:38:42.107
STEP: create a pod that should be denied by the webhook 12/14/22 09:38:42.187
STEP: create a pod that causes the webhook to hang 12/14/22 09:38:42.285
STEP: create a configmap that should be denied by the webhook 12/14/22 09:38:52.301
STEP: create a configmap that should be admitted by the webhook 12/14/22 09:38:52.387
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 09:38:52.49
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 09:38:52.591
STEP: create a namespace that bypass the webhook 12/14/22 09:38:52.644
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/14/22 09:38:52.65
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 14 09:38:52.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7762" for this suite. 12/14/22 09:38:52.724
STEP: Destroying namespace "webhook-7762-markers" for this suite. 12/14/22 09:38:52.729
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":337,"skipped":6206,"failed":0}
------------------------------
• [14.517 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:38:38.274
    Dec 14 09:38:38.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 12/14/22 09:38:38.275
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:38.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:38.292
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/14/22 09:38:38.306
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 09:38:39.035
    STEP: Deploying the webhook pod 12/14/22 09:38:39.043
    STEP: Wait for the deployment to be ready 12/14/22 09:38:39.072
    Dec 14 09:38:39.082: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 09:38:41.094
    STEP: Verifying the service has paired with the endpoint 12/14/22 09:38:41.103
    Dec 14 09:38:42.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 09:38:42.107
    STEP: create a pod that should be denied by the webhook 12/14/22 09:38:42.187
    STEP: create a pod that causes the webhook to hang 12/14/22 09:38:42.285
    STEP: create a configmap that should be denied by the webhook 12/14/22 09:38:52.301
    STEP: create a configmap that should be admitted by the webhook 12/14/22 09:38:52.387
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 09:38:52.49
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 09:38:52.591
    STEP: create a namespace that bypass the webhook 12/14/22 09:38:52.644
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/14/22 09:38:52.65
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 14 09:38:52.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7762" for this suite. 12/14/22 09:38:52.724
    STEP: Destroying namespace "webhook-7762-markers" for this suite. 12/14/22 09:38:52.729
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:38:52.792
Dec 14 09:38:52.792: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:38:52.793
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:52.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:52.809
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-9804/secret-test-9d3b57cc-c319-42b9-a845-2a2afe737026 12/14/22 09:38:52.814
STEP: Creating a pod to test consume secrets 12/14/22 09:38:52.819
Dec 14 09:38:52.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1" in namespace "secrets-9804" to be "Succeeded or Failed"
Dec 14 09:38:52.844: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.991806ms
Dec 14 09:38:54.850: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010995978s
Dec 14 09:38:56.850: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011024433s
STEP: Saw pod success 12/14/22 09:38:56.85
Dec 14 09:38:56.851: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1" satisfied condition "Succeeded or Failed"
Dec 14 09:38:56.854: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1 container env-test: <nil>
STEP: delete the pod 12/14/22 09:38:56.865
Dec 14 09:38:56.874: INFO: Waiting for pod pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1 to disappear
Dec 14 09:38:56.878: INFO: Pod pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:38:56.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9804" for this suite. 12/14/22 09:38:56.883
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":338,"skipped":6256,"failed":0}
------------------------------
• [4.096 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:38:52.792
    Dec 14 09:38:52.792: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:38:52.793
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:52.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:52.809
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-9804/secret-test-9d3b57cc-c319-42b9-a845-2a2afe737026 12/14/22 09:38:52.814
    STEP: Creating a pod to test consume secrets 12/14/22 09:38:52.819
    Dec 14 09:38:52.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1" in namespace "secrets-9804" to be "Succeeded or Failed"
    Dec 14 09:38:52.844: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.991806ms
    Dec 14 09:38:54.850: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010995978s
    Dec 14 09:38:56.850: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011024433s
    STEP: Saw pod success 12/14/22 09:38:56.85
    Dec 14 09:38:56.851: INFO: Pod "pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1" satisfied condition "Succeeded or Failed"
    Dec 14 09:38:56.854: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1 container env-test: <nil>
    STEP: delete the pod 12/14/22 09:38:56.865
    Dec 14 09:38:56.874: INFO: Waiting for pod pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1 to disappear
    Dec 14 09:38:56.878: INFO: Pod pod-configmaps-ffaba5d7-d434-41bc-8e40-14c30fe7e6b1 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:38:56.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9804" for this suite. 12/14/22 09:38:56.883
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:38:56.889
Dec 14 09:38:56.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 12/14/22 09:38:56.89
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:56.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:56.907
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 12/14/22 09:38:56.913
Dec 14 09:38:56.917: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 12/14/22 09:38:56.917
Dec 14 09:38:56.923: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 12/14/22 09:38:56.923
Dec 14 09:38:56.932: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 14 09:38:56.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6750" for this suite. 12/14/22 09:38:56.943
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":339,"skipped":6256,"failed":0}
------------------------------
• [0.061 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:38:56.889
    Dec 14 09:38:56.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 12/14/22 09:38:56.89
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:56.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:56.907
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 12/14/22 09:38:56.913
    Dec 14 09:38:56.917: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 12/14/22 09:38:56.917
    Dec 14 09:38:56.923: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 12/14/22 09:38:56.923
    Dec 14 09:38:56.932: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 14 09:38:56.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6750" for this suite. 12/14/22 09:38:56.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:38:56.951
Dec 14 09:38:56.951: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:38:56.952
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:56.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:56.973
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:38:56.978
Dec 14 09:38:56.979: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 14 09:38:57.051: INFO: stderr: ""
Dec 14 09:38:57.051: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 12/14/22 09:38:57.051
STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 09:39:02.103
Dec 14 09:39:02.103: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 get pod e2e-test-httpd-pod -o json'
Dec 14 09:39:02.164: INFO: stderr: ""
Dec 14 09:39:02.164: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"8506470d6061a7a55d2f4734b2017a298a0f52d39a9239c13373c6849691b4bd\",\n            \"cni.projectcalico.org/podIP\": \"100.64.1.30/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.64.1.30/32\"\n        },\n        \"creationTimestamp\": \"2022-12-14T09:38:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1140\",\n        \"resourceVersion\": \"51340\",\n        \"uid\": \"8f12adb5-b773-4392-9cca-d7233bb5c989\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tm5on-jne.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r6vk6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r6vk6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c8c9ebd8b98d0fcf2a46ed9e46c4ab796c49a2cc283ce131dbee836908df1bb2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-14T09:38:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.3.210\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.30\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.30\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-14T09:38:57Z\"\n    }\n}\n"
STEP: replace the image in the pod 12/14/22 09:39:02.165
Dec 14 09:39:02.165: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 replace -f -'
Dec 14 09:39:02.774: INFO: stderr: ""
Dec 14 09:39:02.774: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 12/14/22 09:39:02.774
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Dec 14 09:39:02.778: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 delete pods e2e-test-httpd-pod'
Dec 14 09:39:04.943: INFO: stderr: ""
Dec 14 09:39:04.943: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:39:04.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1140" for this suite. 12/14/22 09:39:04.953
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":340,"skipped":6324,"failed":0}
------------------------------
• [8.007 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:38:56.951
    Dec 14 09:38:56.951: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:38:56.952
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:38:56.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:38:56.973
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:38:56.978
    Dec 14 09:38:56.979: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 14 09:38:57.051: INFO: stderr: ""
    Dec 14 09:38:57.051: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 12/14/22 09:38:57.051
    STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 09:39:02.103
    Dec 14 09:39:02.103: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 get pod e2e-test-httpd-pod -o json'
    Dec 14 09:39:02.164: INFO: stderr: ""
    Dec 14 09:39:02.164: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"8506470d6061a7a55d2f4734b2017a298a0f52d39a9239c13373c6849691b4bd\",\n            \"cni.projectcalico.org/podIP\": \"100.64.1.30/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.64.1.30/32\"\n        },\n        \"creationTimestamp\": \"2022-12-14T09:38:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1140\",\n        \"resourceVersion\": \"51340\",\n        \"uid\": \"8f12adb5-b773-4392-9cca-d7233bb5c989\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tm5on-jne.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r6vk6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r6vk6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T09:38:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c8c9ebd8b98d0fcf2a46ed9e46c4ab796c49a2cc283ce131dbee836908df1bb2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-14T09:38:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.3.210\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.30\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.30\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-14T09:38:57Z\"\n    }\n}\n"
    STEP: replace the image in the pod 12/14/22 09:39:02.165
    Dec 14 09:39:02.165: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 replace -f -'
    Dec 14 09:39:02.774: INFO: stderr: ""
    Dec 14 09:39:02.774: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 12/14/22 09:39:02.774
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Dec 14 09:39:02.778: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1140 delete pods e2e-test-httpd-pod'
    Dec 14 09:39:04.943: INFO: stderr: ""
    Dec 14 09:39:04.943: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:39:04.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1140" for this suite. 12/14/22 09:39:04.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:39:04.958
Dec 14 09:39:04.958: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:39:04.959
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:04.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:04.974
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-8047 12/14/22 09:39:04.979
STEP: creating service affinity-clusterip in namespace services-8047 12/14/22 09:39:04.979
STEP: creating replication controller affinity-clusterip in namespace services-8047 12/14/22 09:39:04.989
I1214 09:39:04.994327    4635 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8047, replica count: 3
I1214 09:39:08.046237    4635 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:39:08.056: INFO: Creating new exec pod
Dec 14 09:39:08.067: INFO: Waiting up to 5m0s for pod "execpod-affinityxhgt7" in namespace "services-8047" to be "running"
Dec 14 09:39:08.070: INFO: Pod "execpod-affinityxhgt7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313217ms
Dec 14 09:39:10.075: INFO: Pod "execpod-affinityxhgt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008590832s
Dec 14 09:39:10.075: INFO: Pod "execpod-affinityxhgt7" satisfied condition "running"
Dec 14 09:39:11.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8047 exec execpod-affinityxhgt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Dec 14 09:39:11.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 14 09:39:11.439: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:39:11.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8047 exec execpod-affinityxhgt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.108.13.147 80'
Dec 14 09:39:11.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.108.13.147 80\nConnection to 100.108.13.147 80 port [tcp/http] succeeded!\n"
Dec 14 09:39:11.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 09:39:11.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8047 exec execpod-affinityxhgt7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.13.147:80/ ; done'
Dec 14 09:39:12.339: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n"
Dec 14 09:39:12.339: INFO: stdout: "\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l"
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
Dec 14 09:39:12.339: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8047, will wait for the garbage collector to delete the pods 12/14/22 09:39:12.348
Dec 14 09:39:12.408: INFO: Deleting ReplicationController affinity-clusterip took: 5.891086ms
Dec 14 09:39:12.509: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.862204ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:39:14.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8047" for this suite. 12/14/22 09:39:14.029
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":341,"skipped":6332,"failed":0}
------------------------------
• [9.077 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:39:04.958
    Dec 14 09:39:04.958: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:39:04.959
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:04.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:04.974
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-8047 12/14/22 09:39:04.979
    STEP: creating service affinity-clusterip in namespace services-8047 12/14/22 09:39:04.979
    STEP: creating replication controller affinity-clusterip in namespace services-8047 12/14/22 09:39:04.989
    I1214 09:39:04.994327    4635 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8047, replica count: 3
    I1214 09:39:08.046237    4635 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:39:08.056: INFO: Creating new exec pod
    Dec 14 09:39:08.067: INFO: Waiting up to 5m0s for pod "execpod-affinityxhgt7" in namespace "services-8047" to be "running"
    Dec 14 09:39:08.070: INFO: Pod "execpod-affinityxhgt7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313217ms
    Dec 14 09:39:10.075: INFO: Pod "execpod-affinityxhgt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008590832s
    Dec 14 09:39:10.075: INFO: Pod "execpod-affinityxhgt7" satisfied condition "running"
    Dec 14 09:39:11.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8047 exec execpod-affinityxhgt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Dec 14 09:39:11.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Dec 14 09:39:11.439: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:39:11.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8047 exec execpod-affinityxhgt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.108.13.147 80'
    Dec 14 09:39:11.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.108.13.147 80\nConnection to 100.108.13.147 80 port [tcp/http] succeeded!\n"
    Dec 14 09:39:11.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 14 09:39:11.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8047 exec execpod-affinityxhgt7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.13.147:80/ ; done'
    Dec 14 09:39:12.339: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.13.147:80/\n"
    Dec 14 09:39:12.339: INFO: stdout: "\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l\naffinity-clusterip-9xn6l"
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Received response from host: affinity-clusterip-9xn6l
    Dec 14 09:39:12.339: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8047, will wait for the garbage collector to delete the pods 12/14/22 09:39:12.348
    Dec 14 09:39:12.408: INFO: Deleting ReplicationController affinity-clusterip took: 5.891086ms
    Dec 14 09:39:12.509: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.862204ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:39:14.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8047" for this suite. 12/14/22 09:39:14.029
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:39:14.035
Dec 14 09:39:14.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:39:14.036
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:14.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:14.053
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 12/14/22 09:39:14.058
Dec 14 09:39:14.058: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2858 create -f -'
Dec 14 09:39:14.707: INFO: stderr: ""
Dec 14 09:39:14.707: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/14/22 09:39:14.707
Dec 14 09:39:15.713: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:39:15.713: INFO: Found 0 / 1
Dec 14 09:39:16.714: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:39:16.714: INFO: Found 1 / 1
Dec 14 09:39:16.714: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 12/14/22 09:39:16.714
Dec 14 09:39:16.718: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:39:16.718: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 09:39:16.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2858 patch pod agnhost-primary-v585f -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 14 09:39:16.788: INFO: stderr: ""
Dec 14 09:39:16.788: INFO: stdout: "pod/agnhost-primary-v585f patched\n"
STEP: checking annotations 12/14/22 09:39:16.788
Dec 14 09:39:16.844: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 09:39:16.844: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:39:16.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2858" for this suite. 12/14/22 09:39:16.85
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":342,"skipped":6356,"failed":0}
------------------------------
• [2.821 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:39:14.035
    Dec 14 09:39:14.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:39:14.036
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:14.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:14.053
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 12/14/22 09:39:14.058
    Dec 14 09:39:14.058: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2858 create -f -'
    Dec 14 09:39:14.707: INFO: stderr: ""
    Dec 14 09:39:14.707: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/14/22 09:39:14.707
    Dec 14 09:39:15.713: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:39:15.713: INFO: Found 0 / 1
    Dec 14 09:39:16.714: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:39:16.714: INFO: Found 1 / 1
    Dec 14 09:39:16.714: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 12/14/22 09:39:16.714
    Dec 14 09:39:16.718: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:39:16.718: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 14 09:39:16.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2858 patch pod agnhost-primary-v585f -p {"metadata":{"annotations":{"x":"y"}}}'
    Dec 14 09:39:16.788: INFO: stderr: ""
    Dec 14 09:39:16.788: INFO: stdout: "pod/agnhost-primary-v585f patched\n"
    STEP: checking annotations 12/14/22 09:39:16.788
    Dec 14 09:39:16.844: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 09:39:16.844: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:39:16.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2858" for this suite. 12/14/22 09:39:16.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:39:16.857
Dec 14 09:39:16.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:39:16.857
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:16.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:16.874
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 12/14/22 09:39:16.879
STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:39:16.884
STEP: Creating a ResourceQuota with not terminating scope 12/14/22 09:39:18.89
STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:39:18.895
STEP: Creating a long running pod 12/14/22 09:39:20.901
STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/14/22 09:39:20.914
STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/14/22 09:39:22.919
STEP: Deleting the pod 12/14/22 09:39:24.924
STEP: Ensuring resource quota status released the pod usage 12/14/22 09:39:24.931
STEP: Creating a terminating pod 12/14/22 09:39:26.936
STEP: Ensuring resource quota with terminating scope captures the pod usage 12/14/22 09:39:26.949
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/14/22 09:39:28.954
STEP: Deleting the pod 12/14/22 09:39:30.958
STEP: Ensuring resource quota status released the pod usage 12/14/22 09:39:30.966
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:39:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4220" for this suite. 12/14/22 09:39:32.976
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":343,"skipped":6361,"failed":0}
------------------------------
• [16.124 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:39:16.857
    Dec 14 09:39:16.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:39:16.857
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:16.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:16.874
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 12/14/22 09:39:16.879
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:39:16.884
    STEP: Creating a ResourceQuota with not terminating scope 12/14/22 09:39:18.89
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 09:39:18.895
    STEP: Creating a long running pod 12/14/22 09:39:20.901
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/14/22 09:39:20.914
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/14/22 09:39:22.919
    STEP: Deleting the pod 12/14/22 09:39:24.924
    STEP: Ensuring resource quota status released the pod usage 12/14/22 09:39:24.931
    STEP: Creating a terminating pod 12/14/22 09:39:26.936
    STEP: Ensuring resource quota with terminating scope captures the pod usage 12/14/22 09:39:26.949
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/14/22 09:39:28.954
    STEP: Deleting the pod 12/14/22 09:39:30.958
    STEP: Ensuring resource quota status released the pod usage 12/14/22 09:39:30.966
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:39:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4220" for this suite. 12/14/22 09:39:32.976
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:39:32.981
Dec 14 09:39:32.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 09:39:32.982
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:32.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:32.997
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 12/14/22 09:39:33.002
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2131;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2131;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +notcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_tcp@PTR;sleep 1; done
 12/14/22 09:39:33.014
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2131;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2131;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +notcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_tcp@PTR;sleep 1; done
 12/14/22 09:39:33.014
STEP: creating a pod to probe DNS 12/14/22 09:39:33.014
STEP: submitting the pod to kubernetes 12/14/22 09:39:33.014
Dec 14 09:39:33.026: INFO: Waiting up to 15m0s for pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f" in namespace "dns-2131" to be "running"
Dec 14 09:39:33.030: INFO: Pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881882ms
Dec 14 09:39:35.036: INFO: Pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009870137s
Dec 14 09:39:35.036: INFO: Pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f" satisfied condition "running"
STEP: retrieving the pod 12/14/22 09:39:35.036
STEP: looking for the results for each expected name from probers 12/14/22 09:39:35.04
Dec 14 09:39:35.141: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.190: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.198: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.207: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.218: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.225: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.233: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.285: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.292: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.299: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.306: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.314: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.328: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.335: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:35.365: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:39:40.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.418: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.433: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.446: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.497: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.503: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.511: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.518: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.525: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.544: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:40.572: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:39:45.373: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.418: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.427: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.487: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.498: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.507: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.515: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.565: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.615: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.624: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.631: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.645: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:45.676: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:39:50.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.418: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.426: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.433: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.446: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.459: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.490: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.497: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.503: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.514: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.527: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.533: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:50.559: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:39:55.378: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.429: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.438: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.459: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.468: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.474: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.481: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.516: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.525: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.534: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.546: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.554: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.561: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.567: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.574: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:39:55.600: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:40:00.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.423: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.439: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.473: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.528: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.535: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.543: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.552: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.560: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.568: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.575: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.582: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:00.616: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:40:05.469: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:05.561: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:05.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
Dec 14 09:40:05.600: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

Dec 14 09:40:10.591: INFO: DNS probes using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f succeeded

STEP: deleting the pod 12/14/22 09:40:10.591
STEP: deleting the test service 12/14/22 09:40:10.601
STEP: deleting the test headless service 12/14/22 09:40:10.614
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 09:40:10.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2131" for this suite. 12/14/22 09:40:10.627
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":344,"skipped":6365,"failed":0}
------------------------------
• [37.652 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:39:32.981
    Dec 14 09:39:32.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 09:39:32.982
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:39:32.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:39:32.997
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 12/14/22 09:39:33.002
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2131;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2131;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +notcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_tcp@PTR;sleep 1; done
     12/14/22 09:39:33.014
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2131;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2131;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2131.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2131.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2131.svc;check="$$(dig +notcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.130.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.130.239_tcp@PTR;sleep 1; done
     12/14/22 09:39:33.014
    STEP: creating a pod to probe DNS 12/14/22 09:39:33.014
    STEP: submitting the pod to kubernetes 12/14/22 09:39:33.014
    Dec 14 09:39:33.026: INFO: Waiting up to 15m0s for pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f" in namespace "dns-2131" to be "running"
    Dec 14 09:39:33.030: INFO: Pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881882ms
    Dec 14 09:39:35.036: INFO: Pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009870137s
    Dec 14 09:39:35.036: INFO: Pod "dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 09:39:35.036
    STEP: looking for the results for each expected name from probers 12/14/22 09:39:35.04
    Dec 14 09:39:35.141: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.190: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.198: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.207: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.218: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.225: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.233: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.285: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.292: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.299: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.306: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.314: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.328: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.335: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:35.365: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:39:40.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.418: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.433: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.446: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.497: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.503: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.511: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.518: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.525: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.544: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:40.572: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:39:45.373: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.418: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.427: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.487: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.498: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.507: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.515: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.565: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.615: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.624: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.631: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.645: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:45.676: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:39:50.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.418: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.426: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.433: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.446: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.459: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.490: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.497: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.503: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.514: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.527: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.533: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:50.559: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:39:55.378: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.429: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.438: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.459: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.468: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.474: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.481: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.516: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.525: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.534: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.546: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.554: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.561: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.567: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.574: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:39:55.600: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:40:00.374: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.423: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.439: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.473: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.528: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.535: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.543: INFO: Unable to read jessie_udp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.552: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131 from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.560: INFO: Unable to read jessie_udp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.568: INFO: Unable to read jessie_tcp@dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.575: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.582: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:00.616: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2131 wheezy_tcp@dns-test-service.dns-2131 wheezy_udp@dns-test-service.dns-2131.svc wheezy_tcp@dns-test-service.dns-2131.svc wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2131 jessie_tcp@dns-test-service.dns-2131 jessie_udp@dns-test-service.dns-2131.svc jessie_tcp@dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:40:05.469: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:05.561: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:05.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc from pod dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f: the server could not find the requested resource (get pods dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f)
    Dec 14 09:40:05.600: INFO: Lookups using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_udp@_http._tcp.dns-test-service.dns-2131.svc jessie_tcp@_http._tcp.dns-test-service.dns-2131.svc]

    Dec 14 09:40:10.591: INFO: DNS probes using dns-2131/dns-test-50801fd6-f2f0-4ea0-a550-a7ce11007b0f succeeded

    STEP: deleting the pod 12/14/22 09:40:10.591
    STEP: deleting the test service 12/14/22 09:40:10.601
    STEP: deleting the test headless service 12/14/22 09:40:10.614
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 09:40:10.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2131" for this suite. 12/14/22 09:40:10.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:10.634
Dec 14 09:40:10.634: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 12/14/22 09:40:10.634
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:10.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:10.652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:40:10.657
Dec 14 09:40:10.657: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3225 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Dec 14 09:40:10.795: INFO: stderr: ""
Dec 14 09:40:10.795: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 09:40:10.795
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Dec 14 09:40:10.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3225 delete pods e2e-test-httpd-pod'
Dec 14 09:40:13.110: INFO: stderr: ""
Dec 14 09:40:13.110: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 14 09:40:13.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3225" for this suite. 12/14/22 09:40:13.117
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":345,"skipped":6388,"failed":0}
------------------------------
• [2.489 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:10.634
    Dec 14 09:40:10.634: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 12/14/22 09:40:10.634
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:10.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:10.652
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/14/22 09:40:10.657
    Dec 14 09:40:10.657: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3225 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Dec 14 09:40:10.795: INFO: stderr: ""
    Dec 14 09:40:10.795: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 09:40:10.795
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Dec 14 09:40:10.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3225 delete pods e2e-test-httpd-pod'
    Dec 14 09:40:13.110: INFO: stderr: ""
    Dec 14 09:40:13.110: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 14 09:40:13.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3225" for this suite. 12/14/22 09:40:13.117
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:13.123
Dec 14 09:40:13.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:40:13.123
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:13.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:13.142
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-873d7f50-6188-4dc0-8842-f7de1da7d1dd 12/14/22 09:40:13.147
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:40:13.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2714" for this suite. 12/14/22 09:40:13.156
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":346,"skipped":6392,"failed":0}
------------------------------
• [0.037 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:13.123
    Dec 14 09:40:13.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:40:13.123
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:13.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:13.142
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-873d7f50-6188-4dc0-8842-f7de1da7d1dd 12/14/22 09:40:13.147
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:40:13.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2714" for this suite. 12/14/22 09:40:13.156
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:13.16
Dec 14 09:40:13.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 12/14/22 09:40:13.161
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:13.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:13.177
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-846c8f1c-79a2-45b4-b3fa-9a1e8b8e350d 12/14/22 09:40:13.199
STEP: Creating a pod to test consume secrets 12/14/22 09:40:13.203
Dec 14 09:40:13.217: INFO: Waiting up to 5m0s for pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d" in namespace "secrets-1536" to be "Succeeded or Failed"
Dec 14 09:40:13.221: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646769ms
Dec 14 09:40:15.227: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009770883s
Dec 14 09:40:17.227: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010130072s
STEP: Saw pod success 12/14/22 09:40:17.227
Dec 14 09:40:17.228: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d" satisfied condition "Succeeded or Failed"
Dec 14 09:40:17.232: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:40:17.242
Dec 14 09:40:17.249: INFO: Waiting for pod pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d to disappear
Dec 14 09:40:17.252: INFO: Pod pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 14 09:40:17.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1536" for this suite. 12/14/22 09:40:17.259
STEP: Destroying namespace "secret-namespace-3396" for this suite. 12/14/22 09:40:17.263
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":347,"skipped":6394,"failed":0}
------------------------------
• [4.107 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:13.16
    Dec 14 09:40:13.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 12/14/22 09:40:13.161
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:13.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:13.177
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-846c8f1c-79a2-45b4-b3fa-9a1e8b8e350d 12/14/22 09:40:13.199
    STEP: Creating a pod to test consume secrets 12/14/22 09:40:13.203
    Dec 14 09:40:13.217: INFO: Waiting up to 5m0s for pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d" in namespace "secrets-1536" to be "Succeeded or Failed"
    Dec 14 09:40:13.221: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646769ms
    Dec 14 09:40:15.227: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009770883s
    Dec 14 09:40:17.227: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010130072s
    STEP: Saw pod success 12/14/22 09:40:17.227
    Dec 14 09:40:17.228: INFO: Pod "pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d" satisfied condition "Succeeded or Failed"
    Dec 14 09:40:17.232: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:40:17.242
    Dec 14 09:40:17.249: INFO: Waiting for pod pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d to disappear
    Dec 14 09:40:17.252: INFO: Pod pod-secrets-7358c8ba-9b67-4758-9e74-57f1ea559d1d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 14 09:40:17.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1536" for this suite. 12/14/22 09:40:17.259
    STEP: Destroying namespace "secret-namespace-3396" for this suite. 12/14/22 09:40:17.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:17.268
Dec 14 09:40:17.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 12/14/22 09:40:17.268
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:17.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:17.285
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8035-delete-me 12/14/22 09:40:17.295
STEP: Waiting for the RuntimeClass to disappear 12/14/22 09:40:17.3
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 14 09:40:17.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8035" for this suite. 12/14/22 09:40:17.316
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":348,"skipped":6407,"failed":0}
------------------------------
• [0.053 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:17.268
    Dec 14 09:40:17.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 09:40:17.268
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:17.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:17.285
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8035-delete-me 12/14/22 09:40:17.295
    STEP: Waiting for the RuntimeClass to disappear 12/14/22 09:40:17.3
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 14 09:40:17.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8035" for this suite. 12/14/22 09:40:17.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:17.322
Dec 14 09:40:17.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:40:17.322
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:17.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:17.339
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-7137/configmap-test-3e81ef39-0554-411b-b8ff-9577bb424961 12/14/22 09:40:17.344
STEP: Creating a pod to test consume configMaps 12/14/22 09:40:17.348
Dec 14 09:40:17.359: INFO: Waiting up to 5m0s for pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352" in namespace "configmap-7137" to be "Succeeded or Failed"
Dec 14 09:40:17.363: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307266ms
Dec 14 09:40:19.368: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009518033s
Dec 14 09:40:21.369: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010469349s
STEP: Saw pod success 12/14/22 09:40:21.369
Dec 14 09:40:21.369: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352" satisfied condition "Succeeded or Failed"
Dec 14 09:40:21.374: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352 container env-test: <nil>
STEP: delete the pod 12/14/22 09:40:21.386
Dec 14 09:40:21.394: INFO: Waiting for pod pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352 to disappear
Dec 14 09:40:21.397: INFO: Pod pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:40:21.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7137" for this suite. 12/14/22 09:40:21.403
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":349,"skipped":6434,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:17.322
    Dec 14 09:40:17.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:40:17.322
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:17.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:17.339
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-7137/configmap-test-3e81ef39-0554-411b-b8ff-9577bb424961 12/14/22 09:40:17.344
    STEP: Creating a pod to test consume configMaps 12/14/22 09:40:17.348
    Dec 14 09:40:17.359: INFO: Waiting up to 5m0s for pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352" in namespace "configmap-7137" to be "Succeeded or Failed"
    Dec 14 09:40:17.363: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307266ms
    Dec 14 09:40:19.368: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009518033s
    Dec 14 09:40:21.369: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010469349s
    STEP: Saw pod success 12/14/22 09:40:21.369
    Dec 14 09:40:21.369: INFO: Pod "pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352" satisfied condition "Succeeded or Failed"
    Dec 14 09:40:21.374: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352 container env-test: <nil>
    STEP: delete the pod 12/14/22 09:40:21.386
    Dec 14 09:40:21.394: INFO: Waiting for pod pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352 to disappear
    Dec 14 09:40:21.397: INFO: Pod pod-configmaps-8cc5e842-6c7d-43d1-891d-0f2efd716352 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:40:21.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7137" for this suite. 12/14/22 09:40:21.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:21.408
Dec 14 09:40:21.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:40:21.409
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:21.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:21.424
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-ac9b1384-c61c-4c5d-b38f-9bc3eb14e696 12/14/22 09:40:21.429
STEP: Creating a pod to test consume secrets 12/14/22 09:40:21.434
Dec 14 09:40:21.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578" in namespace "projected-6053" to be "Succeeded or Failed"
Dec 14 09:40:21.448: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578": Phase="Pending", Reason="", readiness=false. Elapsed: 3.282744ms
Dec 14 09:40:23.454: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009717794s
Dec 14 09:40:25.453: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008204774s
STEP: Saw pod success 12/14/22 09:40:25.453
Dec 14 09:40:25.453: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578" satisfied condition "Succeeded or Failed"
Dec 14 09:40:25.457: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:40:25.472
Dec 14 09:40:25.479: INFO: Waiting for pod pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578 to disappear
Dec 14 09:40:25.483: INFO: Pod pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 09:40:25.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6053" for this suite. 12/14/22 09:40:25.489
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":350,"skipped":6448,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:21.408
    Dec 14 09:40:21.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:40:21.409
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:21.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:21.424
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-ac9b1384-c61c-4c5d-b38f-9bc3eb14e696 12/14/22 09:40:21.429
    STEP: Creating a pod to test consume secrets 12/14/22 09:40:21.434
    Dec 14 09:40:21.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578" in namespace "projected-6053" to be "Succeeded or Failed"
    Dec 14 09:40:21.448: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578": Phase="Pending", Reason="", readiness=false. Elapsed: 3.282744ms
    Dec 14 09:40:23.454: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009717794s
    Dec 14 09:40:25.453: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008204774s
    STEP: Saw pod success 12/14/22 09:40:25.453
    Dec 14 09:40:25.453: INFO: Pod "pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578" satisfied condition "Succeeded or Failed"
    Dec 14 09:40:25.457: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:40:25.472
    Dec 14 09:40:25.479: INFO: Waiting for pod pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578 to disappear
    Dec 14 09:40:25.483: INFO: Pod pod-projected-secrets-314e501e-be74-4f6e-af46-c4c5548ae578 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 09:40:25.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6053" for this suite. 12/14/22 09:40:25.489
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:25.493
Dec 14 09:40:25.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:40:25.494
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:25.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:25.509
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Dec 14 09:40:25.531: INFO: created pod pod-service-account-defaultsa
Dec 14 09:40:25.531: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 14 09:40:25.541: INFO: created pod pod-service-account-mountsa
Dec 14 09:40:25.541: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 14 09:40:25.555: INFO: created pod pod-service-account-nomountsa
Dec 14 09:40:25.555: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 14 09:40:25.564: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 14 09:40:25.564: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 14 09:40:25.571: INFO: created pod pod-service-account-mountsa-mountspec
Dec 14 09:40:25.571: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 14 09:40:25.591: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 14 09:40:25.591: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 14 09:40:25.598: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 14 09:40:25.598: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 14 09:40:25.605: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 14 09:40:25.605: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 14 09:40:25.611: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 14 09:40:25.611: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 14 09:40:25.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4531" for this suite. 12/14/22 09:40:25.615
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":351,"skipped":6449,"failed":0}
------------------------------
• [0.127 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:25.493
    Dec 14 09:40:25.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 09:40:25.494
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:25.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:25.509
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Dec 14 09:40:25.531: INFO: created pod pod-service-account-defaultsa
    Dec 14 09:40:25.531: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Dec 14 09:40:25.541: INFO: created pod pod-service-account-mountsa
    Dec 14 09:40:25.541: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Dec 14 09:40:25.555: INFO: created pod pod-service-account-nomountsa
    Dec 14 09:40:25.555: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Dec 14 09:40:25.564: INFO: created pod pod-service-account-defaultsa-mountspec
    Dec 14 09:40:25.564: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Dec 14 09:40:25.571: INFO: created pod pod-service-account-mountsa-mountspec
    Dec 14 09:40:25.571: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Dec 14 09:40:25.591: INFO: created pod pod-service-account-nomountsa-mountspec
    Dec 14 09:40:25.591: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Dec 14 09:40:25.598: INFO: created pod pod-service-account-defaultsa-nomountspec
    Dec 14 09:40:25.598: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Dec 14 09:40:25.605: INFO: created pod pod-service-account-mountsa-nomountspec
    Dec 14 09:40:25.605: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Dec 14 09:40:25.611: INFO: created pod pod-service-account-nomountsa-nomountspec
    Dec 14 09:40:25.611: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 14 09:40:25.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4531" for this suite. 12/14/22 09:40:25.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:25.621
Dec 14 09:40:25.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 12/14/22 09:40:25.621
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:25.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:25.635
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-1361 12/14/22 09:40:25.64
STEP: creating replication controller nodeport-test in namespace services-1361 12/14/22 09:40:25.649
I1214 09:40:25.655105    4635 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1361, replica count: 2
I1214 09:40:28.706536    4635 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 09:40:28.706: INFO: Creating new exec pod
Dec 14 09:40:28.719: INFO: Waiting up to 5m0s for pod "execpodpkc8q" in namespace "services-1361" to be "running"
Dec 14 09:40:28.722: INFO: Pod "execpodpkc8q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364767ms
Dec 14 09:40:30.727: INFO: Pod "execpodpkc8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.008221519s
Dec 14 09:40:30.727: INFO: Pod "execpodpkc8q" satisfied condition "running"
Dec 14 09:40:31.734: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec 14 09:40:32.126: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 14 09:40:32.127: INFO: stdout: "nodeport-test-kcd2d"
Dec 14 09:40:32.127: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.107.210.210 80'
Dec 14 09:40:32.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.107.210.210 80\nConnection to 100.107.210.210 80 port [tcp/http] succeeded!\n"
Dec 14 09:40:32.543: INFO: stdout: ""
Dec 14 09:40:33.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.107.210.210 80'
Dec 14 09:40:33.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.107.210.210 80\nConnection to 100.107.210.210 80 port [tcp/http] succeeded!\n"
Dec 14 09:40:33.958: INFO: stdout: "nodeport-test-kcd2d"
Dec 14 09:40:33.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 32420'
Dec 14 09:40:34.334: INFO: stderr: "+ nc -v -t -w 2 10.250.3.58 32420\n+ echo hostName\nConnection to 10.250.3.58 32420 port [tcp/*] succeeded!\n"
Dec 14 09:40:34.334: INFO: stdout: "nodeport-test-ppxgh"
Dec 14 09:40:34.334: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 32420'
Dec 14 09:40:34.784: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 32420\nConnection to 10.250.3.210 32420 port [tcp/*] succeeded!\n"
Dec 14 09:40:34.784: INFO: stdout: "nodeport-test-kcd2d"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 14 09:40:34.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1361" for this suite. 12/14/22 09:40:34.791
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":352,"skipped":6458,"failed":0}
------------------------------
• [9.177 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:25.621
    Dec 14 09:40:25.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 12/14/22 09:40:25.621
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:25.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:25.635
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-1361 12/14/22 09:40:25.64
    STEP: creating replication controller nodeport-test in namespace services-1361 12/14/22 09:40:25.649
    I1214 09:40:25.655105    4635 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1361, replica count: 2
    I1214 09:40:28.706536    4635 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 09:40:28.706: INFO: Creating new exec pod
    Dec 14 09:40:28.719: INFO: Waiting up to 5m0s for pod "execpodpkc8q" in namespace "services-1361" to be "running"
    Dec 14 09:40:28.722: INFO: Pod "execpodpkc8q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364767ms
    Dec 14 09:40:30.727: INFO: Pod "execpodpkc8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.008221519s
    Dec 14 09:40:30.727: INFO: Pod "execpodpkc8q" satisfied condition "running"
    Dec 14 09:40:31.734: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Dec 14 09:40:32.126: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Dec 14 09:40:32.127: INFO: stdout: "nodeport-test-kcd2d"
    Dec 14 09:40:32.127: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.107.210.210 80'
    Dec 14 09:40:32.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.107.210.210 80\nConnection to 100.107.210.210 80 port [tcp/http] succeeded!\n"
    Dec 14 09:40:32.543: INFO: stdout: ""
    Dec 14 09:40:33.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.107.210.210 80'
    Dec 14 09:40:33.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.107.210.210 80\nConnection to 100.107.210.210 80 port [tcp/http] succeeded!\n"
    Dec 14 09:40:33.958: INFO: stdout: "nodeport-test-kcd2d"
    Dec 14 09:40:33.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.58 32420'
    Dec 14 09:40:34.334: INFO: stderr: "+ nc -v -t -w 2 10.250.3.58 32420\n+ echo hostName\nConnection to 10.250.3.58 32420 port [tcp/*] succeeded!\n"
    Dec 14 09:40:34.334: INFO: stdout: "nodeport-test-ppxgh"
    Dec 14 09:40:34.334: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1361 exec execpodpkc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.210 32420'
    Dec 14 09:40:34.784: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.210 32420\nConnection to 10.250.3.210 32420 port [tcp/*] succeeded!\n"
    Dec 14 09:40:34.784: INFO: stdout: "nodeport-test-kcd2d"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 14 09:40:34.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1361" for this suite. 12/14/22 09:40:34.791
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:34.797
Dec 14 09:40:34.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 12/14/22 09:40:34.798
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:34.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:34.821
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Dec 14 09:40:34.841: INFO: Waiting up to 5m0s for pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94" in namespace "containers-8602" to be "running"
Dec 14 09:40:34.845: INFO: Pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37488ms
Dec 14 09:40:36.851: INFO: Pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94": Phase="Running", Reason="", readiness=true. Elapsed: 2.010194395s
Dec 14 09:40:36.851: INFO: Pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 14 09:40:36.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8602" for this suite. 12/14/22 09:40:36.867
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":353,"skipped":6459,"failed":0}
------------------------------
• [2.075 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:34.797
    Dec 14 09:40:34.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 12/14/22 09:40:34.798
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:34.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:34.821
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Dec 14 09:40:34.841: INFO: Waiting up to 5m0s for pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94" in namespace "containers-8602" to be "running"
    Dec 14 09:40:34.845: INFO: Pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37488ms
    Dec 14 09:40:36.851: INFO: Pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94": Phase="Running", Reason="", readiness=true. Elapsed: 2.010194395s
    Dec 14 09:40:36.851: INFO: Pod "client-containers-eacbce03-12fd-49d1-9231-ae0f81851d94" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 14 09:40:36.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8602" for this suite. 12/14/22 09:40:36.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:36.873
Dec 14 09:40:36.873: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 12/14/22 09:40:36.873
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:36.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:36.891
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:40:36.897
Dec 14 09:40:36.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32" in namespace "downward-api-815" to be "Succeeded or Failed"
Dec 14 09:40:36.912: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470094ms
Dec 14 09:40:38.918: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009258648s
Dec 14 09:40:40.917: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008619974s
STEP: Saw pod success 12/14/22 09:40:40.917
Dec 14 09:40:40.917: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32" satisfied condition "Succeeded or Failed"
Dec 14 09:40:40.921: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f pod downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32 container client-container: <nil>
STEP: delete the pod 12/14/22 09:40:40.938
Dec 14 09:40:40.947: INFO: Waiting for pod downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32 to disappear
Dec 14 09:40:40.950: INFO: Pod downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 14 09:40:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-815" for this suite. 12/14/22 09:40:40.957
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":354,"skipped":6484,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:36.873
    Dec 14 09:40:36.873: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 12/14/22 09:40:36.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:36.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:36.891
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:40:36.897
    Dec 14 09:40:36.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32" in namespace "downward-api-815" to be "Succeeded or Failed"
    Dec 14 09:40:36.912: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470094ms
    Dec 14 09:40:38.918: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009258648s
    Dec 14 09:40:40.917: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008619974s
    STEP: Saw pod success 12/14/22 09:40:40.917
    Dec 14 09:40:40.917: INFO: Pod "downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32" satisfied condition "Succeeded or Failed"
    Dec 14 09:40:40.921: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-hx72f pod downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:40:40.938
    Dec 14 09:40:40.947: INFO: Waiting for pod downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32 to disappear
    Dec 14 09:40:40.950: INFO: Pod downwardapi-volume-7b2bc271-a9d6-4c88-bcf5-5634393a4b32 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 14 09:40:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-815" for this suite. 12/14/22 09:40:40.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:40.963
Dec 14 09:40:40.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 12/14/22 09:40:40.964
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:40.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:40.98
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 12/14/22 09:40:40.985
STEP: setting up watch 12/14/22 09:40:40.985
STEP: submitting the pod to kubernetes 12/14/22 09:40:41.088
STEP: verifying the pod is in kubernetes 12/14/22 09:40:41.099
STEP: verifying pod creation was observed 12/14/22 09:40:41.104
Dec 14 09:40:41.104: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765" in namespace "pods-5193" to be "running"
Dec 14 09:40:41.108: INFO: Pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457365ms
Dec 14 09:40:43.114: INFO: Pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765": Phase="Running", Reason="", readiness=true. Elapsed: 2.009288461s
Dec 14 09:40:43.114: INFO: Pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765" satisfied condition "running"
STEP: deleting the pod gracefully 12/14/22 09:40:43.118
STEP: verifying pod deletion was observed 12/14/22 09:40:43.123
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 14 09:40:45.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5193" for this suite. 12/14/22 09:40:45.246
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":355,"skipped":6530,"failed":0}
------------------------------
• [4.287 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:40.963
    Dec 14 09:40:40.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 12/14/22 09:40:40.964
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:40.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:40.98
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 12/14/22 09:40:40.985
    STEP: setting up watch 12/14/22 09:40:40.985
    STEP: submitting the pod to kubernetes 12/14/22 09:40:41.088
    STEP: verifying the pod is in kubernetes 12/14/22 09:40:41.099
    STEP: verifying pod creation was observed 12/14/22 09:40:41.104
    Dec 14 09:40:41.104: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765" in namespace "pods-5193" to be "running"
    Dec 14 09:40:41.108: INFO: Pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457365ms
    Dec 14 09:40:43.114: INFO: Pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765": Phase="Running", Reason="", readiness=true. Elapsed: 2.009288461s
    Dec 14 09:40:43.114: INFO: Pod "pod-submit-remove-fbf2d1e3-e801-452b-83e3-9f99aa4c1765" satisfied condition "running"
    STEP: deleting the pod gracefully 12/14/22 09:40:43.118
    STEP: verifying pod deletion was observed 12/14/22 09:40:43.123
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 14 09:40:45.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5193" for this suite. 12/14/22 09:40:45.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:45.251
Dec 14 09:40:45.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 12/14/22 09:40:45.251
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:45.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:45.266
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/14/22 09:40:45.271
Dec 14 09:40:45.281: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5530  23e5cf51-f1c9-4ef5-8d49-32267967c7f0 52416 0 2022-12-14 09:40:45 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-14 09:40:45 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76289,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76289,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 09:40:45.281: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5530" to be "running and ready"
Dec 14 09:40:45.284: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133675ms
Dec 14 09:40:45.284: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 14 09:40:47.290: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009330079s
Dec 14 09:40:47.291: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Dec 14 09:40:47.291: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 12/14/22 09:40:47.291
Dec 14 09:40:47.291: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5530 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:40:47.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:40:47.291: INFO: ExecWithOptions: Clientset creation
Dec 14 09:40:47.291: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5530/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 12/14/22 09:40:47.715
Dec 14 09:40:47.715: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5530 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 09:40:47.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec 14 09:40:47.716: INFO: ExecWithOptions: Clientset creation
Dec 14 09:40:47.716: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5530/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 09:40:48.148: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 14 09:40:48.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5530" for this suite. 12/14/22 09:40:48.163
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":356,"skipped":6548,"failed":0}
------------------------------
• [2.918 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:45.251
    Dec 14 09:40:45.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 12/14/22 09:40:45.251
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:45.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:45.266
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/14/22 09:40:45.271
    Dec 14 09:40:45.281: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5530  23e5cf51-f1c9-4ef5-8d49-32267967c7f0 52416 0 2022-12-14 09:40:45 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-14 09:40:45 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76289,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tm5on-jne.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76289,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 09:40:45.281: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5530" to be "running and ready"
    Dec 14 09:40:45.284: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133675ms
    Dec 14 09:40:45.284: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 09:40:47.290: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009330079s
    Dec 14 09:40:47.291: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Dec 14 09:40:47.291: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 12/14/22 09:40:47.291
    Dec 14 09:40:47.291: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5530 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:40:47.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:40:47.291: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:40:47.291: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5530/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 12/14/22 09:40:47.715
    Dec 14 09:40:47.715: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5530 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 09:40:47.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Dec 14 09:40:47.716: INFO: ExecWithOptions: Clientset creation
    Dec 14 09:40:47.716: INFO: ExecWithOptions: execute(POST https://api.tm5on-jne.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5530/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 09:40:48.148: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 14 09:40:48.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5530" for this suite. 12/14/22 09:40:48.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:48.169
Dec 14 09:40:48.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:40:48.17
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:48.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:48.19
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-99213813-d583-442e-8453-6a0598ddd4af 12/14/22 09:40:48.195
STEP: Creating a pod to test consume secrets 12/14/22 09:40:48.2
Dec 14 09:40:48.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0" in namespace "projected-4166" to be "Succeeded or Failed"
Dec 14 09:40:48.216: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520925ms
Dec 14 09:40:50.221: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009102615s
Dec 14 09:40:52.221: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009108669s
STEP: Saw pod success 12/14/22 09:40:52.221
Dec 14 09:40:52.221: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0" satisfied condition "Succeeded or Failed"
Dec 14 09:40:52.225: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 09:40:52.236
Dec 14 09:40:52.243: INFO: Waiting for pod pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0 to disappear
Dec 14 09:40:52.247: INFO: Pod pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 14 09:40:52.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4166" for this suite. 12/14/22 09:40:52.253
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":357,"skipped":6592,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:48.169
    Dec 14 09:40:48.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:40:48.17
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:48.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:48.19
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-99213813-d583-442e-8453-6a0598ddd4af 12/14/22 09:40:48.195
    STEP: Creating a pod to test consume secrets 12/14/22 09:40:48.2
    Dec 14 09:40:48.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0" in namespace "projected-4166" to be "Succeeded or Failed"
    Dec 14 09:40:48.216: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520925ms
    Dec 14 09:40:50.221: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009102615s
    Dec 14 09:40:52.221: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009108669s
    STEP: Saw pod success 12/14/22 09:40:52.221
    Dec 14 09:40:52.221: INFO: Pod "pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0" satisfied condition "Succeeded or Failed"
    Dec 14 09:40:52.225: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 09:40:52.236
    Dec 14 09:40:52.243: INFO: Waiting for pod pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0 to disappear
    Dec 14 09:40:52.247: INFO: Pod pod-projected-secrets-e2450c9c-03f5-4196-a025-ca5c5fad51b0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 14 09:40:52.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4166" for this suite. 12/14/22 09:40:52.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:52.259
Dec 14 09:40:52.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:40:52.259
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:52.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:52.276
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-b7124104-9e57-49a4-a918-abfb937306db 12/14/22 09:40:52.282
STEP: Creating a pod to test consume configMaps 12/14/22 09:40:52.287
Dec 14 09:40:52.298: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b" in namespace "configmap-869" to be "Succeeded or Failed"
Dec 14 09:40:52.302: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54838ms
Dec 14 09:40:54.308: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009610185s
Dec 14 09:40:56.308: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009148206s
STEP: Saw pod success 12/14/22 09:40:56.308
Dec 14 09:40:56.308: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b" satisfied condition "Succeeded or Failed"
Dec 14 09:40:56.313: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b container agnhost-container: <nil>
STEP: delete the pod 12/14/22 09:40:56.323
Dec 14 09:40:56.333: INFO: Waiting for pod pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b to disappear
Dec 14 09:40:56.337: INFO: Pod pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:40:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-869" for this suite. 12/14/22 09:40:56.345
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":358,"skipped":6615,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:52.259
    Dec 14 09:40:52.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:40:52.259
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:52.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:52.276
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-b7124104-9e57-49a4-a918-abfb937306db 12/14/22 09:40:52.282
    STEP: Creating a pod to test consume configMaps 12/14/22 09:40:52.287
    Dec 14 09:40:52.298: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b" in namespace "configmap-869" to be "Succeeded or Failed"
    Dec 14 09:40:52.302: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54838ms
    Dec 14 09:40:54.308: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009610185s
    Dec 14 09:40:56.308: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009148206s
    STEP: Saw pod success 12/14/22 09:40:56.308
    Dec 14 09:40:56.308: INFO: Pod "pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b" satisfied condition "Succeeded or Failed"
    Dec 14 09:40:56.313: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 09:40:56.323
    Dec 14 09:40:56.333: INFO: Waiting for pod pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b to disappear
    Dec 14 09:40:56.337: INFO: Pod pod-configmaps-0c325553-1162-4f34-b99b-b1243a3e919b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:40:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-869" for this suite. 12/14/22 09:40:56.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:40:56.351
Dec 14 09:40:56.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 12/14/22 09:40:56.352
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:56.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:56.375
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 12/14/22 09:40:56.38
Dec 14 09:40:56.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2" in namespace "projected-1601" to be "Succeeded or Failed"
Dec 14 09:40:56.395: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048619ms
Dec 14 09:40:58.401: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009523546s
Dec 14 09:41:00.401: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009339455s
STEP: Saw pod success 12/14/22 09:41:00.401
Dec 14 09:41:00.401: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2" satisfied condition "Succeeded or Failed"
Dec 14 09:41:00.405: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2 container client-container: <nil>
STEP: delete the pod 12/14/22 09:41:00.415
Dec 14 09:41:00.424: INFO: Waiting for pod downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2 to disappear
Dec 14 09:41:00.427: INFO: Pod downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 14 09:41:00.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1601" for this suite. 12/14/22 09:41:00.434
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":359,"skipped":6623,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:40:56.351
    Dec 14 09:40:56.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 12/14/22 09:40:56.352
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:40:56.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:40:56.375
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 12/14/22 09:40:56.38
    Dec 14 09:40:56.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2" in namespace "projected-1601" to be "Succeeded or Failed"
    Dec 14 09:40:56.395: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048619ms
    Dec 14 09:40:58.401: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009523546s
    Dec 14 09:41:00.401: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009339455s
    STEP: Saw pod success 12/14/22 09:41:00.401
    Dec 14 09:41:00.401: INFO: Pod "downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2" satisfied condition "Succeeded or Failed"
    Dec 14 09:41:00.405: INFO: Trying to get logs from node shoot--it--tm5on-jne-worker-1-z1-78f85-ztcjb pod downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2 container client-container: <nil>
    STEP: delete the pod 12/14/22 09:41:00.415
    Dec 14 09:41:00.424: INFO: Waiting for pod downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2 to disappear
    Dec 14 09:41:00.427: INFO: Pod downwardapi-volume-06d1ab41-4f5c-44f1-ab84-a11439e3f4f2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 14 09:41:00.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1601" for this suite. 12/14/22 09:41:00.434
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:41:00.439
Dec 14 09:41:00.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 12/14/22 09:41:00.44
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:41:00.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:41:00.457
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 12/14/22 09:41:00.462
STEP: Creating a ResourceQuota 12/14/22 09:41:05.467
STEP: Ensuring resource quota status is calculated 12/14/22 09:41:05.476
STEP: Creating a ReplicaSet 12/14/22 09:41:07.48
STEP: Ensuring resource quota status captures replicaset creation 12/14/22 09:41:07.489
STEP: Deleting a ReplicaSet 12/14/22 09:41:09.494
STEP: Ensuring resource quota status released usage 12/14/22 09:41:09.5
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 14 09:41:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5719" for this suite. 12/14/22 09:41:11.511
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":360,"skipped":6626,"failed":0}
------------------------------
• [11.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:41:00.439
    Dec 14 09:41:00.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 12/14/22 09:41:00.44
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:41:00.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:41:00.457
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 12/14/22 09:41:00.462
    STEP: Creating a ResourceQuota 12/14/22 09:41:05.467
    STEP: Ensuring resource quota status is calculated 12/14/22 09:41:05.476
    STEP: Creating a ReplicaSet 12/14/22 09:41:07.48
    STEP: Ensuring resource quota status captures replicaset creation 12/14/22 09:41:07.489
    STEP: Deleting a ReplicaSet 12/14/22 09:41:09.494
    STEP: Ensuring resource quota status released usage 12/14/22 09:41:09.5
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 14 09:41:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5719" for this suite. 12/14/22 09:41:11.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:41:11.517
Dec 14 09:41:11.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 12/14/22 09:41:11.517
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:41:11.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:41:11.534
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 12/14/22 09:41:11.539
STEP: creating 12/14/22 09:41:11.539
STEP: getting 12/14/22 09:41:11.545
STEP: listing 12/14/22 09:41:11.548
STEP: watching 12/14/22 09:41:11.552
Dec 14 09:41:11.552: INFO: starting watch
STEP: cluster-wide listing 12/14/22 09:41:11.554
STEP: cluster-wide watching 12/14/22 09:41:11.576
Dec 14 09:41:11.576: INFO: starting watch
STEP: patching 12/14/22 09:41:11.579
STEP: updating 12/14/22 09:41:11.586
Dec 14 09:41:11.595: INFO: waiting for watch events with expected annotations
Dec 14 09:41:11.595: INFO: saw patched and updated annotations
STEP: patching /status 12/14/22 09:41:11.595
STEP: updating /status 12/14/22 09:41:11.601
STEP: get /status 12/14/22 09:41:11.609
STEP: deleting 12/14/22 09:41:11.613
STEP: deleting a collection 12/14/22 09:41:11.625
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 14 09:41:11.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7879" for this suite. 12/14/22 09:41:11.638
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":361,"skipped":6656,"failed":0}
------------------------------
• [0.126 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:41:11.517
    Dec 14 09:41:11.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 12/14/22 09:41:11.517
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:41:11.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:41:11.534
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 12/14/22 09:41:11.539
    STEP: creating 12/14/22 09:41:11.539
    STEP: getting 12/14/22 09:41:11.545
    STEP: listing 12/14/22 09:41:11.548
    STEP: watching 12/14/22 09:41:11.552
    Dec 14 09:41:11.552: INFO: starting watch
    STEP: cluster-wide listing 12/14/22 09:41:11.554
    STEP: cluster-wide watching 12/14/22 09:41:11.576
    Dec 14 09:41:11.576: INFO: starting watch
    STEP: patching 12/14/22 09:41:11.579
    STEP: updating 12/14/22 09:41:11.586
    Dec 14 09:41:11.595: INFO: waiting for watch events with expected annotations
    Dec 14 09:41:11.595: INFO: saw patched and updated annotations
    STEP: patching /status 12/14/22 09:41:11.595
    STEP: updating /status 12/14/22 09:41:11.601
    STEP: get /status 12/14/22 09:41:11.609
    STEP: deleting 12/14/22 09:41:11.613
    STEP: deleting a collection 12/14/22 09:41:11.625
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 14 09:41:11.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7879" for this suite. 12/14/22 09:41:11.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/14/22 09:41:11.644
Dec 14 09:41:11.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 12/14/22 09:41:11.645
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:41:11.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:41:11.662
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 14 09:41:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-746" for this suite. 12/14/22 09:41:11.712
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":362,"skipped":6705,"failed":0}
------------------------------
• [0.072 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/14/22 09:41:11.644
    Dec 14 09:41:11.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 12/14/22 09:41:11.645
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 09:41:11.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 09:41:11.662
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 14 09:41:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-746" for this suite. 12/14/22 09:41:11.712
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Dec 14 09:41:11.717: INFO: Running AfterSuite actions on all nodes
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Dec 14 09:41:11.717: INFO: Running AfterSuite actions on node 1
Dec 14 09:41:11.717: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Dec 14 09:41:11.717: INFO: Running AfterSuite actions on all nodes
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Dec 14 09:41:11.717: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Dec 14 09:41:11.717: INFO: Running AfterSuite actions on node 1
    Dec 14 09:41:11.717: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.051 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6077.085 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--ginkgo.flakeAttempts is deprecated, use --ginkgo.flake-attempts instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m
  [38;5;11m--ginkgo.dryRun is deprecated, use --ginkgo.dry-run instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m


Ginkgo ran 1 suite in 1h41m17.339440044s
Test Suite Passed
