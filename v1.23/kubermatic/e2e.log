I0829 15:07:31.139636      18 e2e.go:132] Starting e2e run "4cd5f496-ae4c-4f18-a736-d00bc57922d3" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1661785651 - Will randomize all specs
Will run 346 of 7050 specs

Aug 29 15:07:34.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:07:34.237: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 29 15:07:34.269: INFO: Condition Ready of node ip-172-31-20-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:29 +0000 UTC}]. Failure
Aug 29 15:07:34.269: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:07:34.269: INFO: Unschedulable nodes= 2, maximum value for starting tests= 0
Aug 29 15:07:34.269: INFO: 	-> Node ip-172-31-20-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:29 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:07:34.269: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:07:34.270: INFO: ==== node wait: 1 out of 3 nodes are ready, max notReady allowed 0.  Need 2 more before starting.
Aug 29 15:08:04.287: INFO: Condition Ready of node ip-172-31-20-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:29 +0000 UTC}]. Failure
Aug 29 15:08:04.287: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:08:04.288: INFO: Unschedulable nodes= 2, maximum value for starting tests= 0
Aug 29 15:08:04.289: INFO: 	-> Node ip-172-31-20-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:29 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:08:04.289: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:08:04.289: INFO: ==== node wait: 1 out of 3 nodes are ready, max notReady allowed 0.  Need 2 more before starting.
Aug 29 15:08:34.282: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:08:34.282: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:08:34.282: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:08:34.282: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:09:04.286: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:09:04.287: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:09:04.287: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:09:04.287: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:09:34.289: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:09:34.289: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:09:34.289: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:09:34.289: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:10:04.286: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:10:04.287: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:10:04.287: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:10:04.288: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:10:34.283: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:10:34.284: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:10:34.284: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:10:34.284: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:11:04.291: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:11:04.291: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:11:04.291: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:11:04.292: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:11:34.289: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:11:34.289: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:11:34.290: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:11:34.290: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:12:04.293: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:12:04.293: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:12:04.294: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:12:04.294: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:12:34.285: INFO: Condition Ready of node ip-172-31-23-90.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}]. Failure
Aug 29 15:12:34.285: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:12:34.286: INFO: 	-> Node ip-172-31-23-90.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:07:09 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:12:34.286: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:13:04.301: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 29 15:13:04.389: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 29 15:13:04.391: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Aug 29 15:13:04.391: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 29 15:13:04.421: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'aws-node-termination-handler' (0 seconds elapsed)
Aug 29 15:13:04.421: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Aug 29 15:13:04.421: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'envoy-agent' (0 seconds elapsed)
Aug 29 15:13:04.421: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 29 15:13:04.421: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Aug 29 15:13:04.421: INFO: e2e test version: v1.23.9
Aug 29 15:13:04.429: INFO: kube-apiserver version: v1.23.9
Aug 29 15:13:04.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:13:04.442: INFO: Cluster IP family: ipv4
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:13:04.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
W0829 15:13:04.489558      18 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Aug 29 15:13:04.489: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Aug 29 15:13:04.510: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:13:06.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 15:13:08.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:13:10.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:13:12.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:13:14.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:13:16.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:13:18.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:13:22.003: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:13:22.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1909" for this suite.
STEP: Destroying namespace "webhook-1909-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.962 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":1,"skipped":9,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:13:22.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:13:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7217" for this suite.

• [SLOW TEST:9.945 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":2,"skipped":19,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:13:32.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea
Aug 29 15:13:32.475: INFO: Pod name my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea: Found 0 pods out of 1
Aug 29 15:13:37.544: INFO: Pod name my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea: Found 1 pods out of 1
Aug 29 15:13:37.545: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea" are running
Aug 29 15:13:57.587: INFO: Pod "my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea-jczfs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:13:32 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:13:32 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:13:32 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:13:32 +0000 UTC Reason: Message:}])
Aug 29 15:13:57.587: INFO: Trying to dial the pod
Aug 29 15:14:02.623: INFO: Controller my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea: Got expected result from replica 1 [my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea-jczfs]: "my-hostname-basic-79a16e18-fa8c-48e3-8b83-f398f65ca6ea-jczfs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:02.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7065" for this suite.

• [SLOW TEST:30.478 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":3,"skipped":29,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:02.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:14:02.897: INFO: Got root ca configmap in namespace "svcaccounts-1423"
Aug 29 15:14:02.908: INFO: Deleted root ca configmap in namespace "svcaccounts-1423"
STEP: waiting for a new root ca configmap created
Aug 29 15:14:03.561: INFO: Recreated root ca configmap in namespace "svcaccounts-1423"
Aug 29 15:14:03.569: INFO: Updated root ca configmap in namespace "svcaccounts-1423"
STEP: waiting for the root ca configmap reconciled
Aug 29 15:14:04.081: INFO: Reconciled root ca configmap in namespace "svcaccounts-1423"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:04.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1423" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":4,"skipped":38,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:04.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 29 15:14:04.185: INFO: Waiting up to 5m0s for pod "pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de" in namespace "emptydir-223" to be "Succeeded or Failed"
Aug 29 15:14:04.196: INFO: Pod "pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de": Phase="Pending", Reason="", readiness=false. Elapsed: 10.972134ms
Aug 29 15:14:06.206: INFO: Pod "pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021399977s
Aug 29 15:14:08.217: INFO: Pod "pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032058672s
Aug 29 15:14:10.225: INFO: Pod "pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040293797s
STEP: Saw pod success
Aug 29 15:14:10.225: INFO: Pod "pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de" satisfied condition "Succeeded or Failed"
Aug 29 15:14:10.236: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de container test-container: <nil>
STEP: delete the pod
Aug 29 15:14:10.277: INFO: Waiting for pod pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de to disappear
Aug 29 15:14:10.285: INFO: Pod pod-8dacd897-06d5-4c2b-b1dd-d2f3b26d48de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:10.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-223" for this suite.

• [SLOW TEST:6.197 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":41,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:10.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-2182ca23-d375-499f-b492-a252b6e23b1b
STEP: Creating a pod to test consume configMaps
Aug 29 15:14:10.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293" in namespace "projected-5444" to be "Succeeded or Failed"
Aug 29 15:14:10.438: INFO: Pod "pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293": Phase="Pending", Reason="", readiness=false. Elapsed: 13.300541ms
Aug 29 15:14:12.447: INFO: Pod "pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022030633s
Aug 29 15:14:14.458: INFO: Pod "pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033062625s
STEP: Saw pod success
Aug 29 15:14:14.458: INFO: Pod "pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293" satisfied condition "Succeeded or Failed"
Aug 29 15:14:14.468: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:14:14.515: INFO: Waiting for pod pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293 to disappear
Aug 29 15:14:14.522: INFO: Pod pod-projected-configmaps-2112c70a-d2f7-49e7-a919-5adec81d4293 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:14.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5444" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":41,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:14.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:16.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1207" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":7,"skipped":59,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:16.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 29 15:14:16.864: INFO: Waiting up to 5m0s for pod "pod-41678fc9-b6be-458b-911f-e1620c1aa400" in namespace "emptydir-979" to be "Succeeded or Failed"
Aug 29 15:14:16.875: INFO: Pod "pod-41678fc9-b6be-458b-911f-e1620c1aa400": Phase="Pending", Reason="", readiness=false. Elapsed: 10.001476ms
Aug 29 15:14:18.890: INFO: Pod "pod-41678fc9-b6be-458b-911f-e1620c1aa400": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025402172s
Aug 29 15:14:20.899: INFO: Pod "pod-41678fc9-b6be-458b-911f-e1620c1aa400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034677402s
STEP: Saw pod success
Aug 29 15:14:20.899: INFO: Pod "pod-41678fc9-b6be-458b-911f-e1620c1aa400" satisfied condition "Succeeded or Failed"
Aug 29 15:14:20.906: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-41678fc9-b6be-458b-911f-e1620c1aa400 container test-container: <nil>
STEP: delete the pod
Aug 29 15:14:20.941: INFO: Waiting for pod pod-41678fc9-b6be-458b-911f-e1620c1aa400 to disappear
Aug 29 15:14:20.950: INFO: Pod pod-41678fc9-b6be-458b-911f-e1620c1aa400 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:20.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-979" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:20.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-90959d29-10f4-471d-89b2-5ef95729b09d
STEP: Creating the pod
Aug 29 15:14:21.074: INFO: The status of Pod pod-configmaps-af0d9e24-0ebc-42dd-9c9d-928c5369945b is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:14:23.082: INFO: The status of Pod pod-configmaps-af0d9e24-0ebc-42dd-9c9d-928c5369945b is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-90959d29-10f4-471d-89b2-5ef95729b09d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:25.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1583" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:25.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-50fac071-cef7-4797-95ee-549341c73b7b in namespace container-probe-141
Aug 29 15:14:27.655: INFO: Started pod liveness-50fac071-cef7-4797-95ee-549341c73b7b in namespace container-probe-141
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:14:28.044: INFO: Initial restart count of pod liveness-50fac071-cef7-4797-95ee-549341c73b7b is 0
Aug 29 15:14:48.204: INFO: Restart count of pod container-probe-141/liveness-50fac071-cef7-4797-95ee-549341c73b7b is now 1 (20.159284448s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:48.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-141" for this suite.

• [SLOW TEST:22.725 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":187,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:48.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-d348dc3b-904b-40c3-98f1-cd16669fd88b
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:48.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1861" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":11,"skipped":202,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:48.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:14:52.582: INFO: Deleting pod "var-expansion-2fb1a6ff-35a3-4270-99bf-493b44b6ca9b" in namespace "var-expansion-864"
Aug 29 15:14:52.601: INFO: Wait up to 5m0s for pod "var-expansion-2fb1a6ff-35a3-4270-99bf-493b44b6ca9b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:14:54.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-864" for this suite.

• [SLOW TEST:6.248 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":12,"skipped":214,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:14:54.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:14:54.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e" in namespace "projected-6161" to be "Succeeded or Failed"
Aug 29 15:14:54.772: INFO: Pod "downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.855811ms
Aug 29 15:14:56.788: INFO: Pod "downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027322575s
Aug 29 15:14:58.798: INFO: Pod "downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037679139s
Aug 29 15:15:00.811: INFO: Pod "downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050658251s
STEP: Saw pod success
Aug 29 15:15:00.811: INFO: Pod "downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e" satisfied condition "Succeeded or Failed"
Aug 29 15:15:01.567: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e container client-container: <nil>
STEP: delete the pod
Aug 29 15:15:01.678: INFO: Waiting for pod downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e to disappear
Aug 29 15:15:01.687: INFO: Pod downwardapi-volume-a5256338-a22f-4b6d-af7e-de9bb2e2d29e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:01.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6161" for this suite.

• [SLOW TEST:7.072 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":225,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:01.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
Aug 29 15:15:02.279: INFO: created test-pod-1
Aug 29 15:15:06.312: INFO: running and ready test-pod-1
Aug 29 15:15:06.450: INFO: created test-pod-2
Aug 29 15:15:10.484: INFO: running and ready test-pod-2
Aug 29 15:15:10.716: INFO: created test-pod-3
Aug 29 15:15:12.755: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Aug 29 15:15:14.183: INFO: Pod quantity 3 is different from expected quantity 0
Aug 29 15:15:15.597: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:16.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5715" for this suite.

• [SLOW TEST:14.474 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":14,"skipped":235,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:16.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Aug 29 15:15:16.539: INFO: Pod name sample-pod: Found 1 pods out of 3
Aug 29 15:15:21.556: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Aug 29 15:15:23.616: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:23.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2480" for this suite.

• [SLOW TEST:7.705 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":15,"skipped":242,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:23.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-4dfce82a-352a-44c7-82af-478f29b3d005
STEP: Creating a pod to test consume configMaps
Aug 29 15:15:24.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92" in namespace "configmap-5277" to be "Succeeded or Failed"
Aug 29 15:15:24.033: INFO: Pod "pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92": Phase="Pending", Reason="", readiness=false. Elapsed: 14.904465ms
Aug 29 15:15:26.057: INFO: Pod "pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038799907s
Aug 29 15:15:28.068: INFO: Pod "pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049726418s
Aug 29 15:15:30.081: INFO: Pod "pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062375928s
STEP: Saw pod success
Aug 29 15:15:30.081: INFO: Pod "pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92" satisfied condition "Succeeded or Failed"
Aug 29 15:15:30.087: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:15:30.138: INFO: Waiting for pod pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92 to disappear
Aug 29 15:15:30.144: INFO: Pod pod-configmaps-e2dba22d-bc68-41af-a7e9-e6c910df4c92 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5277" for this suite.

• [SLOW TEST:6.237 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":243,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:30.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:15:30.298: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e0ce7e28-dc24-401d-8234-9e2dbb51cacd", Controller:(*bool)(0xc0029ab516), BlockOwnerDeletion:(*bool)(0xc0029ab517)}}
Aug 29 15:15:30.314: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"df5762c3-253a-44a5-b55d-f27b00530685", Controller:(*bool)(0xc0029ab7e6), BlockOwnerDeletion:(*bool)(0xc0029ab7e7)}}
Aug 29 15:15:30.328: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"828f96c4-4879-4e0a-8791-938136a55846", Controller:(*bool)(0xc002a4038e), BlockOwnerDeletion:(*bool)(0xc002a4038f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:35.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3923" for this suite.

• [SLOW TEST:5.578 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":17,"skipped":250,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:35.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-1675
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1675 to expose endpoints map[]
Aug 29 15:15:35.938: INFO: successfully validated that service multi-endpoint-test in namespace services-1675 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1675
Aug 29 15:15:36.010: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:15:38.027: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1675 to expose endpoints map[pod1:[100]]
Aug 29 15:15:38.085: INFO: successfully validated that service multi-endpoint-test in namespace services-1675 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1675
Aug 29 15:15:38.108: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:15:40.119: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1675 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 29 15:15:40.157: INFO: successfully validated that service multi-endpoint-test in namespace services-1675 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Aug 29 15:15:40.157: INFO: Creating new exec pod
Aug 29 15:15:43.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1675 exec execpod4hms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 29 15:15:43.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 29 15:15:43.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:15:43.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1675 exec execpod4hms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.182 80'
Aug 29 15:15:44.101: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.182 80\nConnection to 10.240.20.182 80 port [tcp/http] succeeded!\n"
Aug 29 15:15:44.101: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:15:44.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1675 exec execpod4hms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 29 15:15:44.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 29 15:15:44.378: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:15:44.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1675 exec execpod4hms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.182 81'
Aug 29 15:15:44.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.182 81\nConnection to 10.240.20.182 81 port [tcp/*] succeeded!\n"
Aug 29 15:15:44.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1675
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1675 to expose endpoints map[pod2:[101]]
Aug 29 15:15:45.228: INFO: successfully validated that service multi-endpoint-test in namespace services-1675 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1675
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1675 to expose endpoints map[]
Aug 29 15:15:46.310: INFO: successfully validated that service multi-endpoint-test in namespace services-1675 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:46.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1675" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:10.825 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":18,"skipped":267,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:46.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:15:46.670: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 29 15:15:51.696: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 29 15:15:51.696: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 15:15:51.767: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8453  878a2fcf-5a3b-4845-95db-f5ce4db57cad 4582 1 2022-08-29 15:15:51 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-08-29 15:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a0e0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 29 15:15:51.784: INFO: New ReplicaSet "test-cleanup-deployment-5dbdbf94dc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5dbdbf94dc  deployment-8453  15853235-0056-49e0-9ef0-1a56cc0c477d 4584 1 2022-08-29 15:15:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 878a2fcf-5a3b-4845-95db-f5ce4db57cad 0xc000a0fd07 0xc000a0fd08}] []  [{kube-controller-manager Update apps/v1 2022-08-29 15:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"878a2fcf-5a3b-4845-95db-f5ce4db57cad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5dbdbf94dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a0ff98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:15:51.785: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 29 15:15:51.785: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8453  55fb83cf-dae4-4d69-8acd-611af804ee49 4583 1 2022-08-29 15:15:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 878a2fcf-5a3b-4845-95db-f5ce4db57cad 0xc000a0ef7f 0xc000a0efa0}] []  [{e2e.test Update apps/v1 2022-08-29 15:15:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:15:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-29 15:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"878a2fcf-5a3b-4845-95db-f5ce4db57cad\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a0f3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:15:51.802: INFO: Pod "test-cleanup-controller-frfpk" is available:
&Pod{ObjectMeta:{test-cleanup-controller-frfpk test-cleanup-controller- deployment-8453  96815acd-89a3-4898-9449-3552e430badc 4567 0 2022-08-29 15:15:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:9077dddb8f6a6eb4ae73942d62e9a6e0c693d9bfe218e0270afc24b7b1467fe5 cni.projectcalico.org/podIP:172.25.1.14/32 cni.projectcalico.org/podIPs:172.25.1.14/32] [{apps/v1 ReplicaSet test-cleanup-controller 55fb83cf-dae4-4d69-8acd-611af804ee49 0xc0042b823f 0xc0042b8270}] []  [{kube-controller-manager Update v1 2022-08-29 15:15:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55fb83cf-dae4-4d69-8acd-611af804ee49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 15:15:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 15:15:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4gkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4gkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:15:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.14,StartTime:2022-08-29 15:15:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 15:15:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ee1825395ce4803fda96b90ccc420b3d4dcf908236e0d1792e1cc53167d81787,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 15:15:51.802: INFO: Pod "test-cleanup-deployment-5dbdbf94dc-gp6st" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5dbdbf94dc-gp6st test-cleanup-deployment-5dbdbf94dc- deployment-8453  25ecaf1c-d380-48e2-8fab-2cae1d172b90 4587 0 2022-08-29 15:15:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5dbdbf94dc 15853235-0056-49e0-9ef0-1a56cc0c477d 0xc0042b8477 0xc0042b8478}] []  [{kube-controller-manager Update v1 2022-08-29 15:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15853235-0056-49e0-9ef0-1a56cc0c477d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbmhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbmhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:15:51.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8453" for this suite.

• [SLOW TEST:5.276 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":19,"skipped":271,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:15:51.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 15:16:02.460: INFO: DNS probes using dns-6860/dns-test-d4475ed2-8eb0-4954-8b23-7ffa92bd3333 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:16:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6860" for this suite.

• [SLOW TEST:10.668 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":20,"skipped":273,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:16:02.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-jgqd
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:16:02.633: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jgqd" in namespace "subpath-2115" to be "Succeeded or Failed"
Aug 29 15:16:02.648: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.599916ms
Aug 29 15:16:04.666: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033105538s
Aug 29 15:16:06.680: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.046995029s
Aug 29 15:16:08.705: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.071831156s
Aug 29 15:16:10.716: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.082469507s
Aug 29 15:16:12.724: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.090767396s
Aug 29 15:16:14.735: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.102044017s
Aug 29 15:16:16.747: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.11374456s
Aug 29 15:16:18.761: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.127353914s
Aug 29 15:16:20.771: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.138114894s
Aug 29 15:16:22.793: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.159361487s
Aug 29 15:16:24.805: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=true. Elapsed: 22.171990476s
Aug 29 15:16:26.828: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Running", Reason="", readiness=false. Elapsed: 24.194548132s
Aug 29 15:16:28.839: INFO: Pod "pod-subpath-test-secret-jgqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.20559927s
STEP: Saw pod success
Aug 29 15:16:28.839: INFO: Pod "pod-subpath-test-secret-jgqd" satisfied condition "Succeeded or Failed"
Aug 29 15:16:28.844: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-subpath-test-secret-jgqd container test-container-subpath-secret-jgqd: <nil>
STEP: delete the pod
Aug 29 15:16:28.893: INFO: Waiting for pod pod-subpath-test-secret-jgqd to disappear
Aug 29 15:16:28.902: INFO: Pod pod-subpath-test-secret-jgqd no longer exists
STEP: Deleting pod pod-subpath-test-secret-jgqd
Aug 29 15:16:28.902: INFO: Deleting pod "pod-subpath-test-secret-jgqd" in namespace "subpath-2115"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:16:28.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2115" for this suite.

• [SLOW TEST:26.423 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":21,"skipped":277,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:16:28.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-579d2ea6-2fd4-4e5d-aed9-636e19616d39
STEP: Creating a pod to test consume secrets
Aug 29 15:16:29.052: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec" in namespace "projected-6083" to be "Succeeded or Failed"
Aug 29 15:16:29.074: INFO: Pod "pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec": Phase="Pending", Reason="", readiness=false. Elapsed: 21.845129ms
Aug 29 15:16:31.082: INFO: Pod "pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029892797s
Aug 29 15:16:33.095: INFO: Pod "pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042898861s
Aug 29 15:16:35.116: INFO: Pod "pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06431122s
STEP: Saw pod success
Aug 29 15:16:35.116: INFO: Pod "pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec" satisfied condition "Succeeded or Failed"
Aug 29 15:16:35.126: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:16:35.168: INFO: Waiting for pod pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec to disappear
Aug 29 15:16:35.179: INFO: Pod pod-projected-secrets-65e15c48-d32f-4fa2-9780-f5d636380dec no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:16:35.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6083" for this suite.

• [SLOW TEST:6.268 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":277,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:16:35.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-4971
Aug 29 15:16:35.294: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:16:37.305: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 29 15:16:37.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 29 15:16:37.712: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 29 15:16:37.712: INFO: stdout: "ipvs"
Aug 29 15:16:37.712: INFO: proxyMode: ipvs
Aug 29 15:16:37.738: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 29 15:16:37.751: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4971
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4971
I0829 15:16:37.800376      18 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4971, replica count: 3
I0829 15:16:40.867992      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:16:41.467: INFO: Creating new exec pod
Aug 29 15:16:46.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Aug 29 15:16:47.039: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 29 15:16:47.039: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:16:47.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.55 80'
Aug 29 15:16:47.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.55 80\nConnection to 10.240.26.55 80 port [tcp/http] succeeded!\n"
Aug 29 15:16:47.423: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:16:47.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.14 31682'
Aug 29 15:16:47.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.14 31682\nConnection to 172.31.16.14 31682 port [tcp/*] succeeded!\n"
Aug 29 15:16:47.702: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:16:47.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 31682'
Aug 29 15:16:47.994: INFO: stderr: "+ + nc -v -t -w 2 172.31.23.90 31682\necho hostName\nConnection to 172.31.23.90 31682 port [tcp/*] succeeded!\n"
Aug 29 15:16:47.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:16:47.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.14:31682/ ; done'
Aug 29 15:16:48.325: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n"
Aug 29 15:16:48.325: INFO: stdout: "\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl\naffinity-nodeport-timeout-dblkl"
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Received response from host: affinity-nodeport-timeout-dblkl
Aug 29 15:16:48.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.16.14:31682/'
Aug 29 15:16:48.568: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n"
Aug 29 15:16:48.568: INFO: stdout: "affinity-nodeport-timeout-dblkl"
Aug 29 15:18:58.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4971 exec execpod-affinitynrhjn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.16.14:31682/'
Aug 29 15:18:58.937: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.16.14:31682/\n"
Aug 29 15:18:58.937: INFO: stdout: "affinity-nodeport-timeout-5qkkb"
Aug 29 15:18:58.937: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4971, will wait for the garbage collector to delete the pods
Aug 29 15:18:59.037: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 14.177951ms
Aug 29 15:18:59.239: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 201.416503ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:02.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4971" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:147.143 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":23,"skipped":277,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:02.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-9416/configmap-test-68b96c87-da2d-4a7e-a4bf-a1d62cc67968
STEP: Creating a pod to test consume configMaps
Aug 29 15:19:02.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c" in namespace "configmap-9416" to be "Succeeded or Failed"
Aug 29 15:19:02.489: INFO: Pod "pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.805378ms
Aug 29 15:19:04.505: INFO: Pod "pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029010095s
Aug 29 15:19:06.635: INFO: Pod "pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159127062s
Aug 29 15:19:08.658: INFO: Pod "pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.182190286s
STEP: Saw pod success
Aug 29 15:19:08.659: INFO: Pod "pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c" satisfied condition "Succeeded or Failed"
Aug 29 15:19:08.673: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c container env-test: <nil>
STEP: delete the pod
Aug 29 15:19:08.738: INFO: Waiting for pod pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c to disappear
Aug 29 15:19:08.748: INFO: Pod pod-configmaps-a54d3ef9-5f67-4453-b621-e77b0009085c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:08.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9416" for this suite.

• [SLOW TEST:6.408 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:08.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-35fe7204-53a7-4d92-aeb4-d5cd8b9df36e
STEP: Creating a pod to test consume secrets
Aug 29 15:19:08.894: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc" in namespace "projected-7441" to be "Succeeded or Failed"
Aug 29 15:19:08.903: INFO: Pod "pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.139169ms
Aug 29 15:19:11.270: INFO: Pod "pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37573295s
Aug 29 15:19:13.281: INFO: Pod "pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.386517328s
STEP: Saw pod success
Aug 29 15:19:13.281: INFO: Pod "pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc" satisfied condition "Succeeded or Failed"
Aug 29 15:19:13.291: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:19:13.333: INFO: Waiting for pod pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc to disappear
Aug 29 15:19:13.341: INFO: Pod pod-projected-secrets-d4962e1e-18ce-43c0-8392-bc172e0dbbbc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7441" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:13.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 29 15:19:13.437: INFO: Waiting up to 5m0s for pod "pod-055704e3-3f10-460d-bae6-ee63c85fe724" in namespace "emptydir-5150" to be "Succeeded or Failed"
Aug 29 15:19:13.866: INFO: Pod "pod-055704e3-3f10-460d-bae6-ee63c85fe724": Phase="Pending", Reason="", readiness=false. Elapsed: 427.940348ms
Aug 29 15:19:16.013: INFO: Pod "pod-055704e3-3f10-460d-bae6-ee63c85fe724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.575743839s
Aug 29 15:19:18.021: INFO: Pod "pod-055704e3-3f10-460d-bae6-ee63c85fe724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.583563285s
STEP: Saw pod success
Aug 29 15:19:18.022: INFO: Pod "pod-055704e3-3f10-460d-bae6-ee63c85fe724" satisfied condition "Succeeded or Failed"
Aug 29 15:19:18.030: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-055704e3-3f10-460d-bae6-ee63c85fe724 container test-container: <nil>
STEP: delete the pod
Aug 29 15:19:18.075: INFO: Waiting for pod pod-055704e3-3f10-460d-bae6-ee63c85fe724 to disappear
Aug 29 15:19:18.084: INFO: Pod pod-055704e3-3f10-460d-bae6-ee63c85fe724 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:18.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5150" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:18.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
Aug 29 15:19:18.177: INFO: Waiting up to 5m0s for pod "var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209" in namespace "var-expansion-1399" to be "Succeeded or Failed"
Aug 29 15:19:18.194: INFO: Pod "var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209": Phase="Pending", Reason="", readiness=false. Elapsed: 16.907539ms
Aug 29 15:19:20.204: INFO: Pod "var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026898506s
Aug 29 15:19:22.219: INFO: Pod "var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041490321s
STEP: Saw pod success
Aug 29 15:19:22.219: INFO: Pod "var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209" satisfied condition "Succeeded or Failed"
Aug 29 15:19:22.225: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209 container dapi-container: <nil>
STEP: delete the pod
Aug 29 15:19:22.534: INFO: Waiting for pod var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209 to disappear
Aug 29 15:19:22.545: INFO: Pod var-expansion-1beed0e7-76fe-42e1-9dfd-ee46ef2fd209 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:22.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1399" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":374,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:22.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:19:23.588: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:19:26.663: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:19:26.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:30.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8940" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.594 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":28,"skipped":375,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:30.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:58.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1571" for this suite.

• [SLOW TEST:27.967 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":387,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:58.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:19:58.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1246" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":30,"skipped":392,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:19:58.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Aug 29 15:19:58.381: INFO: Waiting up to 5m0s for pod "downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c" in namespace "downward-api-3523" to be "Succeeded or Failed"
Aug 29 15:19:58.392: INFO: Pod "downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.496072ms
Aug 29 15:20:00.401: INFO: Pod "downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020250323s
Aug 29 15:20:02.410: INFO: Pod "downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028720374s
STEP: Saw pod success
Aug 29 15:20:02.410: INFO: Pod "downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c" satisfied condition "Succeeded or Failed"
Aug 29 15:20:02.419: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c container dapi-container: <nil>
STEP: delete the pod
Aug 29 15:20:02.462: INFO: Waiting for pod downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c to disappear
Aug 29 15:20:02.472: INFO: Pod downward-api-4bed422b-71ea-41f3-8570-3961057a4e6c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:20:02.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3523" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:20:02.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5165
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Aug 29 15:20:02.693: INFO: Found 0 stateful pods, waiting for 3
Aug 29 15:20:12.706: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:20:12.707: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:20:12.707: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 29 15:20:12.772: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 29 15:20:22.853: INFO: Updating stateful set ss2
Aug 29 15:20:22.888: INFO: Waiting for Pod statefulset-5165/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Aug 29 15:20:33.014: INFO: Found 2 stateful pods, waiting for 3
Aug 29 15:20:43.023: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:20:43.023: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:20:43.023: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 29 15:20:43.073: INFO: Updating stateful set ss2
Aug 29 15:20:43.093: INFO: Waiting for Pod statefulset-5165/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 29 15:20:53.172: INFO: Updating stateful set ss2
Aug 29 15:20:53.209: INFO: Waiting for StatefulSet statefulset-5165/ss2 to complete update
Aug 29 15:20:53.209: INFO: Waiting for Pod statefulset-5165/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 15:21:03.228: INFO: Deleting all statefulset in ns statefulset-5165
Aug 29 15:21:03.235: INFO: Scaling statefulset ss2 to 0
Aug 29 15:21:13.279: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:21:13.292: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:21:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5165" for this suite.

• [SLOW TEST:70.859 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":32,"skipped":438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:21:13.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Aug 29 15:21:13.444: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:21:15.462: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Aug 29 15:21:16.574: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:21:18.585: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 29 15:21:18.757: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:18.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:18.758: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:18.758: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:19.144: INFO: Exec stderr: ""
Aug 29 15:21:19.144: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:19.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:19.145: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:19.145: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:19.384: INFO: Exec stderr: ""
Aug 29 15:21:19.384: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:19.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:19.385: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:19.385: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:19.630: INFO: Exec stderr: ""
Aug 29 15:21:19.630: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:19.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:19.632: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:19.632: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:19.975: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 29 15:21:19.975: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:19.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:19.976: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:19.976: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:20.136: INFO: Exec stderr: ""
Aug 29 15:21:20.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:20.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:20.137: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:20.137: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:20.324: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 29 15:21:20.324: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:20.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:20.327: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:20.327: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:20.568: INFO: Exec stderr: ""
Aug 29 15:21:20.568: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:20.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:20.570: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:20.570: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:20.766: INFO: Exec stderr: ""
Aug 29 15:21:20.766: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:20.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:20.767: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:20.768: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:20.928: INFO: Exec stderr: ""
Aug 29 15:21:20.929: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5925 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:21:20.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:21:20.929: INFO: ExecWithOptions: Clientset creation
Aug 29 15:21:20.930: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5925/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:21:21.067: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:21:21.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5925" for this suite.

• [SLOW TEST:7.722 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":481,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:21:21.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 29 15:21:21.780: INFO: Waiting up to 5m0s for pod "pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461" in namespace "emptydir-1229" to be "Succeeded or Failed"
Aug 29 15:21:21.791: INFO: Pod "pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461": Phase="Pending", Reason="", readiness=false. Elapsed: 10.68492ms
Aug 29 15:21:23.807: INFO: Pod "pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026819149s
Aug 29 15:21:25.821: INFO: Pod "pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040832902s
STEP: Saw pod success
Aug 29 15:21:25.821: INFO: Pod "pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461" satisfied condition "Succeeded or Failed"
Aug 29 15:21:25.829: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461 container test-container: <nil>
STEP: delete the pod
Aug 29 15:21:25.863: INFO: Waiting for pod pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461 to disappear
Aug 29 15:21:25.871: INFO: Pod pod-4c6b66c1-35e9-49e7-a47d-fdba2d1c5461 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:21:25.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1229" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":492,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:21:25.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:21:27.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:21:30.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:21:30.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-976" for this suite.
STEP: Destroying namespace "webhook-976-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":35,"skipped":512,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:21:30.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Aug 29 15:21:30.510: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:21:32.520: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Aug 29 15:21:32.809: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:21:34.823: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 29 15:21:34.841: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 15:21:34.857: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 29 15:21:36.857: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 15:21:36.866: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 29 15:21:38.858: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 15:21:38.870: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:21:38.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2743" for this suite.

• [SLOW TEST:8.534 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":522,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:21:38.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:21:39.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 15:21:39.034: INFO: The status of Pod pod-exec-websocket-95fc5e00-6283-44d7-b38c-2dc43de57cfe is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:21:41.485: INFO: The status of Pod pod-exec-websocket-95fc5e00-6283-44d7-b38c-2dc43de57cfe is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:21:41.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3926" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":523,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:21:41.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:21:41.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 29 15:21:51.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-8661 --namespace=crd-publish-openapi-8661 create -f -'
Aug 29 15:21:52.585: INFO: stderr: ""
Aug 29 15:21:52.585: INFO: stdout: "e2e-test-crd-publish-openapi-8837-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 29 15:21:52.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-8661 --namespace=crd-publish-openapi-8661 delete e2e-test-crd-publish-openapi-8837-crds test-cr'
Aug 29 15:21:52.728: INFO: stderr: ""
Aug 29 15:21:52.728: INFO: stdout: "e2e-test-crd-publish-openapi-8837-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 29 15:21:52.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-8661 --namespace=crd-publish-openapi-8661 apply -f -'
Aug 29 15:21:53.023: INFO: stderr: ""
Aug 29 15:21:53.023: INFO: stdout: "e2e-test-crd-publish-openapi-8837-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 29 15:21:53.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-8661 --namespace=crd-publish-openapi-8661 delete e2e-test-crd-publish-openapi-8837-crds test-cr'
Aug 29 15:21:53.162: INFO: stderr: ""
Aug 29 15:21:53.162: INFO: stdout: "e2e-test-crd-publish-openapi-8837-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 29 15:21:53.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-8661 explain e2e-test-crd-publish-openapi-8837-crds'
Aug 29 15:21:53.475: INFO: stderr: ""
Aug 29 15:21:53.475: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8837-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:04.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8661" for this suite.

• [SLOW TEST:23.090 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":38,"skipped":527,"failed":0}
SSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
Aug 29 15:22:05.336: INFO: Waiting up to 5m0s for pod "client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf" in namespace "containers-2512" to be "Succeeded or Failed"
Aug 29 15:22:07.005: INFO: Pod "client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.669310244s
Aug 29 15:22:09.022: INFO: Pod "client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685941355s
Aug 29 15:22:11.045: INFO: Pod "client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.709353562s
STEP: Saw pod success
Aug 29 15:22:11.045: INFO: Pod "client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf" satisfied condition "Succeeded or Failed"
Aug 29 15:22:11.052: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:22:11.102: INFO: Waiting for pod client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf to disappear
Aug 29 15:22:11.107: INFO: Pod client-containers-417b4cc6-4745-48d5-913e-2e688a2403cf no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:11.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2512" for this suite.

• [SLOW TEST:6.390 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Aug 29 15:22:11.223: INFO: The status of Pod annotationupdate7bf5ce92-fe85-4e23-8c91-14a42642b478 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:22:13.237: INFO: The status of Pod annotationupdate7bf5ce92-fe85-4e23-8c91-14a42642b478 is Running (Ready = true)
Aug 29 15:22:13.782: INFO: Successfully updated pod "annotationupdate7bf5ce92-fe85-4e23-8c91-14a42642b478"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:15.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1039" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":40,"skipped":567,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:15.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Aug 29 15:22:15.938: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Aug 29 15:22:17.975: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Aug 29 15:22:18.277: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Aug 29 15:22:18.286: INFO: Observed &ReplicaSet event: ADDED
Aug 29 15:22:18.286: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.286: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.287: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.287: INFO: Found replicaset test-rs in namespace replicaset-5739 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 15:22:18.287: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Aug 29 15:22:18.287: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 15:22:18.301: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Aug 29 15:22:18.307: INFO: Observed &ReplicaSet event: ADDED
Aug 29 15:22:18.308: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.308: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.308: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.309: INFO: Observed replicaset test-rs in namespace replicaset-5739 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 15:22:18.309: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 15:22:18.309: INFO: Found replicaset test-rs in namespace replicaset-5739 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 29 15:22:18.309: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5739" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":41,"skipped":578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:18.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:29.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2829" for this suite.

• [SLOW TEST:11.230 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":42,"skipped":603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:29.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 29 15:22:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:22:33.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:48.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2376" for this suite.

• [SLOW TEST:18.569 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":43,"skipped":655,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:48.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:22:48.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001" in namespace "downward-api-3531" to be "Succeeded or Failed"
Aug 29 15:22:48.219: INFO: Pod "downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001": Phase="Pending", Reason="", readiness=false. Elapsed: 4.978553ms
Aug 29 15:22:50.242: INFO: Pod "downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027958879s
Aug 29 15:22:52.269: INFO: Pod "downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055004188s
STEP: Saw pod success
Aug 29 15:22:52.269: INFO: Pod "downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001" satisfied condition "Succeeded or Failed"
Aug 29 15:22:52.276: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001 container client-container: <nil>
STEP: delete the pod
Aug 29 15:22:52.320: INFO: Waiting for pod downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001 to disappear
Aug 29 15:22:52.327: INFO: Pod downwardapi-volume-43679fcd-9564-42c6-84e1-e521817c2001 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:52.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3531" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:52.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:22:53.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 create -f -'
Aug 29 15:22:54.134: INFO: stderr: ""
Aug 29 15:22:54.134: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 29 15:22:54.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 create -f -'
Aug 29 15:22:54.383: INFO: stderr: ""
Aug 29 15:22:54.383: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 29 15:22:55.399: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:22:55.399: INFO: Found 1 / 1
Aug 29 15:22:55.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 29 15:22:55.406: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:22:55.406: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 15:22:55.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 describe pod agnhost-primary-qktxm'
Aug 29 15:22:55.518: INFO: stderr: ""
Aug 29 15:22:55.518: INFO: stdout: "Name:         agnhost-primary-qktxm\nNamespace:    kubectl-2542\nPriority:     0\nNode:         ip-172-31-20-142.eu-central-1.compute.internal/172.31.20.142\nStart Time:   Mon, 29 Aug 2022 15:22:54 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 0617be53639d68ed61dfae9cd756408e18617bda371993f6bfc177b8a7ccc5c0\n              cni.projectcalico.org/podIP: 172.25.1.30/32\n              cni.projectcalico.org/podIPs: 172.25.1.30/32\nStatus:       Running\nIP:           172.25.1.30\nIPs:\n  IP:           172.25.1.30\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6783098ff2ebb0846e86c6ab5f4f8345b476bf13f9c793b06ab4128f6d6a21ef\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 29 Aug 2022 15:22:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b7zs7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-b7zs7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2542/agnhost-primary-qktxm to ip-172-31-20-142.eu-central-1.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Aug 29 15:22:55.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 describe rc agnhost-primary'
Aug 29 15:22:55.627: INFO: stderr: ""
Aug 29 15:22:55.627: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2542\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-qktxm\n"
Aug 29 15:22:55.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 describe service agnhost-primary'
Aug 29 15:22:55.729: INFO: stderr: ""
Aug 29 15:22:55.729: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2542\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.29.67\nIPs:               10.240.29.67\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.1.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 29 15:22:55.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 describe node ip-172-31-16-14.eu-central-1.compute.internal'
Aug 29 15:22:55.901: INFO: stderr: ""
Aug 29 15:22:55.901: INFO: stdout: "Name:               ip-172-31-16-14.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3a.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-16-14\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=969469f5-8620-44c8-b28c-b4f379924f04\n                    node.kubernetes.io/instance-type=t3a.medium\n                    system/cluster=gt2cwz5c4t\n                    system/project=jtnngz4ghk\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1a\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        cluster.k8s.io/machine: kube-system/gt2cwz5c4t-worker-6dg846-64774556d6-xzvv9\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"46:a9:98:a9:00:39\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.16.14\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.16.14/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 29 Aug 2022 15:05:26 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-16-14.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 29 Aug 2022 15:22:50 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 29 Aug 2022 15:06:32 +0000   Mon, 29 Aug 2022 15:06:32 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 29 Aug 2022 15:18:14 +0000   Mon, 29 Aug 2022 15:05:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 29 Aug 2022 15:18:14 +0000   Mon, 29 Aug 2022 15:05:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 29 Aug 2022 15:18:14 +0000   Mon, 29 Aug 2022 15:05:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 29 Aug 2022 15:18:14 +0000   Mon, 29 Aug 2022 15:06:18 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.16.14\n  ExternalIP:   3.70.242.194\n  Hostname:     ip-172-31-16-14.eu-central-1.compute.internal\n  InternalDNS:  ip-172-31-16-14.eu-central-1.compute.internal\n  ExternalDNS:  ec2-3-70-242-194.eu-central-1.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           25215872Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3969012Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         1600m\n  ephemeral-storage:           21091463949\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3457012Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec2f6128888a3486a5b2a24ff17ad34f\n  System UUID:                ec2f6128-888a-3486-a5b2-a24ff17ad34f\n  Boot ID:                    251ffe96-88da-4ffa-948a-1fb75ec8879f\n  Kernel Version:             5.15.0-1017-aws\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.11\n  Kubelet Version:            v1.23.9\n  Kube-Proxy Version:         v1.23.9\nPodCIDR:                      172.25.0.0/24\nPodCIDRs:                     172.25.0.0/24\nProviderID:                   aws:///eu-central-1a/i-076f6230dffc5c28f\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-57fb8785bf-xg2r9                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                 canal-lzt6p                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 coredns-5848f745f7-mkm4d                                   50m (3%)      100m (6%)   32Mi (0%)        64Mi (1%)      20m\n  kube-system                 coredns-5848f745f7-ntmz2                                   50m (3%)      100m (6%)   32Mi (0%)        64Mi (1%)      20m\n  kube-system                 envoy-agent-7wzrr                                          50m (3%)      1 (62%)     32Mi (0%)        64Mi (1%)      17m\n  kube-system                 konnectivity-agent-b7c8486c7-dr2vw                         10m (0%)      2 (125%)    10Mi (0%)        100Mi (2%)     20m\n  kube-system                 konnectivity-agent-b7c8486c7-r476v                         10m (0%)      2 (125%)    10Mi (0%)        100Mi (2%)     20m\n  kube-system                 kube-proxy-fjrpb                                           75m (4%)      250m (15%)  50Mi (1%)        250Mi (7%)     17m\n  kube-system                 metrics-server-7c94595b7c-d66rr                            100m (6%)     1 (62%)     200Mi (5%)       512Mi (15%)    20m\n  kube-system                 metrics-server-7c94595b7c-xjds6                            100m (6%)     1 (62%)     200Mi (5%)       512Mi (15%)    20m\n  kube-system                 node-local-dns-jdx9q                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  sonobuoy                    sonobuoy-e2e-job-0af6d8c74f8640cb                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-lffk7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         695m (43%)   7450m (465%)\n  memory                      566Mi (16%)  1666Mi (49%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  hugepages-1Gi               0 (0%)       0 (0%)\n  hugepages-2Mi               0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:\n  Type     Reason                   Age                From        Message\n  ----     ------                   ----               ----        -------\n  Normal   Starting                 16m                kube-proxy  \n  Warning  InvalidDiskCapacity      17m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  17m (x2 over 17m)  kubelet     Node ip-172-31-16-14.eu-central-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    17m (x2 over 17m)  kubelet     Node ip-172-31-16-14.eu-central-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     17m (x2 over 17m)  kubelet     Node ip-172-31-16-14.eu-central-1.compute.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  17m                kubelet     Updated Node Allocatable limit across pods\n  Normal   Starting                 17m                kubelet     Starting kubelet.\n  Normal   NodeReady                16m                kubelet     Node ip-172-31-16-14.eu-central-1.compute.internal status is now: NodeReady\n"
Aug 29 15:22:55.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-2542 describe namespace kubectl-2542'
Aug 29 15:22:56.004: INFO: stderr: ""
Aug 29 15:22:56.004: INFO: stdout: "Name:         kubectl-2542\nLabels:       e2e-framework=kubectl\n              e2e-run=4cd5f496-ae4c-4f18-a736-d00bc57922d3\n              kubernetes.io/metadata.name=kubectl-2542\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:22:56.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2542" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":45,"skipped":691,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:22:56.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-3707
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 15:22:56.077: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 15:22:56.140: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:22:58.150: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:23:00.158: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:23:02.153: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:04.160: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:06.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:08.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:10.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:12.153: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:14.153: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:16.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:23:18.152: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 15:23:18.169: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 15:23:18.186: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 15:23:20.237: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 15:23:20.237: INFO: Breadth first check of 172.25.0.11 on host 172.31.16.14...
Aug 29 15:23:20.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.32:9080/dial?request=hostname&protocol=http&host=172.25.0.11&port=8083&tries=1'] Namespace:pod-network-test-3707 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:23:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:23:20.247: INFO: ExecWithOptions: Clientset creation
Aug 29 15:23:20.247: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3707/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.11%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:23:20.484: INFO: Waiting for responses: map[]
Aug 29 15:23:20.484: INFO: reached 172.25.0.11 after 0/1 tries
Aug 29 15:23:20.484: INFO: Breadth first check of 172.25.1.31 on host 172.31.20.142...
Aug 29 15:23:20.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.32:9080/dial?request=hostname&protocol=http&host=172.25.1.31&port=8083&tries=1'] Namespace:pod-network-test-3707 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:23:20.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:23:20.494: INFO: ExecWithOptions: Clientset creation
Aug 29 15:23:20.494: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3707/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.31%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:23:20.639: INFO: Waiting for responses: map[]
Aug 29 15:23:20.640: INFO: reached 172.25.1.31 after 0/1 tries
Aug 29 15:23:20.640: INFO: Breadth first check of 172.25.2.30 on host 172.31.23.90...
Aug 29 15:23:20.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.32:9080/dial?request=hostname&protocol=http&host=172.25.2.30&port=8083&tries=1'] Namespace:pod-network-test-3707 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:23:20.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:23:20.772: INFO: ExecWithOptions: Clientset creation
Aug 29 15:23:20.772: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3707/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.30%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:23:20.927: INFO: Waiting for responses: map[]
Aug 29 15:23:20.927: INFO: reached 172.25.2.30 after 0/1 tries
Aug 29 15:23:20.927: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:23:20.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3707" for this suite.

• [SLOW TEST:24.929 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":694,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:23:20.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-04b4a09d-9ffd-4217-8810-51751f3306c7
STEP: Creating secret with name secret-projected-all-test-volume-163fcfe5-a112-419e-b9cd-3db233033a44
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 29 15:23:21.491: INFO: Waiting up to 5m0s for pod "projected-volume-570e1142-e027-496b-ba58-a4b380599a2b" in namespace "projected-617" to be "Succeeded or Failed"
Aug 29 15:23:21.498: INFO: Pod "projected-volume-570e1142-e027-496b-ba58-a4b380599a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.137926ms
Aug 29 15:23:23.512: INFO: Pod "projected-volume-570e1142-e027-496b-ba58-a4b380599a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020523512s
Aug 29 15:23:26.052: INFO: Pod "projected-volume-570e1142-e027-496b-ba58-a4b380599a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.560443881s
STEP: Saw pod success
Aug 29 15:23:26.052: INFO: Pod "projected-volume-570e1142-e027-496b-ba58-a4b380599a2b" satisfied condition "Succeeded or Failed"
Aug 29 15:23:26.060: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod projected-volume-570e1142-e027-496b-ba58-a4b380599a2b container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 29 15:23:26.107: INFO: Waiting for pod projected-volume-570e1142-e027-496b-ba58-a4b380599a2b to disappear
Aug 29 15:23:26.113: INFO: Pod projected-volume-570e1142-e027-496b-ba58-a4b380599a2b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:23:26.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-617" for this suite.

• [SLOW TEST:5.187 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":47,"skipped":734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:23:26.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:23:27.275: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:23:30.319: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:23:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5885-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:23:33.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3254" for this suite.
STEP: Destroying namespace "webhook-3254-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":48,"skipped":756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:23:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:23:33.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8" in namespace "downward-api-268" to be "Succeeded or Failed"
Aug 29 15:23:33.812: INFO: Pod "downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.087046ms
Aug 29 15:23:35.826: INFO: Pod "downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029220531s
Aug 29 15:23:37.842: INFO: Pod "downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045256489s
STEP: Saw pod success
Aug 29 15:23:37.842: INFO: Pod "downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8" satisfied condition "Succeeded or Failed"
Aug 29 15:23:37.850: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8 container client-container: <nil>
STEP: delete the pod
Aug 29 15:23:37.897: INFO: Waiting for pod downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8 to disappear
Aug 29 15:23:37.903: INFO: Pod downwardapi-volume-77b01778-870f-4fd7-885c-f382d891fdc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:23:37.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-268" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":785,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:23:37.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:23:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3517" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":50,"skipped":797,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:23:40.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:23:40.898: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 29 15:23:40.915: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 15:23:45.935: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 29 15:23:45.935: INFO: Creating deployment "test-rolling-update-deployment"
Aug 29 15:23:45.967: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 29 15:23:45.980: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 29 15:23:47.998: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 29 15:23:48.013: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 15:23:48.034: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8566  c644d9b8-49ef-4235-ab9e-3c89ef77720f 7980 1 2022-08-29 15:23:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-08-29 15:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:23:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404d5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-29 15:23:46 +0000 UTC,LastTransitionTime:2022-08-29 15:23:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-8656fc4b57" has successfully progressed.,LastUpdateTime:2022-08-29 15:23:47 +0000 UTC,LastTransitionTime:2022-08-29 15:23:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 15:23:48.047: INFO: New ReplicaSet "test-rolling-update-deployment-8656fc4b57" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-8656fc4b57  deployment-8566  f3e5189d-dcd5-41c1-9460-da971c1604ce 7970 1 2022-08-29 15:23:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c644d9b8-49ef-4235-ab9e-3c89ef77720f 0xc0040c0247 0xc0040c0248}] []  [{kube-controller-manager Update apps/v1 2022-08-29 15:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c644d9b8-49ef-4235-ab9e-3c89ef77720f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:23:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 8656fc4b57,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040c02f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:23:48.048: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 29 15:23:48.048: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8566  09a32f32-f302-47b4-ac36-4c8296fe84e1 7979 2 2022-08-29 15:23:40 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c644d9b8-49ef-4235-ab9e-3c89ef77720f 0xc0040c011f 0xc0040c0130}] []  [{e2e.test Update apps/v1 2022-08-29 15:23:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:23:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c644d9b8-49ef-4235-ab9e-3c89ef77720f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:23:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0040c01e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:23:48.059: INFO: Pod "test-rolling-update-deployment-8656fc4b57-22mvt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-8656fc4b57-22mvt test-rolling-update-deployment-8656fc4b57- deployment-8566  277c4048-87bf-4f1d-b0ab-0f7ce1c71223 7969 0 2022-08-29 15:23:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[cni.projectcalico.org/containerID:3038946f90e07892a629395acc7f1edeb9568d5cec6f444203275606da23880e cni.projectcalico.org/podIP:172.25.1.33/32 cni.projectcalico.org/podIPs:172.25.1.33/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-8656fc4b57 f3e5189d-dcd5-41c1-9460-da971c1604ce 0xc0040c0767 0xc0040c0768}] []  [{kube-controller-manager Update v1 2022-08-29 15:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3e5189d-dcd5-41c1-9460-da971c1604ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 15:23:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 15:23:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vjtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vjtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:23:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:23:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:23:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:23:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.33,StartTime:2022-08-29 15:23:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 15:23:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://c6f3a23f69277749523a569f4ca6002c393c0d248136c6b311851a9390e47064,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:23:48.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8566" for this suite.

• [SLOW TEST:7.245 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":51,"skipped":816,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:23:48.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5406
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Aug 29 15:23:48.176: INFO: Found 0 stateful pods, waiting for 3
Aug 29 15:23:58.206: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:23:58.206: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:23:58.206: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:23:58.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5406 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 15:23:58.491: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 15:23:58.491: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 15:23:58.491: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 29 15:24:08.570: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 29 15:24:18.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5406 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 15:24:19.014: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 15:24:19.014: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 15:24:19.014: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Aug 29 15:24:29.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5406 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 15:24:29.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 15:24:29.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 15:24:29.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 15:24:39.712: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 29 15:24:49.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5406 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 15:24:49.980: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 15:24:49.981: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 15:24:49.981: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 15:25:00.064: INFO: Deleting all statefulset in ns statefulset-5406
Aug 29 15:25:00.070: INFO: Scaling statefulset ss2 to 0
Aug 29 15:25:10.111: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:25:10.117: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:10.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5406" for this suite.

• [SLOW TEST:82.078 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":52,"skipped":820,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:10.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
Aug 29 15:25:10.315: INFO: Waiting up to 5m0s for pod "var-expansion-5ed78061-f9ec-4118-9589-697778cab445" in namespace "var-expansion-7141" to be "Succeeded or Failed"
Aug 29 15:25:10.323: INFO: Pod "var-expansion-5ed78061-f9ec-4118-9589-697778cab445": Phase="Pending", Reason="", readiness=false. Elapsed: 8.208004ms
Aug 29 15:25:12.346: INFO: Pod "var-expansion-5ed78061-f9ec-4118-9589-697778cab445": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030383849s
Aug 29 15:25:14.368: INFO: Pod "var-expansion-5ed78061-f9ec-4118-9589-697778cab445": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052702908s
STEP: Saw pod success
Aug 29 15:25:14.368: INFO: Pod "var-expansion-5ed78061-f9ec-4118-9589-697778cab445" satisfied condition "Succeeded or Failed"
Aug 29 15:25:14.375: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod var-expansion-5ed78061-f9ec-4118-9589-697778cab445 container dapi-container: <nil>
STEP: delete the pod
Aug 29 15:25:14.434: INFO: Waiting for pod var-expansion-5ed78061-f9ec-4118-9589-697778cab445 to disappear
Aug 29 15:25:14.447: INFO: Pod var-expansion-5ed78061-f9ec-4118-9589-697778cab445 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:14.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7141" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":822,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:14.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:14.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9055" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":54,"skipped":826,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:14.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:25:14.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:17.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7316" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":55,"skipped":838,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:17.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 29 15:25:18.339: INFO: Waiting up to 5m0s for pod "pod-3556f507-f25b-443a-808e-65dd06326f55" in namespace "emptydir-9861" to be "Succeeded or Failed"
Aug 29 15:25:18.348: INFO: Pod "pod-3556f507-f25b-443a-808e-65dd06326f55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.607656ms
Aug 29 15:25:20.358: INFO: Pod "pod-3556f507-f25b-443a-808e-65dd06326f55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01827966s
Aug 29 15:25:22.370: INFO: Pod "pod-3556f507-f25b-443a-808e-65dd06326f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030659955s
STEP: Saw pod success
Aug 29 15:25:22.371: INFO: Pod "pod-3556f507-f25b-443a-808e-65dd06326f55" satisfied condition "Succeeded or Failed"
Aug 29 15:25:22.382: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-3556f507-f25b-443a-808e-65dd06326f55 container test-container: <nil>
STEP: delete the pod
Aug 29 15:25:22.420: INFO: Waiting for pod pod-3556f507-f25b-443a-808e-65dd06326f55 to disappear
Aug 29 15:25:22.435: INFO: Pod pod-3556f507-f25b-443a-808e-65dd06326f55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:22.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9861" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":840,"failed":0}
SSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:22.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-2999
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2999
STEP: Deleting pre-stop pod
Aug 29 15:25:32.806: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:32.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2999" for this suite.

• [SLOW TEST:10.434 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":57,"skipped":844,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:32.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:25:33.008: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Aug 29 15:25:35.050: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Aug 29 15:25:35.068: INFO: observed ReplicaSet test-rs in namespace replicaset-6042 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 15:25:35.097: INFO: observed ReplicaSet test-rs in namespace replicaset-6042 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 15:25:35.651: INFO: observed ReplicaSet test-rs in namespace replicaset-6042 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 15:25:35.652: INFO: observed ReplicaSet test-rs in namespace replicaset-6042 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 15:25:36.559: INFO: observed ReplicaSet test-rs in namespace replicaset-6042 with ReadyReplicas 2, AvailableReplicas 2
Aug 29 15:25:38.639: INFO: observed Replicaset test-rs in namespace replicaset-6042 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:38.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6042" for this suite.

• [SLOW TEST:5.779 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":58,"skipped":856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:38.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-c2ce238e-b19b-4dca-9f38-b09bee9c934e
STEP: Creating a pod to test consume secrets
Aug 29 15:25:38.763: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4" in namespace "projected-5263" to be "Succeeded or Failed"
Aug 29 15:25:38.779: INFO: Pod "pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.878809ms
Aug 29 15:25:40.789: INFO: Pod "pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02604721s
Aug 29 15:25:42.805: INFO: Pod "pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041805955s
STEP: Saw pod success
Aug 29 15:25:42.805: INFO: Pod "pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4" satisfied condition "Succeeded or Failed"
Aug 29 15:25:42.812: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:25:43.306: INFO: Waiting for pod pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4 to disappear
Aug 29 15:25:43.313: INFO: Pod pod-projected-secrets-edbb858c-750d-4e3a-9270-d45574dccff4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:43.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5263" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:43.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:43.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-5954
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:45.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6603" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:45.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5954" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":60,"skipped":912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:45.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 29 15:25:45.806: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7182  15414962-bf02-4878-93e0-067a4ebd819e 9092 0 2022-08-29 15:25:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-29 15:25:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:25:45.807: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7182  15414962-bf02-4878-93e0-067a4ebd819e 9093 0 2022-08-29 15:25:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-29 15:25:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:45.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7182" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":61,"skipped":943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-78caa2e9-747c-4332-b25b-327a1c07e048
STEP: Creating a pod to test consume secrets
Aug 29 15:25:45.908: INFO: Waiting up to 5m0s for pod "pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb" in namespace "secrets-813" to be "Succeeded or Failed"
Aug 29 15:25:45.919: INFO: Pod "pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.735002ms
Aug 29 15:25:47.931: INFO: Pod "pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021687804s
Aug 29 15:25:49.943: INFO: Pod "pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033573764s
STEP: Saw pod success
Aug 29 15:25:49.943: INFO: Pod "pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb" satisfied condition "Succeeded or Failed"
Aug 29 15:25:49.950: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:25:49.990: INFO: Waiting for pod pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb to disappear
Aug 29 15:25:49.995: INFO: Pod pod-secrets-0110bd35-75d0-4a9f-8922-5fd4e9c2d3bb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:49.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-813" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":981,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:50.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
Aug 29 15:25:50.066: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8786 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:25:50.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8786" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":63,"skipped":983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:25:50.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-2595
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 15:25:50.252: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 15:25:50.331: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:25:52.349: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:25:54.357: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:25:56.343: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:25:58.346: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:00.709: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:02.346: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:04.346: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:06.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:08.342: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:10.342: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:26:12.344: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 15:26:12.363: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 15:26:12.376: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 15:26:14.482: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 15:26:14.482: INFO: Going to poll 172.25.0.12 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 29 15:26:14.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.12:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2595 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:26:14.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:26:14.489: INFO: ExecWithOptions: Clientset creation
Aug 29 15:26:14.489: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2595/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.12%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:26:14.649: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 29 15:26:14.649: INFO: Going to poll 172.25.1.41 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 29 15:26:14.659: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.41:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2595 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:26:14.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:26:14.660: INFO: ExecWithOptions: Clientset creation
Aug 29 15:26:14.660: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2595/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.41%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:26:14.837: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 29 15:26:14.837: INFO: Going to poll 172.25.2.46 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 29 15:26:14.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.46:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2595 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:26:14.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:26:14.846: INFO: ExecWithOptions: Clientset creation
Aug 29 15:26:14.846: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-2595/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.46%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:26:15.052: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:26:15.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2595" for this suite.

• [SLOW TEST:24.903 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1010,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:26:15.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-e1bfa8d7-8449-42e9-b8cb-e83bba5321c9 in namespace container-probe-5424
Aug 29 15:26:17.179: INFO: Started pod test-webserver-e1bfa8d7-8449-42e9-b8cb-e83bba5321c9 in namespace container-probe-5424
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:26:17.188: INFO: Initial restart count of pod test-webserver-e1bfa8d7-8449-42e9-b8cb-e83bba5321c9 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:30:19.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5424" for this suite.

• [SLOW TEST:244.063 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":65,"skipped":1030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:30:19.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-6ff3d894-3b0a-41ff-8848-35251a6cd397
STEP: Creating configMap with name cm-test-opt-upd-2d6e12f6-4905-4cda-8d2f-00c632b0a420
STEP: Creating the pod
Aug 29 15:30:19.287: INFO: The status of Pod pod-configmaps-50725448-a48b-46ba-b835-702db73c18f9 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:30:21.312: INFO: The status of Pod pod-configmaps-50725448-a48b-46ba-b835-702db73c18f9 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:30:23.298: INFO: The status of Pod pod-configmaps-50725448-a48b-46ba-b835-702db73c18f9 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-6ff3d894-3b0a-41ff-8848-35251a6cd397
STEP: Updating configmap cm-test-opt-upd-2d6e12f6-4905-4cda-8d2f-00c632b0a420
STEP: Creating configMap with name cm-test-opt-create-89f8c279-df9d-47f2-a886-a6ee8c4d622f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:31:52.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9161" for this suite.

• [SLOW TEST:93.063 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1053,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:31:52.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 29 15:31:52.291: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 15:31:52.313: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 15:31:52.319: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-14.eu-central-1.compute.internal before test
Aug 29 15:31:52.332: INFO: calico-kube-controllers-57fb8785bf-xg2r9 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 15:31:52.332: INFO: canal-lzt6p from kube-system started at 2022-08-29 15:05:27 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 15:31:52.332: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 15:31:52.332: INFO: coredns-5848f745f7-mkm4d from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container coredns ready: true, restart count 0
Aug 29 15:31:52.332: INFO: coredns-5848f745f7-ntmz2 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container coredns ready: true, restart count 0
Aug 29 15:31:52.332: INFO: envoy-agent-7wzrr from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 15:31:52.332: INFO: konnectivity-agent-b7c8486c7-dr2vw from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 15:31:52.332: INFO: konnectivity-agent-b7c8486c7-r476v from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 15:31:52.332: INFO: kube-proxy-fjrpb from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 15:31:52.332: INFO: metrics-server-7c94595b7c-d66rr from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 15:31:52.332: INFO: metrics-server-7c94595b7c-xjds6 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 15:31:52.332: INFO: node-local-dns-jdx9q from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 15:31:52.332: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:53 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 15:31:52.332: INFO: sonobuoy-e2e-job-0af6d8c74f8640cb from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container e2e ready: true, restart count 0
Aug 29 15:31:52.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 15:31:52.332: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-lffk7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 15:31:52.332: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 15:31:52.332: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-142.eu-central-1.compute.internal before test
Aug 29 15:31:52.348: INFO: canal-twqkh from kube-system started at 2022-08-29 15:05:56 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.348: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 15:31:52.348: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 15:31:52.348: INFO: envoy-agent-rmdxv from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.349: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 15:31:52.349: INFO: kube-proxy-rdwz4 from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.349: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 15:31:52.349: INFO: node-local-dns-x9dxw from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.349: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 15:31:52.349: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-dlbm7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.349: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 15:31:52.349: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 15:31:52.349: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-23-90.eu-central-1.compute.internal before test
Aug 29 15:31:52.361: INFO: pod-configmaps-50725448-a48b-46ba-b835-702db73c18f9 from configmap-9161 started at 2022-08-29 15:30:19 +0000 UTC (3 container statuses recorded)
Aug 29 15:31:52.361: INFO: 	Container createcm-volume-test ready: true, restart count 0
Aug 29 15:31:52.361: INFO: 	Container delcm-volume-test ready: true, restart count 0
Aug 29 15:31:52.361: INFO: 	Container updcm-volume-test ready: true, restart count 0
Aug 29 15:31:52.361: INFO: canal-vngk2 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.361: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 15:31:52.361: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 15:31:52.361: INFO: envoy-agent-48bh4 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.361: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 15:31:52.361: INFO: kube-proxy-qjw5j from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.361: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 15:31:52.361: INFO: node-local-dns-lxgw9 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 15:31:52.361: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 15:31:52.361: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-snzb6 from sonobuoy started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 15:31:52.361: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Aug 29 15:31:52.361: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node ip-172-31-16-14.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-20-142.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-23-90.eu-central-1.compute.internal
Aug 29 15:31:52.500: INFO: Pod pod-configmaps-50725448-a48b-46ba-b835-702db73c18f9 requesting resource cpu=0m on Node ip-172-31-23-90.eu-central-1.compute.internal
Aug 29 15:31:52.500: INFO: Pod calico-kube-controllers-57fb8785bf-xg2r9 requesting resource cpu=0m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod canal-lzt6p requesting resource cpu=250m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod canal-twqkh requesting resource cpu=250m on Node ip-172-31-20-142.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod canal-vngk2 requesting resource cpu=250m on Node ip-172-31-23-90.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod coredns-5848f745f7-mkm4d requesting resource cpu=50m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod coredns-5848f745f7-ntmz2 requesting resource cpu=50m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod envoy-agent-48bh4 requesting resource cpu=50m on Node ip-172-31-23-90.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod envoy-agent-7wzrr requesting resource cpu=50m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod envoy-agent-rmdxv requesting resource cpu=50m on Node ip-172-31-20-142.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod konnectivity-agent-b7c8486c7-dr2vw requesting resource cpu=10m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod konnectivity-agent-b7c8486c7-r476v requesting resource cpu=10m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod kube-proxy-fjrpb requesting resource cpu=75m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod kube-proxy-qjw5j requesting resource cpu=75m on Node ip-172-31-23-90.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod kube-proxy-rdwz4 requesting resource cpu=75m on Node ip-172-31-20-142.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod metrics-server-7c94595b7c-d66rr requesting resource cpu=100m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod metrics-server-7c94595b7c-xjds6 requesting resource cpu=100m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod node-local-dns-jdx9q requesting resource cpu=0m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod node-local-dns-lxgw9 requesting resource cpu=0m on Node ip-172-31-23-90.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod node-local-dns-x9dxw requesting resource cpu=0m on Node ip-172-31-20-142.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod sonobuoy-e2e-job-0af6d8c74f8640cb requesting resource cpu=0m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-dlbm7 requesting resource cpu=0m on Node ip-172-31-20-142.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-lffk7 requesting resource cpu=0m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.501: INFO: Pod sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-snzb6 requesting resource cpu=0m on Node ip-172-31-23-90.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Aug 29 15:31:52.501: INFO: Creating a pod which consumes cpu=633m on Node ip-172-31-16-14.eu-central-1.compute.internal
Aug 29 15:31:52.516: INFO: Creating a pod which consumes cpu=857m on Node ip-172-31-20-142.eu-central-1.compute.internal
Aug 29 15:31:52.528: INFO: Creating a pod which consumes cpu=857m on Node ip-172-31-23-90.eu-central-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26aa5791-0770-408b-82d1-69514ee169f7.170fda932045996b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-908/filler-pod-26aa5791-0770-408b-82d1-69514ee169f7 to ip-172-31-23-90.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26aa5791-0770-408b-82d1-69514ee169f7.170fda9348d8282d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26aa5791-0770-408b-82d1-69514ee169f7.170fda934a972f13], Reason = [Created], Message = [Created container filler-pod-26aa5791-0770-408b-82d1-69514ee169f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26aa5791-0770-408b-82d1-69514ee169f7.170fda9351724929], Reason = [Started], Message = [Started container filler-pod-26aa5791-0770-408b-82d1-69514ee169f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e.170fda9305e46c72], Reason = [Scheduled], Message = [Successfully assigned sched-pred-908/filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e to ip-172-31-20-142.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e.170fda9343575882], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e.170fda93796e3137], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 907.451073ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e.170fda937aad91ad], Reason = [Created], Message = [Created container filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e.170fda938041fa5a], Reason = [Started], Message = [Started container filler-pod-2864d4a8-63dc-4ca9-b956-e81eeaf51b2e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90af3128-c911-460f-b66d-df5f3edddaca.170fda9306215667], Reason = [Scheduled], Message = [Successfully assigned sched-pred-908/filler-pod-90af3128-c911-460f-b66d-df5f3edddaca to ip-172-31-16-14.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90af3128-c911-460f-b66d-df5f3edddaca.170fda93458b2e47], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90af3128-c911-460f-b66d-df5f3edddaca.170fda93778f19a3], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 839.100429ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90af3128-c911-460f-b66d-df5f3edddaca.170fda9379bd30c2], Reason = [Created], Message = [Created container filler-pod-90af3128-c911-460f-b66d-df5f3edddaca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90af3128-c911-460f-b66d-df5f3edddaca.170fda938022451c], Reason = [Started], Message = [Started container filler-pod-90af3128-c911-460f-b66d-df5f3edddaca]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.170fda94140aa760], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-16-14.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-20-142.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-23-90.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:31:58.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-908" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.041 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":67,"skipped":1073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:31:58.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:31:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5024" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":68,"skipped":1132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:31:58.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-4ab54f08-01eb-4ce0-90e5-81b7008f5003
STEP: Creating a pod to test consume configMaps
Aug 29 15:31:58.636: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4" in namespace "configmap-956" to be "Succeeded or Failed"
Aug 29 15:31:58.645: INFO: Pod "pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.61799ms
Aug 29 15:32:00.661: INFO: Pod "pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4": Phase="Running", Reason="", readiness=false. Elapsed: 2.024664977s
Aug 29 15:32:02.690: INFO: Pod "pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053753294s
STEP: Saw pod success
Aug 29 15:32:02.691: INFO: Pod "pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4" satisfied condition "Succeeded or Failed"
Aug 29 15:32:02.709: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:32:02.757: INFO: Waiting for pod pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4 to disappear
Aug 29 15:32:02.766: INFO: Pod pod-configmaps-3d89ea3a-db74-4c4e-97cf-e89c986afba4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:32:02.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-956" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1160,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:32:02.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Aug 29 15:32:02.916: INFO: observed Pod pod-test in namespace pods-4609 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 29 15:32:02.930: INFO: observed Pod pod-test in namespace pods-4609 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  }]
Aug 29 15:32:02.960: INFO: observed Pod pod-test in namespace pods-4609 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  }]
Aug 29 15:32:03.809: INFO: observed Pod pod-test in namespace pods-4609 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  }]
Aug 29 15:32:04.603: INFO: Found Pod pod-test in namespace pods-4609 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:32:02 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Aug 29 15:32:04.652: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Aug 29 15:32:04.705: INFO: observed event type ADDED
Aug 29 15:32:04.705: INFO: observed event type MODIFIED
Aug 29 15:32:04.706: INFO: observed event type MODIFIED
Aug 29 15:32:04.706: INFO: observed event type MODIFIED
Aug 29 15:32:04.707: INFO: observed event type MODIFIED
Aug 29 15:32:04.707: INFO: observed event type MODIFIED
Aug 29 15:32:04.708: INFO: observed event type MODIFIED
Aug 29 15:32:04.708: INFO: observed event type MODIFIED
Aug 29 15:32:06.605: INFO: observed event type MODIFIED
Aug 29 15:32:07.125: INFO: observed event type MODIFIED
Aug 29 15:32:07.624: INFO: observed event type MODIFIED
Aug 29 15:32:07.662: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:32:07.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4609" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":70,"skipped":1179,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:32:07.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 29 15:32:07.801: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:33:07.880: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:33:07.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Aug 29 15:33:10.026: INFO: found a healthy node: ip-172-31-20-142.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:33:18.230: INFO: pods created so far: [1 1 1]
Aug 29 15:33:18.230: INFO: length of pods created so far: 3
Aug 29 15:33:22.253: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:33:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7556" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:33:29.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4982" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:81.770 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":71,"skipped":1183,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:33:29.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-0c4915b0-0c7e-40c1-b84a-a8eeb2aa42f4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:33:31.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8627" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:33:31.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:33:31.960: INFO: The status of Pod busybox-readonly-fscbce70fa-dd7e-46ee-8e03-5d675ae4622a is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:33:33.973: INFO: The status of Pod busybox-readonly-fscbce70fa-dd7e-46ee-8e03-5d675ae4622a is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:33:33.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9614" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:33:34.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:33:34.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 29 15:33:38.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-5798 --namespace=crd-publish-openapi-5798 create -f -'
Aug 29 15:33:39.966: INFO: stderr: ""
Aug 29 15:33:39.966: INFO: stdout: "e2e-test-crd-publish-openapi-4690-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 29 15:33:39.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-5798 --namespace=crd-publish-openapi-5798 delete e2e-test-crd-publish-openapi-4690-crds test-cr'
Aug 29 15:33:40.108: INFO: stderr: ""
Aug 29 15:33:40.108: INFO: stdout: "e2e-test-crd-publish-openapi-4690-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 29 15:33:40.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-5798 --namespace=crd-publish-openapi-5798 apply -f -'
Aug 29 15:33:40.364: INFO: stderr: ""
Aug 29 15:33:40.364: INFO: stdout: "e2e-test-crd-publish-openapi-4690-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 29 15:33:40.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-5798 --namespace=crd-publish-openapi-5798 delete e2e-test-crd-publish-openapi-4690-crds test-cr'
Aug 29 15:33:40.481: INFO: stderr: ""
Aug 29 15:33:40.481: INFO: stdout: "e2e-test-crd-publish-openapi-4690-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 29 15:33:40.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-5798 explain e2e-test-crd-publish-openapi-4690-crds'
Aug 29 15:33:41.240: INFO: stderr: ""
Aug 29 15:33:41.240: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4690-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:33:44.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5798" for this suite.

• [SLOW TEST:10.532 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":74,"skipped":1258,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:33:44.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1443
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1443
STEP: creating replication controller externalsvc in namespace services-1443
I0829 15:33:44.739330      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1443, replica count: 2
I0829 15:33:47.791178      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 29 15:33:47.852: INFO: Creating new exec pod
Aug 29 15:33:49.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1443 exec execpodqcxk9 -- /bin/sh -x -c nslookup nodeport-service.services-1443.svc.cluster.local'
Aug 29 15:33:50.177: INFO: stderr: "+ nslookup nodeport-service.services-1443.svc.cluster.local\n"
Aug 29 15:33:50.177: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-1443.svc.cluster.local\tcanonical name = externalsvc.services-1443.svc.cluster.local.\nName:\texternalsvc.services-1443.svc.cluster.local\nAddress: 10.240.31.17\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1443, will wait for the garbage collector to delete the pods
Aug 29 15:33:50.252: INFO: Deleting ReplicationController externalsvc took: 11.930689ms
Aug 29 15:33:50.353: INFO: Terminating ReplicationController externalsvc pods took: 100.283892ms
Aug 29 15:33:52.882: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:33:52.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1443" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:8.354 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":75,"skipped":1265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:33:52.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 15:33:52.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1779 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 29 15:33:53.119: INFO: stderr: ""
Aug 29 15:33:53.119: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 29 15:33:58.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1779 get pod e2e-test-httpd-pod -o json'
Aug 29 15:33:58.265: INFO: stderr: ""
Aug 29 15:33:58.265: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"e1c02bbf72f9f0eefc704f62a7394eaa1093cf50af1c0676195fea43d42be3b1\",\n            \"cni.projectcalico.org/podIP\": \"172.25.2.56/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.56/32\"\n        },\n        \"creationTimestamp\": \"2022-08-29T15:33:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1779\",\n        \"resourceVersion\": \"11361\",\n        \"uid\": \"677ef722-ee83-4007-add4-a84e0fb2ca2e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-mn6hc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-23-90.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-mn6hc\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:33:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:33:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:33:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:33:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a33276847314a5f338f6abe352238f54b2644464bc8763e2b7de99072b1e1781\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-08-29T15:33:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.23.90\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.56\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.56\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-08-29T15:33:53Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 29 15:33:58.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1779 replace -f -'
Aug 29 15:33:59.127: INFO: stderr: ""
Aug 29 15:33:59.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
Aug 29 15:33:59.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1779 delete pods e2e-test-httpd-pod'
Aug 29 15:34:00.967: INFO: stderr: ""
Aug 29 15:34:00.967: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:34:00.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1779" for this suite.

• [SLOW TEST:8.075 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":76,"skipped":1305,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:34:01.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 29 15:34:02.206: INFO: Pod name wrapped-volume-race-183c5fcf-e1b4-466e-97cd-8e2bfc536f0d: Found 0 pods out of 5
Aug 29 15:34:07.231: INFO: Pod name wrapped-volume-race-183c5fcf-e1b4-466e-97cd-8e2bfc536f0d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-183c5fcf-e1b4-466e-97cd-8e2bfc536f0d in namespace emptydir-wrapper-1163, will wait for the garbage collector to delete the pods
Aug 29 15:34:19.393: INFO: Deleting ReplicationController wrapped-volume-race-183c5fcf-e1b4-466e-97cd-8e2bfc536f0d took: 12.989837ms
Aug 29 15:34:19.519: INFO: Terminating ReplicationController wrapped-volume-race-183c5fcf-e1b4-466e-97cd-8e2bfc536f0d pods took: 125.391478ms
STEP: Creating RC which spawns configmap-volume pods
Aug 29 15:34:22.828: INFO: Pod name wrapped-volume-race-60f59a8b-24de-4134-9e63-25898604e0b8: Found 0 pods out of 5
Aug 29 15:34:27.854: INFO: Pod name wrapped-volume-race-60f59a8b-24de-4134-9e63-25898604e0b8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-60f59a8b-24de-4134-9e63-25898604e0b8 in namespace emptydir-wrapper-1163, will wait for the garbage collector to delete the pods
Aug 29 15:34:38.027: INFO: Deleting ReplicationController wrapped-volume-race-60f59a8b-24de-4134-9e63-25898604e0b8 took: 19.157544ms
Aug 29 15:34:38.228: INFO: Terminating ReplicationController wrapped-volume-race-60f59a8b-24de-4134-9e63-25898604e0b8 pods took: 200.903556ms
STEP: Creating RC which spawns configmap-volume pods
Aug 29 15:34:42.674: INFO: Pod name wrapped-volume-race-1914f233-b480-447d-a686-5b96bc16ce7a: Found 0 pods out of 5
Aug 29 15:34:47.694: INFO: Pod name wrapped-volume-race-1914f233-b480-447d-a686-5b96bc16ce7a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1914f233-b480-447d-a686-5b96bc16ce7a in namespace emptydir-wrapper-1163, will wait for the garbage collector to delete the pods
Aug 29 15:34:58.011: INFO: Deleting ReplicationController wrapped-volume-race-1914f233-b480-447d-a686-5b96bc16ce7a took: 17.272236ms
Aug 29 15:34:58.212: INFO: Terminating ReplicationController wrapped-volume-race-1914f233-b480-447d-a686-5b96bc16ce7a pods took: 200.768007ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1163" for this suite.

• [SLOW TEST:61.523 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":77,"skipped":1316,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:02.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-952ccaaa-970e-47cb-8139-b34860f764dc
STEP: Creating a pod to test consume secrets
Aug 29 15:35:02.602: INFO: Waiting up to 5m0s for pod "pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266" in namespace "secrets-8599" to be "Succeeded or Failed"
Aug 29 15:35:02.611: INFO: Pod "pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266": Phase="Pending", Reason="", readiness=false. Elapsed: 8.476504ms
Aug 29 15:35:04.622: INFO: Pod "pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019914603s
Aug 29 15:35:07.505: INFO: Pod "pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.903168234s
STEP: Saw pod success
Aug 29 15:35:07.505: INFO: Pod "pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266" satisfied condition "Succeeded or Failed"
Aug 29 15:35:07.519: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:35:07.602: INFO: Waiting for pod pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266 to disappear
Aug 29 15:35:07.625: INFO: Pod pod-secrets-4b14c798-ff05-4e15-aef5-7ab2442fa266 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:07.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8599" for this suite.

• [SLOW TEST:5.137 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:07.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:24.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3435" for this suite.

• [SLOW TEST:16.375 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":79,"skipped":1352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:24.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 15:35:24.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-5674 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 29 15:35:24.237: INFO: stderr: ""
Aug 29 15:35:24.237: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 29 15:35:24.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-5674 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Aug 29 15:35:25.108: INFO: stderr: ""
Aug 29 15:35:25.108: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 15:35:25.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-5674 delete pods e2e-test-httpd-pod'
Aug 29 15:35:28.109: INFO: stderr: ""
Aug 29 15:35:28.109: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:28.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5674" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":80,"skipped":1400,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:28.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:35:28.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1" in namespace "downward-api-9534" to be "Succeeded or Failed"
Aug 29 15:35:28.231: INFO: Pod "downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.846018ms
Aug 29 15:35:30.248: INFO: Pod "downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1": Phase="Running", Reason="", readiness=false. Elapsed: 2.029829168s
Aug 29 15:35:32.263: INFO: Pod "downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043901047s
STEP: Saw pod success
Aug 29 15:35:32.263: INFO: Pod "downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1" satisfied condition "Succeeded or Failed"
Aug 29 15:35:32.269: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1 container client-container: <nil>
STEP: delete the pod
Aug 29 15:35:32.332: INFO: Waiting for pod downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1 to disappear
Aug 29 15:35:32.343: INFO: Pod downwardapi-volume-c1dd9165-8ef9-4468-96c3-0b8a378951a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:32.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9534" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1406,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2180.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2180.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2180.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2180.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2180.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 15:35:34.651: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.662: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.673: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.685: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.694: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.705: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.836: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.845: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2180.svc.cluster.local from pod dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3: the server could not find the requested resource (get pods dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3)
Aug 29 15:35:34.845: INFO: Lookups using dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2180.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2180.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2180.svc.cluster.local jessie_udp@dns-test-service-2.dns-2180.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2180.svc.cluster.local]

Aug 29 15:35:39.920: INFO: DNS probes using dns-2180/dns-test-231fe850-6cc4-4796-a7f1-8706e97e18a3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:40.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2180" for this suite.

• [SLOW TEST:7.791 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":82,"skipped":1419,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:40.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:35:50.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2886" for this suite.

• [SLOW TEST:10.125 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":83,"skipped":1428,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:35:50.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-1484
STEP: creating service affinity-nodeport in namespace services-1484
STEP: creating replication controller affinity-nodeport in namespace services-1484
I0829 15:35:50.426005      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1484, replica count: 3
I0829 15:35:53.478132      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:35:53.534: INFO: Creating new exec pod
Aug 29 15:35:56.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1484 exec execpod-affinitymplh7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 29 15:35:56.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 29 15:35:56.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:35:56.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1484 exec execpod-affinitymplh7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.242 80'
Aug 29 15:35:57.127: INFO: stderr: "+ nc -v -t -w 2 10.240.26.242 80\n+ echo hostName\nConnection to 10.240.26.242 80 port [tcp/http] succeeded!\n"
Aug 29 15:35:57.127: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:35:57.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1484 exec execpod-affinitymplh7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 31268'
Aug 29 15:35:57.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.23.90 31268\nConnection to 172.31.23.90 31268 port [tcp/*] succeeded!\n"
Aug 29 15:35:57.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:35:57.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1484 exec execpod-affinitymplh7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.14 31268'
Aug 29 15:35:57.696: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.14 31268\nConnection to 172.31.16.14 31268 port [tcp/*] succeeded!\n"
Aug 29 15:35:57.696: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:35:57.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1484 exec execpod-affinitymplh7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.14:31268/ ; done'
Aug 29 15:35:58.010: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:31268/\n"
Aug 29 15:35:58.010: INFO: stdout: "\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr\naffinity-nodeport-8dwqr"
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Received response from host: affinity-nodeport-8dwqr
Aug 29 15:35:58.010: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1484, will wait for the garbage collector to delete the pods
Aug 29 15:35:58.128: INFO: Deleting ReplicationController affinity-nodeport took: 13.357053ms
Aug 29 15:35:58.229: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.518686ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:00.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1484" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:10.533 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":84,"skipped":1429,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:00.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
Aug 29 15:36:00.902: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1389 proxy --unix-socket=/tmp/kubectl-proxy-unix1176065144/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1389" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":85,"skipped":1432,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:01.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:01.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9721" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":86,"skipped":1438,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 15:36:01.245: INFO: The status of Pod pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:36:03.265: INFO: The status of Pod pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:36:05.361: INFO: The status of Pod pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 29 15:36:05.905: INFO: Successfully updated pod "pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a"
Aug 29 15:36:05.905: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a" in namespace "pods-4552" to be "terminated due to deadline exceeded"
Aug 29 15:36:06.628: INFO: Pod "pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a": Phase="Running", Reason="", readiness=true. Elapsed: 723.263675ms
Aug 29 15:36:08.638: INFO: Pod "pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.733876351s
Aug 29 15:36:08.640: INFO: Pod "pod-update-activedeadlineseconds-78d8ce7c-ab15-47a4-84e5-a5d9f16da80a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:08.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4552" for this suite.

• [SLOW TEST:7.574 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1446,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 29 15:36:08.773: INFO: Waiting up to 5m0s for pod "pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9" in namespace "emptydir-7957" to be "Succeeded or Failed"
Aug 29 15:36:08.779: INFO: Pod "pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.499738ms
Aug 29 15:36:10.787: INFO: Pod "pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013426307s
Aug 29 15:36:12.796: INFO: Pod "pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023087253s
STEP: Saw pod success
Aug 29 15:36:12.796: INFO: Pod "pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9" satisfied condition "Succeeded or Failed"
Aug 29 15:36:12.804: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9 container test-container: <nil>
STEP: delete the pod
Aug 29 15:36:12.839: INFO: Waiting for pod pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9 to disappear
Aug 29 15:36:12.854: INFO: Pod pod-065a4a55-15b2-43f5-bd3e-7aaa491006d9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:12.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7957" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:12.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2844
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:36:12.974: INFO: Found 0 stateful pods, waiting for 1
Aug 29 15:36:22.985: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Aug 29 15:36:23.044: INFO: Found 1 stateful pods, waiting for 2
Aug 29 15:36:33.053: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:36:33.053: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 15:36:33.092: INFO: Deleting all statefulset in ns statefulset-2844
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:33.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2844" for this suite.

• [SLOW TEST:20.273 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":89,"skipped":1473,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:33.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Aug 29 15:36:37.260: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1204 PodName:pod-sharedvolume-df933dbd-2b17-4ccc-9326-2ef988846c14 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:36:37.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:36:37.261: INFO: ExecWithOptions: Clientset creation
Aug 29 15:36:37.261: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-1204/pods/pod-sharedvolume-df933dbd-2b17-4ccc-9326-2ef988846c14/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:36:37.432: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:36:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1204" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":90,"skipped":1476,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:36:37.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8897, will wait for the garbage collector to delete the pods
Aug 29 15:36:39.623: INFO: Deleting Job.batch foo took: 11.571934ms
Aug 29 15:36:39.724: INFO: Terminating Job.batch foo pods took: 101.038089ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8897" for this suite.

• [SLOW TEST:35.305 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":91,"skipped":1485,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:12.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:37:12.821: INFO: Creating ReplicaSet my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490
Aug 29 15:37:12.849: INFO: Pod name my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490: Found 0 pods out of 1
Aug 29 15:37:17.858: INFO: Pod name my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490: Found 1 pods out of 1
Aug 29 15:37:17.858: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490" is running
Aug 29 15:37:17.879: INFO: Pod "my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490-ctln6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:37:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:37:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:37:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:37:12 +0000 UTC Reason: Message:}])
Aug 29 15:37:17.879: INFO: Trying to dial the pod
Aug 29 15:37:22.916: INFO: Controller my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490: Got expected result from replica 1 [my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490-ctln6]: "my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490-ctln6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:22.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1633" for this suite.

• [SLOW TEST:10.178 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":92,"skipped":1488,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:22.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 29 15:37:23.022: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 15:37:23.069: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 15:37:23.075: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-14.eu-central-1.compute.internal before test
Aug 29 15:37:23.094: INFO: calico-kube-controllers-57fb8785bf-xg2r9 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 15:37:23.094: INFO: canal-lzt6p from kube-system started at 2022-08-29 15:05:27 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 15:37:23.094: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 15:37:23.094: INFO: coredns-5848f745f7-mkm4d from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container coredns ready: true, restart count 0
Aug 29 15:37:23.094: INFO: coredns-5848f745f7-ntmz2 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container coredns ready: true, restart count 0
Aug 29 15:37:23.094: INFO: envoy-agent-7wzrr from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 15:37:23.094: INFO: konnectivity-agent-b7c8486c7-dr2vw from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 15:37:23.094: INFO: konnectivity-agent-b7c8486c7-r476v from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 15:37:23.094: INFO: kube-proxy-fjrpb from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 15:37:23.094: INFO: metrics-server-7c94595b7c-d66rr from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 15:37:23.094: INFO: metrics-server-7c94595b7c-xjds6 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 15:37:23.094: INFO: node-local-dns-jdx9q from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 15:37:23.094: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:53 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 15:37:23.094: INFO: sonobuoy-e2e-job-0af6d8c74f8640cb from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container e2e ready: true, restart count 0
Aug 29 15:37:23.094: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 15:37:23.094: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-lffk7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.094: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 15:37:23.094: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 15:37:23.094: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-142.eu-central-1.compute.internal before test
Aug 29 15:37:23.114: INFO: canal-twqkh from kube-system started at 2022-08-29 15:05:56 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.114: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 15:37:23.114: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 15:37:23.114: INFO: envoy-agent-rmdxv from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.115: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 15:37:23.115: INFO: kube-proxy-rdwz4 from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.115: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 15:37:23.115: INFO: node-local-dns-x9dxw from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.115: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 15:37:23.115: INFO: my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490-ctln6 from replicaset-1633 started at 2022-08-29 15:37:12 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.115: INFO: 	Container my-hostname-basic-afaea586-0eb8-4a8e-94c7-463fb8705490 ready: true, restart count 0
Aug 29 15:37:23.115: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-dlbm7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 15:37:23.115: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 15:37:23.115: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-23-90.eu-central-1.compute.internal before test
Aug 29 15:37:23.140: INFO: canal-vngk2 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.140: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 15:37:23.140: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 15:37:23.140: INFO: envoy-agent-48bh4 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.140: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 15:37:23.140: INFO: kube-proxy-qjw5j from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.140: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 15:37:23.140: INFO: node-local-dns-lxgw9 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 15:37:23.140: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 15:37:23.140: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-snzb6 from sonobuoy started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 15:37:23.140: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Aug 29 15:37:23.140: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.170fdae0024ba3f5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:24.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7031" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":93,"skipped":1495,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:37:24.853: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:37:27.898: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:37:27.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1356-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:31.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5987" for this suite.
STEP: Destroying namespace "webhook-5987-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":94,"skipped":1495,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:32.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:36.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6525" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":95,"skipped":1516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:36.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-eb4cd3d2-9abe-4315-96d4-767392a24395
STEP: Creating configMap with name cm-test-opt-upd-e589e70b-5f02-4907-86f6-eacc43f505ea
STEP: Creating the pod
Aug 29 15:37:36.893: INFO: The status of Pod pod-projected-configmaps-32c0302b-6eb0-412f-82a8-a91c937ea8c4 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:37:38.911: INFO: The status of Pod pod-projected-configmaps-32c0302b-6eb0-412f-82a8-a91c937ea8c4 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-eb4cd3d2-9abe-4315-96d4-767392a24395
STEP: Updating configmap cm-test-opt-upd-e589e70b-5f02-4907-86f6-eacc43f505ea
STEP: Creating configMap with name cm-test-opt-create-3e186c96-513d-48b7-8e60-20ea4f96ac16
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:41.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1074" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:41.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Aug 29 15:37:41.182: INFO: pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 29 15:37:43.364: INFO: running pods: 2 < 3
Aug 29 15:37:45.391: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:47.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2534" for this suite.

• [SLOW TEST:6.526 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":97,"skipped":1694,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:47.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
Aug 29 15:37:47.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8745 create -f -'
Aug 29 15:37:48.907: INFO: stderr: ""
Aug 29 15:37:48.907: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 29 15:37:48.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8745 diff -f -'
Aug 29 15:37:50.260: INFO: rc: 1
Aug 29 15:37:50.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8745 delete -f -'
Aug 29 15:37:50.360: INFO: stderr: ""
Aug 29 15:37:50.360: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:50.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8745" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":98,"skipped":1720,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:50.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
Aug 29 15:37:50.449: INFO: The status of Pod pod-hostip-7ed961ed-25d0-4ffc-acd0-cc7d7cdb7f2e is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:37:52.458: INFO: The status of Pod pod-hostip-7ed961ed-25d0-4ffc-acd0-cc7d7cdb7f2e is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:37:54.459: INFO: The status of Pod pod-hostip-7ed961ed-25d0-4ffc-acd0-cc7d7cdb7f2e is Running (Ready = true)
Aug 29 15:37:54.476: INFO: Pod pod-hostip-7ed961ed-25d0-4ffc-acd0-cc7d7cdb7f2e has hostIP: 172.31.20.142
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:37:54.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2496" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1722,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:37:54.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:37:54.597: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 29 15:37:54.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:54.619: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Aug 29 15:37:54.678: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:54.678: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:37:55.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:55.693: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:37:56.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:56.690: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:37:57.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:57.690: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:37:58.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:58.694: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:37:59.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:37:59.690: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:38:00.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 15:38:00.689: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 29 15:38:00.747: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 15:38:00.747: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 29 15:38:01.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:38:01.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 29 15:38:01.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:38:01.790: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:38:02.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:38:02.804: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:38:03.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:38:03.801: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:38:04.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:38:04.806: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:38:06.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 15:38:06.437: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8926, will wait for the garbage collector to delete the pods
Aug 29 15:38:06.531: INFO: Deleting DaemonSet.extensions daemon-set took: 17.287487ms
Aug 29 15:38:06.632: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.199712ms
Aug 29 15:38:08.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:38:08.849: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 15:38:08.855: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14369"},"items":null}

Aug 29 15:38:08.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14369"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:08.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8926" for this suite.

• [SLOW TEST:14.446 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":100,"skipped":1735,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:08.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-64bbc3e2-66c9-4b0c-9bb6-0a4e52997a75
STEP: Creating a pod to test consume secrets
Aug 29 15:38:09.011: INFO: Waiting up to 5m0s for pod "pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2" in namespace "secrets-3117" to be "Succeeded or Failed"
Aug 29 15:38:09.021: INFO: Pod "pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.163802ms
Aug 29 15:38:11.034: INFO: Pod "pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023170136s
Aug 29 15:38:13.050: INFO: Pod "pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039769569s
STEP: Saw pod success
Aug 29 15:38:13.050: INFO: Pod "pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2" satisfied condition "Succeeded or Failed"
Aug 29 15:38:13.057: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:38:13.102: INFO: Waiting for pod pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2 to disappear
Aug 29 15:38:13.119: INFO: Pod pod-secrets-0e621c97-2f71-4233-8dce-58cb7d3048b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:13.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3117" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1736,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:13.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:38:13.204: INFO: Creating simple deployment test-new-deployment
Aug 29 15:38:13.237: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 15:38:15.354: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5302  82c1a9a6-5421-46d8-b3ca-245adf9e9422 14477 3 2022-08-29 15:38:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-08-29 15:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:38:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b22a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-08-29 15:38:14 +0000 UTC,LastTransitionTime:2022-08-29 15:38:13 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-29 15:38:15 +0000 UTC,LastTransitionTime:2022-08-29 15:38:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 15:38:15.360: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-5302  23fb20a0-50f1-4e7d-8795-77388063ce33 14475 3 2022-08-29 15:38:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 82c1a9a6-5421-46d8-b3ca-245adf9e9422 0xc000b23027 0xc000b23028}] []  [{kube-controller-manager Update apps/v1 2022-08-29 15:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82c1a9a6-5421-46d8-b3ca-245adf9e9422\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:38:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b230b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:38:15.373: INFO: Pod "test-new-deployment-5d9fdcc779-fn8w2" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-fn8w2 test-new-deployment-5d9fdcc779- deployment-5302  f5be54e1-f426-43c4-84dd-bc7ff2d7380f 14456 0 2022-08-29 15:38:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:12bdb84c122392abdbe9ef38281b3b4a3115f05f97b79de1b07a5e954fc850c8 cni.projectcalico.org/podIP:172.25.1.72/32 cni.projectcalico.org/podIPs:172.25.1.72/32] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 23fb20a0-50f1-4e7d-8795-77388063ce33 0xc00120e497 0xc00120e498}] []  [{Go-http-client Update v1 2022-08-29 15:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 15:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fb20a0-50f1-4e7d-8795-77388063ce33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 15:38:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpfc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpfc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.72,StartTime:2022-08-29 15:38:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 15:38:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b96d9716f87b9da1391201034ec35dd1022a0e3ed01dcac7e0becc0e00dc1d21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 15:38:15.374: INFO: Pod "test-new-deployment-5d9fdcc779-g4jnz" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-g4jnz test-new-deployment-5d9fdcc779- deployment-5302  f6cfcb9a-a725-4b7a-a2db-33e8c15b683d 14479 0 2022-08-29 15:38:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 23fb20a0-50f1-4e7d-8795-77388063ce33 0xc00120e6b0 0xc00120e6b1}] []  [{kube-controller-manager Update v1 2022-08-29 15:38:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fb20a0-50f1-4e7d-8795-77388063ce33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 15:38:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gm9bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gm9bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:38:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 15:38:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:15.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5302" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":102,"skipped":1738,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:15.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Aug 29 15:38:21.549: INFO: 80 pods remaining
Aug 29 15:38:21.549: INFO: 80 pods has nil DeletionTimestamp
Aug 29 15:38:21.549: INFO: 
Aug 29 15:38:22.684: INFO: 68 pods remaining
Aug 29 15:38:22.684: INFO: 68 pods has nil DeletionTimestamp
Aug 29 15:38:22.684: INFO: 
Aug 29 15:38:23.523: INFO: 60 pods remaining
Aug 29 15:38:23.523: INFO: 60 pods has nil DeletionTimestamp
Aug 29 15:38:23.523: INFO: 
Aug 29 15:38:24.521: INFO: 40 pods remaining
Aug 29 15:38:24.521: INFO: 40 pods has nil DeletionTimestamp
Aug 29 15:38:24.521: INFO: 
Aug 29 15:38:25.530: INFO: 30 pods remaining
Aug 29 15:38:25.530: INFO: 30 pods has nil DeletionTimestamp
Aug 29 15:38:25.530: INFO: 
Aug 29 15:38:26.532: INFO: 20 pods remaining
Aug 29 15:38:26.532: INFO: 20 pods has nil DeletionTimestamp
Aug 29 15:38:26.532: INFO: 
STEP: Gathering metrics
W0829 15:38:27.539648      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 15:38:27.539: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:27.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2833" for this suite.

• [SLOW TEST:12.168 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":103,"skipped":1744,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:27.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:38:27.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 15:38:27.698: INFO: The status of Pod pod-logs-websocket-cb28e823-628c-4b3a-b850-3b9525037334 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:38:29.708: INFO: The status of Pod pod-logs-websocket-cb28e823-628c-4b3a-b850-3b9525037334 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:38:31.800: INFO: The status of Pod pod-logs-websocket-cb28e823-628c-4b3a-b850-3b9525037334 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:38:33.713: INFO: The status of Pod pod-logs-websocket-cb28e823-628c-4b3a-b850-3b9525037334 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:38:35.739: INFO: The status of Pod pod-logs-websocket-cb28e823-628c-4b3a-b850-3b9525037334 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7438" for this suite.

• [SLOW TEST:8.448 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:36.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:38:37.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 15:38:39.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:38:42.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:43.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1269" for this suite.
STEP: Destroying namespace "webhook-1269-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.251 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":105,"skipped":1786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6258.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6258.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6258.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6258.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 15:38:47.523: INFO: DNS probes using dns-6258/dns-test-51d37492-6598-4413-8b52-d153a361d42d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:38:47.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6258" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":106,"skipped":1823,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:38:47.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Aug 29 15:38:47.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
Aug 29 15:38:48.281: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 29 15:38:50.756: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:38:52.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:38:54.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:38:56.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:38:58.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 38, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:39:02.118: INFO: Waited 1.335498647s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Aug 29 15:39:03.210: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:03.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6784" for this suite.

• [SLOW TEST:16.181 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":107,"skipped":1835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:03.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:39:03.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 29 15:39:08.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 create -f -'
Aug 29 15:39:09.199: INFO: stderr: ""
Aug 29 15:39:09.199: INFO: stdout: "e2e-test-crd-publish-openapi-6495-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 29 15:39:09.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 delete e2e-test-crd-publish-openapi-6495-crds test-foo'
Aug 29 15:39:09.340: INFO: stderr: ""
Aug 29 15:39:09.340: INFO: stdout: "e2e-test-crd-publish-openapi-6495-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 29 15:39:09.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 apply -f -'
Aug 29 15:39:09.612: INFO: stderr: ""
Aug 29 15:39:09.612: INFO: stdout: "e2e-test-crd-publish-openapi-6495-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 29 15:39:09.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 delete e2e-test-crd-publish-openapi-6495-crds test-foo'
Aug 29 15:39:09.718: INFO: stderr: ""
Aug 29 15:39:09.718: INFO: stdout: "e2e-test-crd-publish-openapi-6495-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Aug 29 15:39:09.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 create -f -'
Aug 29 15:39:09.932: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 29 15:39:09.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 create -f -'
Aug 29 15:39:10.145: INFO: rc: 1
Aug 29 15:39:10.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 apply -f -'
Aug 29 15:39:10.789: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 29 15:39:10.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 create -f -'
Aug 29 15:39:10.988: INFO: rc: 1
Aug 29 15:39:10.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 --namespace=crd-publish-openapi-9815 apply -f -'
Aug 29 15:39:11.267: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 29 15:39:11.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 explain e2e-test-crd-publish-openapi-6495-crds'
Aug 29 15:39:11.472: INFO: stderr: ""
Aug 29 15:39:11.472: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6495-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 29 15:39:11.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 explain e2e-test-crd-publish-openapi-6495-crds.metadata'
Aug 29 15:39:11.685: INFO: stderr: ""
Aug 29 15:39:11.685: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6495-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 29 15:39:11.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 explain e2e-test-crd-publish-openapi-6495-crds.spec'
Aug 29 15:39:11.945: INFO: stderr: ""
Aug 29 15:39:11.945: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6495-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 29 15:39:11.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 explain e2e-test-crd-publish-openapi-6495-crds.spec.bars'
Aug 29 15:39:12.162: INFO: stderr: ""
Aug 29 15:39:12.162: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6495-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 29 15:39:12.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-9815 explain e2e-test-crd-publish-openapi-6495-crds.spec.bars2'
Aug 29 15:39:12.418: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:16.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9815" for this suite.

• [SLOW TEST:12.402 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":108,"skipped":1880,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:16.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
Aug 29 15:39:16.228: INFO: created test-podtemplate-1
Aug 29 15:39:16.238: INFO: created test-podtemplate-2
Aug 29 15:39:16.245: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 29 15:39:16.251: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 29 15:39:16.279: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8178" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":109,"skipped":1894,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:16.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-4794
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4794 to expose endpoints map[]
Aug 29 15:39:16.414: INFO: successfully validated that service endpoint-test2 in namespace services-4794 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4794
Aug 29 15:39:16.452: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:39:18.467: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4794 to expose endpoints map[pod1:[80]]
Aug 29 15:39:18.513: INFO: successfully validated that service endpoint-test2 in namespace services-4794 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Aug 29 15:39:18.513: INFO: Creating new exec pod
Aug 29 15:39:21.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4794 exec execpodgvmpg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 29 15:39:21.887: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 15:39:21.887: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:39:21.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4794 exec execpodgvmpg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.252 80'
Aug 29 15:39:22.211: INFO: stderr: "+ nc -v -t -w 2 10.240.19.252 80\nConnection to 10.240.19.252 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Aug 29 15:39:22.211: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4794
Aug 29 15:39:22.235: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:39:24.243: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4794 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 29 15:39:24.279: INFO: successfully validated that service endpoint-test2 in namespace services-4794 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Aug 29 15:39:25.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4794 exec execpodgvmpg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 29 15:39:25.571: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 15:39:25.571: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:39:25.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4794 exec execpodgvmpg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.252 80'
Aug 29 15:39:25.935: INFO: stderr: "+ + ncecho hostName -v -t\n -w 2 10.240.19.252 80\nConnection to 10.240.19.252 80 port [tcp/http] succeeded!\n"
Aug 29 15:39:25.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4794
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4794 to expose endpoints map[pod2:[80]]
Aug 29 15:39:27.040: INFO: successfully validated that service endpoint-test2 in namespace services-4794 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Aug 29 15:39:28.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4794 exec execpodgvmpg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 29 15:39:28.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 15:39:28.371: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:39:28.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4794 exec execpodgvmpg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.252 80'
Aug 29 15:39:28.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.252 80\nConnection to 10.240.19.252 80 port [tcp/http] succeeded!\n"
Aug 29 15:39:28.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4794
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4794 to expose endpoints map[]
Aug 29 15:39:29.257: INFO: successfully validated that service endpoint-test2 in namespace services-4794 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4794" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:13.010 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":110,"skipped":1905,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:29.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 15:39:33.074: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:33.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4639" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":1924,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 15:39:33.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:39:33.305: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:39:34.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:39:34.802: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:39:35.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:39:35.324: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 29 15:39:36.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:39:36.106: INFO: Node ip-172-31-20-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:39:37.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:39:37.132: INFO: Node ip-172-31-20-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:39:38.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:39:38.125: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-185, will wait for the garbage collector to delete the pods
Aug 29 15:39:38.212: INFO: Deleting DaemonSet.extensions daemon-set took: 10.828371ms
Aug 29 15:39:38.316: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.90089ms
Aug 29 15:39:40.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:39:40.428: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 15:39:40.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16443"},"items":null}

Aug 29 15:39:40.442: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16443"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:40.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-185" for this suite.

• [SLOW TEST:7.357 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":112,"skipped":1932,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:40.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bbb99600-4afb-41ed-bec8-69e198708b55
STEP: Creating the pod
Aug 29 15:39:40.691: INFO: The status of Pod pod-projected-configmaps-89c5ad9b-206f-41b6-bc0d-4e5f3210612f is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:39:42.706: INFO: The status of Pod pod-projected-configmaps-89c5ad9b-206f-41b6-bc0d-4e5f3210612f is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-bbb99600-4afb-41ed-bec8-69e198708b55
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:44.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4629" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":1935,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:44.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:39:58.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3198" for this suite.
STEP: Destroying namespace "nsdeletetest-5610" for this suite.
Aug 29 15:39:58.066: INFO: Namespace nsdeletetest-5610 was already deleted
STEP: Destroying namespace "nsdeletetest-4805" for this suite.

• [SLOW TEST:13.273 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":114,"skipped":1936,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:39:58.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:40:00.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:40:03.501: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:40:17.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1186" for this suite.
STEP: Destroying namespace "webhook-1186-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:20.163 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":115,"skipped":1955,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:40:18.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:40:18.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:40:19.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7951" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":116,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:40:19.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 29 15:40:19.366: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 29 15:40:19.395: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 29 15:40:19.395: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 29 15:40:19.436: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 29 15:40:19.437: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 29 15:40:19.486: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 29 15:40:19.487: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 29 15:40:26.617: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:40:26.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-487" for this suite.

• [SLOW TEST:7.397 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":117,"skipped":1998,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:40:26.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:40:26.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5324" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":118,"skipped":2010,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:40:26.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:40:29.103: INFO: Deleting pod "var-expansion-155388eb-56cb-4d62-8f69-3b8f55a874a2" in namespace "var-expansion-8391"
Aug 29 15:40:29.119: INFO: Wait up to 5m0s for pod "var-expansion-155388eb-56cb-4d62-8f69-3b8f55a874a2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:40:33.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8391" for this suite.

• [SLOW TEST:6.299 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":119,"skipped":2016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:40:33.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-33ed46f7-ebab-4a95-86af-58e51a5642de in namespace container-probe-1224
Aug 29 15:40:35.491: INFO: Started pod busybox-33ed46f7-ebab-4a95-86af-58e51a5642de in namespace container-probe-1224
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:40:35.499: INFO: Initial restart count of pod busybox-33ed46f7-ebab-4a95-86af-58e51a5642de is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:44:36.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1224" for this suite.

• [SLOW TEST:243.168 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":120,"skipped":2063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:44:36.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 29 15:44:37.092: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:44:39.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9269" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":121,"skipped":2092,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:44:39.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:44:39.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01" in namespace "projected-3419" to be "Succeeded or Failed"
Aug 29 15:44:39.220: INFO: Pod "downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01": Phase="Pending", Reason="", readiness=false. Elapsed: 14.098586ms
Aug 29 15:44:41.230: INFO: Pod "downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01": Phase="Running", Reason="", readiness=false. Elapsed: 2.0244429s
Aug 29 15:44:43.239: INFO: Pod "downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033479824s
STEP: Saw pod success
Aug 29 15:44:43.239: INFO: Pod "downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01" satisfied condition "Succeeded or Failed"
Aug 29 15:44:43.246: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01 container client-container: <nil>
STEP: delete the pod
Aug 29 15:44:43.286: INFO: Waiting for pod downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01 to disappear
Aug 29 15:44:43.292: INFO: Pod downwardapi-volume-c2a17ff3-b265-4b16-933e-5ff2b502ef01 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:44:43.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3419" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2113,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:44:43.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 29 15:44:43.414: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:45:43.494: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Aug 29 15:45:43.551: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 29 15:45:43.577: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 29 15:45:43.618: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 29 15:45:43.634: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 29 15:45:43.690: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 29 15:45:43.700: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:45:57.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7780" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.743 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":123,"skipped":2128,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:45:58.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:45:58.140: INFO: Endpoints addresses: [192.168.30.10] , ports: [6443]
Aug 29 15:45:58.141: INFO: EndpointSlices addresses: [192.168.30.10] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:45:58.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-910" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":124,"skipped":2143,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:45:58.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-394edd0d-bd6e-4844-973f-c15ae46c688f
STEP: Creating a pod to test consume configMaps
Aug 29 15:45:58.311: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74" in namespace "projected-6152" to be "Succeeded or Failed"
Aug 29 15:45:58.324: INFO: Pod "pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74": Phase="Pending", Reason="", readiness=false. Elapsed: 12.365628ms
Aug 29 15:46:00.564: INFO: Pod "pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252198172s
Aug 29 15:46:02.574: INFO: Pod "pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.262759348s
STEP: Saw pod success
Aug 29 15:46:02.574: INFO: Pod "pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74" satisfied condition "Succeeded or Failed"
Aug 29 15:46:02.584: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:46:02.626: INFO: Waiting for pod pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74 to disappear
Aug 29 15:46:02.633: INFO: Pod pod-projected-configmaps-7c72b80c-a245-4252-b686-ee4b2781ed74 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:02.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6152" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:02.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
Aug 29 15:46:02.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1224 api-versions'
Aug 29 15:46:02.845: INFO: stderr: ""
Aug 29 15:46:02.845: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps.kubermatic.k8c.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1224" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":126,"skipped":2264,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:02.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-sfxp
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:46:02.970: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sfxp" in namespace "subpath-255" to be "Succeeded or Failed"
Aug 29 15:46:02.982: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Pending", Reason="", readiness=false. Elapsed: 11.521712ms
Aug 29 15:46:04.992: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021537938s
Aug 29 15:46:07.001: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 4.030602201s
Aug 29 15:46:09.014: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 6.043513136s
Aug 29 15:46:11.025: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 8.054957213s
Aug 29 15:46:13.034: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 10.064074182s
Aug 29 15:46:15.045: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 12.075074816s
Aug 29 15:46:17.055: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 14.084936851s
Aug 29 15:46:19.067: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 16.096900523s
Aug 29 15:46:21.077: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 18.106720882s
Aug 29 15:46:23.087: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 20.11723783s
Aug 29 15:46:25.099: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=true. Elapsed: 22.128826137s
Aug 29 15:46:27.113: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Running", Reason="", readiness=false. Elapsed: 24.143186188s
Aug 29 15:46:29.127: INFO: Pod "pod-subpath-test-configmap-sfxp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.157038278s
STEP: Saw pod success
Aug 29 15:46:29.127: INFO: Pod "pod-subpath-test-configmap-sfxp" satisfied condition "Succeeded or Failed"
Aug 29 15:46:29.135: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-subpath-test-configmap-sfxp container test-container-subpath-configmap-sfxp: <nil>
STEP: delete the pod
Aug 29 15:46:29.173: INFO: Waiting for pod pod-subpath-test-configmap-sfxp to disappear
Aug 29 15:46:29.182: INFO: Pod pod-subpath-test-configmap-sfxp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sfxp
Aug 29 15:46:29.183: INFO: Deleting pod "pod-subpath-test-configmap-sfxp" in namespace "subpath-255"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:29.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-255" for this suite.

• [SLOW TEST:26.334 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":127,"skipped":2279,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:29.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:29.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2017" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":128,"skipped":2279,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:29.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-1109
STEP: creating replication controller nodeport-test in namespace services-1109
I0829 15:46:29.428577      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1109, replica count: 2
Aug 29 15:46:32.480: INFO: Creating new exec pod
I0829 15:46:32.480578      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:46:36.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 29 15:46:36.492: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 29 15:46:36.492: INFO: stdout: ""
Aug 29 15:46:37.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 29 15:46:37.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 29 15:46:37.732: INFO: stdout: ""
Aug 29 15:46:38.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 29 15:46:38.771: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echoConnection to nodeport-test 80 port [tcp/http] succeeded!\n hostName\n"
Aug 29 15:46:38.771: INFO: stdout: "nodeport-test-lcmw9"
Aug 29 15:46:38.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.24.229 80'
Aug 29 15:46:39.050: INFO: stderr: "+ nc -v -t -w 2 10.240.24.229 80\n+ echo hostName\nConnection to 10.240.24.229 80 port [tcp/http] succeeded!\n"
Aug 29 15:46:39.050: INFO: stdout: "nodeport-test-rd4tb"
Aug 29 15:46:39.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 32537'
Aug 29 15:46:39.260: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 172.31.23.90 32537\nConnection to 172.31.23.90 32537 port [tcp/*] succeeded!\n"
Aug 29 15:46:39.260: INFO: stdout: ""
Aug 29 15:46:40.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 32537'
Aug 29 15:46:40.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.23.90 32537\nConnection to 172.31.23.90 32537 port [tcp/*] succeeded!\n"
Aug 29 15:46:40.495: INFO: stdout: "nodeport-test-rd4tb"
Aug 29 15:46:40.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.14 32537'
Aug 29 15:46:40.790: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.14 32537\nConnection to 172.31.16.14 32537 port [tcp/*] succeeded!\n"
Aug 29 15:46:40.790: INFO: stdout: ""
Aug 29 15:46:41.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-1109 exec execpodpplsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.14 32537'
Aug 29 15:46:42.050: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 172.31.16.14 32537\nConnection to 172.31.16.14 32537 port [tcp/*] succeeded!\n"
Aug 29 15:46:42.050: INFO: stdout: "nodeport-test-rd4tb"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:42.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1109" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:12.737 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":129,"skipped":2291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:42.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:42.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7440" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":130,"skipped":2323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:42.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:42.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4178" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":131,"skipped":2350,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:42.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:46:42.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-413 version'
Aug 29 15:46:42.829: INFO: stderr: ""
Aug 29 15:46:42.829: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.9\", GitCommit:\"c1de2d70269039fe55efb98e737d9a29f9155246\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:26:51Z\", GoVersion:\"go1.17.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.9\", GitCommit:\"c1de2d70269039fe55efb98e737d9a29f9155246\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:19:57Z\", GoVersion:\"go1.17.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:46:42.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-413" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":132,"skipped":2356,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:46:42.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4300
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-4300
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4300
Aug 29 15:46:42.953: INFO: Found 0 stateful pods, waiting for 1
Aug 29 15:46:52.963: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 29 15:46:52.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 15:46:53.433: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 15:46:53.433: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 15:46:53.433: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 15:46:53.441: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 29 15:47:03.456: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 15:47:03.456: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:47:04.102: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Aug 29 15:47:04.102: INFO: ss-0  ip-172-31-23-90.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:42 +0000 UTC  }]
Aug 29 15:47:04.102: INFO: 
Aug 29 15:47:04.102: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 29 15:47:05.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991258109s
Aug 29 15:47:06.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979202529s
Aug 29 15:47:07.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.954621529s
Aug 29 15:47:08.159: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944914601s
Aug 29 15:47:09.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.93499756s
Aug 29 15:47:10.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.92185375s
Aug 29 15:47:11.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.910017396s
Aug 29 15:47:12.220: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.88859708s
Aug 29 15:47:13.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 874.71047ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4300
Aug 29 15:47:14.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 15:47:14.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 15:47:14.927: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 15:47:14.927: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 15:47:14.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 15:47:15.197: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 29 15:47:15.197: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 15:47:15.197: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 15:47:15.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 15:47:15.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 29 15:47:15.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 15:47:15.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 15:47:15.442: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:47:15.442: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:47:15.442: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 29 15:47:15.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 15:47:15.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 15:47:15.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 15:47:15.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 15:47:15.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 15:47:15.941: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 15:47:15.941: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 15:47:15.941: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 15:47:15.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-4300 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 15:47:16.191: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 15:47:16.191: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 15:47:16.191: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 15:47:16.191: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:47:16.198: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 29 15:47:26.224: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 15:47:26.224: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 15:47:26.224: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 15:47:26.248: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 15:47:26.248: INFO: ss-0  ip-172-31-23-90.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:42 +0000 UTC  }]
Aug 29 15:47:26.248: INFO: ss-1  ip-172-31-20-142.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  }]
Aug 29 15:47:26.248: INFO: ss-2  ip-172-31-20-142.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  }]
Aug 29 15:47:26.248: INFO: 
Aug 29 15:47:26.248: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 29 15:47:27.259: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 15:47:27.259: INFO: ss-0  ip-172-31-23-90.eu-central-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:46:42 +0000 UTC  }]
Aug 29 15:47:27.260: INFO: ss-1  ip-172-31-20-142.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  }]
Aug 29 15:47:27.260: INFO: ss-2  ip-172-31-20-142.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:47:04 +0000 UTC  }]
Aug 29 15:47:27.260: INFO: 
Aug 29 15:47:27.260: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 29 15:47:28.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.97968816s
Aug 29 15:47:29.510: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.968413533s
Aug 29 15:47:30.518: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.728839451s
Aug 29 15:47:31.526: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.721209118s
Aug 29 15:47:32.540: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.713715964s
Aug 29 15:47:33.549: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.698677006s
Aug 29 15:47:34.560: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.689693198s
Aug 29 15:47:35.574: INFO: Verifying statefulset ss doesn't scale past 0 for another 678.787646ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4300
Aug 29 15:47:36.584: INFO: Scaling statefulset ss to 0
Aug 29 15:47:36.899: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 15:47:36.919: INFO: Deleting all statefulset in ns statefulset-4300
Aug 29 15:47:36.925: INFO: Scaling statefulset ss to 0
Aug 29 15:47:36.955: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:47:36.965: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:47:37.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4300" for this suite.

• [SLOW TEST:54.907 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":133,"skipped":2366,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:47:37.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-2e34332f-d77f-4b91-b574-174601f4700a
STEP: Creating a pod to test consume secrets
Aug 29 15:47:37.830: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835" in namespace "projected-9346" to be "Succeeded or Failed"
Aug 29 15:47:37.841: INFO: Pod "pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835": Phase="Pending", Reason="", readiness=false. Elapsed: 10.541841ms
Aug 29 15:47:39.854: INFO: Pod "pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835": Phase="Running", Reason="", readiness=false. Elapsed: 2.024235946s
Aug 29 15:47:41.864: INFO: Pod "pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033843576s
STEP: Saw pod success
Aug 29 15:47:41.864: INFO: Pod "pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835" satisfied condition "Succeeded or Failed"
Aug 29 15:47:41.870: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:47:41.898: INFO: Waiting for pod pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835 to disappear
Aug 29 15:47:41.904: INFO: Pod pod-projected-secrets-1bcb42c6-ccf0-486d-9fda-d2d031bfd835 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:47:41.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9346" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2368,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:47:41.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Aug 29 15:47:41.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3547 create -f -'
Aug 29 15:47:42.399: INFO: stderr: ""
Aug 29 15:47:42.399: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 29 15:47:43.412: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:47:43.412: INFO: Found 0 / 1
Aug 29 15:47:44.407: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:47:44.407: INFO: Found 0 / 1
Aug 29 15:47:45.409: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:47:45.409: INFO: Found 1 / 1
Aug 29 15:47:45.409: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 29 15:47:45.415: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:47:45.415: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 15:47:45.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3547 patch pod agnhost-primary-j9gsz -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 29 15:47:45.534: INFO: stderr: ""
Aug 29 15:47:45.534: INFO: stdout: "pod/agnhost-primary-j9gsz patched\n"
STEP: checking annotations
Aug 29 15:47:45.540: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:47:45.540: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:47:45.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3547" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":135,"skipped":2386,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:47:45.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 29 15:47:45.640: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19070 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:47:45.641: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19070 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 29 15:47:45.658: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19071 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:47:45.659: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19071 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 29 15:47:45.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19072 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:47:45.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19072 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 29 15:47:45.692: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19073 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:47:45.692: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2762  b88f1ced-839d-4fd0-bfcd-485bb700c702 19073 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 29 15:47:45.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2762  12cb8138-b3fb-48fa-b8b5-01b7a4fd542e 19074 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:47:45.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2762  12cb8138-b3fb-48fa-b8b5-01b7a4fd542e 19074 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 29 15:47:55.725: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2762  12cb8138-b3fb-48fa-b8b5-01b7a4fd542e 19137 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:47:55.725: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2762  12cb8138-b3fb-48fa-b8b5-01b7a4fd542e 19137 0 2022-08-29 15:47:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:48:05.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2762" for this suite.

• [SLOW TEST:20.189 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":136,"skipped":2403,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:48:05.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-38cc2eb3-01e0-461f-bb34-66515eb78754
STEP: Creating a pod to test consume configMaps
Aug 29 15:48:05.946: INFO: Waiting up to 5m0s for pod "pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c" in namespace "configmap-1281" to be "Succeeded or Failed"
Aug 29 15:48:05.960: INFO: Pod "pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.509883ms
Aug 29 15:48:07.970: INFO: Pod "pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024551898s
Aug 29 15:48:09.986: INFO: Pod "pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04072811s
STEP: Saw pod success
Aug 29 15:48:09.987: INFO: Pod "pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c" satisfied condition "Succeeded or Failed"
Aug 29 15:48:09.993: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:48:10.027: INFO: Waiting for pod pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c to disappear
Aug 29 15:48:10.034: INFO: Pod pod-configmaps-d60c3318-042d-4033-a8fa-d660a2a6b72c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:48:10.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1281" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2416,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:48:10.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:48:27.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8795" for this suite.

• [SLOW TEST:17.236 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":138,"skipped":2419,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:48:27.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 29 15:48:28.125: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0829 15:48:28.125518      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:48:28.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4466" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":139,"skipped":2421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:48:28.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Aug 29 15:48:28.858: INFO: The status of Pod labelsupdate4c8de1c0-b5e5-46cf-97de-79e1a2b29e28 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:48:30.867: INFO: The status of Pod labelsupdate4c8de1c0-b5e5-46cf-97de-79e1a2b29e28 is Running (Ready = true)
Aug 29 15:48:31.417: INFO: Successfully updated pod "labelsupdate4c8de1c0-b5e5-46cf-97de-79e1a2b29e28"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:48:33.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8781" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2508,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:48:33.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6743
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6743
I0829 15:48:34.208120      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6743, replica count: 2
Aug 29 15:48:37.258: INFO: Creating new exec pod
I0829 15:48:37.258668      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:48:40.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:48:40.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:40.620: INFO: stdout: ""
Aug 29 15:48:41.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:48:41.870: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:41.870: INFO: stdout: ""
Aug 29 15:48:42.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:48:42.839: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:42.839: INFO: stdout: ""
Aug 29 15:48:43.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:48:43.875: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:43.875: INFO: stdout: ""
Aug 29 15:48:44.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:48:44.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:44.864: INFO: stdout: ""
Aug 29 15:48:45.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:48:45.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:45.862: INFO: stdout: "externalname-service-wb9s4"
Aug 29 15:48:45.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.50 80'
Aug 29 15:48:46.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.50 80\nConnection to 10.240.16.50 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:46.103: INFO: stdout: ""
Aug 29 15:48:47.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.50 80'
Aug 29 15:48:47.348: INFO: stderr: "+ + nc -v -t -w 2 10.240.16.50 80\necho hostName\nConnection to 10.240.16.50 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:47.348: INFO: stdout: ""
Aug 29 15:48:48.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.50 80'
Aug 29 15:48:48.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.50 80\nConnection to 10.240.16.50 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:48.337: INFO: stdout: ""
Aug 29 15:48:49.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.50 80'
Aug 29 15:48:49.334: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.50 80\nConnection to 10.240.16.50 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:49.334: INFO: stdout: ""
Aug 29 15:48:50.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.50 80'
Aug 29 15:48:50.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.50 80\nConnection to 10.240.16.50 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:50.865: INFO: stdout: ""
Aug 29 15:48:51.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6743 exec execpodc2kv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.50 80'
Aug 29 15:48:51.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.50 80\nConnection to 10.240.16.50 80 port [tcp/http] succeeded!\n"
Aug 29 15:48:51.429: INFO: stdout: "externalname-service-wb9s4"
Aug 29 15:48:51.429: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:48:51.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6743" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:18.000 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":141,"skipped":2525,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:48:51.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Aug 29 15:48:51.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:50:17.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5473" for this suite.

• [SLOW TEST:85.655 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":142,"skipped":2537,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:50:17.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Aug 29 15:50:17.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 create -f -'
Aug 29 15:50:18.203: INFO: stderr: ""
Aug 29 15:50:18.203: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 15:50:18.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 15:50:18.317: INFO: stderr: ""
Aug 29 15:50:18.317: INFO: stdout: "update-demo-nautilus-7xgzr update-demo-nautilus-bnl7k "
Aug 29 15:50:18.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-7xgzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:50:18.403: INFO: stderr: ""
Aug 29 15:50:18.403: INFO: stdout: ""
Aug 29 15:50:18.403: INFO: update-demo-nautilus-7xgzr is created but not running
Aug 29 15:50:23.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 15:50:23.526: INFO: stderr: ""
Aug 29 15:50:23.526: INFO: stdout: "update-demo-nautilus-7xgzr update-demo-nautilus-bnl7k "
Aug 29 15:50:23.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-7xgzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:50:23.642: INFO: stderr: ""
Aug 29 15:50:23.642: INFO: stdout: "true"
Aug 29 15:50:23.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-7xgzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 15:50:23.744: INFO: stderr: ""
Aug 29 15:50:23.744: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 15:50:23.744: INFO: validating pod update-demo-nautilus-7xgzr
Aug 29 15:50:23.763: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 15:50:23.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 15:50:23.763: INFO: update-demo-nautilus-7xgzr is verified up and running
Aug 29 15:50:23.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-bnl7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:50:23.852: INFO: stderr: ""
Aug 29 15:50:23.852: INFO: stdout: ""
Aug 29 15:50:23.852: INFO: update-demo-nautilus-bnl7k is created but not running
Aug 29 15:50:28.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 15:50:28.954: INFO: stderr: ""
Aug 29 15:50:28.954: INFO: stdout: "update-demo-nautilus-7xgzr update-demo-nautilus-bnl7k "
Aug 29 15:50:28.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-7xgzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:50:29.062: INFO: stderr: ""
Aug 29 15:50:29.062: INFO: stdout: "true"
Aug 29 15:50:29.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-7xgzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 15:50:29.150: INFO: stderr: ""
Aug 29 15:50:29.150: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 15:50:29.150: INFO: validating pod update-demo-nautilus-7xgzr
Aug 29 15:50:29.161: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 15:50:29.161: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 15:50:29.161: INFO: update-demo-nautilus-7xgzr is verified up and running
Aug 29 15:50:29.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-bnl7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:50:29.247: INFO: stderr: ""
Aug 29 15:50:29.247: INFO: stdout: "true"
Aug 29 15:50:29.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods update-demo-nautilus-bnl7k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 15:50:29.338: INFO: stderr: ""
Aug 29 15:50:29.338: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 15:50:29.338: INFO: validating pod update-demo-nautilus-bnl7k
Aug 29 15:50:29.357: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 15:50:29.357: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 15:50:29.357: INFO: update-demo-nautilus-bnl7k is verified up and running
STEP: using delete to clean up resources
Aug 29 15:50:29.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 delete --grace-period=0 --force -f -'
Aug 29 15:50:29.465: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 15:50:29.465: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 29 15:50:29.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get rc,svc -l name=update-demo --no-headers'
Aug 29 15:50:29.835: INFO: stderr: "No resources found in kubectl-1000 namespace.\n"
Aug 29 15:50:29.835: INFO: stdout: ""
Aug 29 15:50:29.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-1000 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 15:50:29.935: INFO: stderr: ""
Aug 29 15:50:29.935: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:50:29.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1000" for this suite.

• [SLOW TEST:13.306 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":143,"skipped":2539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:50:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:50:30.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0" in namespace "projected-634" to be "Succeeded or Failed"
Aug 29 15:50:30.540: INFO: Pod "downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.827747ms
Aug 29 15:50:32.553: INFO: Pod "downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025079392s
Aug 29 15:50:34.567: INFO: Pod "downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039174743s
STEP: Saw pod success
Aug 29 15:50:34.568: INFO: Pod "downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0" satisfied condition "Succeeded or Failed"
Aug 29 15:50:34.575: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0 container client-container: <nil>
STEP: delete the pod
Aug 29 15:50:34.638: INFO: Waiting for pod downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0 to disappear
Aug 29 15:50:34.647: INFO: Pod downwardapi-volume-c34abb44-af8b-4681-848d-4b46a53879e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:50:34.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-634" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2591,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:50:34.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:50:34.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2" in namespace "projected-7671" to be "Succeeded or Failed"
Aug 29 15:50:34.740: INFO: Pod "downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.862587ms
Aug 29 15:50:36.758: INFO: Pod "downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024341741s
Aug 29 15:50:38.786: INFO: Pod "downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051859989s
STEP: Saw pod success
Aug 29 15:50:38.786: INFO: Pod "downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2" satisfied condition "Succeeded or Failed"
Aug 29 15:50:38.795: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2 container client-container: <nil>
STEP: delete the pod
Aug 29 15:50:38.865: INFO: Waiting for pod downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2 to disappear
Aug 29 15:50:38.872: INFO: Pod downwardapi-volume-26ee6263-e813-4ddf-b1cc-14d6480959d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:50:38.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7671" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:50:38.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 15:50:42.052: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:50:42.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2528" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:50:42.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
Aug 29 15:50:42.517: INFO: Waiting up to 5m0s for pod "var-expansion-e471066f-dfec-4d26-854f-e5d630989a73" in namespace "var-expansion-5898" to be "Succeeded or Failed"
Aug 29 15:50:42.530: INFO: Pod "var-expansion-e471066f-dfec-4d26-854f-e5d630989a73": Phase="Pending", Reason="", readiness=false. Elapsed: 12.696177ms
Aug 29 15:50:44.540: INFO: Pod "var-expansion-e471066f-dfec-4d26-854f-e5d630989a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022469829s
Aug 29 15:50:46.553: INFO: Pod "var-expansion-e471066f-dfec-4d26-854f-e5d630989a73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035528904s
STEP: Saw pod success
Aug 29 15:50:46.553: INFO: Pod "var-expansion-e471066f-dfec-4d26-854f-e5d630989a73" satisfied condition "Succeeded or Failed"
Aug 29 15:50:46.568: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod var-expansion-e471066f-dfec-4d26-854f-e5d630989a73 container dapi-container: <nil>
STEP: delete the pod
Aug 29 15:50:46.628: INFO: Waiting for pod var-expansion-e471066f-dfec-4d26-854f-e5d630989a73 to disappear
Aug 29 15:50:46.640: INFO: Pod var-expansion-e471066f-dfec-4d26-854f-e5d630989a73 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:50:46.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5898" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:50:46.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Aug 29 15:50:46.797: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:51:46.888: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:51:46.896: INFO: Starting informer...
STEP: Starting pods...
Aug 29 15:51:47.144: INFO: Pod1 is running on ip-172-31-23-90.eu-central-1.compute.internal. Tainting Node
Aug 29 15:51:49.398: INFO: Pod2 is running on ip-172-31-23-90.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 29 15:51:55.447: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 29 15:52:15.486: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:52:15.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-954" for this suite.

• [SLOW TEST:88.839 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":148,"skipped":2743,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:52:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 15:52:15.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:52:15.725: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:16.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:52:16.748: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:17.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:52:17.746: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 29 15:52:18.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:52:18.271: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:19.300: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:52:19.300: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:20.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:52:20.299: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:21.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:52:21.309: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:22.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:52:22.289: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:52:23.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:52:23.301: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7979, will wait for the garbage collector to delete the pods
Aug 29 15:52:23.405: INFO: Deleting DaemonSet.extensions daemon-set took: 28.906879ms
Aug 29 15:52:23.506: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.30313ms
Aug 29 15:52:25.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:52:25.415: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 15:52:25.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20551"},"items":null}

Aug 29 15:52:25.428: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20551"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:52:25.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7979" for this suite.

• [SLOW TEST:9.928 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":149,"skipped":2759,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:52:25.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4043
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-4043
Aug 29 15:52:25.591: INFO: Found 0 stateful pods, waiting for 1
Aug 29 15:52:35.605: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Aug 29 15:52:35.647: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Aug 29 15:52:35.673: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Aug 29 15:52:35.677: INFO: Observed &StatefulSet event: ADDED
Aug 29 15:52:35.677: INFO: Found Statefulset ss in namespace statefulset-4043 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 15:52:35.677: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Aug 29 15:52:35.677: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 15:52:35.689: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Aug 29 15:52:35.694: INFO: Observed &StatefulSet event: ADDED
Aug 29 15:52:35.694: INFO: Observed Statefulset ss in namespace statefulset-4043 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 15:52:35.694: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 15:52:35.695: INFO: Deleting all statefulset in ns statefulset-4043
Aug 29 15:52:35.701: INFO: Scaling statefulset ss to 0
Aug 29 15:52:45.757: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:52:45.764: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:52:45.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4043" for this suite.

• [SLOW TEST:20.323 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":150,"skipped":2759,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:52:45.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:52:45.887: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b" in namespace "security-context-test-5296" to be "Succeeded or Failed"
Aug 29 15:52:45.899: INFO: Pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.734066ms
Aug 29 15:52:47.922: INFO: Pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035229583s
Aug 29 15:52:49.939: INFO: Pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b": Phase="Running", Reason="", readiness=true. Elapsed: 4.052116261s
Aug 29 15:52:51.956: INFO: Pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b": Phase="Running", Reason="", readiness=false. Elapsed: 6.069071558s
Aug 29 15:52:53.970: INFO: Pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.083094362s
Aug 29 15:52:53.970: INFO: Pod "alpine-nnp-false-815f464a-d654-4d98-80f7-1227c26d543b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:52:53.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5296" for this suite.

• [SLOW TEST:8.218 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2766,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:52:54.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 29 15:53:04.192: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:53:04.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0829 15:53:04.192368      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-6366" for this suite.

• [SLOW TEST:10.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":152,"skipped":2784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:53:04.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-45238b29-f80d-43e2-95b7-bcc5a19e5597
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:53:04.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3563" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":153,"skipped":2852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:53:04.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
Aug 29 15:53:04.437: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:53:06.448: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:53:07.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9910" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":154,"skipped":2898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:53:07.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:53:07.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8895" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":155,"skipped":2931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:53:07.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 29 15:53:07.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:53:19.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:54:19.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4177" for this suite.

• [SLOW TEST:71.560 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":156,"skipped":2970,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:54:19.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-781a97a1-b441-4d47-b85f-64d50e91ea77
STEP: Creating a pod to test consume configMaps
Aug 29 15:54:19.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a" in namespace "projected-2051" to be "Succeeded or Failed"
Aug 29 15:54:19.357: INFO: Pod "pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.527415ms
Aug 29 15:54:21.382: INFO: Pod "pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034393112s
Aug 29 15:54:23.395: INFO: Pod "pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047262048s
STEP: Saw pod success
Aug 29 15:54:23.395: INFO: Pod "pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a" satisfied condition "Succeeded or Failed"
Aug 29 15:54:23.403: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 29 15:54:23.437: INFO: Waiting for pod pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a to disappear
Aug 29 15:54:23.446: INFO: Pod pod-projected-configmaps-2c31a614-9a77-4e14-a0e9-1729c02e537a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:54:23.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2051" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2983,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:54:23.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-7681
Aug 29 15:54:23.570: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:54:25.583: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 29 15:54:25.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-7681 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 29 15:54:25.853: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 29 15:54:25.853: INFO: stdout: "ipvs"
Aug 29 15:54:25.853: INFO: proxyMode: ipvs
Aug 29 15:54:25.881: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 29 15:54:25.892: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7681
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7681
I0829 15:54:25.923902      18 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7681, replica count: 3
I0829 15:54:28.976153      18 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:54:29.007: INFO: Creating new exec pod
Aug 29 15:54:34.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-7681 exec execpod-affinitycft9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Aug 29 15:54:34.442: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 29 15:54:34.442: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:54:34.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-7681 exec execpod-affinitycft9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.28.141 80'
Aug 29 15:54:34.694: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.28.141 80\nConnection to 10.240.28.141 80 port [tcp/http] succeeded!\n"
Aug 29 15:54:34.694: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:54:34.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-7681 exec execpod-affinitycft9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.28.141:80/ ; done'
Aug 29 15:54:35.303: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n"
Aug 29 15:54:35.303: INFO: stdout: "\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4\naffinity-clusterip-timeout-gr8v4"
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Received response from host: affinity-clusterip-timeout-gr8v4
Aug 29 15:54:35.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-7681 exec execpod-affinitycft9d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.28.141:80/'
Aug 29 15:54:35.517: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n"
Aug 29 15:54:35.517: INFO: stdout: "affinity-clusterip-timeout-gr8v4"
Aug 29 15:56:45.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-7681 exec execpod-affinitycft9d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.28.141:80/'
Aug 29 15:56:45.883: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.28.141:80/\n"
Aug 29 15:56:45.883: INFO: stdout: "affinity-clusterip-timeout-pf57s"
Aug 29 15:56:45.883: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7681, will wait for the garbage collector to delete the pods
Aug 29 15:56:46.002: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 13.056204ms
Aug 29 15:56:46.104: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.864591ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:56:48.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7681" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:144.797 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":158,"skipped":2989,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:56:48.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-nd2t
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:56:48.391: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nd2t" in namespace "subpath-1454" to be "Succeeded or Failed"
Aug 29 15:56:48.406: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Pending", Reason="", readiness=false. Elapsed: 14.416518ms
Aug 29 15:56:50.417: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.025679554s
Aug 29 15:56:52.432: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.040537624s
Aug 29 15:56:54.452: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.060647965s
Aug 29 15:56:56.464: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.072553966s
Aug 29 15:56:58.473: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.081439595s
Aug 29 15:57:00.485: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.093815804s
Aug 29 15:57:02.501: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.109803155s
Aug 29 15:57:04.513: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.121893554s
Aug 29 15:57:06.543: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.151207946s
Aug 29 15:57:08.556: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.164790815s
Aug 29 15:57:10.567: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Running", Reason="", readiness=false. Elapsed: 22.175989418s
Aug 29 15:57:12.578: INFO: Pod "pod-subpath-test-projected-nd2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.18699184s
STEP: Saw pod success
Aug 29 15:57:12.578: INFO: Pod "pod-subpath-test-projected-nd2t" satisfied condition "Succeeded or Failed"
Aug 29 15:57:12.585: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-subpath-test-projected-nd2t container test-container-subpath-projected-nd2t: <nil>
STEP: delete the pod
Aug 29 15:57:12.631: INFO: Waiting for pod pod-subpath-test-projected-nd2t to disappear
Aug 29 15:57:12.639: INFO: Pod pod-subpath-test-projected-nd2t no longer exists
STEP: Deleting pod pod-subpath-test-projected-nd2t
Aug 29 15:57:12.639: INFO: Deleting pod "pod-subpath-test-projected-nd2t" in namespace "subpath-1454"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:12.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1454" for this suite.

• [SLOW TEST:24.396 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":159,"skipped":2996,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Aug 29 15:57:12.728: INFO: Waiting up to 5m0s for pod "downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f" in namespace "downward-api-6148" to be "Succeeded or Failed"
Aug 29 15:57:12.742: INFO: Pod "downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.308145ms
Aug 29 15:57:14.754: INFO: Pod "downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025435468s
Aug 29 15:57:16.766: INFO: Pod "downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037184034s
STEP: Saw pod success
Aug 29 15:57:16.766: INFO: Pod "downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f" satisfied condition "Succeeded or Failed"
Aug 29 15:57:16.772: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f container dapi-container: <nil>
STEP: delete the pod
Aug 29 15:57:16.821: INFO: Waiting for pod downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f to disappear
Aug 29 15:57:16.839: INFO: Pod downward-api-30b553f7-1d65-4112-90ab-d8c8c399f62f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:16.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6148" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":3006,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:16.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-7690
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 15:57:16.910: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 15:57:16.989: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:57:18.999: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:57:21.003: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:23.011: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:25.001: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:27.010: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:28.998: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:31.008: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:33.001: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:35.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:37.002: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:57:39.000: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 15:57:39.015: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 15:57:39.028: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 15:57:41.101: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 15:57:41.101: INFO: Going to poll 172.25.0.60 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 29 15:57:41.107: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.60 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:57:41.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:57:41.108: INFO: ExecWithOptions: Clientset creation
Aug 29 15:57:41.108: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.60+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:57:42.246: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 29 15:57:42.246: INFO: Going to poll 172.25.1.131 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 29 15:57:42.257: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.131 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:57:42.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:57:42.258: INFO: ExecWithOptions: Clientset creation
Aug 29 15:57:42.259: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.131+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:57:43.383: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 29 15:57:43.383: INFO: Going to poll 172.25.2.128 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 29 15:57:43.398: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.128 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:57:43.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 15:57:43.399: INFO: ExecWithOptions: Clientset creation
Aug 29 15:57:43.399: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.128+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 15:57:44.528: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:44.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7690" for this suite.

• [SLOW TEST:27.689 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":3016,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:44.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 29 15:57:44.632: INFO: Waiting up to 5m0s for pod "pod-549cab2c-555b-4b36-b612-236fbf388f5c" in namespace "emptydir-4269" to be "Succeeded or Failed"
Aug 29 15:57:44.642: INFO: Pod "pod-549cab2c-555b-4b36-b612-236fbf388f5c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.14448ms
Aug 29 15:57:46.653: INFO: Pod "pod-549cab2c-555b-4b36-b612-236fbf388f5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021739622s
Aug 29 15:57:48.670: INFO: Pod "pod-549cab2c-555b-4b36-b612-236fbf388f5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038358242s
STEP: Saw pod success
Aug 29 15:57:48.670: INFO: Pod "pod-549cab2c-555b-4b36-b612-236fbf388f5c" satisfied condition "Succeeded or Failed"
Aug 29 15:57:48.700: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-549cab2c-555b-4b36-b612-236fbf388f5c container test-container: <nil>
STEP: delete the pod
Aug 29 15:57:48.750: INFO: Waiting for pod pod-549cab2c-555b-4b36-b612-236fbf388f5c to disappear
Aug 29 15:57:48.756: INFO: Pod pod-549cab2c-555b-4b36-b612-236fbf388f5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:48.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4269" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":3028,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:48.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:57:48.835: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-60638a58-5d1a-4559-bfa1-b2150ee58b41" in namespace "security-context-test-2294" to be "Succeeded or Failed"
Aug 29 15:57:48.845: INFO: Pod "busybox-readonly-false-60638a58-5d1a-4559-bfa1-b2150ee58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 9.523219ms
Aug 29 15:57:50.859: INFO: Pod "busybox-readonly-false-60638a58-5d1a-4559-bfa1-b2150ee58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023189029s
Aug 29 15:57:52.868: INFO: Pod "busybox-readonly-false-60638a58-5d1a-4559-bfa1-b2150ee58b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032914448s
Aug 29 15:57:52.868: INFO: Pod "busybox-readonly-false-60638a58-5d1a-4559-bfa1-b2150ee58b41" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:52.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2294" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3030,"failed":0}
SSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:52.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 15:57:53.006: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 15:57:53.036: INFO: waiting for watch events with expected annotations
Aug 29 15:57:53.036: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:53.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8909" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":164,"skipped":3033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:53.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:57:53.217: INFO: The status of Pod busybox-scheduling-9225bd49-5b19-417e-81c2-0b357f136e82 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:57:55.229: INFO: The status of Pod busybox-scheduling-9225bd49-5b19-417e-81c2-0b357f136e82 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:57:55.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2363" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":3092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:57:55.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 29 15:57:55.349: INFO: Waiting up to 5m0s for pod "pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81" in namespace "emptydir-231" to be "Succeeded or Failed"
Aug 29 15:57:55.364: INFO: Pod "pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81": Phase="Pending", Reason="", readiness=false. Elapsed: 14.673822ms
Aug 29 15:57:57.374: INFO: Pod "pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024090332s
Aug 29 15:57:59.385: INFO: Pod "pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035243962s
Aug 29 15:58:01.397: INFO: Pod "pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046867914s
STEP: Saw pod success
Aug 29 15:58:01.397: INFO: Pod "pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81" satisfied condition "Succeeded or Failed"
Aug 29 15:58:01.404: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81 container test-container: <nil>
STEP: delete the pod
Aug 29 15:58:01.440: INFO: Waiting for pod pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81 to disappear
Aug 29 15:58:01.447: INFO: Pod pod-6f10c629-bc7b-4cfd-a1ac-05272dce3c81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:58:01.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-231" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":3136,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:58:01.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-c47c
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:58:01.558: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c47c" in namespace "subpath-4212" to be "Succeeded or Failed"
Aug 29 15:58:01.570: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.693969ms
Aug 29 15:58:03.582: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02391379s
Aug 29 15:58:05.599: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 4.040681295s
Aug 29 15:58:07.610: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 6.052201906s
Aug 29 15:58:09.623: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 8.064698142s
Aug 29 15:58:11.642: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 10.084027767s
Aug 29 15:58:13.653: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 12.094606592s
Aug 29 15:58:15.672: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 14.114227078s
Aug 29 15:58:17.682: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 16.123382787s
Aug 29 15:58:19.698: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 18.139945141s
Aug 29 15:58:21.709: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=true. Elapsed: 20.150668648s
Aug 29 15:58:23.716: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Running", Reason="", readiness=false. Elapsed: 22.157821693s
Aug 29 15:58:25.728: INFO: Pod "pod-subpath-test-configmap-c47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.169379897s
STEP: Saw pod success
Aug 29 15:58:25.728: INFO: Pod "pod-subpath-test-configmap-c47c" satisfied condition "Succeeded or Failed"
Aug 29 15:58:25.734: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-subpath-test-configmap-c47c container test-container-subpath-configmap-c47c: <nil>
STEP: delete the pod
Aug 29 15:58:25.768: INFO: Waiting for pod pod-subpath-test-configmap-c47c to disappear
Aug 29 15:58:25.778: INFO: Pod pod-subpath-test-configmap-c47c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c47c
Aug 29 15:58:25.778: INFO: Deleting pod "pod-subpath-test-configmap-c47c" in namespace "subpath-4212"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:58:25.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4212" for this suite.

• [SLOW TEST:24.332 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":167,"skipped":3157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:58:25.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-w6svg in namespace proxy-4897
I0829 15:58:25.931488      18 runners.go:193] Created replication controller with name: proxy-service-w6svg, namespace: proxy-4897, replica count: 1
I0829 15:58:26.982656      18 runners.go:193] proxy-service-w6svg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 15:58:27.983942      18 runners.go:193] proxy-service-w6svg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:58:27.996: INFO: setup took 2.129279291s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 29 15:58:28.023: INFO: (0) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 25.627142ms)
Aug 29 15:58:28.024: INFO: (0) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 22.642132ms)
Aug 29 15:58:28.024: INFO: (0) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 26.213927ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 41.139533ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 40.784481ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 46.955495ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 42.963526ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 43.494831ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 40.790491ms)
Aug 29 15:58:28.043: INFO: (0) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 42.942817ms)
Aug 29 15:58:28.044: INFO: (0) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 47.428978ms)
Aug 29 15:58:28.044: INFO: (0) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 41.613667ms)
Aug 29 15:58:28.044: INFO: (0) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 42.07128ms)
Aug 29 15:58:28.047: INFO: (0) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 49.07491ms)
Aug 29 15:58:28.047: INFO: (0) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 46.342131ms)
Aug 29 15:58:28.047: INFO: (0) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 48.338085ms)
Aug 29 15:58:28.074: INFO: (1) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 22.003997ms)
Aug 29 15:58:28.074: INFO: (1) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 23.231856ms)
Aug 29 15:58:28.076: INFO: (1) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 24.327683ms)
Aug 29 15:58:28.076: INFO: (1) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 26.60974ms)
Aug 29 15:58:28.076: INFO: (1) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 25.449921ms)
Aug 29 15:58:28.079: INFO: (1) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 27.304644ms)
Aug 29 15:58:28.079: INFO: (1) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 28.689885ms)
Aug 29 15:58:28.085: INFO: (1) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 33.211597ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 33.409009ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 33.63399ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 33.586349ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 33.627531ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 35.551784ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 34.025993ms)
Aug 29 15:58:28.086: INFO: (1) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 35.682275ms)
Aug 29 15:58:28.090: INFO: (1) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 39.853345ms)
Aug 29 15:58:28.103: INFO: (2) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 12.58704ms)
Aug 29 15:58:28.103: INFO: (2) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 12.736571ms)
Aug 29 15:58:28.103: INFO: (2) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 10.137862ms)
Aug 29 15:58:28.105: INFO: (2) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 11.16573ms)
Aug 29 15:58:28.108: INFO: (2) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 16.435097ms)
Aug 29 15:58:28.108: INFO: (2) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 17.026961ms)
Aug 29 15:58:28.108: INFO: (2) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 16.275886ms)
Aug 29 15:58:28.111: INFO: (2) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 19.799352ms)
Aug 29 15:58:28.112: INFO: (2) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 20.91368ms)
Aug 29 15:58:28.114: INFO: (2) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 18.760434ms)
Aug 29 15:58:28.114: INFO: (2) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 18.701124ms)
Aug 29 15:58:28.115: INFO: (2) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 19.69207ms)
Aug 29 15:58:28.115: INFO: (2) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 24.106732ms)
Aug 29 15:58:28.121: INFO: (2) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 27.011792ms)
Aug 29 15:58:28.122: INFO: (2) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 27.902019ms)
Aug 29 15:58:28.122: INFO: (2) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 27.566167ms)
Aug 29 15:58:28.133: INFO: (3) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 10.413314ms)
Aug 29 15:58:28.137: INFO: (3) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 13.008163ms)
Aug 29 15:58:28.137: INFO: (3) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 12.880792ms)
Aug 29 15:58:28.137: INFO: (3) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 14.212102ms)
Aug 29 15:58:28.137: INFO: (3) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 12.62756ms)
Aug 29 15:58:28.138: INFO: (3) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 12.546209ms)
Aug 29 15:58:28.138: INFO: (3) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 12.73288ms)
Aug 29 15:58:28.139: INFO: (3) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 15.318779ms)
Aug 29 15:58:28.139: INFO: (3) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 13.895099ms)
Aug 29 15:58:28.140: INFO: (3) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 15.544411ms)
Aug 29 15:58:28.147: INFO: (3) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 22.034307ms)
Aug 29 15:58:28.148: INFO: (3) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 23.243925ms)
Aug 29 15:58:28.152: INFO: (3) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 27.088634ms)
Aug 29 15:58:28.155: INFO: (3) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 30.067065ms)
Aug 29 15:58:28.155: INFO: (3) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 30.008154ms)
Aug 29 15:58:28.156: INFO: (3) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 30.875131ms)
Aug 29 15:58:28.170: INFO: (4) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 13.411526ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 18.12962ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 18.953465ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 18.29877ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 18.357341ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 18.607302ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 18.684874ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 18.223151ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 18.517663ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 18.557153ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 18.477832ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 18.615393ms)
Aug 29 15:58:28.175: INFO: (4) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 18.356281ms)
Aug 29 15:58:28.202: INFO: (4) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 45.591605ms)
Aug 29 15:58:28.202: INFO: (4) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 45.521185ms)
Aug 29 15:58:28.202: INFO: (4) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 45.374694ms)
Aug 29 15:58:28.217: INFO: (5) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 13.506626ms)
Aug 29 15:58:28.217: INFO: (5) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 15.043767ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 15.260859ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 14.444863ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 12.712201ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 13.287654ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 13.668817ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 12.260748ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 15.160928ms)
Aug 29 15:58:28.218: INFO: (5) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 13.006183ms)
Aug 29 15:58:28.219: INFO: (5) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 14.605594ms)
Aug 29 15:58:28.229: INFO: (5) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 26.078106ms)
Aug 29 15:58:28.229: INFO: (5) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 23.143976ms)
Aug 29 15:58:28.229: INFO: (5) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 23.88779ms)
Aug 29 15:58:28.231: INFO: (5) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 28.365533ms)
Aug 29 15:58:28.232: INFO: (5) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 28.521463ms)
Aug 29 15:58:28.246: INFO: (6) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 13.766408ms)
Aug 29 15:58:28.247: INFO: (6) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 11.045899ms)
Aug 29 15:58:28.247: INFO: (6) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 13.849229ms)
Aug 29 15:58:28.247: INFO: (6) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 12.69133ms)
Aug 29 15:58:28.250: INFO: (6) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 15.855583ms)
Aug 29 15:58:28.250: INFO: (6) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 15.010857ms)
Aug 29 15:58:28.250: INFO: (6) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 13.95542ms)
Aug 29 15:58:28.253: INFO: (6) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 19.875962ms)
Aug 29 15:58:28.253: INFO: (6) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 16.404687ms)
Aug 29 15:58:28.275: INFO: (6) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 41.167264ms)
Aug 29 15:58:28.275: INFO: (6) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 39.031769ms)
Aug 29 15:58:28.275: INFO: (6) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 40.293368ms)
Aug 29 15:58:28.275: INFO: (6) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 38.528795ms)
Aug 29 15:58:28.277: INFO: (6) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 40.409278ms)
Aug 29 15:58:28.277: INFO: (6) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 41.041673ms)
Aug 29 15:58:28.277: INFO: (6) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 43.47155ms)
Aug 29 15:58:28.289: INFO: (7) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 10.408705ms)
Aug 29 15:58:28.289: INFO: (7) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 11.060869ms)
Aug 29 15:58:28.290: INFO: (7) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 12.368808ms)
Aug 29 15:58:28.290: INFO: (7) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 12.307818ms)
Aug 29 15:58:28.290: INFO: (7) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 11.880124ms)
Aug 29 15:58:28.297: INFO: (7) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 18.356262ms)
Aug 29 15:58:28.297: INFO: (7) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 18.466552ms)
Aug 29 15:58:28.297: INFO: (7) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 18.449292ms)
Aug 29 15:58:28.300: INFO: (7) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 21.239992ms)
Aug 29 15:58:28.343: INFO: (7) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 64.988324ms)
Aug 29 15:58:28.343: INFO: (7) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 64.749752ms)
Aug 29 15:58:28.344: INFO: (7) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 65.490268ms)
Aug 29 15:58:28.345: INFO: (7) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 68.087036ms)
Aug 29 15:58:28.354: INFO: (7) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 75.450249ms)
Aug 29 15:58:28.355: INFO: (7) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 76.344445ms)
Aug 29 15:58:28.358: INFO: (7) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 80.455874ms)
Aug 29 15:58:28.391: INFO: (8) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 30.739929ms)
Aug 29 15:58:28.392: INFO: (8) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 32.670223ms)
Aug 29 15:58:28.392: INFO: (8) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 33.528599ms)
Aug 29 15:58:28.392: INFO: (8) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 32.19688ms)
Aug 29 15:58:28.394: INFO: (8) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 34.545206ms)
Aug 29 15:58:28.397: INFO: (8) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 37.253596ms)
Aug 29 15:58:28.402: INFO: (8) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 42.472823ms)
Aug 29 15:58:28.402: INFO: (8) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 42.946767ms)
Aug 29 15:58:28.403: INFO: (8) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 43.069768ms)
Aug 29 15:58:28.403: INFO: (8) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 44.015245ms)
Aug 29 15:58:28.422: INFO: (8) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 62.147804ms)
Aug 29 15:58:28.422: INFO: (8) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 62.226954ms)
Aug 29 15:58:28.422: INFO: (8) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 63.12505ms)
Aug 29 15:58:28.422: INFO: (8) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 62.95174ms)
Aug 29 15:58:28.422: INFO: (8) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 62.384575ms)
Aug 29 15:58:28.424: INFO: (8) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 65.063884ms)
Aug 29 15:58:28.436: INFO: (9) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 10.902778ms)
Aug 29 15:58:28.456: INFO: (9) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 31.080722ms)
Aug 29 15:58:28.459: INFO: (9) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 33.464719ms)
Aug 29 15:58:28.460: INFO: (9) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 32.998916ms)
Aug 29 15:58:28.460: INFO: (9) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 33.099456ms)
Aug 29 15:58:28.461: INFO: (9) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 35.357863ms)
Aug 29 15:58:28.461: INFO: (9) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 35.661724ms)
Aug 29 15:58:28.462: INFO: (9) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 34.95223ms)
Aug 29 15:58:28.462: INFO: (9) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 35.176912ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 41.320306ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 41.019393ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 41.899589ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 41.96353ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 40.775311ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 40.748032ms)
Aug 29 15:58:28.467: INFO: (9) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 41.754888ms)
Aug 29 15:58:28.482: INFO: (10) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 13.906949ms)
Aug 29 15:58:28.482: INFO: (10) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 10.396234ms)
Aug 29 15:58:28.482: INFO: (10) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 14.144981ms)
Aug 29 15:58:28.488: INFO: (10) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 15.266399ms)
Aug 29 15:58:28.488: INFO: (10) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 17.935098ms)
Aug 29 15:58:28.489: INFO: (10) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 20.447096ms)
Aug 29 15:58:28.489: INFO: (10) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 18.15249ms)
Aug 29 15:58:28.489: INFO: (10) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 18.039269ms)
Aug 29 15:58:28.493: INFO: (10) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 25.202471ms)
Aug 29 15:58:28.493: INFO: (10) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 23.489918ms)
Aug 29 15:58:28.494: INFO: (10) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 22.39058ms)
Aug 29 15:58:28.494: INFO: (10) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 21.944317ms)
Aug 29 15:58:28.494: INFO: (10) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 19.197497ms)
Aug 29 15:58:28.495: INFO: (10) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 22.467761ms)
Aug 29 15:58:28.495: INFO: (10) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 23.882491ms)
Aug 29 15:58:28.498: INFO: (10) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 28.518214ms)
Aug 29 15:58:28.525: INFO: (11) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 26.990623ms)
Aug 29 15:58:28.545: INFO: (11) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 46.812735ms)
Aug 29 15:58:28.545: INFO: (11) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 46.438822ms)
Aug 29 15:58:28.545: INFO: (11) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 46.302911ms)
Aug 29 15:58:28.546: INFO: (11) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 47.034096ms)
Aug 29 15:58:28.547: INFO: (11) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 46.842045ms)
Aug 29 15:58:28.547: INFO: (11) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 47.168997ms)
Aug 29 15:58:28.547: INFO: (11) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 48.219714ms)
Aug 29 15:58:28.547: INFO: (11) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 48.190664ms)
Aug 29 15:58:28.547: INFO: (11) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 48.854239ms)
Aug 29 15:58:28.577: INFO: (11) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 78.936323ms)
Aug 29 15:58:28.578: INFO: (11) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 78.156218ms)
Aug 29 15:58:28.578: INFO: (11) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 78.861533ms)
Aug 29 15:58:28.578: INFO: (11) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 78.516651ms)
Aug 29 15:58:28.578: INFO: (11) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 78.118288ms)
Aug 29 15:58:28.578: INFO: (11) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 78.547461ms)
Aug 29 15:58:28.605: INFO: (12) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 25.409491ms)
Aug 29 15:58:28.605: INFO: (12) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 26.248478ms)
Aug 29 15:58:28.605: INFO: (12) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 25.798454ms)
Aug 29 15:58:28.606: INFO: (12) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 27.097043ms)
Aug 29 15:58:28.606: INFO: (12) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 25.855285ms)
Aug 29 15:58:28.606: INFO: (12) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 27.535197ms)
Aug 29 15:58:28.606: INFO: (12) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 25.931655ms)
Aug 29 15:58:28.606: INFO: (12) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 26.412599ms)
Aug 29 15:58:28.606: INFO: (12) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 27.260674ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 39.14656ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 39.101379ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 39.453252ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 39.434662ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 39.535402ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 39.696663ms)
Aug 29 15:58:28.619: INFO: (12) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 39.24159ms)
Aug 29 15:58:28.646: INFO: (13) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 21.630845ms)
Aug 29 15:58:28.647: INFO: (13) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 21.393213ms)
Aug 29 15:58:28.647: INFO: (13) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 19.890032ms)
Aug 29 15:58:28.647: INFO: (13) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 18.257181ms)
Aug 29 15:58:28.648: INFO: (13) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 28.625504ms)
Aug 29 15:58:28.648: INFO: (13) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 24.957058ms)
Aug 29 15:58:28.648: INFO: (13) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 26.951713ms)
Aug 29 15:58:28.654: INFO: (13) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 26.350778ms)
Aug 29 15:58:28.654: INFO: (13) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 25.426731ms)
Aug 29 15:58:28.655: INFO: (13) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 28.228891ms)
Aug 29 15:58:28.655: INFO: (13) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 30.585838ms)
Aug 29 15:58:28.655: INFO: (13) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 31.889568ms)
Aug 29 15:58:28.655: INFO: (13) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 26.999923ms)
Aug 29 15:58:28.655: INFO: (13) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 32.661013ms)
Aug 29 15:58:28.658: INFO: (13) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 29.799413ms)
Aug 29 15:58:28.659: INFO: (13) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 29.591171ms)
Aug 29 15:58:28.688: INFO: (14) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 28.992347ms)
Aug 29 15:58:28.688: INFO: (14) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 28.392373ms)
Aug 29 15:58:28.689: INFO: (14) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 26.904602ms)
Aug 29 15:58:28.689: INFO: (14) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 27.167234ms)
Aug 29 15:58:28.689: INFO: (14) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 29.692552ms)
Aug 29 15:58:28.690: INFO: (14) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 30.745039ms)
Aug 29 15:58:28.691: INFO: (14) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 31.403714ms)
Aug 29 15:58:28.692: INFO: (14) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 30.546268ms)
Aug 29 15:58:28.692: INFO: (14) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 31.785087ms)
Aug 29 15:58:28.692: INFO: (14) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 30.93168ms)
Aug 29 15:58:28.693: INFO: (14) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 30.79576ms)
Aug 29 15:58:28.693: INFO: (14) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 31.279463ms)
Aug 29 15:58:28.693: INFO: (14) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 31.219273ms)
Aug 29 15:58:28.693: INFO: (14) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 31.825647ms)
Aug 29 15:58:28.702: INFO: (14) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 42.987116ms)
Aug 29 15:58:28.702: INFO: (14) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 40.949542ms)
Aug 29 15:58:28.723: INFO: (15) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 20.023043ms)
Aug 29 15:58:28.727: INFO: (15) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 23.566898ms)
Aug 29 15:58:28.738: INFO: (15) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 35.221292ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 35.158361ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 34.493716ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 35.780815ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 35.08155ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 34.932949ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 34.641877ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 35.443863ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 34.749578ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 34.238305ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 34.366466ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 34.521337ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 34.95497ms)
Aug 29 15:58:28.739: INFO: (15) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 35.821635ms)
Aug 29 15:58:28.753: INFO: (16) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 14.04976ms)
Aug 29 15:58:28.757: INFO: (16) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 17.116803ms)
Aug 29 15:58:28.758: INFO: (16) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 17.931008ms)
Aug 29 15:58:28.759: INFO: (16) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 18.736074ms)
Aug 29 15:58:28.761: INFO: (16) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 20.531366ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 23.114604ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 22.105437ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 22.033927ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 22.864003ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 23.79937ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 23.706309ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 23.437227ms)
Aug 29 15:58:28.763: INFO: (16) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 22.814363ms)
Aug 29 15:58:28.767: INFO: (16) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 26.351609ms)
Aug 29 15:58:28.768: INFO: (16) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 27.756278ms)
Aug 29 15:58:28.768: INFO: (16) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 28.00691ms)
Aug 29 15:58:28.801: INFO: (17) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 31.319813ms)
Aug 29 15:58:28.798: INFO: (17) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 25.20358ms)
Aug 29 15:58:28.801: INFO: (17) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 31.870987ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 33.419388ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 30.440938ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 35.122041ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 31.298553ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 30.456227ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 34.991009ms)
Aug 29 15:58:28.804: INFO: (17) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 34.909738ms)
Aug 29 15:58:28.805: INFO: (17) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 35.463233ms)
Aug 29 15:58:28.816: INFO: (17) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 42.394903ms)
Aug 29 15:58:28.816: INFO: (17) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 42.666294ms)
Aug 29 15:58:28.816: INFO: (17) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 42.820106ms)
Aug 29 15:58:28.816: INFO: (17) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 46.808554ms)
Aug 29 15:58:28.817: INFO: (17) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 47.407008ms)
Aug 29 15:58:28.836: INFO: (18) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 17.473415ms)
Aug 29 15:58:28.837: INFO: (18) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 18.677863ms)
Aug 29 15:58:28.837: INFO: (18) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 18.878735ms)
Aug 29 15:58:28.838: INFO: (18) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 19.108807ms)
Aug 29 15:58:28.838: INFO: (18) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 18.983716ms)
Aug 29 15:58:28.852: INFO: (18) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 33.366059ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 35.492063ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 35.217402ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 35.667995ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 35.431873ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 35.493034ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 35.601475ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 37.360617ms)
Aug 29 15:58:28.854: INFO: (18) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 35.685165ms)
Aug 29 15:58:28.856: INFO: (18) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 36.940293ms)
Aug 29 15:58:28.856: INFO: (18) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 37.673009ms)
Aug 29 15:58:28.880: INFO: (19) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 22.128549ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname2/proxy/: bar (200; 24.366594ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">test<... (200; 23.037945ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw/proxy/rewriteme">test</a> (200; 21.727465ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname2/proxy/: tls qux (200; 23.88286ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:460/proxy/: tls baz (200; 22.093948ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:462/proxy/: tls qux (200; 24.780118ms)
Aug 29 15:58:28.881: INFO: (19) /api/v1/namespaces/proxy-4897/services/https:proxy-service-w6svg:tlsportname1/proxy/: tls baz (200; 24.139573ms)
Aug 29 15:58:28.885: INFO: (19) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:162/proxy/: bar (200; 26.125596ms)
Aug 29 15:58:28.886: INFO: (19) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname2/proxy/: bar (200; 26.470769ms)
Aug 29 15:58:28.886: INFO: (19) /api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/https:proxy-service-w6svg-2jxrw:443/proxy/tlsrewritem... (200; 26.174077ms)
Aug 29 15:58:28.886: INFO: (19) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 26.927173ms)
Aug 29 15:58:28.886: INFO: (19) /api/v1/namespaces/proxy-4897/services/http:proxy-service-w6svg:portname1/proxy/: foo (200; 28.399463ms)
Aug 29 15:58:28.887: INFO: (19) /api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4897/pods/http:proxy-service-w6svg-2jxrw:1080/proxy/rewriteme">... (200; 28.263302ms)
Aug 29 15:58:28.888: INFO: (19) /api/v1/namespaces/proxy-4897/pods/proxy-service-w6svg-2jxrw:160/proxy/: foo (200; 28.816976ms)
Aug 29 15:58:28.888: INFO: (19) /api/v1/namespaces/proxy-4897/services/proxy-service-w6svg:portname1/proxy/: foo (200; 28.723095ms)
STEP: deleting ReplicationController proxy-service-w6svg in namespace proxy-4897, will wait for the garbage collector to delete the pods
Aug 29 15:58:28.957: INFO: Deleting ReplicationController proxy-service-w6svg took: 11.07857ms
Aug 29 15:58:29.058: INFO: Terminating ReplicationController proxy-service-w6svg pods took: 100.953823ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:58:31.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4897" for this suite.

• [SLOW TEST:5.576 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":168,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:58:31.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:58:31.849: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:58:33.863: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:35.862: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:37.865: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:39.865: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:41.861: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:43.864: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:45.862: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:47.859: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:49.859: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:51.859: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = false)
Aug 29 15:58:53.860: INFO: The status of Pod test-webserver-77146bab-a022-46d5-90e6-2406ee476b43 is Running (Ready = true)
Aug 29 15:58:53.869: INFO: Container started at 2022-08-29 15:58:33 +0000 UTC, pod became ready at 2022-08-29 15:58:52 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:58:53.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3618" for this suite.

• [SLOW TEST:22.507 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":3230,"failed":0}
SSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:58:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 29 15:58:53.967: INFO: Waiting up to 5m0s for pod "security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee" in namespace "security-context-3995" to be "Succeeded or Failed"
Aug 29 15:58:53.976: INFO: Pod "security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.299729ms
Aug 29 15:58:55.987: INFO: Pod "security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01924367s
Aug 29 15:58:57.996: INFO: Pod "security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028796277s
STEP: Saw pod success
Aug 29 15:58:57.997: INFO: Pod "security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee" satisfied condition "Succeeded or Failed"
Aug 29 15:58:58.004: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee container test-container: <nil>
STEP: delete the pod
Aug 29 15:58:58.040: INFO: Waiting for pod security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee to disappear
Aug 29 15:58:58.048: INFO: Pod security-context-2ac84b3c-1cba-427c-ba48-b03eeea3b9ee no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:58:58.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3995" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":170,"skipped":3233,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:58:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5101
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5101
I0829 15:58:58.205335      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5101, replica count: 2
I0829 15:59:01.257161      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:59:01.257: INFO: Creating new exec pod
Aug 29 15:59:04.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-5101 exec execpod9bczm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:59:04.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:59:04.821: INFO: stdout: "externalname-service-wg9l2"
Aug 29 15:59:04.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-5101 exec execpod9bczm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.25.209 80'
Aug 29 15:59:05.367: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.25.209 80\nConnection to 10.240.25.209 80 port [tcp/http] succeeded!\n"
Aug 29 15:59:05.367: INFO: stdout: "externalname-service-9x4dv"
Aug 29 15:59:05.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-5101 exec execpod9bczm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 30465'
Aug 29 15:59:06.398: INFO: stderr: "+ nc -v -t -w 2 172.31.23.90 30465\n+ echo hostName\nConnection to 172.31.23.90 30465 port [tcp/*] succeeded!\n"
Aug 29 15:59:06.398: INFO: stdout: ""
Aug 29 15:59:07.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-5101 exec execpod9bczm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 30465'
Aug 29 15:59:08.032: INFO: stderr: "+ nc -v -t -w 2 172.31.23.90 30465\n+ echo hostName\nConnection to 172.31.23.90 30465 port [tcp/*] succeeded!\n"
Aug 29 15:59:08.032: INFO: stdout: "externalname-service-9x4dv"
Aug 29 15:59:08.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-5101 exec execpod9bczm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.14 30465'
Aug 29 15:59:08.451: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.14 30465\nConnection to 172.31.16.14 30465 port [tcp/*] succeeded!\n"
Aug 29 15:59:08.451: INFO: stdout: "externalname-service-wg9l2"
Aug 29 15:59:08.451: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:08.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5101" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:10.431 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":171,"skipped":3253,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:08.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
Aug 29 15:59:08.576: INFO: Major version: 1
STEP: Confirm minor version
Aug 29 15:59:08.576: INFO: cleanMinorVersion: 23
Aug 29 15:59:08.576: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3451" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":172,"skipped":3268,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:08.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:10.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1399" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3272,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:10.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
Aug 29 15:59:10.826: INFO: Waiting up to 5m0s for pod "test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9" in namespace "svcaccounts-6185" to be "Succeeded or Failed"
Aug 29 15:59:10.832: INFO: Pod "test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.774492ms
Aug 29 15:59:12.844: INFO: Pod "test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017558575s
Aug 29 15:59:14.860: INFO: Pod "test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033648312s
STEP: Saw pod success
Aug 29 15:59:14.860: INFO: Pod "test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9" satisfied condition "Succeeded or Failed"
Aug 29 15:59:14.867: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:59:14.905: INFO: Waiting for pod test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9 to disappear
Aug 29 15:59:14.910: INFO: Pod test-pod-7124dda4-216d-471c-b1ac-4b226e49beb9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6185" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":174,"skipped":3276,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:14.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 15:59:15.021: INFO: The status of Pod pod-update-8955b6eb-2ce4-479a-ac1c-269f3c164ead is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:59:17.041: INFO: The status of Pod pod-update-8955b6eb-2ce4-479a-ac1c-269f3c164ead is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 29 15:59:17.593: INFO: Successfully updated pod "pod-update-8955b6eb-2ce4-479a-ac1c-269f3c164ead"
STEP: verifying the updated pod is in kubernetes
Aug 29 15:59:17.629: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:17.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3821" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3285,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:17.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Aug 29 15:59:17.771: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:59:19.782: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Aug 29 15:59:19.815: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:59:21.825: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 29 15:59:21.863: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 15:59:21.870: INFO: Pod pod-with-poststart-http-hook still exists
Aug 29 15:59:23.870: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 15:59:23.881: INFO: Pod pod-with-poststart-http-hook still exists
Aug 29 15:59:25.870: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 15:59:25.885: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6144" for this suite.

• [SLOW TEST:8.249 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3286,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:25.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 15:59:25.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:33.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9809" for this suite.

• [SLOW TEST:7.951 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":177,"skipped":3301,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 29 15:59:33.926: INFO: Waiting up to 5m0s for pod "security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc" in namespace "security-context-9201" to be "Succeeded or Failed"
Aug 29 15:59:33.941: INFO: Pod "security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.969549ms
Aug 29 15:59:35.979: INFO: Pod "security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052686749s
Aug 29 15:59:37.988: INFO: Pod "security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061468903s
STEP: Saw pod success
Aug 29 15:59:37.988: INFO: Pod "security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc" satisfied condition "Succeeded or Failed"
Aug 29 15:59:37.995: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc container test-container: <nil>
STEP: delete the pod
Aug 29 15:59:38.024: INFO: Waiting for pod security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc to disappear
Aug 29 15:59:38.040: INFO: Pod security-context-efe1e1b8-e62c-4845-a03c-9e00399ba5fc no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:38.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9201" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":178,"skipped":3306,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:38.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
Aug 29 15:59:38.136: INFO: Waiting up to 5m0s for pod "var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4" in namespace "var-expansion-2374" to be "Succeeded or Failed"
Aug 29 15:59:38.141: INFO: Pod "var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.302074ms
Aug 29 15:59:40.152: INFO: Pod "var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016128378s
Aug 29 15:59:42.163: INFO: Pod "var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027036707s
Aug 29 15:59:44.174: INFO: Pod "var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038439345s
STEP: Saw pod success
Aug 29 15:59:44.175: INFO: Pod "var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4" satisfied condition "Succeeded or Failed"
Aug 29 15:59:44.187: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4 container dapi-container: <nil>
STEP: delete the pod
Aug 29 15:59:44.250: INFO: Waiting for pod var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4 to disappear
Aug 29 15:59:44.258: INFO: Pod var-expansion-1d662bbe-7719-46a8-9843-4ad1a3f84ae4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:44.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2374" for this suite.

• [SLOW TEST:6.212 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":179,"skipped":3310,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:44.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 29 15:59:44.339: INFO: Waiting up to 5m0s for pod "pod-33758bdd-6743-46f8-84d6-746e44489281" in namespace "emptydir-5676" to be "Succeeded or Failed"
Aug 29 15:59:44.346: INFO: Pod "pod-33758bdd-6743-46f8-84d6-746e44489281": Phase="Pending", Reason="", readiness=false. Elapsed: 6.423137ms
Aug 29 15:59:46.358: INFO: Pod "pod-33758bdd-6743-46f8-84d6-746e44489281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018871981s
Aug 29 15:59:48.369: INFO: Pod "pod-33758bdd-6743-46f8-84d6-746e44489281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029061814s
STEP: Saw pod success
Aug 29 15:59:48.369: INFO: Pod "pod-33758bdd-6743-46f8-84d6-746e44489281" satisfied condition "Succeeded or Failed"
Aug 29 15:59:48.375: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-33758bdd-6743-46f8-84d6-746e44489281 container test-container: <nil>
STEP: delete the pod
Aug 29 15:59:48.409: INFO: Waiting for pod pod-33758bdd-6743-46f8-84d6-746e44489281 to disappear
Aug 29 15:59:48.420: INFO: Pod pod-33758bdd-6743-46f8-84d6-746e44489281 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:48.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5676" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3318,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:48.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:59:49.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:59:52.465: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 29 15:59:54.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=webhook-4931 attach --namespace=webhook-4931 to-be-attached-pod -i -c=container1'
Aug 29 15:59:54.680: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 15:59:54.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4931" for this suite.
STEP: Destroying namespace "webhook-4931-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.362 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":181,"skipped":3334,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 15:59:54.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:00:01.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-339" for this suite.
STEP: Destroying namespace "nsdeletetest-4747" for this suite.
Aug 29 16:00:01.090: INFO: Namespace nsdeletetest-4747 was already deleted
STEP: Destroying namespace "nsdeletetest-7720" for this suite.

• [SLOW TEST:6.286 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":182,"skipped":3338,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:00:01.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:00:01.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2671" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":183,"skipped":3359,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:00:01.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:00:01.411: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 29 16:00:01.444: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:00:01.480: INFO: waiting for watch events with expected annotations
Aug 29 16:00:01.480: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:00:01.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-99" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":184,"skipped":3368,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:00:01.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-211
STEP: creating service affinity-nodeport-transition in namespace services-211
STEP: creating replication controller affinity-nodeport-transition in namespace services-211
I0829 16:00:01.792307      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-211, replica count: 3
I0829 16:00:04.851711      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:00:04.880: INFO: Creating new exec pod
Aug 29 16:00:07.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 29 16:00:08.278: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 29 16:00:08.278: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:00:08.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.49 80'
Aug 29 16:00:08.707: INFO: stderr: "+ + nc -v -t -w 2 10.240.20.49 80\nechoConnection to 10.240.20.49 80 port [tcp/http] succeeded!\n hostName\n"
Aug 29 16:00:08.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:00:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.23.90 30221'
Aug 29 16:00:09.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.23.90 30221\nConnection to 172.31.23.90 30221 port [tcp/*] succeeded!\n"
Aug 29 16:00:09.237: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:00:09.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.14 30221'
Aug 29 16:00:09.595: INFO: stderr: "+ nc -v -t -w 2 172.31.16.14 30221\n+ echo hostName\nConnection to 172.31.16.14 30221 port [tcp/*] succeeded!\n"
Aug 29 16:00:09.595: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:00:09.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.14:30221/ ; done'
Aug 29 16:00:10.078: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n"
Aug 29 16:00:10.078: INFO: stdout: "\naffinity-nodeport-transition-4qtqk\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-4qtqk\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-4qtqk\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-4qtqk\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-4qtqk\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-4qtqk"
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.078: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.14:30221/ ; done'
Aug 29 16:00:10.579: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n"
Aug 29 16:00:10.579: INFO: stdout: "\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-4qtqk\naffinity-nodeport-transition-vklgh\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp"
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-4qtqk
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-vklgh
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:10.579: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:40.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-211 exec execpod-affinityz4tlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.14:30221/ ; done'
Aug 29 16:00:41.506: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.14:30221/\n"
Aug 29 16:00:41.506: INFO: stdout: "\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp\naffinity-nodeport-transition-cjtvp"
Aug 29 16:00:41.506: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.506: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.506: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.506: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.506: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.506: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Received response from host: affinity-nodeport-transition-cjtvp
Aug 29 16:00:41.507: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-211, will wait for the garbage collector to delete the pods
Aug 29 16:00:41.610: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.052868ms
Aug 29 16:00:41.710: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.506675ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:00:43.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-211" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:42.364 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":185,"skipped":3387,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:00:43.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-f9ef68d7-72d7-4844-8e4d-b696e5028e96
STEP: Creating a pod to test consume configMaps
Aug 29 16:00:44.103: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2" in namespace "projected-9355" to be "Succeeded or Failed"
Aug 29 16:00:44.110: INFO: Pod "pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.757979ms
Aug 29 16:00:46.147: INFO: Pod "pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044267532s
Aug 29 16:00:48.161: INFO: Pod "pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058117179s
STEP: Saw pod success
Aug 29 16:00:48.161: INFO: Pod "pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2" satisfied condition "Succeeded or Failed"
Aug 29 16:00:48.167: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:00:48.216: INFO: Waiting for pod pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2 to disappear
Aug 29 16:00:48.222: INFO: Pod pod-projected-configmaps-8399ea57-f4b5-4a80-aa5e-876bd304e3e2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:00:48.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9355" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:00:48.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-02302f53-0ee6-44a8-b86a-7326cf0c60da
STEP: Creating a pod to test consume configMaps
Aug 29 16:00:48.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2" in namespace "configmap-2200" to be "Succeeded or Failed"
Aug 29 16:00:48.355: INFO: Pod "pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.495519ms
Aug 29 16:00:50.365: INFO: Pod "pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019005361s
Aug 29 16:00:52.379: INFO: Pod "pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033020941s
STEP: Saw pod success
Aug 29 16:00:52.379: INFO: Pod "pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2" satisfied condition "Succeeded or Failed"
Aug 29 16:00:52.386: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:00:52.422: INFO: Waiting for pod pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2 to disappear
Aug 29 16:00:52.429: INFO: Pod pod-configmaps-52cdf7a3-20b8-4d3c-8e9d-2011b3da44b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:00:52.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2200" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3416,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:00:52.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-8ljj
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 16:00:52.545: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8ljj" in namespace "subpath-2930" to be "Succeeded or Failed"
Aug 29 16:00:52.552: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.870989ms
Aug 29 16:00:54.564: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 2.018798586s
Aug 29 16:00:56.574: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 4.028131431s
Aug 29 16:00:58.584: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 6.038384201s
Aug 29 16:01:00.600: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 8.053921079s
Aug 29 16:01:03.035: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 10.488967352s
Aug 29 16:01:05.043: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 12.497056849s
Aug 29 16:01:07.062: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 14.516784244s
Aug 29 16:01:09.077: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 16.531591148s
Aug 29 16:01:11.085: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 18.53962957s
Aug 29 16:01:13.096: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=true. Elapsed: 20.550592469s
Aug 29 16:01:15.108: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Running", Reason="", readiness=false. Elapsed: 22.562103313s
Aug 29 16:01:17.131: INFO: Pod "pod-subpath-test-downwardapi-8ljj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.585824984s
STEP: Saw pod success
Aug 29 16:01:17.136: INFO: Pod "pod-subpath-test-downwardapi-8ljj" satisfied condition "Succeeded or Failed"
Aug 29 16:01:17.150: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-8ljj container test-container-subpath-downwardapi-8ljj: <nil>
STEP: delete the pod
Aug 29 16:01:17.188: INFO: Waiting for pod pod-subpath-test-downwardapi-8ljj to disappear
Aug 29 16:01:17.197: INFO: Pod pod-subpath-test-downwardapi-8ljj no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8ljj
Aug 29 16:01:17.197: INFO: Deleting pod "pod-subpath-test-downwardapi-8ljj" in namespace "subpath-2930"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:01:17.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2930" for this suite.

• [SLOW TEST:24.795 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":188,"skipped":3432,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:01:17.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:01:18.281: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:01:21.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:01:21.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-996" for this suite.
STEP: Destroying namespace "webhook-996-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":189,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:01:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-377cbd2b-8250-432b-9a71-443a1e08982e
STEP: Creating a pod to test consume secrets
Aug 29 16:01:21.643: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e" in namespace "projected-1224" to be "Succeeded or Failed"
Aug 29 16:01:21.654: INFO: Pod "pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.712979ms
Aug 29 16:01:23.801: INFO: Pod "pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158039127s
Aug 29 16:01:25.818: INFO: Pod "pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174610128s
Aug 29 16:01:27.826: INFO: Pod "pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.18307896s
STEP: Saw pod success
Aug 29 16:01:27.827: INFO: Pod "pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e" satisfied condition "Succeeded or Failed"
Aug 29 16:01:27.842: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:01:27.872: INFO: Waiting for pod pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e to disappear
Aug 29 16:01:27.889: INFO: Pod pod-projected-secrets-0445229f-778c-400a-afe7-39a7c79a899e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:01:27.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1224" for this suite.

• [SLOW TEST:6.369 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3502,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:01:27.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Aug 29 16:01:28.015: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 16:02:28.087: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:02:28.099: INFO: Starting informer...
STEP: Starting pod...
Aug 29 16:02:28.331: INFO: Pod is running on ip-172-31-23-90.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 29 16:02:28.371: INFO: Pod wasn't evicted. Proceeding
Aug 29 16:02:28.371: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 29 16:03:43.416: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:03:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6337" for this suite.

• [SLOW TEST:135.537 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":191,"skipped":3513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:03:43.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:03:43.573: INFO: The status of Pod server-envvars-25c3740a-0187-4bf0-acb3-14e448439e01 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:03:45.586: INFO: The status of Pod server-envvars-25c3740a-0187-4bf0-acb3-14e448439e01 is Running (Ready = true)
Aug 29 16:03:45.626: INFO: Waiting up to 5m0s for pod "client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d" in namespace "pods-9294" to be "Succeeded or Failed"
Aug 29 16:03:45.637: INFO: Pod "client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.681194ms
Aug 29 16:03:47.652: INFO: Pod "client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025593369s
Aug 29 16:03:49.661: INFO: Pod "client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034716968s
STEP: Saw pod success
Aug 29 16:03:49.661: INFO: Pod "client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d" satisfied condition "Succeeded or Failed"
Aug 29 16:03:49.668: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d container env3cont: <nil>
STEP: delete the pod
Aug 29 16:03:49.705: INFO: Waiting for pod client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d to disappear
Aug 29 16:03:49.712: INFO: Pod client-envvars-37352619-c57b-47df-838d-3721bcd4bf6d no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:03:49.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9294" for this suite.

• [SLOW TEST:6.281 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3543,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:03:49.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Aug 29 16:03:49.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:05:22.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5177" for this suite.

• [SLOW TEST:92.388 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":193,"skipped":3552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:05:22.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:00.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2233" for this suite.

• [SLOW TEST:98.160 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":194,"skipped":3576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:00.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:07:00.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5" in namespace "projected-8544" to be "Succeeded or Failed"
Aug 29 16:07:00.386: INFO: Pod "downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.502299ms
Aug 29 16:07:02.395: INFO: Pod "downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01750541s
Aug 29 16:07:04.410: INFO: Pod "downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032566127s
STEP: Saw pod success
Aug 29 16:07:04.411: INFO: Pod "downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5" satisfied condition "Succeeded or Failed"
Aug 29 16:07:04.422: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5 container client-container: <nil>
STEP: delete the pod
Aug 29 16:07:04.500: INFO: Waiting for pod downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5 to disappear
Aug 29 16:07:04.518: INFO: Pod downwardapi-volume-943ab3bb-b81c-4468-9e55-bafb75b946d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:04.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8544" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3630,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:04.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:07:06.175: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Aug 29 16:07:08.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 7, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 7, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 7, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 7, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-67c86bcf4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:07:11.243: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:07:11.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:14.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8904" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.060 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":196,"skipped":3639,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:14.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:14.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2795" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":197,"skipped":3647,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:14.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Aug 29 16:07:14.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 create -f -'
Aug 29 16:07:16.719: INFO: stderr: ""
Aug 29 16:07:16.719: INFO: stdout: "pod/pause created\n"
Aug 29 16:07:16.719: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 29 16:07:16.719: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-928" to be "running and ready"
Aug 29 16:07:16.737: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.061966ms
Aug 29 16:07:18.762: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.043102039s
Aug 29 16:07:18.762: INFO: Pod "pause" satisfied condition "running and ready"
Aug 29 16:07:18.762: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 29 16:07:18.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 label pods pause testing-label=testing-label-value'
Aug 29 16:07:19.024: INFO: stderr: ""
Aug 29 16:07:19.024: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 29 16:07:19.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 get pod pause -L testing-label'
Aug 29 16:07:19.282: INFO: stderr: ""
Aug 29 16:07:19.282: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 29 16:07:19.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 label pods pause testing-label-'
Aug 29 16:07:19.788: INFO: stderr: ""
Aug 29 16:07:19.789: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 29 16:07:19.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 get pod pause -L testing-label'
Aug 29 16:07:19.882: INFO: stderr: ""
Aug 29 16:07:19.882: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
Aug 29 16:07:19.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 delete --grace-period=0 --force -f -'
Aug 29 16:07:19.977: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:07:19.977: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 29 16:07:19.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 get rc,svc -l name=pause --no-headers'
Aug 29 16:07:20.102: INFO: stderr: "No resources found in kubectl-928 namespace.\n"
Aug 29 16:07:20.102: INFO: stdout: ""
Aug 29 16:07:20.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-928 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 16:07:20.187: INFO: stderr: ""
Aug 29 16:07:20.187: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:20.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-928" for this suite.

• [SLOW TEST:5.486 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":198,"skipped":3659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:20.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:07:20.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:07:24.028: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:24.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-300" for this suite.
STEP: Destroying namespace "webhook-300-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":199,"skipped":3681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:24.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:35.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7539" for this suite.

• [SLOW TEST:11.362 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":200,"skipped":3703,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:35.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 16:07:35.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-9888 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Aug 29 16:07:36.164: INFO: stderr: ""
Aug 29 16:07:36.164: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
Aug 29 16:07:36.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-9888 delete pods e2e-test-httpd-pod'
Aug 29 16:07:38.690: INFO: stderr: ""
Aug 29 16:07:38.690: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9888" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":201,"skipped":3706,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:38.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 29 16:07:38.844: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5688  0278b197-a0d2-42f3-b4ea-946a915e19a1 25700 0 2022-08-29 16:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:07:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:07:38.844: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5688  0278b197-a0d2-42f3-b4ea-946a915e19a1 25701 0 2022-08-29 16:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:07:38.845: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5688  0278b197-a0d2-42f3-b4ea-946a915e19a1 25702 0 2022-08-29 16:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 29 16:07:48.931: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5688  0278b197-a0d2-42f3-b4ea-946a915e19a1 25758 0 2022-08-29 16:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:07:48.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5688  0278b197-a0d2-42f3-b4ea-946a915e19a1 25759 0 2022-08-29 16:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:07:48.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5688  0278b197-a0d2-42f3-b4ea-946a915e19a1 25760 0 2022-08-29 16:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:48.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5688" for this suite.

• [SLOW TEST:10.242 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":202,"skipped":3726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:48.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:07:50.318: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:07:50.339: INFO: waiting for watch events with expected annotations
Aug 29 16:07:50.339: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:07:50.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-94" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":203,"skipped":3750,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:07:50.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:07:50.576: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 16:07:50.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:07:50.615: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:07:51.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:07:51.639: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:07:52.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:07:52.646: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 29 16:07:52.761: INFO: Wrong image for pod: daemon-set-976qf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:52.761: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:53.783: INFO: Wrong image for pod: daemon-set-976qf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:53.783: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:54.785: INFO: Wrong image for pod: daemon-set-976qf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:54.785: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:55.802: INFO: Wrong image for pod: daemon-set-976qf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:55.802: INFO: Pod daemon-set-fnbbf is not available
Aug 29 16:07:55.802: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:56.783: INFO: Wrong image for pod: daemon-set-976qf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:56.783: INFO: Pod daemon-set-fnbbf is not available
Aug 29 16:07:56.783: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:57.786: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:58.784: INFO: Pod daemon-set-79dct is not available
Aug 29 16:07:58.784: INFO: Wrong image for pod: daemon-set-lhqls. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 16:07:59.799: INFO: Pod daemon-set-cnp2n is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 29 16:07:59.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:07:59.847: INFO: Node ip-172-31-20-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:08:00.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:08:00.869: INFO: Node ip-172-31-20-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:08:01.867: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:08:01.867: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9227, will wait for the garbage collector to delete the pods
Aug 29 16:08:02.003: INFO: Deleting DaemonSet.extensions daemon-set took: 37.470619ms
Aug 29 16:08:02.104: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.112886ms
Aug 29 16:08:04.820: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:08:04.821: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 16:08:04.830: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25977"},"items":null}

Aug 29 16:08:04.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25977"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:08:04.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9227" for this suite.

• [SLOW TEST:14.445 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":204,"skipped":3751,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:08:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:08:05.001: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 29 16:08:07.907: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:08:07.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8030" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":205,"skipped":3758,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:08:07.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:14:02.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9030" for this suite.

• [SLOW TEST:354.202 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":206,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:14:02.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 29 16:14:02.215: INFO: Waiting up to 5m0s for pod "pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8" in namespace "emptydir-3161" to be "Succeeded or Failed"
Aug 29 16:14:02.230: INFO: Pod "pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.767511ms
Aug 29 16:14:04.245: INFO: Pod "pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029262598s
Aug 29 16:14:06.483: INFO: Pod "pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.267351658s
STEP: Saw pod success
Aug 29 16:14:06.483: INFO: Pod "pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8" satisfied condition "Succeeded or Failed"
Aug 29 16:14:06.492: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8 container test-container: <nil>
STEP: delete the pod
Aug 29 16:14:06.545: INFO: Waiting for pod pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8 to disappear
Aug 29 16:14:06.566: INFO: Pod pod-57c092ff-9e09-4ac6-8c01-b26afb0d8bc8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:14:06.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3161" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3794,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:14:06.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 29 16:14:06.698: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 16:14:06.728: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 16:14:06.735: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-14.eu-central-1.compute.internal before test
Aug 29 16:14:06.761: INFO: calico-kube-controllers-57fb8785bf-xg2r9 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.761: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 16:14:06.761: INFO: canal-lzt6p from kube-system started at 2022-08-29 15:05:27 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:14:06.762: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:14:06.762: INFO: coredns-5848f745f7-mkm4d from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:14:06.762: INFO: coredns-5848f745f7-ntmz2 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:14:06.762: INFO: envoy-agent-7wzrr from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:14:06.762: INFO: konnectivity-agent-b7c8486c7-dr2vw from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:14:06.762: INFO: konnectivity-agent-b7c8486c7-r476v from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:14:06.762: INFO: kube-proxy-fjrpb from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:14:06.762: INFO: metrics-server-7c94595b7c-d66rr from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.762: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:14:06.762: INFO: metrics-server-7c94595b7c-xjds6 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.763: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:14:06.763: INFO: node-local-dns-jdx9q from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.763: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:14:06.763: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:53 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.763: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 16:14:06.763: INFO: sonobuoy-e2e-job-0af6d8c74f8640cb from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.763: INFO: 	Container e2e ready: true, restart count 0
Aug 29 16:14:06.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:14:06.763: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-lffk7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:14:06.763: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:14:06.763: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-142.eu-central-1.compute.internal before test
Aug 29 16:14:06.779: INFO: canal-twqkh from kube-system started at 2022-08-29 15:05:56 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.779: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:14:06.779: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:14:06.780: INFO: envoy-agent-rmdxv from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.780: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:14:06.780: INFO: kube-proxy-rdwz4 from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.780: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:14:06.780: INFO: node-local-dns-x9dxw from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.780: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:14:06.780: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-dlbm7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.780: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:14:06.780: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:14:06.780: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-23-90.eu-central-1.compute.internal before test
Aug 29 16:14:06.797: INFO: canal-vngk2 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.799: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:14:06.799: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:14:06.799: INFO: envoy-agent-48bh4 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.800: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:14:06.800: INFO: kube-proxy-qjw5j from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.800: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:14:06.800: INFO: node-local-dns-lxgw9 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 16:14:06.800: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:14:06.800: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-snzb6 from sonobuoy started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 16:14:06.800: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Aug 29 16:14:06.800: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5ffd4bbb-89da-4bc5-a84d-583b5ca4dfd4 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5ffd4bbb-89da-4bc5-a84d-583b5ca4dfd4 off the node ip-172-31-20-142.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5ffd4bbb-89da-4bc5-a84d-583b5ca4dfd4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:14:13.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6089" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.486 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":208,"skipped":3795,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:14:13.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 29 16:14:13.154: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9046  5904b9c2-c4fa-4503-afc2-b11aed7574a1 27265 0 2022-08-29 16:14:13 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-08-29 16:14:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99ns7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99ns7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:14:13.164: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:14:15.177: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 29 16:14:15.177: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9046 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:14:15.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:14:15.177: INFO: ExecWithOptions: Clientset creation
Aug 29 16:14:15.177: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-9046/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Aug 29 16:14:15.358: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9046 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:14:15.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:14:15.359: INFO: ExecWithOptions: Clientset creation
Aug 29 16:14:15.359: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-9046/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Aug 29 16:14:15.517: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:14:15.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9046" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":209,"skipped":3814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:14:15.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Aug 29 16:14:15.657: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 16:14:20.667: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:14:20.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9487" for this suite.

• [SLOW TEST:5.170 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":210,"skipped":3859,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:14:20.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:15:20.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7776" for this suite.

• [SLOW TEST:60.096 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3925,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:15:20.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5522.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5522.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5522.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5522.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:15:22.995: INFO: DNS probes using dns-test-11ccd1f3-e809-4534-a76e-f6cab869a0b2 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5522.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5522.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5522.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5522.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:15:25.106: INFO: File wheezy_udp@dns-test-service-3.dns-5522.svc.cluster.local from pod  dns-5522/dns-test-3bb0711a-2dd0-431e-b04a-247f956d95fa contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 29 16:15:25.115: INFO: File jessie_udp@dns-test-service-3.dns-5522.svc.cluster.local from pod  dns-5522/dns-test-3bb0711a-2dd0-431e-b04a-247f956d95fa contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 29 16:15:25.116: INFO: Lookups using dns-5522/dns-test-3bb0711a-2dd0-431e-b04a-247f956d95fa failed for: [wheezy_udp@dns-test-service-3.dns-5522.svc.cluster.local jessie_udp@dns-test-service-3.dns-5522.svc.cluster.local]

Aug 29 16:15:30.139: INFO: DNS probes using dns-test-3bb0711a-2dd0-431e-b04a-247f956d95fa succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5522.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5522.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5522.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5522.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:15:32.328: INFO: DNS probes using dns-test-9bae737a-c039-4ed7-b980-4c7d1bab7f97 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:15:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5522" for this suite.

• [SLOW TEST:11.576 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":212,"skipped":3926,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:15:32.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3406 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3406;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3406 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3406;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3406.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3406.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3406.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3406.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3406.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3406.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3406.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3406.svc;check="$$(dig +notcp +noall +answer +search 66.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.66_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3406 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3406;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3406 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3406;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3406.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3406.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3406.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3406.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3406.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3406.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3406.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3406.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3406.svc;check="$$(dig +notcp +noall +answer +search 66.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.30.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.30.66_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:15:34.623: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.638: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.649: INFO: Unable to read wheezy_udp@dns-test-service.dns-3406 from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.658: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3406 from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.668: INFO: Unable to read wheezy_udp@dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.680: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.691: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.700: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.756: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.769: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.785: INFO: Unable to read jessie_udp@dns-test-service.dns-3406 from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.798: INFO: Unable to read jessie_tcp@dns-test-service.dns-3406 from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.810: INFO: Unable to read jessie_udp@dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.820: INFO: Unable to read jessie_tcp@dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.840: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.850: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3406.svc from pod dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644: the server could not find the requested resource (get pods dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644)
Aug 29 16:15:34.896: INFO: Lookups using dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3406 wheezy_tcp@dns-test-service.dns-3406 wheezy_udp@dns-test-service.dns-3406.svc wheezy_tcp@dns-test-service.dns-3406.svc wheezy_udp@_http._tcp.dns-test-service.dns-3406.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3406.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3406 jessie_tcp@dns-test-service.dns-3406 jessie_udp@dns-test-service.dns-3406.svc jessie_tcp@dns-test-service.dns-3406.svc jessie_udp@_http._tcp.dns-test-service.dns-3406.svc jessie_tcp@_http._tcp.dns-test-service.dns-3406.svc]

Aug 29 16:15:40.180: INFO: DNS probes using dns-3406/dns-test-c6b96ba5-a730-49ea-80ef-ba9da17b9644 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:15:40.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3406" for this suite.

• [SLOW TEST:7.879 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":213,"skipped":3927,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:15:40.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 29 16:15:40.411: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:15:40.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9319" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":214,"skipped":3928,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:15:40.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:15:40.561: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 29 16:15:45.574: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 29 16:15:45.574: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 29 16:15:47.585: INFO: Creating deployment "test-rollover-deployment"
Aug 29 16:15:47.611: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 29 16:15:49.627: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 29 16:15:49.641: INFO: Ensure that both replica sets have 1 created replica
Aug 29 16:15:49.658: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 29 16:15:49.675: INFO: Updating deployment test-rollover-deployment
Aug 29 16:15:49.675: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 29 16:15:51.691: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 29 16:15:51.708: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 29 16:15:51.731: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:15:51.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:15:53.748: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:15:53.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:15:56.095: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:15:56.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:15:57.747: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:15:57.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:15:59.752: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:15:59.752: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 15, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:16:01.956: INFO: 
Aug 29 16:16:01.956: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 16:16:02.031: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6099  68de7f63-e18f-4cef-8efa-879c53382d81 28030 2 2022-08-29 16:15:47 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-29 16:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:16:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067a0508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-29 16:15:47 +0000 UTC,LastTransitionTime:2022-08-29 16:15:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77db6f9f48" has successfully progressed.,LastUpdateTime:2022-08-29 16:16:00 +0000 UTC,LastTransitionTime:2022-08-29 16:15:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 16:16:02.040: INFO: New ReplicaSet "test-rollover-deployment-77db6f9f48" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77db6f9f48  deployment-6099  55ff7b39-7a0a-42dd-87f2-4512a8e6db91 28020 2 2022-08-29 16:15:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 68de7f63-e18f-4cef-8efa-879c53382d81 0xc0067a07f7 0xc0067a07f8}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68de7f63-e18f-4cef-8efa-879c53382d81\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:16:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77db6f9f48,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067a08a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:16:02.040: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 29 16:16:02.040: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6099  6b01e645-e65a-4fae-9adc-e52bfc84f857 28029 2 2022-08-29 16:15:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 68de7f63-e18f-4cef-8efa-879c53382d81 0xc0067a00b7 0xc0067a00b8}] []  [{e2e.test Update apps/v1 2022-08-29 16:15:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:16:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68de7f63-e18f-4cef-8efa-879c53382d81\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:16:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0067a0178 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:16:02.044: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-6099  8b55508d-ce84-4adf-bed6-414acbf46e07 27974 2 2022-08-29 16:15:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 68de7f63-e18f-4cef-8efa-879c53382d81 0xc0067a0917 0xc0067a0918}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:15:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68de7f63-e18f-4cef-8efa-879c53382d81\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:15:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067a09c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:16:02.052: INFO: Pod "test-rollover-deployment-77db6f9f48-hmzsb" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77db6f9f48-hmzsb test-rollover-deployment-77db6f9f48- deployment-6099  53929bd0-19b2-4819-a2ac-1de361af65d7 27988 0 2022-08-29 16:15:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[cni.projectcalico.org/containerID:0ebb44d597fb2183588f9aaf8b85a7b2269e8a0f7ddb6352842ddab3eb3dbfff cni.projectcalico.org/podIP:172.25.1.161/32 cni.projectcalico.org/podIPs:172.25.1.161/32] [{apps/v1 ReplicaSet test-rollover-deployment-77db6f9f48 55ff7b39-7a0a-42dd-87f2-4512a8e6db91 0xc0067a0f37 0xc0067a0f38}] []  [{kube-controller-manager Update v1 2022-08-29 16:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55ff7b39-7a0a-42dd-87f2-4512a8e6db91\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:15:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z4chq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z4chq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:15:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:15:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:15:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.161,StartTime:2022-08-29 16:15:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:15:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://e2ad71c3e1d31d53cc9d1e70c75961e5fa0dd2a6369d58d64a4394f156d21fc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:02.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6099" for this suite.

• [SLOW TEST:21.591 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":215,"skipped":3931,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Aug 29 16:16:02.190: INFO: The status of Pod annotationupdatec44b391d-cbb4-4719-971e-0134d9fd408a is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:16:04.213: INFO: The status of Pod annotationupdatec44b391d-cbb4-4719-971e-0134d9fd408a is Running (Ready = true)
Aug 29 16:16:05.755: INFO: Successfully updated pod "annotationupdatec44b391d-cbb4-4719-971e-0134d9fd408a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:08.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1342" for this suite.

• [SLOW TEST:5.985 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":3932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:08.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-0b8f9235-d5d1-49cb-8bf0-f13d491d1e22
STEP: Creating a pod to test consume configMaps
Aug 29 16:16:08.149: INFO: Waiting up to 5m0s for pod "pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76" in namespace "configmap-1635" to be "Succeeded or Failed"
Aug 29 16:16:08.158: INFO: Pod "pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.723474ms
Aug 29 16:16:10.172: INFO: Pod "pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02206232s
Aug 29 16:16:12.185: INFO: Pod "pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035347097s
STEP: Saw pod success
Aug 29 16:16:12.185: INFO: Pod "pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76" satisfied condition "Succeeded or Failed"
Aug 29 16:16:12.192: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:16:12.229: INFO: Waiting for pod pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76 to disappear
Aug 29 16:16:12.238: INFO: Pod pod-configmaps-d790a765-19fc-4c73-b49d-726455d17f76 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:12.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1635" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":3990,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:16:12.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27" in namespace "downward-api-1993" to be "Succeeded or Failed"
Aug 29 16:16:12.336: INFO: Pod "downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27": Phase="Pending", Reason="", readiness=false. Elapsed: 15.256917ms
Aug 29 16:16:14.361: INFO: Pod "downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039516206s
Aug 29 16:16:16.373: INFO: Pod "downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052088803s
STEP: Saw pod success
Aug 29 16:16:16.373: INFO: Pod "downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27" satisfied condition "Succeeded or Failed"
Aug 29 16:16:16.379: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27 container client-container: <nil>
STEP: delete the pod
Aug 29 16:16:16.419: INFO: Waiting for pod downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27 to disappear
Aug 29 16:16:16.428: INFO: Pod downwardapi-volume-6078fcce-ffc5-4ee6-9fe1-636a510cec27 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:16.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1993" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":3995,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:16.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:16.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2526" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":219,"skipped":4007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:16:17.490: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:16:20.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:20.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4234" for this suite.
STEP: Destroying namespace "webhook-4234-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":220,"skipped":4040,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:21.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:21.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3534" for this suite.
STEP: Destroying namespace "nspatchtest-9c06b10f-8cfe-4c79-9f37-cf5a0022df8e-975" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":221,"skipped":4049,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:21.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-9016/configmap-test-fbaafa27-5b1d-42e2-9f0f-9ee94868bdd3
STEP: Creating a pod to test consume configMaps
Aug 29 16:16:21.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578" in namespace "configmap-9016" to be "Succeeded or Failed"
Aug 29 16:16:21.897: INFO: Pod "pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578": Phase="Pending", Reason="", readiness=false. Elapsed: 8.592071ms
Aug 29 16:16:23.914: INFO: Pod "pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025348203s
Aug 29 16:16:25.928: INFO: Pod "pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040246224s
STEP: Saw pod success
Aug 29 16:16:25.929: INFO: Pod "pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578" satisfied condition "Succeeded or Failed"
Aug 29 16:16:25.937: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578 container env-test: <nil>
STEP: delete the pod
Aug 29 16:16:26.576: INFO: Waiting for pod pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578 to disappear
Aug 29 16:16:26.590: INFO: Pod pod-configmaps-810ba830-7aba-4b57-a282-dd99d7051578 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9016" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":4053,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:26.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:16:26.750: INFO: Create a RollingUpdate DaemonSet
Aug 29 16:16:26.761: INFO: Check that daemon pods launch on every node of the cluster
Aug 29 16:16:26.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:16:26.782: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:16:27.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:16:27.800: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:16:28.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:16:28.803: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 29 16:16:28.803: INFO: Update the DaemonSet to trigger a rollout
Aug 29 16:16:28.825: INFO: Updating DaemonSet daemon-set
Aug 29 16:16:32.866: INFO: Roll back the DaemonSet before rollout is complete
Aug 29 16:16:32.892: INFO: Updating DaemonSet daemon-set
Aug 29 16:16:32.892: INFO: Make sure DaemonSet rollback is complete
Aug 29 16:16:32.903: INFO: Wrong image for pod: daemon-set-4gd6x. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Aug 29 16:16:32.903: INFO: Pod daemon-set-4gd6x is not available
Aug 29 16:16:36.940: INFO: Pod daemon-set-gj2mn is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2078, will wait for the garbage collector to delete the pods
Aug 29 16:16:37.045: INFO: Deleting DaemonSet.extensions daemon-set took: 12.481001ms
Aug 29 16:16:37.146: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.994501ms
Aug 29 16:16:39.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:16:39.463: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 16:16:39.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28585"},"items":null}

Aug 29 16:16:39.478: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28585"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:39.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2078" for this suite.

• [SLOW TEST:12.914 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":223,"skipped":4057,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:39.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6223
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6223
STEP: creating replication controller externalsvc in namespace services-6223
I0829 16:16:39.656909      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6223, replica count: 2
I0829 16:16:42.708351      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 29 16:16:42.742: INFO: Creating new exec pod
Aug 29 16:16:46.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-6223 exec execpodkk5fv -- /bin/sh -x -c nslookup clusterip-service.services-6223.svc.cluster.local'
Aug 29 16:16:47.298: INFO: stderr: "+ nslookup clusterip-service.services-6223.svc.cluster.local\n"
Aug 29 16:16:47.298: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-6223.svc.cluster.local\tcanonical name = externalsvc.services-6223.svc.cluster.local.\nName:\texternalsvc.services-6223.svc.cluster.local\nAddress: 10.240.21.19\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6223, will wait for the garbage collector to delete the pods
Aug 29 16:16:47.370: INFO: Deleting ReplicationController externalsvc took: 11.798262ms
Aug 29 16:16:47.470: INFO: Terminating ReplicationController externalsvc pods took: 100.669957ms
Aug 29 16:16:49.308: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:49.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6223" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.836 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":224,"skipped":4066,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:49.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6190" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":225,"skipped":4068,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:49.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-58c5de61-3500-42e2-8668-e1c362be07d9
STEP: Creating secret with name s-test-opt-upd-3ba3a52e-c169-4a74-b77b-d14dd75d013e
STEP: Creating the pod
Aug 29 16:16:49.581: INFO: The status of Pod pod-projected-secrets-481aa767-5e11-40b5-b7d2-56e7d9e334a6 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:16:51.593: INFO: The status of Pod pod-projected-secrets-481aa767-5e11-40b5-b7d2-56e7d9e334a6 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-58c5de61-3500-42e2-8668-e1c362be07d9
STEP: Updating secret s-test-opt-upd-3ba3a52e-c169-4a74-b77b-d14dd75d013e
STEP: Creating secret with name s-test-opt-create-0ef4b530-22b9-411d-8bd4-fb921db536c0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:16:54.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3113" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":226,"skipped":4072,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:16:54.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 29 16:16:54.501: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 16:17:54.588: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:17:54.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:17:54.987: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Aug 29 16:17:54.997: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:17:55.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-717" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:17:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6604" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.891 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":227,"skipped":4073,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:17:55.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:17:55.287: INFO: Creating pod...
Aug 29 16:17:55.312: INFO: Pod Quantity: 1 Status: Pending
Aug 29 16:17:56.324: INFO: Pod Quantity: 1 Status: Pending
Aug 29 16:17:57.326: INFO: Pod Status: Running
Aug 29 16:17:57.326: INFO: Creating service...
Aug 29 16:17:57.350: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/DELETE
Aug 29 16:17:57.377: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 16:17:57.377: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/GET
Aug 29 16:17:57.388: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 29 16:17:57.388: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/HEAD
Aug 29 16:17:57.400: INFO: http.Client request:HEAD | StatusCode:200
Aug 29 16:17:57.400: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 29 16:17:57.411: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 16:17:57.411: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/PATCH
Aug 29 16:17:57.424: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 16:17:57.424: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/POST
Aug 29 16:17:57.447: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 16:17:57.447: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/PUT
Aug 29 16:17:57.463: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 16:17:57.463: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/DELETE
Aug 29 16:17:57.481: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 16:17:57.481: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/GET
Aug 29 16:17:57.496: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 29 16:17:57.496: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/HEAD
Aug 29 16:17:57.506: INFO: http.Client request:HEAD | StatusCode:200
Aug 29 16:17:57.506: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/OPTIONS
Aug 29 16:17:57.523: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 16:17:57.523: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/PATCH
Aug 29 16:17:57.544: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 16:17:57.544: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/POST
Aug 29 16:17:57.562: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 16:17:57.562: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/PUT
Aug 29 16:17:57.582: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:17:57.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9255" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":228,"skipped":4091,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:17:57.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:17:58.114: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 29 16:17:58.119: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 29 16:17:58.119: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 29 16:17:58.119: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 29 16:17:58.119: INFO: Checking APIGroup: apps
Aug 29 16:17:58.123: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 29 16:17:58.123: INFO: Versions found [{apps/v1 v1}]
Aug 29 16:17:58.123: INFO: apps/v1 matches apps/v1
Aug 29 16:17:58.123: INFO: Checking APIGroup: events.k8s.io
Aug 29 16:17:58.130: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 29 16:17:58.130: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 29 16:17:58.130: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 29 16:17:58.130: INFO: Checking APIGroup: authentication.k8s.io
Aug 29 16:17:58.134: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 29 16:17:58.134: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 29 16:17:58.134: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 29 16:17:58.134: INFO: Checking APIGroup: authorization.k8s.io
Aug 29 16:17:58.139: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 29 16:17:58.139: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 29 16:17:58.139: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 29 16:17:58.139: INFO: Checking APIGroup: autoscaling
Aug 29 16:17:58.144: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 29 16:17:58.144: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 29 16:17:58.144: INFO: autoscaling/v2 matches autoscaling/v2
Aug 29 16:17:58.144: INFO: Checking APIGroup: batch
Aug 29 16:17:58.149: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 29 16:17:58.149: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 29 16:17:58.149: INFO: batch/v1 matches batch/v1
Aug 29 16:17:58.149: INFO: Checking APIGroup: certificates.k8s.io
Aug 29 16:17:58.153: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 29 16:17:58.154: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 29 16:17:58.154: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 29 16:17:58.154: INFO: Checking APIGroup: networking.k8s.io
Aug 29 16:17:58.160: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 29 16:17:58.160: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 29 16:17:58.160: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 29 16:17:58.160: INFO: Checking APIGroup: policy
Aug 29 16:17:58.165: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 29 16:17:58.165: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Aug 29 16:17:58.165: INFO: policy/v1 matches policy/v1
Aug 29 16:17:58.165: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 29 16:17:58.168: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 29 16:17:58.168: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 29 16:17:58.168: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 29 16:17:58.168: INFO: Checking APIGroup: storage.k8s.io
Aug 29 16:17:58.174: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 29 16:17:58.174: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 29 16:17:58.174: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 29 16:17:58.174: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 29 16:17:58.180: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 29 16:17:58.180: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 29 16:17:58.180: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 29 16:17:58.180: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 29 16:17:58.184: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 29 16:17:58.184: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 29 16:17:58.184: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 29 16:17:58.184: INFO: Checking APIGroup: scheduling.k8s.io
Aug 29 16:17:58.187: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 29 16:17:58.187: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 29 16:17:58.187: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 29 16:17:58.187: INFO: Checking APIGroup: coordination.k8s.io
Aug 29 16:17:58.192: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 29 16:17:58.192: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 29 16:17:58.192: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 29 16:17:58.192: INFO: Checking APIGroup: node.k8s.io
Aug 29 16:17:58.196: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 29 16:17:58.196: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Aug 29 16:17:58.196: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 29 16:17:58.196: INFO: Checking APIGroup: discovery.k8s.io
Aug 29 16:17:58.199: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 29 16:17:58.199: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Aug 29 16:17:58.199: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 29 16:17:58.199: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 29 16:17:58.202: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Aug 29 16:17:58.202: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 29 16:17:58.202: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Aug 29 16:17:58.202: INFO: Checking APIGroup: apps.kubermatic.k8c.io
Aug 29 16:17:58.206: INFO: PreferredVersion.GroupVersion: apps.kubermatic.k8c.io/v1
Aug 29 16:17:58.206: INFO: Versions found [{apps.kubermatic.k8c.io/v1 v1}]
Aug 29 16:17:58.206: INFO: apps.kubermatic.k8c.io/v1 matches apps.kubermatic.k8c.io/v1
Aug 29 16:17:58.206: INFO: Checking APIGroup: crd.projectcalico.org
Aug 29 16:17:58.209: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 29 16:17:58.210: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 29 16:17:58.210: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 29 16:17:58.210: INFO: Checking APIGroup: cluster.k8s.io
Aug 29 16:17:58.213: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Aug 29 16:17:58.213: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Aug 29 16:17:58.213: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Aug 29 16:17:58.213: INFO: Checking APIGroup: metrics.k8s.io
Aug 29 16:17:58.219: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 29 16:17:58.227: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 29 16:17:58.227: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:17:58.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9389" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":229,"skipped":4109,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:17:58.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Aug 29 16:17:58.297: INFO: PodSpec: initContainers in spec.initContainers
Aug 29 16:18:42.449: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fa4fd33a-88b8-4b66-bf16-fcd712f0da6b", GenerateName:"", Namespace:"init-container-5115", SelfLink:"", UID:"05109b9d-3ed8-4139-8231-9a9c7763380e", ResourceVersion:"29312", Generation:0, CreationTimestamp:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"297787666"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"6d0a8e6a2feccf70853b8996aa7e350223d9de16ad304b7e63ce5074292bb06a", "cni.projectcalico.org/podIP":"172.25.2.165/32", "cni.projectcalico.org/podIPs":"172.25.2.165/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00753bfe0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 29, 16, 17, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e8e018), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 29, 16, 18, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e8e048), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-xcjmq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00362ed20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xcjmq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xcjmq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xcjmq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005373940), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-23-90.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0036265b0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0053739c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0053739e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0053739e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0053739ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00559fc60), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.23.90", PodIP:"172.25.2.165", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.165"}}, StartTime:time.Date(2022, time.August, 29, 16, 17, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003626690)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003626700)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://0819585f129cd275966a259a394ad817e1c6ef57f9d03b844233687cdf7f64a7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00362eda0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00362ed80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc005373a6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:18:42.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5115" for this suite.

• [SLOW TEST:44.209 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":230,"skipped":4122,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:18:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Aug 29 16:18:42.517: INFO: namespace kubectl-8517
Aug 29 16:18:42.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8517 create -f -'
Aug 29 16:18:43.018: INFO: stderr: ""
Aug 29 16:18:43.018: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 29 16:18:44.025: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:18:44.025: INFO: Found 0 / 1
Aug 29 16:18:45.027: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:18:45.027: INFO: Found 1 / 1
Aug 29 16:18:45.027: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 29 16:18:45.033: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:18:45.033: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 16:18:45.033: INFO: wait on agnhost-primary startup in kubectl-8517 
Aug 29 16:18:45.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8517 logs agnhost-primary-pvvqv agnhost-primary'
Aug 29 16:18:45.143: INFO: stderr: ""
Aug 29 16:18:45.143: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 29 16:18:45.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8517 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 29 16:18:45.321: INFO: stderr: ""
Aug 29 16:18:45.321: INFO: stdout: "service/rm2 exposed\n"
Aug 29 16:18:45.338: INFO: Service rm2 in namespace kubectl-8517 found.
STEP: exposing service
Aug 29 16:18:47.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-8517 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 29 16:18:47.626: INFO: stderr: ""
Aug 29 16:18:47.626: INFO: stdout: "service/rm3 exposed\n"
Aug 29 16:18:47.665: INFO: Service rm3 in namespace kubectl-8517 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:18:49.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8517" for this suite.

• [SLOW TEST:7.235 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":231,"skipped":4130,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:18:49.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-d45668b1-4895-4ec5-9b8e-540bc49507f2
STEP: Creating secret with name s-test-opt-upd-2bb686fa-d780-4a01-9fa8-2090d118b28e
STEP: Creating the pod
Aug 29 16:18:50.533: INFO: The status of Pod pod-secrets-c9b817c3-8eed-4195-97e9-ebbf3a3b35e8 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:18:52.541: INFO: The status of Pod pod-secrets-c9b817c3-8eed-4195-97e9-ebbf3a3b35e8 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d45668b1-4895-4ec5-9b8e-540bc49507f2
STEP: Updating secret s-test-opt-upd-2bb686fa-d780-4a01-9fa8-2090d118b28e
STEP: Creating secret with name s-test-opt-create-5c7a7bf8-910d-4a38-b0aa-4e23bdb8bdaa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:18:56.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7847" for this suite.

• [SLOW TEST:7.185 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4139,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:18:56.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-1950
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-1950
Aug 29 16:18:56.990: INFO: Found 0 stateful pods, waiting for 1
Aug 29 16:19:07.004: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 16:19:07.067: INFO: Deleting all statefulset in ns statefulset-1950
Aug 29 16:19:07.074: INFO: Scaling statefulset ss to 0
Aug 29 16:19:17.151: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:19:17.158: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:17.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1950" for this suite.

• [SLOW TEST:20.337 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":233,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:17.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:19:17.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86" in namespace "downward-api-4864" to be "Succeeded or Failed"
Aug 29 16:19:17.313: INFO: Pod "downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.614519ms
Aug 29 16:19:19.324: INFO: Pod "downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019054673s
Aug 29 16:19:21.335: INFO: Pod "downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030495632s
STEP: Saw pod success
Aug 29 16:19:21.336: INFO: Pod "downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86" satisfied condition "Succeeded or Failed"
Aug 29 16:19:21.343: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86 container client-container: <nil>
STEP: delete the pod
Aug 29 16:19:21.391: INFO: Waiting for pod downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86 to disappear
Aug 29 16:19:21.399: INFO: Pod downwardapi-volume-ba2f44ea-7620-4a52-a20d-589293276a86 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:21.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4864" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:21.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:19:21.519: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6" in namespace "downward-api-3632" to be "Succeeded or Failed"
Aug 29 16:19:21.539: INFO: Pod "downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.777832ms
Aug 29 16:19:23.550: INFO: Pod "downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030640382s
Aug 29 16:19:25.563: INFO: Pod "downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043977224s
STEP: Saw pod success
Aug 29 16:19:25.563: INFO: Pod "downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6" satisfied condition "Succeeded or Failed"
Aug 29 16:19:25.571: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6 container client-container: <nil>
STEP: delete the pod
Aug 29 16:19:25.624: INFO: Waiting for pod downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6 to disappear
Aug 29 16:19:25.630: INFO: Pod downwardapi-volume-9ad19b16-d189-4374-87eb-2580cfbbc4e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:25.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3632" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4235,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:25.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 16:19:26.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:19:26.206: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:19:27.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:19:27.226: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:19:28.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:19:28.225: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Aug 29 16:19:28.311: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29789"},"items":null}

Aug 29 16:19:28.327: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29789"},"items":[{"metadata":{"name":"daemon-set-42nxm","generateName":"daemon-set-","namespace":"daemonsets-1573","uid":"33b5e33f-2922-43c0-8c12-67588f8147c3","resourceVersion":"29786","creationTimestamp":"2022-08-29T16:19:26Z","deletionTimestamp":"2022-08-29T16:19:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"219016d502e217311addd17974f6255f70568195b1ab27b5f0759f579ca4b594","cni.projectcalico.org/podIP":"172.25.2.168/32","cni.projectcalico.org/podIPs":"172.25.2.168/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8e10aeba-c5ee-45f2-95b5-0b1e28940d7a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e10aeba-c5ee-45f2-95b5-0b1e28940d7a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xdzm2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xdzm2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-23-90.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-23-90.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:26Z"}],"hostIP":"172.31.23.90","podIP":"172.25.2.168","podIPs":[{"ip":"172.25.2.168"}],"startTime":"2022-08-29T16:19:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-29T16:19:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://484d813f44eb795f0fb4e2370135a4c3d9220e039ba812635b8cc2c3cd66d1e6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-9t2jg","generateName":"daemon-set-","namespace":"daemonsets-1573","uid":"48808815-5219-4675-9c8e-f0b5f4544cc3","resourceVersion":"29788","creationTimestamp":"2022-08-29T16:19:26Z","deletionTimestamp":"2022-08-29T16:19:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8757a51427160511da8d230f4efe6705e12b69bbf9cbf1d5851070a8ab2452cc","cni.projectcalico.org/podIP":"172.25.0.64/32","cni.projectcalico.org/podIPs":"172.25.0.64/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8e10aeba-c5ee-45f2-95b5-0b1e28940d7a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e10aeba-c5ee-45f2-95b5-0b1e28940d7a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5nnfw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5nnfw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-16-14.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-16-14.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:26Z"}],"hostIP":"172.31.16.14","podIP":"172.25.0.64","podIPs":[{"ip":"172.25.0.64"}],"startTime":"2022-08-29T16:19:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-29T16:19:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://470ae4f45169cfb207d5d3bd53c0350db6c35b95107476f82b0f2da56aa49358","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-zbzdp","generateName":"daemon-set-","namespace":"daemonsets-1573","uid":"937282b1-0b1a-44ce-945a-ea0bef0fb580","resourceVersion":"29789","creationTimestamp":"2022-08-29T16:19:26Z","deletionTimestamp":"2022-08-29T16:19:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"613d0cb81df96106e0e3ce06168a07934cbbc2b394feb30795d3ce5deb16abc0","cni.projectcalico.org/podIP":"172.25.1.174/32","cni.projectcalico.org/podIPs":"172.25.1.174/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8e10aeba-c5ee-45f2-95b5-0b1e28940d7a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e10aeba-c5ee-45f2-95b5-0b1e28940d7a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-29T16:19:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8j9zj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8j9zj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-20-142.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-20-142.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T16:19:26Z"}],"hostIP":"172.31.20.142","podIP":"172.25.1.174","podIPs":[{"ip":"172.25.1.174"}],"startTime":"2022-08-29T16:19:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-29T16:19:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://adf7a201b9fb8b9e73fe02142b4202782d69ddfb96921ab7b6551ff8f22803c5","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:28.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1573" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":236,"skipped":4237,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:28.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:45.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1128" for this suite.

• [SLOW TEST:17.213 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":237,"skipped":4248,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:45.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:19:45.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1" in namespace "projected-7173" to be "Succeeded or Failed"
Aug 29 16:19:45.671: INFO: Pod "downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.69935ms
Aug 29 16:19:47.679: INFO: Pod "downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01935296s
Aug 29 16:19:49.689: INFO: Pod "downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029504777s
Aug 29 16:19:51.699: INFO: Pod "downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039353021s
STEP: Saw pod success
Aug 29 16:19:51.699: INFO: Pod "downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1" satisfied condition "Succeeded or Failed"
Aug 29 16:19:51.857: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1 container client-container: <nil>
STEP: delete the pod
Aug 29 16:19:51.901: INFO: Waiting for pod downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1 to disappear
Aug 29 16:19:51.909: INFO: Pod downwardapi-volume-fb2119a2-a321-4438-8c43-1367e51382b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:51.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7173" for this suite.

• [SLOW TEST:6.335 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4262,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:51.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Aug 29 16:19:52.011: INFO: Waiting up to 5m0s for pod "downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b" in namespace "downward-api-9678" to be "Succeeded or Failed"
Aug 29 16:19:52.023: INFO: Pod "downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.539675ms
Aug 29 16:19:54.038: INFO: Pod "downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026948539s
Aug 29 16:19:56.058: INFO: Pod "downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047179267s
STEP: Saw pod success
Aug 29 16:19:56.058: INFO: Pod "downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b" satisfied condition "Succeeded or Failed"
Aug 29 16:19:56.066: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:19:56.106: INFO: Waiting for pod downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b to disappear
Aug 29 16:19:56.118: INFO: Pod downward-api-aa50986c-c482-4d72-b17d-1ab5d563596b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:19:56.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9678" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:19:56.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:19:56.235: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666" in namespace "security-context-test-4347" to be "Succeeded or Failed"
Aug 29 16:19:56.243: INFO: Pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666": Phase="Pending", Reason="", readiness=false. Elapsed: 7.92427ms
Aug 29 16:19:58.256: INFO: Pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021263105s
Aug 29 16:20:00.271: INFO: Pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035988317s
Aug 29 16:20:02.281: INFO: Pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045815871s
Aug 29 16:20:02.281: INFO: Pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666" satisfied condition "Succeeded or Failed"
Aug 29 16:20:02.298: INFO: Got logs for pod "busybox-privileged-false-3906451a-0758-416d-90a3-114fce9f0666": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:20:02.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4347" for this suite.

• [SLOW TEST:6.168 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:232
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:20:02.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
Aug 29 16:20:02.418: INFO: created test-event-1
Aug 29 16:20:02.429: INFO: created test-event-2
Aug 29 16:20:02.439: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 29 16:20:02.447: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 29 16:20:02.490: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:20:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9878" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":241,"skipped":4362,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:20:02.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Aug 29 16:20:02.619: INFO: Waiting up to 5m0s for pod "downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c" in namespace "downward-api-660" to be "Succeeded or Failed"
Aug 29 16:20:02.632: INFO: Pod "downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.509634ms
Aug 29 16:20:04.647: INFO: Pod "downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026751404s
Aug 29 16:20:06.663: INFO: Pod "downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042804853s
STEP: Saw pod success
Aug 29 16:20:06.663: INFO: Pod "downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c" satisfied condition "Succeeded or Failed"
Aug 29 16:20:06.671: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:20:06.710: INFO: Waiting for pod downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c to disappear
Aug 29 16:20:06.724: INFO: Pod downward-api-03113b6e-5d0f-4b9e-a3b6-4c3dd2e0305c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:20:06.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-660" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4375,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:20:06.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0829 16:20:08.018625      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 16:20:08.018: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:20:08.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4680" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":243,"skipped":4393,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:20:08.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 29 16:20:08.163: INFO: Waiting up to 5m0s for pod "pod-e5cee5b3-e917-4545-9e90-55ad12cbd203" in namespace "emptydir-8176" to be "Succeeded or Failed"
Aug 29 16:20:08.186: INFO: Pod "pod-e5cee5b3-e917-4545-9e90-55ad12cbd203": Phase="Pending", Reason="", readiness=false. Elapsed: 21.885756ms
Aug 29 16:20:10.249: INFO: Pod "pod-e5cee5b3-e917-4545-9e90-55ad12cbd203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084920867s
Aug 29 16:20:12.262: INFO: Pod "pod-e5cee5b3-e917-4545-9e90-55ad12cbd203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098543523s
STEP: Saw pod success
Aug 29 16:20:12.263: INFO: Pod "pod-e5cee5b3-e917-4545-9e90-55ad12cbd203" satisfied condition "Succeeded or Failed"
Aug 29 16:20:12.273: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-e5cee5b3-e917-4545-9e90-55ad12cbd203 container test-container: <nil>
STEP: delete the pod
Aug 29 16:20:12.324: INFO: Waiting for pod pod-e5cee5b3-e917-4545-9e90-55ad12cbd203 to disappear
Aug 29 16:20:12.331: INFO: Pod pod-e5cee5b3-e917-4545-9e90-55ad12cbd203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:20:12.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8176" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":244,"skipped":4409,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:20:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 29 16:20:12.459: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 16:21:12.532: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Aug 29 16:21:13.144: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 29 16:21:13.159: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 29 16:21:13.216: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 29 16:21:13.232: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 29 16:21:13.266: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 29 16:21:13.279: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:21:25.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1959" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:73.291 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":245,"skipped":4415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:21:25.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Aug 29 16:21:25.703: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:21:30.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4915" for this suite.

• [SLOW TEST:5.028 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":246,"skipped":4464,"failed":0}
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:21:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 29 16:21:30.836: INFO: Waiting up to 5m0s for pod "pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76" in namespace "emptydir-9205" to be "Succeeded or Failed"
Aug 29 16:21:30.979: INFO: Pod "pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76": Phase="Pending", Reason="", readiness=false. Elapsed: 142.804503ms
Aug 29 16:21:32.995: INFO: Pod "pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159675334s
Aug 29 16:21:35.287: INFO: Pod "pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45157507s
Aug 29 16:21:37.309: INFO: Pod "pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.473092336s
STEP: Saw pod success
Aug 29 16:21:37.309: INFO: Pod "pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76" satisfied condition "Succeeded or Failed"
Aug 29 16:21:37.317: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76 container test-container: <nil>
STEP: delete the pod
Aug 29 16:21:37.360: INFO: Waiting for pod pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76 to disappear
Aug 29 16:21:37.370: INFO: Pod pod-b90ac21f-ffe0-4ab8-8e32-3e3fa55fbc76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:21:37.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9205" for this suite.

• [SLOW TEST:6.718 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:21:37.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2445.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2445.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2445.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2445.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 233.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.233_udp@PTR;check="$$(dig +tcp +noall +answer +search 233.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.233_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2445.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2445.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2445.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2445.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2445.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 233.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.233_udp@PTR;check="$$(dig +tcp +noall +answer +search 233.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.233_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:21:40.304: INFO: Unable to read wheezy_udp@dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.313: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.337: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.382: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.499: INFO: Unable to read jessie_udp@dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.531: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.545: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local from pod dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293: the server could not find the requested resource (get pods dns-test-f0178353-3753-49d0-ac7d-e358261dc293)
Aug 29 16:21:40.624: INFO: Lookups using dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293 failed for: [wheezy_udp@dns-test-service.dns-2445.svc.cluster.local wheezy_tcp@dns-test-service.dns-2445.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local jessie_udp@dns-test-service.dns-2445.svc.cluster.local jessie_tcp@dns-test-service.dns-2445.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2445.svc.cluster.local]

Aug 29 16:21:45.815: INFO: DNS probes using dns-2445/dns-test-f0178353-3753-49d0-ac7d-e358261dc293 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:21:45.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2445" for this suite.

• [SLOW TEST:8.584 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":248,"skipped":4488,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:21:45.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Aug 29 16:21:46.666: INFO: Waiting up to 5m0s for pod "downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25" in namespace "downward-api-5825" to be "Succeeded or Failed"
Aug 29 16:21:46.683: INFO: Pod "downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25": Phase="Pending", Reason="", readiness=false. Elapsed: 17.422863ms
Aug 29 16:21:48.698: INFO: Pod "downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032284713s
Aug 29 16:21:50.795: INFO: Pod "downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.128916993s
STEP: Saw pod success
Aug 29 16:21:50.795: INFO: Pod "downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25" satisfied condition "Succeeded or Failed"
Aug 29 16:21:50.812: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25 container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:21:50.852: INFO: Waiting for pod downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25 to disappear
Aug 29 16:21:50.857: INFO: Pod downward-api-1045634b-ce69-4b4e-8c37-8eb6c9dd2c25 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:21:50.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5825" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4506,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:21:50.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:21:50.933: INFO: Creating deployment "webserver-deployment"
Aug 29 16:21:50.948: INFO: Waiting for observed generation 1
Aug 29 16:21:52.979: INFO: Waiting for all required pods to come up
Aug 29 16:21:52.990: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 29 16:21:55.021: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 29 16:21:55.040: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 29 16:21:55.064: INFO: Updating deployment webserver-deployment
Aug 29 16:21:55.064: INFO: Waiting for observed generation 2
Aug 29 16:21:57.305: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 29 16:21:57.318: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 29 16:21:57.334: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 29 16:21:57.391: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 29 16:21:57.391: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 29 16:21:57.405: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 29 16:21:57.428: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 29 16:21:57.428: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 29 16:21:57.451: INFO: Updating deployment webserver-deployment
Aug 29 16:21:57.451: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 29 16:21:57.491: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 29 16:21:59.552: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 16:21:59.587: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8141  dcb96e95-451e-462f-b303-bf3eae0c96cc 31185 3 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00551db38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-29 16:21:57 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-08-29 16:21:57 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 29 16:21:59.596: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-8141  6165b98e-d234-4030-908e-fd165a16f096 31180 3 2022-08-29 16:21:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment dcb96e95-451e-462f-b303-bf3eae0c96cc 0xc0050469c7 0xc0050469c8}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcb96e95-451e-462f-b303-bf3eae0c96cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005046a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:21:59.597: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 29 16:21:59.607: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-8141  d25adef5-52db-4320-a4e0-3da7bacb0a54 31160 3 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment dcb96e95-451e-462f-b303-bf3eae0c96cc 0xc005046ac7 0xc005046ac8}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcb96e95-451e-462f-b303-bf3eae0c96cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005046b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:21:59.623: INFO: Pod "webserver-deployment-566f96c878-4qjjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-4qjjg webserver-deployment-566f96c878- deployment-8141  624d2242-0466-4110-911f-83ba314e773b 31109 0 2022-08-29 16:21:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:360dec7b383ba060a507912fd72b9e859ecec58d9354691e53898f70f14606dc cni.projectcalico.org/podIP:172.25.2.181/32 cni.projectcalico.org/podIPs:172.25.2.181/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc005047067 0xc005047068}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jgpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jgpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.623: INFO: Pod "webserver-deployment-566f96c878-5b84s" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-5b84s webserver-deployment-566f96c878- deployment-8141  49fc0005-3307-4c4b-93df-263d6bbcb0a7 31222 0 2022-08-29 16:21:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:3a4140f8c30c0fdf3b00d3e097ba7bde3bae5929387bfb7ee6e25bc7815aad6e cni.projectcalico.org/podIP:172.25.2.182/32 cni.projectcalico.org/podIPs:172.25.2.182/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc005047287 0xc005047288}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxbvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxbvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.623: INFO: Pod "webserver-deployment-566f96c878-6r4zj" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-6r4zj webserver-deployment-566f96c878- deployment-8141  2d316e04-e245-4576-a115-0a55b5fe53dc 31104 0 2022-08-29 16:21:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:3b291932d2d8f003c789808bb10950bcaeed9614451f8d0b10fccc7780c4a41a cni.projectcalico.org/podIP:172.25.0.70/32 cni.projectcalico.org/podIPs:172.25.0.70/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0050474a7 0xc0050474a8}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shn4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shn4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.624: INFO: Pod "webserver-deployment-566f96c878-9l74x" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9l74x webserver-deployment-566f96c878- deployment-8141  ff4ae0f1-458e-49b2-868b-633f5d112c40 31218 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:dd8f8c4f528d8f0e908d8fae15431c877c90bd73ebe8e943fb6797d1fce066df cni.projectcalico.org/podIP:172.25.0.71/32 cni.projectcalico.org/podIPs:172.25.0.71/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0050476c7 0xc0050476c8}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkhpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkhpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.624: INFO: Pod "webserver-deployment-566f96c878-9z6mq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9z6mq webserver-deployment-566f96c878- deployment-8141  eb585695-7dfe-4c00-944f-be9af90934bb 31244 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:712cbbf72b1b5476ed7da319b519b65d6647da0c26051164b8a7a3dc8ff92a92 cni.projectcalico.org/podIP:172.25.2.186/32 cni.projectcalico.org/podIPs:172.25.2.186/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0050478e7 0xc0050478e8}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9q5xk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9q5xk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.624: INFO: Pod "webserver-deployment-566f96c878-bwthl" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-bwthl webserver-deployment-566f96c878- deployment-8141  2e1926db-a6b8-4e46-bfe9-183707d04c54 31261 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:b9f3b1366356a64cee19cdd04b86943f4e6e013aae42bce7bd06b1ad90862f10 cni.projectcalico.org/podIP:172.25.0.76/32 cni.projectcalico.org/podIPs:172.25.0.76/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc005047b07 0xc005047b08}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5cpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5cpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.625: INFO: Pod "webserver-deployment-566f96c878-clrb2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-clrb2 webserver-deployment-566f96c878- deployment-8141  1f88e399-a820-48ae-9d3e-0f077438f64c 31231 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:e2205e7a5e7896d5131b1c519c9d2de5cd8d8fa85a9d3b458ac9d448cb48b9bf cni.projectcalico.org/podIP:172.25.2.184/32 cni.projectcalico.org/podIPs:172.25.2.184/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc005047d37 0xc005047d38}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz25p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz25p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.625: INFO: Pod "webserver-deployment-566f96c878-pr6mk" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-pr6mk webserver-deployment-566f96c878- deployment-8141  cf3e6e9d-0777-4212-ae66-c40df211706a 31097 0 2022-08-29 16:21:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:93600fda2c51108d7ce0a77b7232c599b916d086acefd792b82630e084382d57 cni.projectcalico.org/podIP:172.25.1.186/32 cni.projectcalico.org/podIPs:172.25.1.186/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc005047f67 0xc005047f68}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvszw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvszw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.630: INFO: Pod "webserver-deployment-566f96c878-pvgl8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-pvgl8 webserver-deployment-566f96c878- deployment-8141  bf7d5583-7c1c-4e36-9e12-8d33da53ceae 31240 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:9d884646fc51ca478b355524579246c086e1a8e2dd3895ebae4bfd3b8b3795f8 cni.projectcalico.org/podIP:172.25.1.191/32 cni.projectcalico.org/podIPs:172.25.1.191/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0052b6187 0xc0052b6188}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8whxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8whxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.640: INFO: Pod "webserver-deployment-566f96c878-qjt27" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-qjt27 webserver-deployment-566f96c878- deployment-8141  e279b8c2-e836-4631-961e-e365f8c1c835 31255 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:a3107d20353ad25ecd980df6df72c2a68d9e8d73291699443001a4788f62e5cd cni.projectcalico.org/podIP:172.25.0.75/32 cni.projectcalico.org/podIPs:172.25.0.75/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0052b63a7 0xc0052b63a8}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plpff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plpff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.641: INFO: Pod "webserver-deployment-566f96c878-vdtpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-vdtpp webserver-deployment-566f96c878- deployment-8141  14c16ee9-ab91-4ee8-82ed-11d20b89838d 31197 0 2022-08-29 16:21:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:5876e9f8ed02223de6ec4e91655ba0b55f78e9d61c06f94e046b73627fc0667a cni.projectcalico.org/podIP:172.25.1.187/32 cni.projectcalico.org/podIPs:172.25.1.187/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0052b65c7 0xc0052b65c8}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kw47h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kw47h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.641: INFO: Pod "webserver-deployment-566f96c878-wt5fr" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-wt5fr webserver-deployment-566f96c878- deployment-8141  9ec4f0f4-7d9e-410e-836b-c1c4868c3428 31262 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:5afcef1f6fd2dd5857f6b517957e19bfc802758c83551ee3f58dbd5edbf0db84 cni.projectcalico.org/podIP:172.25.1.193/32 cni.projectcalico.org/podIPs:172.25.1.193/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0052b67e7 0xc0052b67e8}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s92h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s92h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.642: INFO: Pod "webserver-deployment-566f96c878-xwt6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-xwt6m webserver-deployment-566f96c878- deployment-8141  7bcaa737-842b-4d23-943c-f63fd86ca8b3 31234 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:12780a1f6afafae41fad98339786db6f20412cc0f73302a895ede481ce44f60c cni.projectcalico.org/podIP:172.25.2.185/32 cni.projectcalico.org/podIPs:172.25.2.185/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 6165b98e-d234-4030-908e-fd165a16f096 0xc0052b6a07 0xc0052b6a08}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6165b98e-d234-4030-908e-fd165a16f096\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhlsk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhlsk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.642: INFO: Pod "webserver-deployment-5d9fdcc779-284xh" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-284xh webserver-deployment-5d9fdcc779- deployment-8141  47ab8047-8581-481f-a44d-82204f8c8715 31000 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:b56959ed58b7193b47994a4be4ab65f5b82c11efab1bfc42e16cca12990f248e cni.projectcalico.org/podIP:172.25.1.183/32 cni.projectcalico.org/podIPs:172.25.1.183/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b6c27 0xc0052b6c28}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-php56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-php56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.183,StartTime:2022-08-29 16:21:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20fddb2608793864a210b4445b059fc5f0593aaf4b78724eb3e0a2f68de62ba3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.643: INFO: Pod "webserver-deployment-5d9fdcc779-2jtf9" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2jtf9 webserver-deployment-5d9fdcc779- deployment-8141  aa2d9d40-8861-4d92-a17a-82df8cb88788 31246 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:218ede7db0d4642e708ac3240c771c83c865507c3b495aba389fbddbe8b4de8f cni.projectcalico.org/podIP:172.25.1.192/32 cni.projectcalico.org/podIPs:172.25.1.192/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b6e47 0xc0052b6e48}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-29flc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-29flc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.651: INFO: Pod "webserver-deployment-5d9fdcc779-b8cgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-b8cgw webserver-deployment-5d9fdcc779- deployment-8141  5f92c5a9-381a-4082-baf8-3441518ea919 31241 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:42220e78a44dc8c69c9bbb6a9c8ce2d36abe938f31614ffa121d3d5fed309b13 cni.projectcalico.org/podIP:172.25.2.187/32 cni.projectcalico.org/podIPs:172.25.2.187/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7047 0xc0052b7048}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8k7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8k7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.651: INFO: Pod "webserver-deployment-5d9fdcc779-c94xw" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-c94xw webserver-deployment-5d9fdcc779- deployment-8141  963ee0c0-c8e4-434c-b5ee-d488e5aaef69 31230 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:b35a74a96940fee4fa1ba2069c36380711326bcff5e6052addbd0ebd052ab83c cni.projectcalico.org/podIP:172.25.2.183/32 cni.projectcalico.org/podIPs:172.25.2.183/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7267 0xc0052b7268}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxqnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxqnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.651: INFO: Pod "webserver-deployment-5d9fdcc779-d8qvg" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-d8qvg webserver-deployment-5d9fdcc779- deployment-8141  f2912654-2fc7-46f4-b9e4-67fc62885e33 31021 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:dc9a4d124a7e67dd6f9cd6b70e7c2e8f1131aea4c373df07557e41fe1f54a49a cni.projectcalico.org/podIP:172.25.2.179/32 cni.projectcalico.org/podIPs:172.25.2.179/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7467 0xc0052b7468}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mfv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mfv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.179,StartTime:2022-08-29 16:21:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ecdd47b1b9d89ec2e85b7df427e7ebe200d3a8c6ab16ff3f329e70c52254b832,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.652: INFO: Pod "webserver-deployment-5d9fdcc779-djrk7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-djrk7 webserver-deployment-5d9fdcc779- deployment-8141  b2068b11-c124-42a9-9177-60c21e1b01d8 31221 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:9b259a4ada8781a9ded9e761280f8eae4e18ca4b9a2bd25b636c76caaa3e5e51 cni.projectcalico.org/podIP:172.25.1.188/32 cni.projectcalico.org/podIPs:172.25.1.188/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7697 0xc0052b7698}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c42hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c42hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.655: INFO: Pod "webserver-deployment-5d9fdcc779-drx5r" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-drx5r webserver-deployment-5d9fdcc779- deployment-8141  c2c836de-12fd-42df-a9b5-751e609a5bd5 31232 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:6ceb2defc5207f4996f6f98af21ec009e4bd44c63077bd3fe916ac0fbd99af72 cni.projectcalico.org/podIP:172.25.0.74/32 cni.projectcalico.org/podIPs:172.25.0.74/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7897 0xc0052b7898}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7b4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7b4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.656: INFO: Pod "webserver-deployment-5d9fdcc779-dwv6t" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-dwv6t webserver-deployment-5d9fdcc779- deployment-8141  0eef3229-b035-42f1-bbca-eed0c417a9f3 31263 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:494fdab0f3340e2cf36a6941b21df58fac5145ac792e5bf9fa808d4bd888a951 cni.projectcalico.org/podIP:172.25.0.77/32 cni.projectcalico.org/podIPs:172.25.0.77/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7a30 0xc0052b7a31}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr6xb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr6xb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.656: INFO: Pod "webserver-deployment-5d9fdcc779-gh4d2" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gh4d2 webserver-deployment-5d9fdcc779- deployment-8141  82d885bd-6f0a-473d-9874-2cef56bcdd96 30991 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:1eacabd435c54eb3835e89f0cd847dc761e5972af5ba3fdfac6c785a0e9b8fa8 cni.projectcalico.org/podIP:172.25.0.68/32 cni.projectcalico.org/podIPs:172.25.0.68/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7c37 0xc0052b7c38}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-767fw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-767fw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:172.25.0.68,StartTime:2022-08-29 16:21:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://90dbc5e58a54117c39ee4aa30397d13f8596a4731694cda326dc91a398c4e16d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.656: INFO: Pod "webserver-deployment-5d9fdcc779-h7bc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-h7bc4 webserver-deployment-5d9fdcc779- deployment-8141  f28f8e62-d75c-43e3-9676-159d2b900ca6 31220 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:dc425171c346b79e1b3bc3f1701e94d6c53b61d3c4a3494354a1f92d02d16a7c cni.projectcalico.org/podIP:172.25.0.72/32 cni.projectcalico.org/podIPs:172.25.0.72/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0052b7e50 0xc0052b7e51}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4jc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4jc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.657: INFO: Pod "webserver-deployment-5d9fdcc779-hmpgl" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hmpgl webserver-deployment-5d9fdcc779- deployment-8141  8274213d-620a-46a5-9c40-a3e02a4ddb83 31018 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:0bc029a631b701107a0e262ab8b90bde90618c68c9f3588ee62cd73afb2ffea7 cni.projectcalico.org/podIP:172.25.2.180/32 cni.projectcalico.org/podIPs:172.25.2.180/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cc057 0xc0056cc058}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf257,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf257,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.180,StartTime:2022-08-29 16:21:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7626ef7eb9fc2bd2d251d816ffc798f303712a8e2086a22b439ef23f62f2cf85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.662: INFO: Pod "webserver-deployment-5d9fdcc779-jtjln" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jtjln webserver-deployment-5d9fdcc779- deployment-8141  50c49216-2b7f-44e8-af6e-ce98a43634af 31242 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:e2191bec945b23db5bcc9cc20337ebcf4045e33294707135170c40ed1d8b20a3 cni.projectcalico.org/podIP:172.25.2.188/32 cni.projectcalico.org/podIPs:172.25.2.188/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cc287 0xc0056cc288}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nsw8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nsw8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.663: INFO: Pod "webserver-deployment-5d9fdcc779-lp5vm" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-lp5vm webserver-deployment-5d9fdcc779- deployment-8141  dfb65447-09dd-4228-b2e7-bf8cdd8e1eac 31024 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:a64646c871eb7d1c172fb02c29098b77ecf65a12c239517a8a364f23e2eb4f1a cni.projectcalico.org/podIP:172.25.2.178/32 cni.projectcalico.org/podIPs:172.25.2.178/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cc487 0xc0056cc488}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5kptk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kptk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.178,StartTime:2022-08-29 16:21:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://21e2d3cc8705e812d73536d370d7cbaf81026568c76882d671f1926bce1615ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.663: INFO: Pod "webserver-deployment-5d9fdcc779-lrcqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-lrcqz webserver-deployment-5d9fdcc779- deployment-8141  7fd93e44-2ba2-423d-919c-babb5d3667ae 31172 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cc687 0xc0056cc688}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndkzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndkzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.663: INFO: Pod "webserver-deployment-5d9fdcc779-qhwl7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-qhwl7 webserver-deployment-5d9fdcc779- deployment-8141  b7934c44-660c-4137-b936-00ac3225a1f3 31229 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:c352d33eb6f55e2edcd1ade1255f789489f51d28f7184757fa8dfd7881df3aa3 cni.projectcalico.org/podIP:172.25.1.189/32 cni.projectcalico.org/podIPs:172.25.1.189/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cc867 0xc0056cc868}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-886wd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-886wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.663: INFO: Pod "webserver-deployment-5d9fdcc779-rbkml" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rbkml webserver-deployment-5d9fdcc779- deployment-8141  1d2d788a-07ae-4e69-92b1-c592a993173b 31015 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:a7ab9344b08ed9c8a2c548ca63a5a07a98059526cd709a8cadf925f97e403138 cni.projectcalico.org/podIP:172.25.2.177/32 cni.projectcalico.org/podIPs:172.25.2.177/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cca00 0xc0056cca01}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdvrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdvrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.177,StartTime:2022-08-29 16:21:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://53d569652e4626e4036af1735b60598d677053e78aba39e7216db0fbc68dffcc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.664: INFO: Pod "webserver-deployment-5d9fdcc779-rcx2w" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rcx2w webserver-deployment-5d9fdcc779- deployment-8141  d3a78c6f-091a-4821-8563-30f384de808a 30986 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:997721ba5cf078cca64e7292b3c1561be4a099f6f2e9e9d67a48f428807f35b7 cni.projectcalico.org/podIP:172.25.0.69/32 cni.projectcalico.org/podIPs:172.25.0.69/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056ccc37 0xc0056ccc38}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6www5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6www5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:172.25.0.69,StartTime:2022-08-29 16:21:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e9921addb8ae39fdc95922ab964eb0faec6774fde1e45ac1a5ca4f28982bedfe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.664: INFO: Pod "webserver-deployment-5d9fdcc779-rmbd5" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rmbd5 webserver-deployment-5d9fdcc779- deployment-8141  bd35a94c-a690-4123-8fc7-82334e55530e 31003 0 2022-08-29 16:21:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:e155885fa0e6eb71a8f37103d90c904f1ef262653fc0805e2ea188d0ee6685a3 cni.projectcalico.org/podIP:172.25.1.182/32 cni.projectcalico.org/podIPs:172.25.1.182/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cce50 0xc0056cce51}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:21:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:21:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blnlr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blnlr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.182,StartTime:2022-08-29 16:21:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:21:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a79bc93def73f10b792bdfd79d63e6d2a9c6a28efc4af5c4ec4986bccf95e8a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.664: INFO: Pod "webserver-deployment-5d9fdcc779-t5pc9" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-t5pc9 webserver-deployment-5d9fdcc779- deployment-8141  b591c1cb-ba5a-4af6-9de0-23d2934f7a4d 31228 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:f719e640cede10a6e31c251cf0683e9daa17bab075c8f3798aeeb83292c473e4 cni.projectcalico.org/podIP:172.25.0.73/32 cni.projectcalico.org/podIPs:172.25.0.73/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cd077 0xc0056cd078}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22zf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22zf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-14.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.14,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:21:59.665: INFO: Pod "webserver-deployment-5d9fdcc779-v94pz" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-v94pz webserver-deployment-5d9fdcc779- deployment-8141  9d7f5207-6278-49bf-a906-09a06cf612f6 31227 0 2022-08-29 16:21:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:cab3a06b939a81ab1cab4bec6aae8f7d6efd391ccccf2fcebd8102e052a865b2 cni.projectcalico.org/podIP:172.25.1.190/32 cni.projectcalico.org/podIPs:172.25.1.190/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 d25adef5-52db-4320-a4e0-3da7bacb0a54 0xc0056cd277 0xc0056cd278}] []  [{kube-controller-manager Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d25adef5-52db-4320-a4e0-3da7bacb0a54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:21:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h24cl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h24cl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:21:59.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8141" for this suite.

• [SLOW TEST:9.567 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":250,"skipped":4516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:00.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:28.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1160" for this suite.

• [SLOW TEST:28.213 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":251,"skipped":4539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:28.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 29 16:22:28.797: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:33.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8428" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4577,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:33.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-710b355b-8a50-44c5-be73-ec8881a17bdd
STEP: Creating a pod to test consume configMaps
Aug 29 16:22:33.584: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e" in namespace "projected-8984" to be "Succeeded or Failed"
Aug 29 16:22:33.602: INFO: Pod "pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.715963ms
Aug 29 16:22:35.616: INFO: Pod "pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031246666s
Aug 29 16:22:37.671: INFO: Pod "pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08678277s
STEP: Saw pod success
Aug 29 16:22:37.671: INFO: Pod "pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e" satisfied condition "Succeeded or Failed"
Aug 29 16:22:37.777: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:22:37.935: INFO: Waiting for pod pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e to disappear
Aug 29 16:22:37.941: INFO: Pod pod-projected-configmaps-8e44455f-e864-46d9-9695-757cabc6079e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:37.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8984" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":253,"skipped":4577,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:37.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:40.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9487" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":254,"skipped":4589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:40.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:22:40.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56024564-802f-4893-9444-399741e68c59" in namespace "downward-api-7643" to be "Succeeded or Failed"
Aug 29 16:22:40.245: INFO: Pod "downwardapi-volume-56024564-802f-4893-9444-399741e68c59": Phase="Pending", Reason="", readiness=false. Elapsed: 10.743578ms
Aug 29 16:22:42.258: INFO: Pod "downwardapi-volume-56024564-802f-4893-9444-399741e68c59": Phase="Running", Reason="", readiness=false. Elapsed: 2.024244116s
Aug 29 16:22:44.268: INFO: Pod "downwardapi-volume-56024564-802f-4893-9444-399741e68c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034080158s
STEP: Saw pod success
Aug 29 16:22:44.268: INFO: Pod "downwardapi-volume-56024564-802f-4893-9444-399741e68c59" satisfied condition "Succeeded or Failed"
Aug 29 16:22:44.275: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod downwardapi-volume-56024564-802f-4893-9444-399741e68c59 container client-container: <nil>
STEP: delete the pod
Aug 29 16:22:44.316: INFO: Waiting for pod downwardapi-volume-56024564-802f-4893-9444-399741e68c59 to disappear
Aug 29 16:22:44.330: INFO: Pod downwardapi-volume-56024564-802f-4893-9444-399741e68c59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:44.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7643" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:44.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:22:44.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:45.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9446" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":256,"skipped":4688,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:45.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
Aug 29 16:22:45.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-6054 cluster-info'
Aug 29 16:22:45.793: INFO: stderr: ""
Aug 29 16:22:45.793: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:45.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6054" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":257,"skipped":4688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:46.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 29 16:22:46.180: INFO: Waiting up to 5m0s for pod "pod-060285c2-9263-4b8b-90a6-caf0c61370d3" in namespace "emptydir-6262" to be "Succeeded or Failed"
Aug 29 16:22:46.188: INFO: Pod "pod-060285c2-9263-4b8b-90a6-caf0c61370d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.460541ms
Aug 29 16:22:48.196: INFO: Pod "pod-060285c2-9263-4b8b-90a6-caf0c61370d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015756126s
Aug 29 16:22:50.215: INFO: Pod "pod-060285c2-9263-4b8b-90a6-caf0c61370d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034516505s
STEP: Saw pod success
Aug 29 16:22:50.215: INFO: Pod "pod-060285c2-9263-4b8b-90a6-caf0c61370d3" satisfied condition "Succeeded or Failed"
Aug 29 16:22:50.224: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-060285c2-9263-4b8b-90a6-caf0c61370d3 container test-container: <nil>
STEP: delete the pod
Aug 29 16:22:50.269: INFO: Waiting for pod pod-060285c2-9263-4b8b-90a6-caf0c61370d3 to disappear
Aug 29 16:22:50.276: INFO: Pod pod-060285c2-9263-4b8b-90a6-caf0c61370d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:22:50.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6262" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:22:50.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Aug 29 16:22:50.446: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:22:52.455: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:22:54.458: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.23.90 on the node which pod1 resides and expect scheduled
Aug 29 16:22:54.487: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:22:56.500: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:22:58.503: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.23.90 but use UDP protocol on the node which pod2 resides
Aug 29 16:22:58.523: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:23:00.536: INFO: The status of Pod pod3 is Running (Ready = false)
Aug 29 16:23:02.532: INFO: The status of Pod pod3 is Running (Ready = true)
Aug 29 16:23:02.551: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:23:04.779: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Aug 29 16:23:04.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.23.90 http://127.0.0.1:54323/hostname] Namespace:hostport-5628 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:23:04.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:23:04.788: INFO: ExecWithOptions: Clientset creation
Aug 29 16:23:04.789: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-5628/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.23.90+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.23.90, port: 54323
Aug 29 16:23:04.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.23.90:54323/hostname] Namespace:hostport-5628 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:23:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:23:04.973: INFO: ExecWithOptions: Clientset creation
Aug 29 16:23:04.974: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-5628/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.23.90%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.23.90, port: 54323 UDP
Aug 29 16:23:05.185: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.23.90 54323] Namespace:hostport-5628 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:23:05.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:23:05.189: INFO: ExecWithOptions: Clientset creation
Aug 29 16:23:05.189: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-5628/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+172.31.23.90+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:23:10.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5628" for this suite.

• [SLOW TEST:20.074 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":259,"skipped":4787,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:23:10.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
Aug 29 16:23:10.447: INFO: Waiting up to 5m0s for pod "client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0" in namespace "containers-419" to be "Succeeded or Failed"
Aug 29 16:23:10.455: INFO: Pod "client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.674077ms
Aug 29 16:23:12.471: INFO: Pod "client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023490347s
Aug 29 16:23:14.485: INFO: Pod "client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037885295s
STEP: Saw pod success
Aug 29 16:23:14.485: INFO: Pod "client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0" satisfied condition "Succeeded or Failed"
Aug 29 16:23:14.492: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:23:14.524: INFO: Waiting for pod client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0 to disappear
Aug 29 16:23:14.530: INFO: Pod client-containers-de542695-ca6c-4ccc-a9a8-24e889a587a0 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:23:14.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-419" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":4791,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:23:14.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:23:14.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Creating first CR 
Aug 29 16:23:17.285: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T16:23:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T16:23:17Z]] name:name1 resourceVersion:32166 uid:bbe17365-3551-41f5-ad17-bbde689f57ed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 29 16:23:27.311: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T16:23:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T16:23:27Z]] name:name2 resourceVersion:32222 uid:fde6f28a-33ed-4741-98ef-90315e13397d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 29 16:23:37.351: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T16:23:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T16:23:37Z]] name:name1 resourceVersion:32251 uid:bbe17365-3551-41f5-ad17-bbde689f57ed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 29 16:23:47.373: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T16:23:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T16:23:47Z]] name:name2 resourceVersion:32279 uid:fde6f28a-33ed-4741-98ef-90315e13397d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 29 16:23:57.383: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T16:23:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T16:23:37Z]] name:name1 resourceVersion:32308 uid:bbe17365-3551-41f5-ad17-bbde689f57ed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 29 16:24:07.829: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T16:23:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T16:23:47Z]] name:name2 resourceVersion:32332 uid:fde6f28a-33ed-4741-98ef-90315e13397d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:24:18.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9351" for this suite.

• [SLOW TEST:64.175 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":261,"skipped":4801,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:24:18.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:24:18.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655" in namespace "projected-9780" to be "Succeeded or Failed"
Aug 29 16:24:18.906: INFO: Pod "downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655": Phase="Pending", Reason="", readiness=false. Elapsed: 19.523592ms
Aug 29 16:24:20.916: INFO: Pod "downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029662614s
Aug 29 16:24:22.925: INFO: Pod "downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038681018s
STEP: Saw pod success
Aug 29 16:24:22.925: INFO: Pod "downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655" satisfied condition "Succeeded or Failed"
Aug 29 16:24:22.938: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655 container client-container: <nil>
STEP: delete the pod
Aug 29 16:24:22.978: INFO: Waiting for pod downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655 to disappear
Aug 29 16:24:22.986: INFO: Pod downwardapi-volume-21b443e6-0281-4c0e-a5f7-5aaa8a583655 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:24:22.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9780" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":4820,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:24:23.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-6363077f-53b5-4f20-be8b-093dda194200 in namespace container-probe-9555
Aug 29 16:24:25.151: INFO: Started pod busybox-6363077f-53b5-4f20-be8b-093dda194200 in namespace container-probe-9555
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 16:24:25.158: INFO: Initial restart count of pod busybox-6363077f-53b5-4f20-be8b-093dda194200 is 0
Aug 29 16:25:15.738: INFO: Restart count of pod container-probe-9555/busybox-6363077f-53b5-4f20-be8b-093dda194200 is now 1 (50.579763307s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:25:15.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9555" for this suite.

• [SLOW TEST:52.900 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:25:15.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:25:16.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5" in namespace "downward-api-8210" to be "Succeeded or Failed"
Aug 29 16:25:16.032: INFO: Pod "downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.838504ms
Aug 29 16:25:18.044: INFO: Pod "downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5": Phase="Running", Reason="", readiness=false. Elapsed: 2.026480153s
Aug 29 16:25:20.207: INFO: Pod "downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5": Phase="Running", Reason="", readiness=false. Elapsed: 4.189789891s
Aug 29 16:25:22.312: INFO: Pod "downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.294530294s
STEP: Saw pod success
Aug 29 16:25:22.312: INFO: Pod "downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5" satisfied condition "Succeeded or Failed"
Aug 29 16:25:22.335: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5 container client-container: <nil>
STEP: delete the pod
Aug 29 16:25:22.381: INFO: Waiting for pod downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5 to disappear
Aug 29 16:25:22.392: INFO: Pod downwardapi-volume-07100e8a-5833-42e6-a637-556c6cfef9a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:25:22.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8210" for this suite.

• [SLOW TEST:6.601 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4855,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:25:22.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:25:22.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 29 16:25:34.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-3892 --namespace=crd-publish-openapi-3892 create -f -'
Aug 29 16:25:35.744: INFO: stderr: ""
Aug 29 16:25:35.744: INFO: stdout: "e2e-test-crd-publish-openapi-5377-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 29 16:25:35.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-3892 --namespace=crd-publish-openapi-3892 delete e2e-test-crd-publish-openapi-5377-crds test-cr'
Aug 29 16:25:35.855: INFO: stderr: ""
Aug 29 16:25:35.855: INFO: stdout: "e2e-test-crd-publish-openapi-5377-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 29 16:25:35.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-3892 --namespace=crd-publish-openapi-3892 apply -f -'
Aug 29 16:25:36.152: INFO: stderr: ""
Aug 29 16:25:36.152: INFO: stdout: "e2e-test-crd-publish-openapi-5377-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 29 16:25:36.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-3892 --namespace=crd-publish-openapi-3892 delete e2e-test-crd-publish-openapi-5377-crds test-cr'
Aug 29 16:25:36.268: INFO: stderr: ""
Aug 29 16:25:36.268: INFO: stdout: "e2e-test-crd-publish-openapi-5377-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 29 16:25:36.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=crd-publish-openapi-3892 explain e2e-test-crd-publish-openapi-5377-crds'
Aug 29 16:25:36.484: INFO: stderr: ""
Aug 29 16:25:36.484: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5377-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:25:49.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3892" for this suite.

• [SLOW TEST:26.640 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":265,"skipped":4862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:25:49.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 29 16:25:51.802: INFO: Successfully updated pod "adopt-release-djcx4"
STEP: Checking that the Job readopts the Pod
Aug 29 16:25:51.802: INFO: Waiting up to 15m0s for pod "adopt-release-djcx4" in namespace "job-95" to be "adopted"
Aug 29 16:25:51.811: INFO: Pod "adopt-release-djcx4": Phase="Running", Reason="", readiness=true. Elapsed: 9.074113ms
Aug 29 16:25:53.826: INFO: Pod "adopt-release-djcx4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023606367s
Aug 29 16:25:53.826: INFO: Pod "adopt-release-djcx4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 29 16:25:54.356: INFO: Successfully updated pod "adopt-release-djcx4"
STEP: Checking that the Job releases the Pod
Aug 29 16:25:54.356: INFO: Waiting up to 15m0s for pod "adopt-release-djcx4" in namespace "job-95" to be "released"
Aug 29 16:25:54.369: INFO: Pod "adopt-release-djcx4": Phase="Running", Reason="", readiness=true. Elapsed: 12.727131ms
Aug 29 16:25:54.369: INFO: Pod "adopt-release-djcx4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:25:54.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-95" for this suite.

• [SLOW TEST:5.227 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":266,"skipped":4886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:25:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9478
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9478
STEP: Waiting until pod test-pod will start running in namespace statefulset-9478
STEP: Creating statefulset with conflicting port in namespace statefulset-9478
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9478
Aug 29 16:25:56.970: INFO: Observed stateful pod in namespace: statefulset-9478, name: ss-0, uid: 2b00f04e-c828-4786-a516-c723cc483b62, status phase: Pending. Waiting for statefulset controller to delete.
Aug 29 16:25:56.994: INFO: Observed stateful pod in namespace: statefulset-9478, name: ss-0, uid: 2b00f04e-c828-4786-a516-c723cc483b62, status phase: Failed. Waiting for statefulset controller to delete.
Aug 29 16:25:57.008: INFO: Observed stateful pod in namespace: statefulset-9478, name: ss-0, uid: 2b00f04e-c828-4786-a516-c723cc483b62, status phase: Failed. Waiting for statefulset controller to delete.
Aug 29 16:25:57.022: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9478
STEP: Removing pod with conflicting port in namespace statefulset-9478
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9478 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 16:25:59.071: INFO: Deleting all statefulset in ns statefulset-9478
Aug 29 16:25:59.081: INFO: Scaling statefulset ss to 0
Aug 29 16:26:09.133: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:26:09.141: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:26:09.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9478" for this suite.

• [SLOW TEST:14.782 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":267,"skipped":4923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:26:09.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5029
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5029
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5029
Aug 29 16:26:09.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 29 16:26:19.385: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 29 16:26:19.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:26:19.719: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:26:19.719: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:26:19.719: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:26:19.728: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 29 16:26:29.760: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:26:29.761: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:26:29.802: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999957s
Aug 29 16:26:30.814: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989386866s
Aug 29 16:26:31.824: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.977118408s
Aug 29 16:26:32.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966849191s
Aug 29 16:26:33.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.947208273s
Aug 29 16:26:34.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.932078822s
Aug 29 16:26:35.879: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.92106255s
Aug 29 16:26:37.426: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.911647482s
Aug 29 16:26:38.439: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.364603913s
Aug 29 16:26:39.450: INFO: Verifying statefulset ss doesn't scale past 1 for another 351.676506ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5029
Aug 29 16:26:40.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:26:40.689: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:26:40.689: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:26:40.689: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:26:40.702: INFO: Found 1 stateful pods, waiting for 3
Aug 29 16:26:50.731: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:26:50.732: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:26:50.732: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 29 16:26:50.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:26:51.096: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:26:51.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:26:51.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:26:51.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:26:51.522: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:26:51.522: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:26:51.522: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:26:51.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:26:51.780: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:26:51.780: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:26:51.780: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:26:51.780: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:26:51.791: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 29 16:27:01.828: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:27:01.829: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:27:01.830: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:27:01.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999951s
Aug 29 16:27:02.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99011094s
Aug 29 16:27:03.897: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976498949s
Aug 29 16:27:04.912: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955325768s
Aug 29 16:27:05.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.938813331s
Aug 29 16:27:06.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.925377059s
Aug 29 16:27:07.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.909162257s
Aug 29 16:27:08.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897587085s
Aug 29 16:27:09.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.881339457s
Aug 29 16:27:10.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 868.893423ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5029
Aug 29 16:27:12.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:27:12.273: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:27:12.273: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:27:12.273: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:27:12.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:27:12.522: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:27:12.522: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:27:12.522: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:27:12.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=statefulset-5029 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:27:12.852: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:27:12.852: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:27:12.852: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:27:12.852: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Aug 29 16:27:22.915: INFO: Deleting all statefulset in ns statefulset-5029
Aug 29 16:27:22.921: INFO: Scaling statefulset ss to 0
Aug 29 16:27:22.947: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:27:22.952: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:27:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5029" for this suite.

• [SLOW TEST:73.771 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":268,"skipped":4986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:27:23.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:27:34.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5778" for this suite.

• [SLOW TEST:11.989 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":269,"skipped":5028,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:27:35.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-6940b441-ec36-4f36-aa8c-aba90cca00c1
STEP: Creating a pod to test consume secrets
Aug 29 16:27:35.118: INFO: Waiting up to 5m0s for pod "pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40" in namespace "secrets-6105" to be "Succeeded or Failed"
Aug 29 16:27:35.124: INFO: Pod "pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.891948ms
Aug 29 16:27:37.137: INFO: Pod "pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0194373s
Aug 29 16:27:39.150: INFO: Pod "pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03288892s
STEP: Saw pod success
Aug 29 16:27:39.151: INFO: Pod "pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40" satisfied condition "Succeeded or Failed"
Aug 29 16:27:39.158: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:27:39.218: INFO: Waiting for pod pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40 to disappear
Aug 29 16:27:39.225: INFO: Pod pod-secrets-8eff73ea-14d7-4124-aea5-44890840aa40 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:27:39.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6105" for this suite.
STEP: Destroying namespace "secret-namespace-3781" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":5035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:27:39.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Aug 29 16:27:50.495: INFO: 67 pods remaining
Aug 29 16:27:50.495: INFO: 67 pods has nil DeletionTimestamp
Aug 29 16:27:50.495: INFO: 
STEP: Gathering metrics
W0829 16:27:55.520108      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 16:27:55.520: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 29 16:27:55.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-22w49" in namespace "gc-9349"
Aug 29 16:27:55.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-246xw" in namespace "gc-9349"
Aug 29 16:27:55.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bh82" in namespace "gc-9349"
Aug 29 16:27:55.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jzrj" in namespace "gc-9349"
Aug 29 16:27:55.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zj27" in namespace "gc-9349"
Aug 29 16:27:55.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-45gsx" in namespace "gc-9349"
Aug 29 16:27:55.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j6mb" in namespace "gc-9349"
Aug 29 16:27:55.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lrbz" in namespace "gc-9349"
Aug 29 16:27:55.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bbgg" in namespace "gc-9349"
Aug 29 16:27:55.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tm49" in namespace "gc-9349"
Aug 29 16:27:55.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-66bcb" in namespace "gc-9349"
Aug 29 16:27:55.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cm9v" in namespace "gc-9349"
Aug 29 16:27:55.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-6flg8" in namespace "gc-9349"
Aug 29 16:27:55.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gcgr" in namespace "gc-9349"
Aug 29 16:27:55.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-6t9tr" in namespace "gc-9349"
Aug 29 16:27:55.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-79g2r" in namespace "gc-9349"
Aug 29 16:27:55.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b4dz" in namespace "gc-9349"
Aug 29 16:27:56.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h7d8" in namespace "gc-9349"
Aug 29 16:27:56.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-845p5" in namespace "gc-9349"
Aug 29 16:27:56.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-868h8" in namespace "gc-9349"
Aug 29 16:27:56.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-89d6c" in namespace "gc-9349"
Aug 29 16:27:56.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cpn8" in namespace "gc-9349"
Aug 29 16:27:56.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qcl8" in namespace "gc-9349"
Aug 29 16:27:56.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qzb7" in namespace "gc-9349"
Aug 29 16:27:56.187: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xbxw" in namespace "gc-9349"
Aug 29 16:27:56.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rk8g" in namespace "gc-9349"
Aug 29 16:27:56.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zp5s" in namespace "gc-9349"
Aug 29 16:27:56.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zqtt" in namespace "gc-9349"
Aug 29 16:27:56.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zwnq" in namespace "gc-9349"
Aug 29 16:27:56.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-blbfl" in namespace "gc-9349"
Aug 29 16:27:56.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-brwzg" in namespace "gc-9349"
Aug 29 16:27:56.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4q8p" in namespace "gc-9349"
Aug 29 16:27:56.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbx7g" in namespace "gc-9349"
Aug 29 16:27:56.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjnks" in namespace "gc-9349"
Aug 29 16:27:56.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh6mb" in namespace "gc-9349"
Aug 29 16:27:56.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-djff6" in namespace "gc-9349"
Aug 29 16:27:56.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-djkgc" in namespace "gc-9349"
Aug 29 16:27:56.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-dk6x9" in namespace "gc-9349"
Aug 29 16:27:56.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnvj9" in namespace "gc-9349"
Aug 29 16:27:56.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4gbc" in namespace "gc-9349"
Aug 29 16:27:56.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-f94hv" in namespace "gc-9349"
Aug 29 16:27:56.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgg75" in namespace "gc-9349"
Aug 29 16:27:56.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsbhw" in namespace "gc-9349"
Aug 29 16:27:56.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjkvh" in namespace "gc-9349"
Aug 29 16:27:56.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpzpp" in namespace "gc-9349"
Aug 29 16:27:56.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwq6g" in namespace "gc-9349"
Aug 29 16:27:56.842: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2gnz" in namespace "gc-9349"
Aug 29 16:27:56.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-l69zk" in namespace "gc-9349"
Aug 29 16:27:56.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-l88rg" in namespace "gc-9349"
Aug 29 16:27:56.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-lt68r" in namespace "gc-9349"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:27:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9349" for this suite.

• [SLOW TEST:17.998 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":271,"skipped":5057,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:27:57.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 29 16:27:57.347: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 29 16:28:02.571: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:28:03.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9759" for this suite.

• [SLOW TEST:6.376 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":272,"skipped":5074,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:28:03.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:28:04.584: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 16:28:06.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:28:08.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 28, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:28:11.922: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:28:12.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6908" for this suite.
STEP: Destroying namespace "webhook-6908-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.594 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":273,"skipped":5091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:28:12.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:28:12.972: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:28:16.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:28:16.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9593" for this suite.
STEP: Destroying namespace "webhook-9593-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":274,"skipped":5125,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:28:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:28:16.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08" in namespace "downward-api-7131" to be "Succeeded or Failed"
Aug 29 16:28:16.521: INFO: Pod "downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08": Phase="Pending", Reason="", readiness=false. Elapsed: 15.343118ms
Aug 29 16:28:18.535: INFO: Pod "downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029188791s
Aug 29 16:28:20.548: INFO: Pod "downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041809138s
STEP: Saw pod success
Aug 29 16:28:20.548: INFO: Pod "downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08" satisfied condition "Succeeded or Failed"
Aug 29 16:28:20.555: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08 container client-container: <nil>
STEP: delete the pod
Aug 29 16:28:20.603: INFO: Waiting for pod downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08 to disappear
Aug 29 16:28:20.610: INFO: Pod downwardapi-volume-92941287-7853-496d-bb37-53227f14aa08 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:28:20.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7131" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5131,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:28:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Aug 29 16:28:20.746: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:28:22.760: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Aug 29 16:28:22.786: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:28:24.799: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 29 16:28:24.831: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 16:28:24.854: INFO: Pod pod-with-prestop-http-hook still exists
Aug 29 16:28:26.856: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 16:28:26.870: INFO: Pod pod-with-prestop-http-hook still exists
Aug 29 16:28:28.856: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 16:28:28.876: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:28:28.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6471" for this suite.

• [SLOW TEST:8.302 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5133,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:28:28.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:28:36.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2413" for this suite.

• [SLOW TEST:7.157 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":277,"skipped":5147,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:28:36.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a in namespace container-probe-5673
Aug 29 16:28:38.216: INFO: Started pod liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a in namespace container-probe-5673
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 16:28:38.223: INFO: Initial restart count of pod liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a is 0
Aug 29 16:28:58.384: INFO: Restart count of pod container-probe-5673/liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a is now 1 (20.160495017s elapsed)
Aug 29 16:29:19.081: INFO: Restart count of pod container-probe-5673/liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a is now 2 (40.857414801s elapsed)
Aug 29 16:29:39.253: INFO: Restart count of pod container-probe-5673/liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a is now 3 (1m1.029603059s elapsed)
Aug 29 16:29:59.431: INFO: Restart count of pod container-probe-5673/liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a is now 4 (1m21.207768045s elapsed)
Aug 29 16:30:59.892: INFO: Restart count of pod container-probe-5673/liveness-7a2daea8-56cf-4e0c-84ed-24471cb17d6a is now 5 (2m21.668453085s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:30:59.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5673" for this suite.

• [SLOW TEST:143.820 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":5156,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:30:59.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:31:00.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8" in namespace "projected-9771" to be "Succeeded or Failed"
Aug 29 16:31:00.645: INFO: Pod "downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.963376ms
Aug 29 16:31:02.653: INFO: Pod "downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022339745s
Aug 29 16:31:04.676: INFO: Pod "downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044597551s
STEP: Saw pod success
Aug 29 16:31:04.676: INFO: Pod "downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8" satisfied condition "Succeeded or Failed"
Aug 29 16:31:04.684: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8 container client-container: <nil>
STEP: delete the pod
Aug 29 16:31:04.731: INFO: Waiting for pod downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8 to disappear
Aug 29 16:31:04.752: INFO: Pod downwardapi-volume-0717430e-bbd7-48f5-abf2-eeec0272b9e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:04.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9771" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":5173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:04.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:31:04.877: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c186e4db-b37b-496c-a988-f444daac49d9" in namespace "security-context-test-665" to be "Succeeded or Failed"
Aug 29 16:31:04.891: INFO: Pod "busybox-user-65534-c186e4db-b37b-496c-a988-f444daac49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.827819ms
Aug 29 16:31:06.903: INFO: Pod "busybox-user-65534-c186e4db-b37b-496c-a988-f444daac49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02507984s
Aug 29 16:31:08.918: INFO: Pod "busybox-user-65534-c186e4db-b37b-496c-a988-f444daac49d9": Phase="Running", Reason="", readiness=false. Elapsed: 4.040119703s
Aug 29 16:31:10.928: INFO: Pod "busybox-user-65534-c186e4db-b37b-496c-a988-f444daac49d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049720664s
Aug 29 16:31:10.928: INFO: Pod "busybox-user-65534-c186e4db-b37b-496c-a988-f444daac49d9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-665" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":5212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:10.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Aug 29 16:31:11.030: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:16.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7373" for this suite.

• [SLOW TEST:6.060 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":281,"skipped":5245,"failed":0}
SS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:17.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-636" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":282,"skipped":5247,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:17.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-4420/secret-test-5eb58db9-2355-49d8-8e7e-92882d96e3ff
STEP: Creating a pod to test consume secrets
Aug 29 16:31:17.224: INFO: Waiting up to 5m0s for pod "pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1" in namespace "secrets-4420" to be "Succeeded or Failed"
Aug 29 16:31:17.233: INFO: Pod "pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093572ms
Aug 29 16:31:19.250: INFO: Pod "pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025164308s
Aug 29 16:31:21.262: INFO: Pod "pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037671852s
STEP: Saw pod success
Aug 29 16:31:21.262: INFO: Pod "pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1" satisfied condition "Succeeded or Failed"
Aug 29 16:31:21.271: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1 container env-test: <nil>
STEP: delete the pod
Aug 29 16:31:21.310: INFO: Waiting for pod pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1 to disappear
Aug 29 16:31:21.316: INFO: Pod pod-configmaps-d868e8bf-6cb0-483b-8a9b-53a5d2640fa1 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:21.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4420" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5253,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:21.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Aug 29 16:31:21.418: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Aug 29 16:31:21.461: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:21.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6240" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":284,"skipped":5254,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:21.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:31:21.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf" in namespace "projected-8963" to be "Succeeded or Failed"
Aug 29 16:31:21.598: INFO: Pod "downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.055518ms
Aug 29 16:31:23.609: INFO: Pod "downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017945928s
Aug 29 16:31:25.627: INFO: Pod "downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf": Phase="Running", Reason="", readiness=false. Elapsed: 4.03623963s
Aug 29 16:31:27.642: INFO: Pod "downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050869771s
STEP: Saw pod success
Aug 29 16:31:27.642: INFO: Pod "downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf" satisfied condition "Succeeded or Failed"
Aug 29 16:31:27.654: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf container client-container: <nil>
STEP: delete the pod
Aug 29 16:31:27.709: INFO: Waiting for pod downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf to disappear
Aug 29 16:31:27.719: INFO: Pod downwardapi-volume-3974ecf7-1d12-422d-aae4-6b55b00fcfcf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:27.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8963" for this suite.

• [SLOW TEST:6.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5260,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Aug 29 16:31:27.832: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.832: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.849: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.849: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.877: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.877: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.903: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:27.903: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:31:28.795: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 29 16:31:28.795: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 29 16:31:29.021: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Aug 29 16:31:29.043: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 0
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.047: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.061: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.061: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.090: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.090: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:29.112: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:29.112: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:29.125: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:29.125: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:30.032: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:30.032: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:30.074: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
STEP: listing Deployments
Aug 29 16:31:30.087: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Aug 29 16:31:30.106: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Aug 29 16:31:30.119: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:30.136: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:30.171: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:30.193: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:30.207: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:30.216: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:31.044: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:31.100: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:31.121: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:31:32.859: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Aug 29 16:31:32.963: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:32.964: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:32.964: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:32.966: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:32.966: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:32.967: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 1
Aug 29 16:31:32.968: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:32.968: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:32.969: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 2
Aug 29 16:31:32.969: INFO: observed Deployment test-deployment in namespace deployment-6558 with ReadyReplicas 3
STEP: deleting the Deployment
Aug 29 16:31:33.006: INFO: observed event type MODIFIED
Aug 29 16:31:33.007: INFO: observed event type MODIFIED
Aug 29 16:31:33.007: INFO: observed event type MODIFIED
Aug 29 16:31:33.007: INFO: observed event type MODIFIED
Aug 29 16:31:33.007: INFO: observed event type MODIFIED
Aug 29 16:31:33.007: INFO: observed event type MODIFIED
Aug 29 16:31:33.007: INFO: observed event type MODIFIED
Aug 29 16:31:33.008: INFO: observed event type MODIFIED
Aug 29 16:31:33.008: INFO: observed event type MODIFIED
Aug 29 16:31:33.008: INFO: observed event type MODIFIED
Aug 29 16:31:33.008: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 16:31:33.761: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 29 16:31:33.771: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-6558  8a586b41-53a6-4d08-9937-2cd07af74bd5 36581 4 2022-08-29 16:31:29 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ae60a4ab-6bac-4689-a3b9-63d4b76a6d95 0xc00393e947 0xc00393e948}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae60a4ab-6bac-4689-a3b9-63d4b76a6d95\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:31:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393e9d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 29 16:31:33.782: INFO: pod: "test-deployment-5ddd8b47d8-drrmd":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-drrmd test-deployment-5ddd8b47d8- deployment-6558  7735d5ad-74dc-4200-b54c-ff3ef3239251 36577 0 2022-08-29 16:31:29 +0000 UTC 2022-08-29 16:31:33 +0000 UTC 0xc00393ee58 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:22c3670ac06c43e00d26454dd0d9ab315624dd5baef9c897f45d55fe88db9511 cni.projectcalico.org/podIP:172.25.1.241/32 cni.projectcalico.org/podIPs:172.25.1.241/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 8a586b41-53a6-4d08-9937-2cd07af74bd5 0xc00393eea7 0xc00393eea8}] []  [{Go-http-client Update v1 2022-08-29 16:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 16:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a586b41-53a6-4d08-9937-2cd07af74bd5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:31:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87tlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87tlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.241,StartTime:2022-08-29 16:31:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:31:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:containerd://f3a85846e4315589d3832f907615d6c0e68574bcd8e2a428a442557479d7bd09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 29 16:31:33.783: INFO: pod: "test-deployment-5ddd8b47d8-nxrk4":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-nxrk4 test-deployment-5ddd8b47d8- deployment-6558  e2059bc2-6a01-4a24-870b-4044b060902b 36555 0 2022-08-29 16:31:30 +0000 UTC 2022-08-29 16:31:32 +0000 UTC 0xc00393f090 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:372f6b20cd62b76d748bbf716621f3d327dfde86572c8f4f89ea5759b8233959 cni.projectcalico.org/podIP:172.25.2.239/32 cni.projectcalico.org/podIPs:172.25.2.239/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 8a586b41-53a6-4d08-9937-2cd07af74bd5 0xc00393f0e7 0xc00393f0e8}] []  [{Go-http-client Update v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a586b41-53a6-4d08-9937-2cd07af74bd5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:31:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9628,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9628,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.239,StartTime:2022-08-29 16:31:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:31:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:containerd://12da8ec4b27852788785b77d0ed466afd1fa62a5de51ec3ce631480cad5f63ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 29 16:31:33.783: INFO: ReplicaSet "test-deployment-6d7ffcf7fb":
&ReplicaSet{ObjectMeta:{test-deployment-6d7ffcf7fb  deployment-6558  975406a5-312f-4786-b865-1cf983ca5378 36483 3 2022-08-29 16:31:27 +0000 UTC <nil> <nil> map[pod-template-hash:6d7ffcf7fb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ae60a4ab-6bac-4689-a3b9-63d4b76a6d95 0xc00393ea37 0xc00393ea38}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:31:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae60a4ab-6bac-4689-a3b9-63d4b76a6d95\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6d7ffcf7fb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6d7ffcf7fb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393eac0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 29 16:31:33.792: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-6558  73a8da2c-b762-4d45-a5e4-0e2dd56cb18d 36573 2 2022-08-29 16:31:30 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ae60a4ab-6bac-4689-a3b9-63d4b76a6d95 0xc00393eb27 0xc00393eb28}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae60a4ab-6bac-4689-a3b9-63d4b76a6d95\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393ebb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 29 16:31:33.800: INFO: pod: "test-deployment-854fdc678-ffd9h":
&Pod{ObjectMeta:{test-deployment-854fdc678-ffd9h test-deployment-854fdc678- deployment-6558  83327285-ab92-44d9-b322-8f1934255728 36591 0 2022-08-29 16:31:30 +0000 UTC 2022-08-29 16:31:34 +0000 UTC 0xc004009168 map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:d0dcc80ca56e25e9f80cb4642559a85a80e293ba8dbc4c50b0ab3b6093eca8da cni.projectcalico.org/podIP:172.25.1.242/32 cni.projectcalico.org/podIPs:172.25.1.242/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 73a8da2c-b762-4d45-a5e4-0e2dd56cb18d 0xc0040091b7 0xc0040091b8}] []  [{Go-http-client Update v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73a8da2c-b762-4d45-a5e4-0e2dd56cb18d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:31:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xskpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xskpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:172.25.1.242,StartTime:2022-08-29 16:31:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:31:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f84c4246f0c808623f1b2c69645c7eaf9909809abf27af6f6c3f8aa22ef87d09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 29 16:31:33.801: INFO: pod: "test-deployment-854fdc678-lqqr8":
&Pod{ObjectMeta:{test-deployment-854fdc678-lqqr8 test-deployment-854fdc678- deployment-6558  99b445cf-4a6f-478a-9eef-42259a84da8d 36592 0 2022-08-29 16:31:31 +0000 UTC 2022-08-29 16:31:34 +0000 UTC 0xc0040093b0 map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:fa839eb5b10435035ca47420e5038eb1eecf1ec0ab62b8894468c39cbb628d60 cni.projectcalico.org/podIP:172.25.2.240/32 cni.projectcalico.org/podIPs:172.25.2.240/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 73a8da2c-b762-4d45-a5e4-0e2dd56cb18d 0xc004009407 0xc004009408}] []  [{Go-http-client Update v1 2022-08-29 16:31:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 16:31:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73a8da2c-b762-4d45-a5e4-0e2dd56cb18d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:31:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4k6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4k6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:31:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.240,StartTime:2022-08-29 16:31:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:31:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9f59d2dc9d44a7848bfe7c5669c38ab2430340420c543546d2e7e2004d8f768a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.240,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:33.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6558" for this suite.

• [SLOW TEST:6.079 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":286,"skipped":5274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:33.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:31:33.904: INFO: The status of Pod busybox-host-aliasesdfdf10d9-5595-4104-a82e-454b4fbed29e is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:31:35.922: INFO: The status of Pod busybox-host-aliasesdfdf10d9-5595-4104-a82e-454b4fbed29e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:31:35.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4066" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5296,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:31:35.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
Aug 29 16:33:36.611: INFO: Successfully updated pod "var-expansion-95916f22-0b3e-4215-89b4-fbe8a90c70d7"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 29 16:33:38.630: INFO: Deleting pod "var-expansion-95916f22-0b3e-4215-89b4-fbe8a90c70d7" in namespace "var-expansion-9823"
Aug 29 16:33:38.649: INFO: Wait up to 5m0s for pod "var-expansion-95916f22-0b3e-4215-89b4-fbe8a90c70d7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:10.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9823" for this suite.

• [SLOW TEST:154.727 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":288,"skipped":5296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:10.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:34:10.900: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 29 16:34:10.915: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:34:10.946: INFO: waiting for watch events with expected annotations
Aug 29 16:34:10.946: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:11.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5904" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":289,"skipped":5326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:11.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 29 16:34:11.188: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4276  756f7f51-965c-4c5c-b5ca-a1d8e6a8497c 37209 0 2022-08-29 16:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 16:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:34:11.189: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4276  756f7f51-965c-4c5c-b5ca-a1d8e6a8497c 37210 0 2022-08-29 16:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 16:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 29 16:34:11.239: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4276  756f7f51-965c-4c5c-b5ca-a1d8e6a8497c 37211 0 2022-08-29 16:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 16:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:34:11.239: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4276  756f7f51-965c-4c5c-b5ca-a1d8e6a8497c 37212 0 2022-08-29 16:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 16:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:11.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4276" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":290,"skipped":5388,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-3c8b3c3d-b9b8-43dd-bfe7-d0249862a7a3
STEP: Creating a pod to test consume secrets
Aug 29 16:34:11.326: INFO: Waiting up to 5m0s for pod "pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8" in namespace "secrets-7896" to be "Succeeded or Failed"
Aug 29 16:34:11.335: INFO: Pod "pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.236925ms
Aug 29 16:34:13.346: INFO: Pod "pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019028142s
Aug 29 16:34:15.365: INFO: Pod "pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038296019s
Aug 29 16:34:17.371: INFO: Pod "pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044457243s
STEP: Saw pod success
Aug 29 16:34:17.371: INFO: Pod "pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8" satisfied condition "Succeeded or Failed"
Aug 29 16:34:17.380: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:34:17.411: INFO: Waiting for pod pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8 to disappear
Aug 29 16:34:17.420: INFO: Pod pod-secrets-4b84a673-71a9-431c-ad84-57f5957aa5c8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:17.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7896" for this suite.

• [SLOW TEST:6.183 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5400,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:17.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 29 16:34:20.073: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7843 pod-service-account-eb45908d-937c-4f6d-b7be-efea84d956bf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 29 16:34:20.363: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7843 pod-service-account-eb45908d-937c-4f6d-b7be-efea84d956bf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 29 16:34:20.675: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7843 pod-service-account-eb45908d-937c-4f6d-b7be-efea84d956bf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:20.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7843" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":292,"skipped":5404,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:21.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-3779
STEP: creating service affinity-clusterip-transition in namespace services-3779
STEP: creating replication controller affinity-clusterip-transition in namespace services-3779
I0829 16:34:21.228998      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3779, replica count: 3
I0829 16:34:24.279614      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:34:24.295: INFO: Creating new exec pod
Aug 29 16:34:27.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-3779 exec execpod-affinitydbscn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 29 16:34:27.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 29 16:34:27.615: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:34:27.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-3779 exec execpod-affinitydbscn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.40 80'
Aug 29 16:34:27.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.40 80\nConnection to 10.240.19.40 80 port [tcp/http] succeeded!\n"
Aug 29 16:34:27.809: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:34:27.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-3779 exec execpod-affinitydbscn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.19.40:80/ ; done'
Aug 29 16:34:28.160: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n"
Aug 29 16:34:28.160: INFO: stdout: "\naffinity-clusterip-transition-cgpc2\naffinity-clusterip-transition-rdwxk\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-cgpc2\naffinity-clusterip-transition-rdwxk\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-cgpc2\naffinity-clusterip-transition-rdwxk\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-cgpc2\naffinity-clusterip-transition-rdwxk\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-cgpc2\naffinity-clusterip-transition-rdwxk\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-cgpc2"
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-cgpc2
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-rdwxk
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-cgpc2
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-rdwxk
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-cgpc2
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-rdwxk
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-cgpc2
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-rdwxk
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-cgpc2
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-rdwxk
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.160: INFO: Received response from host: affinity-clusterip-transition-cgpc2
Aug 29 16:34:28.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-3779 exec execpod-affinitydbscn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.19.40:80/ ; done'
Aug 29 16:34:28.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.40:80/\n"
Aug 29 16:34:28.615: INFO: stdout: "\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d\naffinity-clusterip-transition-bnq9d"
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Received response from host: affinity-clusterip-transition-bnq9d
Aug 29 16:34:28.615: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3779, will wait for the garbage collector to delete the pods
Aug 29 16:34:28.725: INFO: Deleting ReplicationController affinity-clusterip-transition took: 21.135506ms
Aug 29 16:34:28.826: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.614323ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:32.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3779" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:11.726 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":293,"skipped":5410,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:32.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
Aug 29 16:34:32.853: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 29 16:34:32.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 create -f -'
Aug 29 16:34:33.194: INFO: stderr: ""
Aug 29 16:34:33.194: INFO: stdout: "service/agnhost-replica created\n"
Aug 29 16:34:33.194: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 29 16:34:33.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 create -f -'
Aug 29 16:34:33.800: INFO: stderr: ""
Aug 29 16:34:33.800: INFO: stdout: "service/agnhost-primary created\n"
Aug 29 16:34:33.800: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 29 16:34:33.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 create -f -'
Aug 29 16:34:34.064: INFO: stderr: ""
Aug 29 16:34:34.064: INFO: stdout: "service/frontend created\n"
Aug 29 16:34:34.064: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 29 16:34:34.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 create -f -'
Aug 29 16:34:34.309: INFO: stderr: ""
Aug 29 16:34:34.309: INFO: stdout: "deployment.apps/frontend created\n"
Aug 29 16:34:34.309: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 29 16:34:34.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 create -f -'
Aug 29 16:34:34.622: INFO: stderr: ""
Aug 29 16:34:34.623: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 29 16:34:34.623: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 29 16:34:34.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 create -f -'
Aug 29 16:34:35.084: INFO: stderr: ""
Aug 29 16:34:35.084: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 29 16:34:35.084: INFO: Waiting for all frontend pods to be Running.
Aug 29 16:34:40.135: INFO: Waiting for frontend to serve content.
Aug 29 16:34:40.174: INFO: Trying to add a new entry to the guestbook.
Aug 29 16:34:40.199: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 29 16:34:40.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 delete --grace-period=0 --force -f -'
Aug 29 16:34:40.361: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:34:40.361: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:34:40.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 delete --grace-period=0 --force -f -'
Aug 29 16:34:40.491: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:34:40.491: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:34:40.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 delete --grace-period=0 --force -f -'
Aug 29 16:34:40.647: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:34:40.648: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:34:40.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 delete --grace-period=0 --force -f -'
Aug 29 16:34:41.298: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:34:41.298: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:34:41.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 delete --grace-period=0 --force -f -'
Aug 29 16:34:41.411: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:34:41.411: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:34:41.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-3064 delete --grace-period=0 --force -f -'
Aug 29 16:34:41.506: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:34:41.506: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:41.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3064" for this suite.

• [SLOW TEST:8.738 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":294,"skipped":5411,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:41.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 16:34:45.649: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:45.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6849" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5417,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:45.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:34:45.847: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 29 16:34:45.862: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:34:45.914: INFO: waiting for watch events with expected annotations
Aug 29 16:34:45.914: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-72" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":296,"skipped":5429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:46.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 29 16:34:46.122: INFO: Waiting up to 5m0s for pod "pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d" in namespace "emptydir-8203" to be "Succeeded or Failed"
Aug 29 16:34:46.131: INFO: Pod "pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.783815ms
Aug 29 16:34:48.140: INFO: Pod "pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018045623s
Aug 29 16:34:50.150: INFO: Pod "pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028391372s
STEP: Saw pod success
Aug 29 16:34:50.151: INFO: Pod "pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d" satisfied condition "Succeeded or Failed"
Aug 29 16:34:50.160: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d container test-container: <nil>
STEP: delete the pod
Aug 29 16:34:50.205: INFO: Waiting for pod pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d to disappear
Aug 29 16:34:50.211: INFO: Pod pod-6bf2c3d3-046c-4f0f-b688-1be8b6fe803d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:34:50.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8203" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":5471,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:34:50.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-4174
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 16:34:50.326: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 16:34:50.417: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:34:52.611: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:34:54.426: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:34:56.790: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:34:58.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:00.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:02.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:04.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:08.029: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:08.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:10.427: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:35:12.425: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 16:35:12.441: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 16:35:12.455: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 16:35:14.510: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 16:35:14.510: INFO: Breadth first check of 172.25.0.109 on host 172.31.16.14...
Aug 29 16:35:14.519: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.250:9080/dial?request=hostname&protocol=udp&host=172.25.0.109&port=8081&tries=1'] Namespace:pod-network-test-4174 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:35:14.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:35:14.520: INFO: ExecWithOptions: Clientset creation
Aug 29 16:35:14.520: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4174/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.109%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Aug 29 16:35:14.680: INFO: Waiting for responses: map[]
Aug 29 16:35:14.681: INFO: reached 172.25.0.109 after 0/1 tries
Aug 29 16:35:14.681: INFO: Breadth first check of 172.25.1.249 on host 172.31.20.142...
Aug 29 16:35:14.688: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.250:9080/dial?request=hostname&protocol=udp&host=172.25.1.249&port=8081&tries=1'] Namespace:pod-network-test-4174 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:35:14.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:35:14.689: INFO: ExecWithOptions: Clientset creation
Aug 29 16:35:14.689: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4174/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.249%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Aug 29 16:35:14.820: INFO: Waiting for responses: map[]
Aug 29 16:35:14.820: INFO: reached 172.25.1.249 after 0/1 tries
Aug 29 16:35:14.820: INFO: Breadth first check of 172.25.2.251 on host 172.31.23.90...
Aug 29 16:35:14.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.250:9080/dial?request=hostname&protocol=udp&host=172.25.2.251&port=8081&tries=1'] Namespace:pod-network-test-4174 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:35:14.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:35:14.832: INFO: ExecWithOptions: Clientset creation
Aug 29 16:35:14.832: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4174/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.251%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Aug 29 16:35:14.949: INFO: Waiting for responses: map[]
Aug 29 16:35:14.949: INFO: reached 172.25.2.251 after 0/1 tries
Aug 29 16:35:14.949: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:14.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4174" for this suite.

• [SLOW TEST:24.715 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5476,"failed":0}
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:14.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Aug 29 16:35:15.069: INFO: The status of Pod labelsupdate4d9400cf-aeeb-4047-9db0-b26a9d1f0ae6 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:35:17.081: INFO: The status of Pod labelsupdate4d9400cf-aeeb-4047-9db0-b26a9d1f0ae6 is Running (Ready = true)
Aug 29 16:35:17.629: INFO: Successfully updated pod "labelsupdate4d9400cf-aeeb-4047-9db0-b26a9d1f0ae6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:21.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2210" for this suite.

• [SLOW TEST:6.718 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:35:22.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:35:25.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:35:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1222-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:28.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9902" for this suite.
STEP: Destroying namespace "webhook-9902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.214 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":300,"skipped":5509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:28.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-4457
STEP: creating service affinity-clusterip in namespace services-4457
STEP: creating replication controller affinity-clusterip in namespace services-4457
I0829 16:35:29.021799      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4457, replica count: 3
I0829 16:35:32.073063      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:35:32.090: INFO: Creating new exec pod
Aug 29 16:35:35.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4457 exec execpod-affinityg2lk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 29 16:35:35.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 29 16:35:35.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:35:35.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4457 exec execpod-affinityg2lk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.252 80'
Aug 29 16:35:36.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.18.252 80\nConnection to 10.240.18.252 80 port [tcp/http] succeeded!\n"
Aug 29 16:35:36.234: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:35:36.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=services-4457 exec execpod-affinityg2lk8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.18.252:80/ ; done'
Aug 29 16:35:36.555: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.252:80/\n"
Aug 29 16:35:36.555: INFO: stdout: "\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k\naffinity-clusterip-jqr5k"
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Received response from host: affinity-clusterip-jqr5k
Aug 29 16:35:36.555: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4457, will wait for the garbage collector to delete the pods
Aug 29 16:35:36.673: INFO: Deleting ReplicationController affinity-clusterip took: 13.947441ms
Aug 29 16:35:36.774: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.894131ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:39.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4457" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:10.339 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":301,"skipped":5557,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:39.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-45db45a9-f135-4f48-8c68-e0ef740f6de9
STEP: Creating a pod to test consume configMaps
Aug 29 16:35:39.360: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70" in namespace "projected-1379" to be "Succeeded or Failed"
Aug 29 16:35:39.377: INFO: Pod "pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.741823ms
Aug 29 16:35:41.577: INFO: Pod "pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.216858263s
Aug 29 16:35:43.588: INFO: Pod "pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227766106s
Aug 29 16:35:45.602: INFO: Pod "pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.241645947s
STEP: Saw pod success
Aug 29 16:35:45.602: INFO: Pod "pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70" satisfied condition "Succeeded or Failed"
Aug 29 16:35:45.611: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:35:45.642: INFO: Waiting for pod pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70 to disappear
Aug 29 16:35:45.649: INFO: Pod pod-projected-configmaps-ec8bd298-3d26-4f13-bb97-ce2d75970e70 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:45.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1379" for this suite.

• [SLOW TEST:6.409 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5568,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:45.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Aug 29 16:35:47.771: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:49.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-133" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":303,"skipped":5585,"failed":0}

------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:49.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9816" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":304,"skipped":5585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
Aug 29 16:35:51.406: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:35:53.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6810" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":305,"skipped":5684,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:35:53.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Aug 29 16:35:55.637: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5390 PodName:var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:35:55.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:35:55.638: INFO: ExecWithOptions: Clientset creation
Aug 29 16:35:55.638: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-5390/pods/var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Aug 29 16:35:55.777: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5390 PodName:var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:35:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:35:55.778: INFO: ExecWithOptions: Clientset creation
Aug 29 16:35:55.778: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-5390/pods/var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Aug 29 16:35:56.476: INFO: Successfully updated pod "var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 29 16:35:56.483: INFO: Deleting pod "var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f" in namespace "var-expansion-5390"
Aug 29 16:35:56.501: INFO: Wait up to 5m0s for pod "var-expansion-43bd46f1-6017-45bb-9c25-eab8bb22082f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:36:30.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5390" for this suite.

• [SLOW TEST:37.257 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":306,"skipped":5687,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:36:30.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 29 16:36:31.251: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 16:36:31.271: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 16:36:31.277: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-14.eu-central-1.compute.internal before test
Aug 29 16:36:31.293: INFO: calico-kube-controllers-57fb8785bf-xg2r9 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 16:36:31.293: INFO: canal-lzt6p from kube-system started at 2022-08-29 15:05:27 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:36:31.293: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:36:31.293: INFO: coredns-5848f745f7-mkm4d from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:36:31.293: INFO: coredns-5848f745f7-ntmz2 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:36:31.293: INFO: envoy-agent-7wzrr from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:36:31.293: INFO: konnectivity-agent-b7c8486c7-dr2vw from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:36:31.293: INFO: konnectivity-agent-b7c8486c7-r476v from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:36:31.293: INFO: kube-proxy-fjrpb from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:36:31.293: INFO: metrics-server-7c94595b7c-d66rr from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:36:31.293: INFO: metrics-server-7c94595b7c-xjds6 from kube-system started at 2022-08-29 15:06:18 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:36:31.293: INFO: node-local-dns-jdx9q from kube-system started at 2022-08-29 15:05:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:36:31.293: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:53 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 16:36:31.293: INFO: sonobuoy-e2e-job-0af6d8c74f8640cb from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container e2e ready: true, restart count 0
Aug 29 16:36:31.293: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:36:31.293: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-lffk7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.293: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:36:31.293: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:36:31.293: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-142.eu-central-1.compute.internal before test
Aug 29 16:36:31.307: INFO: canal-twqkh from kube-system started at 2022-08-29 15:05:56 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.308: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:36:31.308: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:36:31.308: INFO: envoy-agent-rmdxv from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.308: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:36:31.308: INFO: kube-proxy-rdwz4 from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.308: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:36:31.308: INFO: node-local-dns-x9dxw from kube-system started at 2022-08-29 15:05:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.308: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:36:31.308: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-dlbm7 from sonobuoy started at 2022-08-29 15:06:59 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:36:31.308: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:36:31.308: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-23-90.eu-central-1.compute.internal before test
Aug 29 16:36:31.319: INFO: canal-vngk2 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.319: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:36:31.319: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:36:31.319: INFO: envoy-agent-48bh4 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.319: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:36:31.319: INFO: kube-proxy-qjw5j from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.319: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:36:31.319: INFO: node-local-dns-lxgw9 from kube-system started at 2022-08-29 15:07:08 +0000 UTC (1 container statuses recorded)
Aug 29 16:36:31.319: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:36:31.319: INFO: sonobuoy-systemd-logs-daemon-set-8aba613fc8154c58-snzb6 from sonobuoy started at 2022-08-29 15:07:08 +0000 UTC (2 container statuses recorded)
Aug 29 16:36:31.319: INFO: 	Container sonobuoy-worker ready: false, restart count 20
Aug 29 16:36:31.319: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b040881c-795e-4c0c-8147-4b546ed2d0f8 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.23.90 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b040881c-795e-4c0c-8147-4b546ed2d0f8 off the node ip-172-31-23-90.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b040881c-795e-4c0c-8147-4b546ed2d0f8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:41:36.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3180" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.221 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":307,"skipped":5708,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:41:36.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-8a8aa4ac-865b-4e67-9abb-fb07c7ff84e7 in namespace container-probe-8666
Aug 29 16:41:41.476: INFO: Started pod liveness-8a8aa4ac-865b-4e67-9abb-fb07c7ff84e7 in namespace container-probe-8666
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 16:41:41.501: INFO: Initial restart count of pod liveness-8a8aa4ac-865b-4e67-9abb-fb07c7ff84e7 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:45:41.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8666" for this suite.

• [SLOW TEST:244.869 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:45:41.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:45:42.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9998" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":309,"skipped":5768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:45:42.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:45:43.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:45:48.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:45:48.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9517" for this suite.
STEP: Destroying namespace "webhook-9517-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.836 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":310,"skipped":5802,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:45:48.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9739.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9739.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9739.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9739.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:45:52.825: INFO: DNS probes using dns-9739/dns-test-b514681a-9778-41be-bb29-65f8bf6c6e5b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:45:52.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9739" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":311,"skipped":5805,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:45:52.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
Aug 29 16:45:53.004: INFO: Found Service test-service-xvmh9 in namespace services-3587 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 29 16:45:53.005: INFO: Service test-service-xvmh9 created
STEP: Getting /status
Aug 29 16:45:53.011: INFO: Service test-service-xvmh9 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Aug 29 16:45:53.028: INFO: observed Service test-service-xvmh9 in namespace services-3587 with annotations: map[] & LoadBalancer: {[]}
Aug 29 16:45:53.036: INFO: Found Service test-service-xvmh9 in namespace services-3587 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 29 16:45:53.036: INFO: Service test-service-xvmh9 has service status patched
STEP: updating the ServiceStatus
Aug 29 16:45:53.059: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Aug 29 16:45:53.063: INFO: Observed Service test-service-xvmh9 in namespace services-3587 with annotations: map[] & Conditions: {[]}
Aug 29 16:45:53.063: INFO: Observed event: &Service{ObjectMeta:{test-service-xvmh9  services-3587  6b2a0852-83b2-4c1c-9e4b-9e83a281cc55 40448 0 2022-08-29 16:45:52 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-08-29 16:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-29 16:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.28.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.28.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 29 16:45:53.063: INFO: Found Service test-service-xvmh9 in namespace services-3587 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 16:45:53.063: INFO: Service test-service-xvmh9 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Aug 29 16:45:53.094: INFO: observed Service test-service-xvmh9 in namespace services-3587 with labels: map[test-service-static:true]
Aug 29 16:45:53.094: INFO: observed Service test-service-xvmh9 in namespace services-3587 with labels: map[test-service-static:true]
Aug 29 16:45:53.094: INFO: observed Service test-service-xvmh9 in namespace services-3587 with labels: map[test-service-static:true]
Aug 29 16:45:53.094: INFO: Found Service test-service-xvmh9 in namespace services-3587 with labels: map[test-service:patched test-service-static:true]
Aug 29 16:45:53.094: INFO: Service test-service-xvmh9 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Aug 29 16:45:53.120: INFO: Observed event: ADDED
Aug 29 16:45:53.121: INFO: Observed event: MODIFIED
Aug 29 16:45:53.121: INFO: Observed event: MODIFIED
Aug 29 16:45:53.121: INFO: Observed event: MODIFIED
Aug 29 16:45:53.122: INFO: Found Service test-service-xvmh9 in namespace services-3587 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 29 16:45:53.122: INFO: Service test-service-xvmh9 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:45:53.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3587" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":312,"skipped":5816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:45:53.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-cefaf1c0-4ea5-4cfc-ad83-fe41fa5cd572
STEP: Creating a pod to test consume configMaps
Aug 29 16:45:53.261: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a" in namespace "configmap-38" to be "Succeeded or Failed"
Aug 29 16:45:53.276: INFO: Pod "pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.663677ms
Aug 29 16:45:55.289: INFO: Pod "pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025995882s
Aug 29 16:45:57.298: INFO: Pod "pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035214762s
STEP: Saw pod success
Aug 29 16:45:57.298: INFO: Pod "pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a" satisfied condition "Succeeded or Failed"
Aug 29 16:45:57.304: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a container configmap-volume-test: <nil>
STEP: delete the pod
Aug 29 16:45:57.358: INFO: Waiting for pod pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a to disappear
Aug 29 16:45:57.367: INFO: Pod pod-configmaps-fe44fb1b-7bc8-4605-9919-ca282fd2480a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:45:57.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-38" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:45:57.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:45:57.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7712
I0829 16:45:57.476818      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7712, replica count: 1
I0829 16:45:58.528356      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 16:45:59.529076      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 16:46:00.530381      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:46:00.662: INFO: Created: latency-svc-qzzgz
Aug 29 16:46:00.678: INFO: Got endpoints: latency-svc-qzzgz [45.763434ms]
Aug 29 16:46:01.263: INFO: Created: latency-svc-dmlhm
Aug 29 16:46:01.279: INFO: Got endpoints: latency-svc-dmlhm [598.89586ms]
Aug 29 16:46:01.280: INFO: Created: latency-svc-88c66
Aug 29 16:46:01.293: INFO: Got endpoints: latency-svc-88c66 [609.889972ms]
Aug 29 16:46:01.298: INFO: Created: latency-svc-fmphl
Aug 29 16:46:01.306: INFO: Created: latency-svc-zj9d2
Aug 29 16:46:01.310: INFO: Got endpoints: latency-svc-fmphl [625.900984ms]
Aug 29 16:46:01.316: INFO: Created: latency-svc-rg4x4
Aug 29 16:46:01.327: INFO: Got endpoints: latency-svc-rg4x4 [642.559073ms]
Aug 29 16:46:01.341: INFO: Got endpoints: latency-svc-zj9d2 [662.175612ms]
Aug 29 16:46:01.354: INFO: Created: latency-svc-r4fgz
Aug 29 16:46:01.360: INFO: Got endpoints: latency-svc-r4fgz [676.539757ms]
Aug 29 16:46:01.354: INFO: Created: latency-svc-snsx4
Aug 29 16:46:01.363: INFO: Got endpoints: latency-svc-snsx4 [679.867971ms]
Aug 29 16:46:01.359: INFO: Created: latency-svc-m5s76
Aug 29 16:46:01.386: INFO: Got endpoints: latency-svc-m5s76 [701.109117ms]
Aug 29 16:46:01.397: INFO: Created: latency-svc-6vhvt
Aug 29 16:46:01.412: INFO: Got endpoints: latency-svc-6vhvt [728.280292ms]
Aug 29 16:46:01.412: INFO: Created: latency-svc-dw74k
Aug 29 16:46:01.417: INFO: Created: latency-svc-flqtd
Aug 29 16:46:01.443: INFO: Got endpoints: latency-svc-dw74k [758.928313ms]
Aug 29 16:46:01.446: INFO: Got endpoints: latency-svc-flqtd [761.943303ms]
Aug 29 16:46:01.469: INFO: Created: latency-svc-6qvtd
Aug 29 16:46:01.577: INFO: Got endpoints: latency-svc-6qvtd [893.210855ms]
Aug 29 16:46:01.580: INFO: Created: latency-svc-hnn9t
Aug 29 16:46:01.583: INFO: Created: latency-svc-wmjd7
Aug 29 16:46:01.590: INFO: Got endpoints: latency-svc-hnn9t [905.44583ms]
Aug 29 16:46:01.601: INFO: Got endpoints: latency-svc-wmjd7 [916.894875ms]
Aug 29 16:46:01.604: INFO: Created: latency-svc-l5vhc
Aug 29 16:46:01.624: INFO: Got endpoints: latency-svc-l5vhc [939.468775ms]
Aug 29 16:46:01.634: INFO: Created: latency-svc-zjb98
Aug 29 16:46:01.642: INFO: Got endpoints: latency-svc-zjb98 [363.429546ms]
Aug 29 16:46:01.645: INFO: Created: latency-svc-rzq2r
Aug 29 16:46:01.661: INFO: Created: latency-svc-lnhbk
Aug 29 16:46:01.671: INFO: Got endpoints: latency-svc-lnhbk [360.322765ms]
Aug 29 16:46:01.672: INFO: Got endpoints: latency-svc-rzq2r [377.245117ms]
Aug 29 16:46:01.678: INFO: Created: latency-svc-vfvqb
Aug 29 16:46:01.695: INFO: Created: latency-svc-cmgj9
Aug 29 16:46:01.697: INFO: Got endpoints: latency-svc-vfvqb [342.637296ms]
Aug 29 16:46:01.705: INFO: Created: latency-svc-dv755
Aug 29 16:46:01.711: INFO: Got endpoints: latency-svc-cmgj9 [353.580687ms]
Aug 29 16:46:01.718: INFO: Created: latency-svc-98zvj
Aug 29 16:46:01.721: INFO: Got endpoints: latency-svc-dv755 [360.601278ms]
Aug 29 16:46:01.749: INFO: Created: latency-svc-h77wf
Aug 29 16:46:01.755: INFO: Got endpoints: latency-svc-98zvj [391.266969ms]
Aug 29 16:46:01.764: INFO: Created: latency-svc-8b9pg
Aug 29 16:46:01.765: INFO: Got endpoints: latency-svc-h77wf [377.661161ms]
Aug 29 16:46:01.776: INFO: Got endpoints: latency-svc-8b9pg [363.75469ms]
Aug 29 16:46:01.780: INFO: Created: latency-svc-vcm5m
Aug 29 16:46:01.787: INFO: Got endpoints: latency-svc-vcm5m [343.483834ms]
Aug 29 16:46:01.791: INFO: Created: latency-svc-hm5g5
Aug 29 16:46:01.799: INFO: Got endpoints: latency-svc-hm5g5 [353.01381ms]
Aug 29 16:46:01.940: INFO: Created: latency-svc-wwrzw
Aug 29 16:46:01.940: INFO: Created: latency-svc-kpl5s
Aug 29 16:46:01.954: INFO: Created: latency-svc-f2jk2
Aug 29 16:46:01.954: INFO: Created: latency-svc-7xbrz
Aug 29 16:46:01.955: INFO: Created: latency-svc-265zk
Aug 29 16:46:01.955: INFO: Created: latency-svc-7d4tr
Aug 29 16:46:01.960: INFO: Created: latency-svc-jfdtb
Aug 29 16:46:01.960: INFO: Created: latency-svc-4gptn
Aug 29 16:46:01.960: INFO: Created: latency-svc-zbs6k
Aug 29 16:46:01.960: INFO: Created: latency-svc-5n6kl
Aug 29 16:46:01.960: INFO: Created: latency-svc-26bnx
Aug 29 16:46:01.960: INFO: Created: latency-svc-xp8wj
Aug 29 16:46:01.961: INFO: Created: latency-svc-2pxj2
Aug 29 16:46:01.961: INFO: Created: latency-svc-w5cfr
Aug 29 16:46:01.961: INFO: Created: latency-svc-rfhgd
Aug 29 16:46:02.008: INFO: Got endpoints: latency-svc-kpl5s [208.774287ms]
Aug 29 16:46:02.058: INFO: Got endpoints: latency-svc-7d4tr [270.445184ms]
Aug 29 16:46:02.062: INFO: Got endpoints: latency-svc-wwrzw [390.174388ms]
Aug 29 16:46:02.063: INFO: Got endpoints: latency-svc-w5cfr [341.435424ms]
Aug 29 16:46:02.063: INFO: Got endpoints: latency-svc-xp8wj [438.255116ms]
Aug 29 16:46:02.077: INFO: Got endpoints: latency-svc-4gptn [487.296943ms]
Aug 29 16:46:02.123: INFO: Created: latency-svc-bhffb
Aug 29 16:46:02.136: INFO: Got endpoints: latency-svc-rfhgd [493.74186ms]
Aug 29 16:46:02.137: INFO: Got endpoints: latency-svc-zbs6k [381.730624ms]
Aug 29 16:46:02.148: INFO: Got endpoints: latency-svc-26bnx [371.61686ms]
Aug 29 16:46:02.154: INFO: Got endpoints: latency-svc-265zk [456.36358ms]
Aug 29 16:46:02.158: INFO: Got endpoints: latency-svc-f2jk2 [580.312288ms]
Aug 29 16:46:02.184: INFO: Got endpoints: latency-svc-5n6kl [513.175927ms]
Aug 29 16:46:02.185: INFO: Got endpoints: latency-svc-jfdtb [419.77715ms]
Aug 29 16:46:02.186: INFO: Got endpoints: latency-svc-2pxj2 [585.093177ms]
Aug 29 16:46:02.186: INFO: Got endpoints: latency-svc-7xbrz [475.437105ms]
Aug 29 16:46:02.190: INFO: Got endpoints: latency-svc-bhffb [181.672035ms]
Aug 29 16:46:02.304: INFO: Created: latency-svc-svkmh
Aug 29 16:46:02.307: INFO: Created: latency-svc-bkxf2
Aug 29 16:46:02.310: INFO: Created: latency-svc-xmtzl
Aug 29 16:46:02.310: INFO: Created: latency-svc-6rcc6
Aug 29 16:46:02.311: INFO: Created: latency-svc-wjm87
Aug 29 16:46:02.311: INFO: Created: latency-svc-qdgt4
Aug 29 16:46:02.316: INFO: Created: latency-svc-7zxzt
Aug 29 16:46:02.316: INFO: Created: latency-svc-hk4ls
Aug 29 16:46:02.317: INFO: Created: latency-svc-v78zx
Aug 29 16:46:02.317: INFO: Created: latency-svc-dm8v4
Aug 29 16:46:02.576: INFO: Created: latency-svc-qn6d7
Aug 29 16:46:02.577: INFO: Created: latency-svc-bzsn9
Aug 29 16:46:02.580: INFO: Created: latency-svc-f6zqh
Aug 29 16:46:02.581: INFO: Created: latency-svc-pwvht
Aug 29 16:46:02.581: INFO: Created: latency-svc-wgs2w
Aug 29 16:46:02.591: INFO: Got endpoints: latency-svc-svkmh [400.886481ms]
Aug 29 16:46:02.599: INFO: Got endpoints: latency-svc-bzsn9 [521.842899ms]
Aug 29 16:46:02.603: INFO: Got endpoints: latency-svc-qn6d7 [464.173203ms]
Aug 29 16:46:02.610: INFO: Got endpoints: latency-svc-bkxf2 [451.936069ms]
Aug 29 16:46:02.613: INFO: Got endpoints: latency-svc-xmtzl [425.904525ms]
Aug 29 16:46:02.620: INFO: Got endpoints: latency-svc-wjm87 [556.536682ms]
Aug 29 16:46:02.640: INFO: Created: latency-svc-5tv29
Aug 29 16:46:02.646: INFO: Got endpoints: latency-svc-6rcc6 [583.479945ms]
Aug 29 16:46:02.653: INFO: Got endpoints: latency-svc-7zxzt [466.334945ms]
Aug 29 16:46:02.655: INFO: Got endpoints: latency-svc-qdgt4 [591.376995ms]
Aug 29 16:46:02.656: INFO: Got endpoints: latency-svc-hk4ls [519.196382ms]
Aug 29 16:46:02.656: INFO: Created: latency-svc-4h94g
Aug 29 16:46:02.662: INFO: Got endpoints: latency-svc-v78zx [603.419888ms]
Aug 29 16:46:02.665: INFO: Got endpoints: latency-svc-dm8v4 [476.721421ms]
Aug 29 16:46:02.672: INFO: Got endpoints: latency-svc-pwvht [518.525346ms]
Aug 29 16:46:02.673: INFO: Created: latency-svc-6ljd7
Aug 29 16:46:02.677: INFO: Got endpoints: latency-svc-f6zqh [491.35737ms]
Aug 29 16:46:02.697: INFO: Created: latency-svc-r84hc
Aug 29 16:46:02.702: INFO: Got endpoints: latency-svc-5tv29 [107.494181ms]
Aug 29 16:46:02.706: INFO: Got endpoints: latency-svc-wgs2w [557.122758ms]
Aug 29 16:46:02.707: INFO: Created: latency-svc-5kxcp
Aug 29 16:46:02.713: INFO: Created: latency-svc-2jr55
Aug 29 16:46:02.727: INFO: Got endpoints: latency-svc-4h94g [106.963267ms]
Aug 29 16:46:02.749: INFO: Created: latency-svc-9chxp
Aug 29 16:46:02.750: INFO: Created: latency-svc-gppmm
Aug 29 16:46:02.750: INFO: Created: latency-svc-8jxwl
Aug 29 16:46:02.765: INFO: Created: latency-svc-pcgv4
Aug 29 16:46:02.782: INFO: Created: latency-svc-rxf7x
Aug 29 16:46:02.783: INFO: Got endpoints: latency-svc-6ljd7 [172.029137ms]
Aug 29 16:46:02.791: INFO: Created: latency-svc-bdsxs
Aug 29 16:46:02.799: INFO: Created: latency-svc-wtdmk
Aug 29 16:46:02.819: INFO: Got endpoints: latency-svc-r84hc [219.410968ms]
Aug 29 16:46:02.828: INFO: Created: latency-svc-2qzbz
Aug 29 16:46:02.831: INFO: Created: latency-svc-4tqpz
Aug 29 16:46:02.831: INFO: Created: latency-svc-225ql
Aug 29 16:46:02.831: INFO: Created: latency-svc-rwq8r
Aug 29 16:46:02.845: INFO: Created: latency-svc-5xp4c
Aug 29 16:46:02.845: INFO: Created: latency-svc-m6w7l
Aug 29 16:46:02.873: INFO: Got endpoints: latency-svc-5kxcp [259.144513ms]
Aug 29 16:46:02.890: INFO: Created: latency-svc-96xgn
Aug 29 16:46:02.919: INFO: Got endpoints: latency-svc-2jr55 [316.210642ms]
Aug 29 16:46:02.942: INFO: Created: latency-svc-vzfw9
Aug 29 16:46:02.973: INFO: Got endpoints: latency-svc-9chxp [326.730289ms]
Aug 29 16:46:02.994: INFO: Created: latency-svc-shdc5
Aug 29 16:46:03.018: INFO: Got endpoints: latency-svc-gppmm [361.547772ms]
Aug 29 16:46:03.040: INFO: Created: latency-svc-wprrn
Aug 29 16:46:03.075: INFO: Got endpoints: latency-svc-8jxwl [419.869416ms]
Aug 29 16:46:03.096: INFO: Created: latency-svc-rvl6b
Aug 29 16:46:03.125: INFO: Got endpoints: latency-svc-pcgv4 [468.44274ms]
Aug 29 16:46:03.158: INFO: Created: latency-svc-ffrvt
Aug 29 16:46:03.170: INFO: Got endpoints: latency-svc-rxf7x [507.009552ms]
Aug 29 16:46:03.187: INFO: Created: latency-svc-fknqj
Aug 29 16:46:03.920: INFO: Got endpoints: latency-svc-bdsxs [1.254507354s]
Aug 29 16:46:03.931: INFO: Got endpoints: latency-svc-wtdmk [1.258523626s]
Aug 29 16:46:03.937: INFO: Got endpoints: latency-svc-2qzbz [1.210137434s]
Aug 29 16:46:03.947: INFO: Got endpoints: latency-svc-4tqpz [1.164354379s]
Aug 29 16:46:03.953: INFO: Got endpoints: latency-svc-225ql [1.275155874s]
Aug 29 16:46:03.962: INFO: Got endpoints: latency-svc-m6w7l [1.255587116s]
Aug 29 16:46:03.973: INFO: Got endpoints: latency-svc-rwq8r [1.271353177s]
Aug 29 16:46:03.985: INFO: Created: latency-svc-2pbbc
Aug 29 16:46:03.985: INFO: Got endpoints: latency-svc-5xp4c [1.166604292s]
Aug 29 16:46:03.987: INFO: Got endpoints: latency-svc-96xgn [1.11414203s]
Aug 29 16:46:03.988: INFO: Got endpoints: latency-svc-shdc5 [1.014834722s]
Aug 29 16:46:03.989: INFO: Got endpoints: latency-svc-vzfw9 [1.06985249s]
Aug 29 16:46:03.991: INFO: Got endpoints: latency-svc-wprrn [972.832834ms]
Aug 29 16:46:03.992: INFO: Got endpoints: latency-svc-ffrvt [866.23277ms]
Aug 29 16:46:03.994: INFO: Got endpoints: latency-svc-rvl6b [918.390851ms]
Aug 29 16:46:04.006: INFO: Got endpoints: latency-svc-fknqj [836.176953ms]
Aug 29 16:46:04.009: INFO: Created: latency-svc-bcnxv
Aug 29 16:46:04.010: INFO: Got endpoints: latency-svc-2pbbc [88.869073ms]
Aug 29 16:46:04.023: INFO: Created: latency-svc-ckptp
Aug 29 16:46:04.028: INFO: Created: latency-svc-l8bkv
Aug 29 16:46:04.028: INFO: Got endpoints: latency-svc-bcnxv [96.653883ms]
Aug 29 16:46:04.074: INFO: Got endpoints: latency-svc-ckptp [128.71341ms]
Aug 29 16:46:04.124: INFO: Got endpoints: latency-svc-l8bkv [173.600437ms]
Aug 29 16:46:04.161: INFO: Created: latency-svc-jsqp2
Aug 29 16:46:04.162: INFO: Created: latency-svc-dl55q
Aug 29 16:46:04.162: INFO: Created: latency-svc-5k8rz
Aug 29 16:46:04.163: INFO: Created: latency-svc-gcf8l
Aug 29 16:46:04.163: INFO: Created: latency-svc-f664q
Aug 29 16:46:04.163: INFO: Created: latency-svc-6wpj2
Aug 29 16:46:04.163: INFO: Created: latency-svc-8bmvk
Aug 29 16:46:04.163: INFO: Created: latency-svc-t8rk8
Aug 29 16:46:04.163: INFO: Created: latency-svc-wmvmz
Aug 29 16:46:04.163: INFO: Created: latency-svc-s7rsk
Aug 29 16:46:04.163: INFO: Created: latency-svc-wjlbw
Aug 29 16:46:04.163: INFO: Created: latency-svc-sjfxc
Aug 29 16:46:04.164: INFO: Created: latency-svc-nw8dz
Aug 29 16:46:04.164: INFO: Created: latency-svc-5x6kv
Aug 29 16:46:04.173: INFO: Got endpoints: latency-svc-jsqp2 [144.041738ms]
Aug 29 16:46:04.175: INFO: Created: latency-svc-scvkv
Aug 29 16:46:04.188: INFO: Created: latency-svc-9v47c
Aug 29 16:46:04.713: INFO: Got endpoints: latency-svc-gcf8l [723.703541ms]
Aug 29 16:46:04.714: INFO: Got endpoints: latency-svc-6wpj2 [728.52712ms]
Aug 29 16:46:04.716: INFO: Got endpoints: latency-svc-8bmvk [729.406219ms]
Aug 29 16:46:04.721: INFO: Got endpoints: latency-svc-dl55q [729.336488ms]
Aug 29 16:46:04.723: INFO: Got endpoints: latency-svc-sjfxc [734.856834ms]
Aug 29 16:46:04.747: INFO: Created: latency-svc-67l4p
Aug 29 16:46:04.748: INFO: Got endpoints: latency-svc-nw8dz [674.286198ms]
Aug 29 16:46:04.748: INFO: Got endpoints: latency-svc-wmvmz [774.746651ms]
Aug 29 16:46:04.749: INFO: Got endpoints: latency-svc-f664q [742.466063ms]
Aug 29 16:46:04.749: INFO: Got endpoints: latency-svc-5x6kv [791.315198ms]
Aug 29 16:46:04.749: INFO: Got endpoints: latency-svc-t8rk8 [757.585486ms]
Aug 29 16:46:04.749: INFO: Got endpoints: latency-svc-5k8rz [754.99859ms]
Aug 29 16:46:04.782: INFO: Got endpoints: latency-svc-s7rsk [772.6595ms]
Aug 29 16:46:04.783: INFO: Created: latency-svc-8ctbv
Aug 29 16:46:04.796: INFO: Created: latency-svc-ff5bv
Aug 29 16:46:04.796: INFO: Created: latency-svc-r4pkw
Aug 29 16:46:04.797: INFO: Created: latency-svc-k6j7x
Aug 29 16:46:04.805: INFO: Created: latency-svc-4zxch
Aug 29 16:46:04.815: INFO: Created: latency-svc-28bmm
Aug 29 16:46:05.443: INFO: Got endpoints: latency-svc-wjlbw [1.475324032s]
Aug 29 16:46:05.455: INFO: Created: latency-svc-n9wfw
Aug 29 16:46:05.456: INFO: Got endpoints: latency-svc-k6j7x [741.51837ms]
Aug 29 16:46:05.456: INFO: Got endpoints: latency-svc-9v47c [1.282998145s]
Aug 29 16:46:05.456: INFO: Got endpoints: latency-svc-scvkv [1.33160197s]
Aug 29 16:46:05.456: INFO: Got endpoints: latency-svc-67l4p [742.693152ms]
Aug 29 16:46:05.497: INFO: Created: latency-svc-6gwx8
Aug 29 16:46:05.502: INFO: Got endpoints: latency-svc-6gwx8 [719.918691ms]
Aug 29 16:46:05.500: INFO: Created: latency-svc-rkgv4
Aug 29 16:46:05.500: INFO: Created: latency-svc-kc982
Aug 29 16:46:05.500: INFO: Got endpoints: latency-svc-n9wfw [733.758001ms]
Aug 29 16:46:05.500: INFO: Got endpoints: latency-svc-ff5bv [778.878681ms]
Aug 29 16:46:05.500: INFO: Got endpoints: latency-svc-r4pkw [777.433746ms]
Aug 29 16:46:05.500: INFO: Got endpoints: latency-svc-8ctbv [783.893932ms]
Aug 29 16:46:05.500: INFO: Got endpoints: latency-svc-28bmm [751.618863ms]
Aug 29 16:46:05.500: INFO: Got endpoints: latency-svc-4zxch [751.956466ms]
Aug 29 16:46:05.518: INFO: Created: latency-svc-frgzm
Aug 29 16:46:05.519: INFO: Got endpoints: latency-svc-kc982 [752.34514ms]
Aug 29 16:46:05.519: INFO: Got endpoints: latency-svc-rkgv4 [752.659653ms]
Aug 29 16:46:05.525: INFO: Got endpoints: latency-svc-frgzm [759.070639ms]
Aug 29 16:46:06.443: INFO: Created: latency-svc-fpmqz
Aug 29 16:46:06.444: INFO: Created: latency-svc-fp77b
Aug 29 16:46:06.463: INFO: Got endpoints: latency-svc-fp77b [959.962557ms]
Aug 29 16:46:06.461: INFO: Created: latency-svc-dw9xs
Aug 29 16:46:06.462: INFO: Created: latency-svc-ch2q7
Aug 29 16:46:06.477: INFO: Created: latency-svc-rvgnm
Aug 29 16:46:06.479: INFO: Created: latency-svc-bvnxz
Aug 29 16:46:06.480: INFO: Got endpoints: latency-svc-dw9xs [966.359693ms]
Aug 29 16:46:06.480: INFO: Created: latency-svc-h58g2
Aug 29 16:46:06.481: INFO: Created: latency-svc-29hrk
Aug 29 16:46:06.481: INFO: Created: latency-svc-6d6c4
Aug 29 16:46:06.481: INFO: Created: latency-svc-7sdsp
Aug 29 16:46:06.486: INFO: Got endpoints: latency-svc-7sdsp [979.502846ms]
Aug 29 16:46:06.493: INFO: Got endpoints: latency-svc-29hrk [973.172322ms]
Aug 29 16:46:06.494: INFO: Got endpoints: latency-svc-h58g2 [1.03570792s]
Aug 29 16:46:06.495: INFO: Got endpoints: latency-svc-rvgnm [977.333185ms]
Aug 29 16:46:06.499: INFO: Got endpoints: latency-svc-fpmqz [981.978272ms]
Aug 29 16:46:06.500: INFO: Got endpoints: latency-svc-6d6c4 [985.425497ms]
Aug 29 16:46:06.505: INFO: Created: latency-svc-vrxvx
Aug 29 16:46:06.505: INFO: Got endpoints: latency-svc-bvnxz [995.555711ms]
Aug 29 16:46:06.508: INFO: Got endpoints: latency-svc-ch2q7 [1.019378663s]
Aug 29 16:46:06.481: INFO: Created: latency-svc-jn7c2
Aug 29 16:46:06.510: INFO: Got endpoints: latency-svc-jn7c2 [1.010204069s]
Aug 29 16:46:06.482: INFO: Created: latency-svc-g5tpg
Aug 29 16:46:06.514: INFO: Got endpoints: latency-svc-g5tpg [993.210437ms]
Aug 29 16:46:06.482: INFO: Created: latency-svc-zdpm7
Aug 29 16:46:06.517: INFO: Got endpoints: latency-svc-zdpm7 [1.072871539s]
Aug 29 16:46:06.483: INFO: Created: latency-svc-fzd6t
Aug 29 16:46:06.520: INFO: Got endpoints: latency-svc-fzd6t [1.064460073s]
Aug 29 16:46:06.484: INFO: Created: latency-svc-f4w9q
Aug 29 16:46:06.521: INFO: Got endpoints: latency-svc-f4w9q [996.051706ms]
Aug 29 16:46:06.562: INFO: Created: latency-svc-xfhfp
Aug 29 16:46:06.562: INFO: Created: latency-svc-z6kqh
Aug 29 16:46:06.563: INFO: Created: latency-svc-5kkb6
Aug 29 16:46:06.564: INFO: Got endpoints: latency-svc-vrxvx [100.798879ms]
Aug 29 16:46:06.568: INFO: Got endpoints: latency-svc-z6kqh [83.752985ms]
Aug 29 16:46:06.569: INFO: Got endpoints: latency-svc-xfhfp [73.917055ms]
Aug 29 16:46:06.569: INFO: Got endpoints: latency-svc-5kkb6 [75.531811ms]
Aug 29 16:46:06.805: INFO: Created: latency-svc-h76m2
Aug 29 16:46:06.828: INFO: Created: latency-svc-69lm8
Aug 29 16:46:06.897: INFO: Created: latency-svc-ppml8
Aug 29 16:46:06.898: INFO: Created: latency-svc-m8fkw
Aug 29 16:46:06.898: INFO: Created: latency-svc-68v8c
Aug 29 16:46:06.899: INFO: Created: latency-svc-mmktt
Aug 29 16:46:06.900: INFO: Created: latency-svc-fk2gh
Aug 29 16:46:06.900: INFO: Created: latency-svc-cp6dw
Aug 29 16:46:06.901: INFO: Created: latency-svc-kxvdd
Aug 29 16:46:06.901: INFO: Created: latency-svc-dfvlb
Aug 29 16:46:06.901: INFO: Created: latency-svc-5d74w
Aug 29 16:46:06.902: INFO: Created: latency-svc-27hsc
Aug 29 16:46:06.902: INFO: Got endpoints: latency-svc-fk2gh [403.314849ms]
Aug 29 16:46:06.902: INFO: Created: latency-svc-kt826
Aug 29 16:46:06.903: INFO: Got endpoints: latency-svc-kt826 [401.954566ms]
Aug 29 16:46:06.903: INFO: Created: latency-svc-knk2p
Aug 29 16:46:06.904: INFO: Got endpoints: latency-svc-knk2p [388.72268ms]
Aug 29 16:46:06.905: INFO: Got endpoints: latency-svc-68v8c [384.241074ms]
Aug 29 16:46:06.906: INFO: Got endpoints: latency-svc-h76m2 [337.121203ms]
Aug 29 16:46:06.908: INFO: Got endpoints: latency-svc-69lm8 [338.351326ms]
Aug 29 16:46:06.908: INFO: Got endpoints: latency-svc-m8fkw [399.359109ms]
Aug 29 16:46:06.902: INFO: Created: latency-svc-9f6vs
Aug 29 16:46:06.911: INFO: Got endpoints: latency-svc-cp6dw [391.363667ms]
Aug 29 16:46:06.929: INFO: Got endpoints: latency-svc-ppml8 [407.078007ms]
Aug 29 16:46:06.948: INFO: Created: latency-svc-nt7bf
Aug 29 16:46:06.949: INFO: Created: latency-svc-qfkks
Aug 29 16:46:06.950: INFO: Created: latency-svc-phxbd
Aug 29 16:46:06.965: INFO: Created: latency-svc-sbjfr
Aug 29 16:46:06.981: INFO: Got endpoints: latency-svc-mmktt [417.158711ms]
Aug 29 16:46:06.982: INFO: Created: latency-svc-gt5p6
Aug 29 16:46:06.992: INFO: Created: latency-svc-2nbrv
Aug 29 16:46:07.004: INFO: Created: latency-svc-q67j6
Aug 29 16:46:07.009: INFO: Created: latency-svc-s6dxj
Aug 29 16:46:07.021: INFO: Created: latency-svc-wtdcq
Aug 29 16:46:07.028: INFO: Got endpoints: latency-svc-dfvlb [522.704749ms]
Aug 29 16:46:07.046: INFO: Created: latency-svc-nhkc6
Aug 29 16:46:07.065: INFO: Created: latency-svc-82wps
Aug 29 16:46:07.471: INFO: Got endpoints: latency-svc-27hsc [903.615486ms]
Aug 29 16:46:07.491: INFO: Got endpoints: latency-svc-5d74w [1.004957701s]
Aug 29 16:46:07.494: INFO: Got endpoints: latency-svc-9f6vs [982.806425ms]
Aug 29 16:46:07.495: INFO: Got endpoints: latency-svc-kxvdd [999.184792ms]
Aug 29 16:46:07.496: INFO: Got endpoints: latency-svc-phxbd [593.851962ms]
Aug 29 16:46:07.518: INFO: Got endpoints: latency-svc-qfkks [614.14023ms]
Aug 29 16:46:07.522: INFO: Got endpoints: latency-svc-gt5p6 [614.647925ms]
Aug 29 16:46:07.523: INFO: Got endpoints: latency-svc-nt7bf [619.278432ms]
Aug 29 16:46:07.555: INFO: Created: latency-svc-9f8jw
Aug 29 16:46:07.555: INFO: Got endpoints: latency-svc-2nbrv [642.292458ms]
Aug 29 16:46:07.555: INFO: Created: latency-svc-cccbj
Aug 29 16:46:07.555: INFO: Got endpoints: latency-svc-sbjfr [648.911325ms]
Aug 29 16:46:07.559: INFO: Created: latency-svc-wlngj
Aug 29 16:46:07.571: INFO: Created: latency-svc-lhwkw
Aug 29 16:46:07.588: INFO: Created: latency-svc-9sjlb
Aug 29 16:46:07.588: INFO: Got endpoints: latency-svc-q67j6 [680.170975ms]
Aug 29 16:46:07.609: INFO: Created: latency-svc-kbqzt
Aug 29 16:46:07.611: INFO: Created: latency-svc-xxwt2
Aug 29 16:46:07.625: INFO: Created: latency-svc-4sjzg
Aug 29 16:46:07.629: INFO: Got endpoints: latency-svc-s6dxj [724.056514ms]
Aug 29 16:46:07.647: INFO: Created: latency-svc-5twwv
Aug 29 16:46:07.647: INFO: Created: latency-svc-c6hqt
Aug 29 16:46:07.659: INFO: Created: latency-svc-l4zhl
Aug 29 16:46:07.667: INFO: Created: latency-svc-5nw56
Aug 29 16:46:07.675: INFO: Got endpoints: latency-svc-wtdcq [745.529624ms]
Aug 29 16:46:07.693: INFO: Created: latency-svc-9fr9l
Aug 29 16:46:07.723: INFO: Got endpoints: latency-svc-nhkc6 [741.795316ms]
Aug 29 16:46:07.753: INFO: Created: latency-svc-hwtw7
Aug 29 16:46:07.771: INFO: Got endpoints: latency-svc-82wps [741.867727ms]
Aug 29 16:46:07.791: INFO: Created: latency-svc-k8hcr
Aug 29 16:46:07.818: INFO: Got endpoints: latency-svc-9f8jw [346.347562ms]
Aug 29 16:46:07.845: INFO: Created: latency-svc-jtmk6
Aug 29 16:46:07.872: INFO: Got endpoints: latency-svc-cccbj [380.849505ms]
Aug 29 16:46:07.889: INFO: Created: latency-svc-c4j4w
Aug 29 16:46:07.924: INFO: Got endpoints: latency-svc-wlngj [430.332821ms]
Aug 29 16:46:07.948: INFO: Created: latency-svc-qhwcb
Aug 29 16:46:07.970: INFO: Got endpoints: latency-svc-lhwkw [475.000327ms]
Aug 29 16:46:07.988: INFO: Created: latency-svc-xtbsq
Aug 29 16:46:08.023: INFO: Got endpoints: latency-svc-9sjlb [526.120551ms]
Aug 29 16:46:08.047: INFO: Created: latency-svc-6bb69
Aug 29 16:46:08.073: INFO: Got endpoints: latency-svc-kbqzt [554.614053ms]
Aug 29 16:46:08.089: INFO: Created: latency-svc-rhtkh
Aug 29 16:46:08.118: INFO: Got endpoints: latency-svc-xxwt2 [595.498672ms]
Aug 29 16:46:08.135: INFO: Created: latency-svc-gdwps
Aug 29 16:46:08.176: INFO: Got endpoints: latency-svc-4sjzg [653.677518ms]
Aug 29 16:46:08.195: INFO: Created: latency-svc-mhbvn
Aug 29 16:46:08.228: INFO: Got endpoints: latency-svc-c6hqt [673.311999ms]
Aug 29 16:46:08.252: INFO: Created: latency-svc-p6ztf
Aug 29 16:46:08.280: INFO: Got endpoints: latency-svc-5twwv [722.533413ms]
Aug 29 16:46:08.303: INFO: Created: latency-svc-slhzr
Aug 29 16:46:08.334: INFO: Got endpoints: latency-svc-l4zhl [745.225466ms]
Aug 29 16:46:08.361: INFO: Created: latency-svc-n4vgs
Aug 29 16:46:08.373: INFO: Got endpoints: latency-svc-5nw56 [744.209876ms]
Aug 29 16:46:08.396: INFO: Created: latency-svc-lm78v
Aug 29 16:46:08.422: INFO: Got endpoints: latency-svc-9fr9l [746.296378ms]
Aug 29 16:46:08.441: INFO: Created: latency-svc-dvgm7
Aug 29 16:46:08.470: INFO: Got endpoints: latency-svc-hwtw7 [746.740753ms]
Aug 29 16:46:08.487: INFO: Created: latency-svc-h2zfp
Aug 29 16:46:08.521: INFO: Got endpoints: latency-svc-k8hcr [750.30748ms]
Aug 29 16:46:08.539: INFO: Created: latency-svc-8vdcz
Aug 29 16:46:08.569: INFO: Got endpoints: latency-svc-jtmk6 [751.263231ms]
Aug 29 16:46:08.582: INFO: Created: latency-svc-lz54w
Aug 29 16:46:08.619: INFO: Got endpoints: latency-svc-c4j4w [746.882026ms]
Aug 29 16:46:08.645: INFO: Created: latency-svc-46kdv
Aug 29 16:46:09.090: INFO: Got endpoints: latency-svc-qhwcb [1.165964741s]
Aug 29 16:46:09.156: INFO: Got endpoints: latency-svc-6bb69 [1.133514911s]
Aug 29 16:46:09.160: INFO: Got endpoints: latency-svc-xtbsq [1.18911655s]
Aug 29 16:46:09.164: INFO: Got endpoints: latency-svc-rhtkh [1.090845633s]
Aug 29 16:46:09.165: INFO: Got endpoints: latency-svc-mhbvn [988.89025ms]
Aug 29 16:46:09.166: INFO: Got endpoints: latency-svc-gdwps [1.047009785s]
Aug 29 16:46:09.178: INFO: Got endpoints: latency-svc-p6ztf [948.925371ms]
Aug 29 16:46:09.185: INFO: Got endpoints: latency-svc-n4vgs [850.807076ms]
Aug 29 16:46:09.185: INFO: Got endpoints: latency-svc-slhzr [904.9604ms]
Aug 29 16:46:09.206: INFO: Created: latency-svc-nrp7n
Aug 29 16:46:09.207: INFO: Got endpoints: latency-svc-dvgm7 [784.555508ms]
Aug 29 16:46:09.207: INFO: Got endpoints: latency-svc-lm78v [831.334126ms]
Aug 29 16:46:09.207: INFO: Created: latency-svc-4fmvl
Aug 29 16:46:09.213: INFO: Created: latency-svc-xw75m
Aug 29 16:46:09.219: INFO: Created: latency-svc-zlx7m
Aug 29 16:46:09.223: INFO: Got endpoints: latency-svc-h2zfp [752.869033ms]
Aug 29 16:46:09.232: INFO: Created: latency-svc-hwh72
Aug 29 16:46:09.247: INFO: Created: latency-svc-rfl7g
Aug 29 16:46:09.479: INFO: Created: latency-svc-98pg2
Aug 29 16:46:09.483: INFO: Got endpoints: latency-svc-4fmvl [326.256775ms]
Aug 29 16:46:09.483: INFO: Got endpoints: latency-svc-8vdcz [961.888608ms]
Aug 29 16:46:09.484: INFO: Got endpoints: latency-svc-lz54w [914.135549ms]
Aug 29 16:46:09.484: INFO: Got endpoints: latency-svc-46kdv [864.417079ms]
Aug 29 16:46:09.484: INFO: Got endpoints: latency-svc-nrp7n [391.981669ms]
Aug 29 16:46:09.503: INFO: Created: latency-svc-6r9bp
Aug 29 16:46:09.523: INFO: Got endpoints: latency-svc-xw75m [360.686169ms]
Aug 29 16:46:09.570: INFO: Got endpoints: latency-svc-zlx7m [403.925902ms]
Aug 29 16:46:09.620: INFO: Got endpoints: latency-svc-hwh72 [455.288969ms]
Aug 29 16:46:09.674: INFO: Got endpoints: latency-svc-rfl7g [508.509135ms]
Aug 29 16:46:09.720: INFO: Got endpoints: latency-svc-98pg2 [542.016298ms]
Aug 29 16:46:09.772: INFO: Got endpoints: latency-svc-6r9bp [587.03388ms]
Aug 29 16:46:09.772: INFO: Latencies: [73.917055ms 75.531811ms 83.752985ms 88.869073ms 96.653883ms 100.798879ms 106.963267ms 107.494181ms 128.71341ms 144.041738ms 172.029137ms 173.600437ms 181.672035ms 208.774287ms 219.410968ms 259.144513ms 270.445184ms 316.210642ms 326.256775ms 326.730289ms 337.121203ms 338.351326ms 341.435424ms 342.637296ms 343.483834ms 346.347562ms 353.01381ms 353.580687ms 360.322765ms 360.601278ms 360.686169ms 361.547772ms 363.429546ms 363.75469ms 371.61686ms 377.245117ms 377.661161ms 380.849505ms 381.730624ms 384.241074ms 388.72268ms 390.174388ms 391.266969ms 391.363667ms 391.981669ms 399.359109ms 400.886481ms 401.954566ms 403.314849ms 403.925902ms 407.078007ms 417.158711ms 419.77715ms 419.869416ms 425.904525ms 430.332821ms 438.255116ms 451.936069ms 455.288969ms 456.36358ms 464.173203ms 466.334945ms 468.44274ms 475.000327ms 475.437105ms 476.721421ms 487.296943ms 491.35737ms 493.74186ms 507.009552ms 508.509135ms 513.175927ms 518.525346ms 519.196382ms 521.842899ms 522.704749ms 526.120551ms 542.016298ms 554.614053ms 556.536682ms 557.122758ms 580.312288ms 583.479945ms 585.093177ms 587.03388ms 591.376995ms 593.851962ms 595.498672ms 598.89586ms 603.419888ms 609.889972ms 614.14023ms 614.647925ms 619.278432ms 625.900984ms 642.292458ms 642.559073ms 648.911325ms 653.677518ms 662.175612ms 673.311999ms 674.286198ms 676.539757ms 679.867971ms 680.170975ms 701.109117ms 719.918691ms 722.533413ms 723.703541ms 724.056514ms 728.280292ms 728.52712ms 729.336488ms 729.406219ms 733.758001ms 734.856834ms 741.51837ms 741.795316ms 741.867727ms 742.466063ms 742.693152ms 744.209876ms 745.225466ms 745.529624ms 746.296378ms 746.740753ms 746.882026ms 750.30748ms 751.263231ms 751.618863ms 751.956466ms 752.34514ms 752.659653ms 752.869033ms 754.99859ms 757.585486ms 758.928313ms 759.070639ms 761.943303ms 772.6595ms 774.746651ms 777.433746ms 778.878681ms 783.893932ms 784.555508ms 791.315198ms 831.334126ms 836.176953ms 850.807076ms 864.417079ms 866.23277ms 893.210855ms 903.615486ms 904.9604ms 905.44583ms 914.135549ms 916.894875ms 918.390851ms 939.468775ms 948.925371ms 959.962557ms 961.888608ms 966.359693ms 972.832834ms 973.172322ms 977.333185ms 979.502846ms 981.978272ms 982.806425ms 985.425497ms 988.89025ms 993.210437ms 995.555711ms 996.051706ms 999.184792ms 1.004957701s 1.010204069s 1.014834722s 1.019378663s 1.03570792s 1.047009785s 1.064460073s 1.06985249s 1.072871539s 1.090845633s 1.11414203s 1.133514911s 1.164354379s 1.165964741s 1.166604292s 1.18911655s 1.210137434s 1.254507354s 1.255587116s 1.258523626s 1.271353177s 1.275155874s 1.282998145s 1.33160197s 1.475324032s]
Aug 29 16:46:09.772: INFO: 50 %ile: 673.311999ms
Aug 29 16:46:09.772: INFO: 90 %ile: 1.047009785s
Aug 29 16:46:09.772: INFO: 99 %ile: 1.33160197s
Aug 29 16:46:09.773: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:46:09.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7712" for this suite.

• [SLOW TEST:12.411 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":314,"skipped":5935,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:46:09.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:46:09.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7774" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":315,"skipped":5946,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:46:09.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Aug 29 16:46:30.902: INFO: EndpointSlice for Service endpointslice-7307/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:46:40.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7307" for this suite.

• [SLOW TEST:30.984 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":316,"skipped":5962,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:46:40.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:46:41.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8108" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":5977,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:46:41.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:46:56.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8100" for this suite.

• [SLOW TEST:15.684 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":318,"skipped":5993,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:46:56.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-a52b8d4c-77e5-40fb-a4db-0dcfe83c66a9
STEP: Creating a pod to test consume configMaps
Aug 29 16:46:56.854: INFO: Waiting up to 5m0s for pod "pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde" in namespace "configmap-4946" to be "Succeeded or Failed"
Aug 29 16:46:56.868: INFO: Pod "pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020152ms
Aug 29 16:46:58.879: INFO: Pod "pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025664373s
Aug 29 16:47:00.892: INFO: Pod "pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038557227s
STEP: Saw pod success
Aug 29 16:47:00.892: INFO: Pod "pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde" satisfied condition "Succeeded or Failed"
Aug 29 16:47:00.900: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:47:00.935: INFO: Waiting for pod pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde to disappear
Aug 29 16:47:00.940: INFO: Pod pod-configmaps-80f3cbdb-6878-4362-ad04-419766a5ebde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:00.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4946" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":6029,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:00.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:47:02.221: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:47:05.278: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:47:05.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Aug 29 16:47:06.500: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:09.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3634" for this suite.
STEP: Destroying namespace "webhook-3634-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.504 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":320,"skipped":6034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:09.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
Aug 29 16:47:09.560: INFO: Creating simple deployment test-deployment-878r5
Aug 29 16:47:09.599: INFO: deployment "test-deployment-878r5" doesn't have the required revision set
STEP: Getting /status
Aug 29 16:47:11.629: INFO: Deployment test-deployment-878r5 has Conditions: [{Available True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-878r5-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Aug 29 16:47:11.646: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 47, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 47, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 47, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 47, 9, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-878r5-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Aug 29 16:47:11.650: INFO: Observed &Deployment event: ADDED
Aug 29 16:47:11.650: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-878r5-764bc7c4b7"}
Aug 29 16:47:11.650: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.650: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-878r5-764bc7c4b7"}
Aug 29 16:47:11.651: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:47:11.651: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.651: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:47:11.651: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-878r5-764bc7c4b7" is progressing.}
Aug 29 16:47:11.651: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.652: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:47:11.652: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-878r5-764bc7c4b7" has successfully progressed.}
Aug 29 16:47:11.652: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.652: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:47:11.652: INFO: Observed Deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-878r5-764bc7c4b7" has successfully progressed.}
Aug 29 16:47:11.652: INFO: Found Deployment test-deployment-878r5 in namespace deployment-8035 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:47:11.652: INFO: Deployment test-deployment-878r5 has an updated status
STEP: patching the Statefulset Status
Aug 29 16:47:11.652: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 16:47:11.665: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Aug 29 16:47:11.673: INFO: Observed &Deployment event: ADDED
Aug 29 16:47:11.673: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-878r5-764bc7c4b7"}
Aug 29 16:47:11.674: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.674: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-878r5-764bc7c4b7"}
Aug 29 16:47:11.675: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:47:11.675: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.675: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:47:11.675: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:09 +0000 UTC 2022-08-29 16:47:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-878r5-764bc7c4b7" is progressing.}
Aug 29 16:47:11.675: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.675: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:47:11.676: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-878r5-764bc7c4b7" has successfully progressed.}
Aug 29 16:47:11.676: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.676: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:47:11.677: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:47:11 +0000 UTC 2022-08-29 16:47:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-878r5-764bc7c4b7" has successfully progressed.}
Aug 29 16:47:11.677: INFO: Observed deployment test-deployment-878r5 in namespace deployment-8035 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:47:11.677: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:47:11.678: INFO: Found deployment test-deployment-878r5 in namespace deployment-8035 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 29 16:47:11.678: INFO: Deployment test-deployment-878r5 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 16:47:11.685: INFO: Deployment "test-deployment-878r5":
&Deployment{ObjectMeta:{test-deployment-878r5  deployment-8035  427a883e-62d8-4e55-908a-049b1b2735c8 42723 1 2022-08-29 16:47:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-08-29 16:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-08-29 16:47:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-08-29 16:47:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00457c8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-878r5-764bc7c4b7",LastUpdateTime:2022-08-29 16:47:11 +0000 UTC,LastTransitionTime:2022-08-29 16:47:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 16:47:11.694: INFO: New ReplicaSet "test-deployment-878r5-764bc7c4b7" of Deployment "test-deployment-878r5":
&ReplicaSet{ObjectMeta:{test-deployment-878r5-764bc7c4b7  deployment-8035  86df4eb4-4204-4209-b448-2ccbe14521d9 42715 1 2022-08-29 16:47:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-878r5 427a883e-62d8-4e55-908a-049b1b2735c8 0xc002d19900 0xc002d19901}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427a883e-62d8-4e55-908a-049b1b2735c8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:47:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d199a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:47:11.702: INFO: Pod "test-deployment-878r5-764bc7c4b7-6p5rn" is available:
&Pod{ObjectMeta:{test-deployment-878r5-764bc7c4b7-6p5rn test-deployment-878r5-764bc7c4b7- deployment-8035  02df7ce9-b313-42a6-b4f1-f38fb9b324d3 42714 0 2022-08-29 16:47:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:94376c3c780157acadde4393385f3e6cc9a7ca16aa44523a4cda750b06d9f115 cni.projectcalico.org/podIP:172.25.2.12/32 cni.projectcalico.org/podIPs:172.25.2.12/32] [{apps/v1 ReplicaSet test-deployment-878r5-764bc7c4b7 86df4eb4-4204-4209-b448-2ccbe14521d9 0xc002d19d80 0xc002d19d81}] []  [{kube-controller-manager Update v1 2022-08-29 16:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86df4eb4-4204-4209-b448-2ccbe14521d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:47:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm24f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm24f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-90.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:47:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:47:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:47:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:47:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.23.90,PodIP:172.25.2.12,StartTime:2022-08-29 16:47:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:47:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://941fb3d60ed178655c934f4e69a2a1444a90ab49456fc85a67618ba30dc4d5ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:11.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8035" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":321,"skipped":6081,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:11.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Aug 29 16:47:11.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 create -f -'
Aug 29 16:47:13.313: INFO: stderr: ""
Aug 29 16:47:13.313: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 16:47:13.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:47:13.412: INFO: stderr: ""
Aug 29 16:47:13.412: INFO: stdout: "update-demo-nautilus-h44m7 update-demo-nautilus-l8hhq "
Aug 29 16:47:13.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-h44m7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:13.517: INFO: stderr: ""
Aug 29 16:47:13.517: INFO: stdout: ""
Aug 29 16:47:13.517: INFO: update-demo-nautilus-h44m7 is created but not running
Aug 29 16:47:18.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:47:18.672: INFO: stderr: ""
Aug 29 16:47:18.672: INFO: stdout: "update-demo-nautilus-h44m7 update-demo-nautilus-l8hhq "
Aug 29 16:47:18.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-h44m7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:18.897: INFO: stderr: ""
Aug 29 16:47:18.897: INFO: stdout: "true"
Aug 29 16:47:18.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-h44m7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:47:19.017: INFO: stderr: ""
Aug 29 16:47:19.018: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:47:19.018: INFO: validating pod update-demo-nautilus-h44m7
Aug 29 16:47:19.032: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:47:19.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:47:19.032: INFO: update-demo-nautilus-h44m7 is verified up and running
Aug 29 16:47:19.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-l8hhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:19.133: INFO: stderr: ""
Aug 29 16:47:19.133: INFO: stdout: "true"
Aug 29 16:47:19.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-l8hhq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:47:19.214: INFO: stderr: ""
Aug 29 16:47:19.214: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:47:19.214: INFO: validating pod update-demo-nautilus-l8hhq
Aug 29 16:47:19.229: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:47:19.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:47:19.229: INFO: update-demo-nautilus-l8hhq is verified up and running
STEP: scaling down the replication controller
Aug 29 16:47:19.231: INFO: scanned /root for discovery docs: <nil>
Aug 29 16:47:19.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 29 16:47:20.361: INFO: stderr: ""
Aug 29 16:47:20.361: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 16:47:20.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:47:20.467: INFO: stderr: ""
Aug 29 16:47:20.467: INFO: stdout: "update-demo-nautilus-h44m7 update-demo-nautilus-l8hhq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 29 16:47:25.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:47:25.614: INFO: stderr: ""
Aug 29 16:47:25.614: INFO: stdout: "update-demo-nautilus-l8hhq "
Aug 29 16:47:25.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-l8hhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:25.759: INFO: stderr: ""
Aug 29 16:47:25.759: INFO: stdout: "true"
Aug 29 16:47:25.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-l8hhq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:47:25.903: INFO: stderr: ""
Aug 29 16:47:25.903: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:47:25.903: INFO: validating pod update-demo-nautilus-l8hhq
Aug 29 16:47:25.914: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:47:25.914: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:47:25.914: INFO: update-demo-nautilus-l8hhq is verified up and running
STEP: scaling up the replication controller
Aug 29 16:47:25.917: INFO: scanned /root for discovery docs: <nil>
Aug 29 16:47:25.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 29 16:47:27.100: INFO: stderr: ""
Aug 29 16:47:27.100: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 16:47:27.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:47:27.190: INFO: stderr: ""
Aug 29 16:47:27.190: INFO: stdout: "update-demo-nautilus-jvdd7 update-demo-nautilus-l8hhq "
Aug 29 16:47:27.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-jvdd7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:27.273: INFO: stderr: ""
Aug 29 16:47:27.273: INFO: stdout: ""
Aug 29 16:47:27.273: INFO: update-demo-nautilus-jvdd7 is created but not running
Aug 29 16:47:32.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:47:32.375: INFO: stderr: ""
Aug 29 16:47:32.376: INFO: stdout: "update-demo-nautilus-jvdd7 update-demo-nautilus-l8hhq "
Aug 29 16:47:32.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-jvdd7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:32.464: INFO: stderr: ""
Aug 29 16:47:32.464: INFO: stdout: "true"
Aug 29 16:47:32.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-jvdd7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:47:32.547: INFO: stderr: ""
Aug 29 16:47:32.547: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:47:32.547: INFO: validating pod update-demo-nautilus-jvdd7
Aug 29 16:47:32.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:47:32.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:47:32.559: INFO: update-demo-nautilus-jvdd7 is verified up and running
Aug 29 16:47:32.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-l8hhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:47:32.644: INFO: stderr: ""
Aug 29 16:47:32.644: INFO: stdout: "true"
Aug 29 16:47:32.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods update-demo-nautilus-l8hhq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:47:32.737: INFO: stderr: ""
Aug 29 16:47:32.737: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:47:32.737: INFO: validating pod update-demo-nautilus-l8hhq
Aug 29 16:47:32.748: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:47:32.748: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:47:32.748: INFO: update-demo-nautilus-l8hhq is verified up and running
STEP: using delete to clean up resources
Aug 29 16:47:32.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 delete --grace-period=0 --force -f -'
Aug 29 16:47:32.853: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:47:32.853: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 29 16:47:32.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get rc,svc -l name=update-demo --no-headers'
Aug 29 16:47:32.964: INFO: stderr: "No resources found in kubectl-7295 namespace.\n"
Aug 29 16:47:32.964: INFO: stdout: ""
Aug 29 16:47:32.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-7295 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 16:47:33.650: INFO: stderr: ""
Aug 29 16:47:33.650: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:33.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7295" for this suite.

• [SLOW TEST:21.988 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":322,"skipped":6095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:33.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-ed3e5490-e5a7-4673-adec-9470c1860b67
STEP: Creating a pod to test consume configMaps
Aug 29 16:47:34.197: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672" in namespace "projected-8936" to be "Succeeded or Failed"
Aug 29 16:47:34.206: INFO: Pod "pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672": Phase="Pending", Reason="", readiness=false. Elapsed: 8.75779ms
Aug 29 16:47:36.217: INFO: Pod "pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020509883s
Aug 29 16:47:38.423: INFO: Pod "pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.226114663s
STEP: Saw pod success
Aug 29 16:47:38.423: INFO: Pod "pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672" satisfied condition "Succeeded or Failed"
Aug 29 16:47:38.430: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:47:38.467: INFO: Waiting for pod pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672 to disappear
Aug 29 16:47:38.474: INFO: Pod pod-projected-configmaps-433507ed-d624-4c3e-a601-2315755b4672 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:38.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8936" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":6138,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:38.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
Aug 29 16:47:39.214: INFO: created pod pod-service-account-defaultsa
Aug 29 16:47:39.214: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 29 16:47:39.227: INFO: created pod pod-service-account-mountsa
Aug 29 16:47:39.227: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 29 16:47:39.238: INFO: created pod pod-service-account-nomountsa
Aug 29 16:47:39.238: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 29 16:47:39.259: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 29 16:47:39.259: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 29 16:47:39.271: INFO: created pod pod-service-account-mountsa-mountspec
Aug 29 16:47:39.272: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 29 16:47:39.283: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 29 16:47:39.283: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 29 16:47:39.293: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 29 16:47:39.294: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 29 16:47:39.304: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 29 16:47:39.304: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 29 16:47:39.313: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 29 16:47:39.313: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:39.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7354" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":324,"skipped":6150,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:39.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Aug 29 16:47:39.441: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:41.752: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:43.453: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 29 16:47:44.516: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:45.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6875" for this suite.

• [SLOW TEST:6.242 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":325,"skipped":6153,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:45.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:47:45.680: INFO: The status of Pod pod-secrets-70e7fefb-44ca-4f4a-80b6-db15e7cf4d14 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:47.692: INFO: The status of Pod pod-secrets-70e7fefb-44ca-4f4a-80b6-db15e7cf4d14 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:47.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7193" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":326,"skipped":6161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:47.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 16:47:51.937: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:51.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9913" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6232,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:52.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
Aug 29 16:47:52.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 29 16:47:52.172: INFO: stderr: ""
Aug 29 16:47:52.172: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
Aug 29 16:47:52.172: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 29 16:47:52.172: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-905" to be "running and ready, or succeeded"
Aug 29 16:47:52.190: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.643814ms
Aug 29 16:47:54.200: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.027572845s
Aug 29 16:47:54.200: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 29 16:47:54.200: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 29 16:47:54.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 logs logs-generator logs-generator'
Aug 29 16:47:54.382: INFO: stderr: ""
Aug 29 16:47:54.385: INFO: stdout: "I0829 16:47:53.045137       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/cgs 354\nI0829 16:47:53.245405       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nlnr 583\nI0829 16:47:53.445959       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/n4bj 450\nI0829 16:47:53.645330       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n72 342\nI0829 16:47:53.845675       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/pss 247\nI0829 16:47:54.046281       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/jdcq 332\nI0829 16:47:54.249678       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qvfm 378\n"
STEP: limiting log lines
Aug 29 16:47:54.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 logs logs-generator logs-generator --tail=1'
Aug 29 16:47:54.512: INFO: stderr: ""
Aug 29 16:47:54.512: INFO: stdout: "I0829 16:47:54.446074       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jpt6 438\n"
Aug 29 16:47:54.512: INFO: got output "I0829 16:47:54.446074       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jpt6 438\n"
STEP: limiting log bytes
Aug 29 16:47:54.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 logs logs-generator logs-generator --limit-bytes=1'
Aug 29 16:47:54.650: INFO: stderr: ""
Aug 29 16:47:54.650: INFO: stdout: "I"
Aug 29 16:47:54.650: INFO: got output "I"
STEP: exposing timestamps
Aug 29 16:47:54.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 29 16:47:54.771: INFO: stderr: ""
Aug 29 16:47:54.771: INFO: stdout: "2022-08-29T16:47:54.645684773Z I0829 16:47:54.645482       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/qpb 517\n"
Aug 29 16:47:54.772: INFO: got output "2022-08-29T16:47:54.645684773Z I0829 16:47:54.645482       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/qpb 517\n"
STEP: restricting to a time range
Aug 29 16:47:57.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 logs logs-generator logs-generator --since=1s'
Aug 29 16:47:57.414: INFO: stderr: ""
Aug 29 16:47:57.415: INFO: stdout: "I0829 16:47:56.445594       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/g67 219\nI0829 16:47:56.646169       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/tdsk 319\nI0829 16:47:56.845669       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/vhf 396\nI0829 16:47:57.045986       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xsc7 262\nI0829 16:47:57.245307       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/gxfx 575\n"
Aug 29 16:47:57.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 logs logs-generator logs-generator --since=24h'
Aug 29 16:47:57.529: INFO: stderr: ""
Aug 29 16:47:57.529: INFO: stdout: "I0829 16:47:53.045137       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/cgs 354\nI0829 16:47:53.245405       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nlnr 583\nI0829 16:47:53.445959       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/n4bj 450\nI0829 16:47:53.645330       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n72 342\nI0829 16:47:53.845675       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/pss 247\nI0829 16:47:54.046281       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/jdcq 332\nI0829 16:47:54.249678       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qvfm 378\nI0829 16:47:54.446074       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jpt6 438\nI0829 16:47:54.645482       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/qpb 517\nI0829 16:47:54.845898       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/mp9n 472\nI0829 16:47:55.045350       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/tjc 357\nI0829 16:47:55.245704       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/5rjz 260\nI0829 16:47:55.446390       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/g6b4 326\nI0829 16:47:55.645801       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/9d5p 247\nI0829 16:47:55.845115       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/657 431\nI0829 16:47:56.045676       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/c4qr 201\nI0829 16:47:56.246257       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/b58t 515\nI0829 16:47:56.445594       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/g67 219\nI0829 16:47:56.646169       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/tdsk 319\nI0829 16:47:56.845669       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/vhf 396\nI0829 16:47:57.045986       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xsc7 262\nI0829 16:47:57.245307       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/gxfx 575\nI0829 16:47:57.445762       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/jf89 305\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
Aug 29 16:47:57.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2626036518 --namespace=kubectl-905 delete pod logs-generator'
Aug 29 16:47:58.757: INFO: stderr: ""
Aug 29 16:47:58.757: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:47:58.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-905" for this suite.

• [SLOW TEST:6.782 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":328,"skipped":6250,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:47:58.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
Aug 29 16:47:58.853: INFO: Waiting up to 5m0s for pod "client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81" in namespace "containers-6344" to be "Succeeded or Failed"
Aug 29 16:47:58.873: INFO: Pod "client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81": Phase="Pending", Reason="", readiness=false. Elapsed: 18.95958ms
Aug 29 16:48:00.885: INFO: Pod "client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81": Phase="Running", Reason="", readiness=false. Elapsed: 2.030921946s
Aug 29 16:48:02.896: INFO: Pod "client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042800705s
STEP: Saw pod success
Aug 29 16:48:02.897: INFO: Pod "client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81" satisfied condition "Succeeded or Failed"
Aug 29 16:48:02.909: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:48:02.962: INFO: Waiting for pod client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81 to disappear
Aug 29 16:48:02.972: INFO: Pod client-containers-a909065e-9d5d-42b1-b8ee-cc486e671d81 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:48:02.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6344" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6254,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:48:03.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 29 16:48:03.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 29 16:48:19.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
Aug 29 16:48:23.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:48:37.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6050" for this suite.

• [SLOW TEST:34.296 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":330,"skipped":6261,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:48:37.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-5c2f49d8-6d49-43d2-b95f-8755e78bcc59
STEP: Creating a pod to test consume secrets
Aug 29 16:48:37.434: INFO: Waiting up to 5m0s for pod "pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707" in namespace "secrets-8364" to be "Succeeded or Failed"
Aug 29 16:48:37.446: INFO: Pod "pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101335ms
Aug 29 16:48:39.459: INFO: Pod "pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025027847s
Aug 29 16:48:41.476: INFO: Pod "pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042622407s
STEP: Saw pod success
Aug 29 16:48:41.476: INFO: Pod "pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707" satisfied condition "Succeeded or Failed"
Aug 29 16:48:41.488: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:48:41.875: INFO: Waiting for pod pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707 to disappear
Aug 29 16:48:41.881: INFO: Pod pod-secrets-247c65d2-cb8a-498e-914f-741288b0e707 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:48:41.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8364" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":6267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:48:41.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:50:02.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-219" for this suite.

• [SLOW TEST:80.191 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":332,"skipped":6324,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:50:02.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 16:50:02.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:50:02.322: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:50:03.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:50:03.351: INFO: Node ip-172-31-16-14.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:50:04.348: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:50:04.349: INFO: Node ip-172-31-20-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:50:05.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:50:05.462: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Aug 29 16:50:05.482: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Aug 29 16:50:06.256: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Aug 29 16:50:06.267: INFO: Observed &DaemonSet event: ADDED
Aug 29 16:50:06.267: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.268: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.268: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.269: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.269: INFO: Found daemon set daemon-set in namespace daemonsets-9964 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 16:50:06.269: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Aug 29 16:50:06.298: INFO: Observed &DaemonSet event: ADDED
Aug 29 16:50:06.300: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.300: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.307: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.307: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.307: INFO: Observed daemon set daemon-set in namespace daemonsets-9964 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 16:50:06.308: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 16:50:06.310: INFO: Found daemon set daemon-set in namespace daemonsets-9964 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 29 16:50:06.310: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9964, will wait for the garbage collector to delete the pods
Aug 29 16:50:06.391: INFO: Deleting DaemonSet.extensions daemon-set took: 14.224154ms
Aug 29 16:50:06.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.24045ms
Aug 29 16:50:09.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:50:09.103: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 16:50:09.108: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43926"},"items":null}

Aug 29 16:50:09.115: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43926"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:50:09.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9964" for this suite.

• [SLOW TEST:7.075 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":333,"skipped":6336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:50:09.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:50:10.414: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 29 16:50:12.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 50, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 50, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 50, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 50, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:50:15.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
Aug 29 16:50:15.615: INFO: Waiting for webhook configuration to be ready...
Aug 29 16:50:15.765: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:50:26.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5932" for this suite.
STEP: Destroying namespace "webhook-5932-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":334,"skipped":6371,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:50:26.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:50:26.414: INFO: created pod
Aug 29 16:50:26.414: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-676" to be "Succeeded or Failed"
Aug 29 16:50:27.166: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 751.701407ms
Aug 29 16:50:29.179: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765331762s
Aug 29 16:50:31.202: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787676644s
Aug 29 16:50:33.215: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.801182543s
STEP: Saw pod success
Aug 29 16:50:33.215: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 29 16:51:03.216: INFO: polling logs
Aug 29 16:51:03.229: INFO: Pod logs: 
I0829 16:50:28.576608       1 log.go:195] OK: Got token
I0829 16:50:28.576789       1 log.go:195] validating with in-cluster discovery
I0829 16:50:28.577237       1 log.go:195] OK: got issuer https://gt2cwz5c4t.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
I0829 16:50:28.577293       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://gt2cwz5c4t.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-676:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661792427, NotBefore:1661791827, IssuedAt:1661791827, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-676", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"591bd813-d175-4463-a50a-430ef8b31b36"}}}
I0829 16:50:28.624682       1 log.go:195] OK: Constructed OIDC provider for issuer https://gt2cwz5c4t.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
I0829 16:50:28.627649       1 log.go:195] OK: Validated signature on JWT
I0829 16:50:28.627891       1 log.go:195] OK: Got valid claims from token!
I0829 16:50:28.627984       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://gt2cwz5c4t.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-676:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661792427, NotBefore:1661791827, IssuedAt:1661791827, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-676", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"591bd813-d175-4463-a50a-430ef8b31b36"}}}

Aug 29 16:51:03.229: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:51:03.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-676" for this suite.

• [SLOW TEST:37.021 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":335,"skipped":6389,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:51:03.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:51:07.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1477" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:51:07.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:51:08.398: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:51:11.461: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 29 16:51:11.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:51:11.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5030" for this suite.
STEP: Destroying namespace "webhook-5030-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":337,"skipped":6451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:51:11.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Aug 29 16:51:11.870: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:51:13.882: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Aug 29 16:51:13.909: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:51:15.920: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 29 16:51:15.967: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 16:51:15.979: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 29 16:51:17.980: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 16:51:17.989: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 29 16:51:19.980: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 16:51:19.991: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:51:19.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7667" for this suite.

• [SLOW TEST:8.329 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":6494,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:51:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-1213fa9c-c9b5-4d33-99a1-06272f75dc7f
STEP: Creating a pod to test consume secrets
Aug 29 16:51:20.125: INFO: Waiting up to 5m0s for pod "pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c" in namespace "secrets-8083" to be "Succeeded or Failed"
Aug 29 16:51:20.137: INFO: Pod "pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.352203ms
Aug 29 16:51:22.150: INFO: Pod "pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c": Phase="Running", Reason="", readiness=true. Elapsed: 2.024756011s
Aug 29 16:51:24.159: INFO: Pod "pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033442409s
Aug 29 16:51:26.170: INFO: Pod "pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044495818s
STEP: Saw pod success
Aug 29 16:51:26.170: INFO: Pod "pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c" satisfied condition "Succeeded or Failed"
Aug 29 16:51:26.176: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c container secret-env-test: <nil>
STEP: delete the pod
Aug 29 16:51:26.207: INFO: Waiting for pod pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c to disappear
Aug 29 16:51:26.216: INFO: Pod pod-secrets-7d279a88-ef48-48d9-997c-386ae032c34c no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:51:26.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8083" for this suite.

• [SLOW TEST:6.211 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":339,"skipped":6508,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:51:26.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-a2611d8a-5ca8-47bd-9aaf-e174d88f9c8a
STEP: Creating a pod to test consume secrets
Aug 29 16:51:26.307: INFO: Waiting up to 5m0s for pod "pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851" in namespace "secrets-3891" to be "Succeeded or Failed"
Aug 29 16:51:26.313: INFO: Pod "pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851": Phase="Pending", Reason="", readiness=false. Elapsed: 6.525399ms
Aug 29 16:51:28.324: INFO: Pod "pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016728199s
Aug 29 16:51:30.336: INFO: Pod "pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02929336s
STEP: Saw pod success
Aug 29 16:51:30.336: INFO: Pod "pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851" satisfied condition "Succeeded or Failed"
Aug 29 16:51:30.345: INFO: Trying to get logs from node ip-172-31-20-142.eu-central-1.compute.internal pod pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:51:30.386: INFO: Waiting for pod pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851 to disappear
Aug 29 16:51:30.392: INFO: Pod pod-secrets-2d3aa594-4012-441a-88dc-c4078215c851 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:51:30.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3891" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:51:30.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:56:30.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9003" for this suite.

• [SLOW TEST:300.278 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":341,"skipped":6594,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:56:30.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Aug 29 16:56:30.760: INFO: Creating deployment "test-recreate-deployment"
Aug 29 16:56:31.769: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 29 16:56:31.812: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 29 16:56:31.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 56, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 56, 31, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-594f666cd9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 56, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 56, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:56:34.267: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 29 16:56:34.288: INFO: Updating deployment test-recreate-deployment
Aug 29 16:56:34.288: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 29 16:56:34.432: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5358  911fd617-c3f2-49b2-a59e-0fb979ce81b0 45465 2 2022-08-29 16:56:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f37618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-29 16:56:34 +0000 UTC,LastTransitionTime:2022-08-29 16:56:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-08-29 16:56:34 +0000 UTC,LastTransitionTime:2022-08-29 16:56:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 29 16:56:34.439: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-5358  ed2b352a-abad-4ff4-8f3a-4356a5614d06 45464 1 2022-08-29 16:56:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 911fd617-c3f2-49b2-a59e-0fb979ce81b0 0xc0045daa37 0xc0045daa38}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"911fd617-c3f2-49b2-a59e-0fb979ce81b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045daad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:56:34.439: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 29 16:56:34.439: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-594f666cd9  deployment-5358  4fb4e5f7-d059-4fe3-809a-a5406380ca17 45453 2 2022-08-29 16:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:594f666cd9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 911fd617-c3f2-49b2-a59e-0fb979ce81b0 0xc0045da917 0xc0045da918}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"911fd617-c3f2-49b2-a59e-0fb979ce81b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 594f666cd9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:594f666cd9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045da9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:56:34.446: INFO: Pod "test-recreate-deployment-5b99bd5487-6c45f" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-6c45f test-recreate-deployment-5b99bd5487- deployment-5358  86baa2e6-20c2-433b-b839-ac059b7959cd 45463 0 2022-08-29 16:56:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 ed2b352a-abad-4ff4-8f3a-4356a5614d06 0xc0045daf67 0xc0045daf68}] []  [{kube-controller-manager Update v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed2b352a-abad-4ff4-8f3a-4356a5614d06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:56:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxc4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxc4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:56:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:56:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:56:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:56:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.142,PodIP:,StartTime:2022-08-29 16:56:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:56:34.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5358" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":342,"skipped":6601,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:56:34.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-bf9164e7-d219-4aba-a133-e1dc2b5ca550
STEP: Creating a pod to test consume secrets
Aug 29 16:56:34.553: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188" in namespace "projected-7907" to be "Succeeded or Failed"
Aug 29 16:56:34.563: INFO: Pod "pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092764ms
Aug 29 16:56:36.955: INFO: Pod "pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402435282s
Aug 29 16:56:38.976: INFO: Pod "pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.423545238s
STEP: Saw pod success
Aug 29 16:56:38.977: INFO: Pod "pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188" satisfied condition "Succeeded or Failed"
Aug 29 16:56:38.987: INFO: Trying to get logs from node ip-172-31-23-90.eu-central-1.compute.internal pod pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:56:39.032: INFO: Waiting for pod pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188 to disappear
Aug 29 16:56:39.039: INFO: Pod pod-projected-secrets-05f5f65e-34e9-4c0e-80ae-4547ca182188 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:56:39.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7907" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:56:39.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Aug 29 16:56:39.113: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:56:43.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8246" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":344,"skipped":6641,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:56:44.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 29 16:57:24.243: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0829 16:57:24.243138      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 16:57:24.243: INFO: Deleting pod "simpletest.rc-2bwbl" in namespace "gc-3052"
Aug 29 16:57:24.374: INFO: Deleting pod "simpletest.rc-2n7zr" in namespace "gc-3052"
Aug 29 16:57:24.728: INFO: Deleting pod "simpletest.rc-46fpz" in namespace "gc-3052"
Aug 29 16:57:24.775: INFO: Deleting pod "simpletest.rc-4hq4r" in namespace "gc-3052"
Aug 29 16:57:24.797: INFO: Deleting pod "simpletest.rc-4jf2f" in namespace "gc-3052"
Aug 29 16:57:24.843: INFO: Deleting pod "simpletest.rc-5bbwm" in namespace "gc-3052"
Aug 29 16:57:24.876: INFO: Deleting pod "simpletest.rc-5sl4g" in namespace "gc-3052"
Aug 29 16:57:24.910: INFO: Deleting pod "simpletest.rc-6546b" in namespace "gc-3052"
Aug 29 16:57:25.056: INFO: Deleting pod "simpletest.rc-65s7r" in namespace "gc-3052"
Aug 29 16:57:25.096: INFO: Deleting pod "simpletest.rc-69q5m" in namespace "gc-3052"
Aug 29 16:57:25.133: INFO: Deleting pod "simpletest.rc-6nphv" in namespace "gc-3052"
Aug 29 16:57:25.254: INFO: Deleting pod "simpletest.rc-6tbml" in namespace "gc-3052"
Aug 29 16:57:25.288: INFO: Deleting pod "simpletest.rc-77h4f" in namespace "gc-3052"
Aug 29 16:57:25.322: INFO: Deleting pod "simpletest.rc-7c8r8" in namespace "gc-3052"
Aug 29 16:57:25.372: INFO: Deleting pod "simpletest.rc-7g5xk" in namespace "gc-3052"
Aug 29 16:57:25.404: INFO: Deleting pod "simpletest.rc-7qc27" in namespace "gc-3052"
Aug 29 16:57:25.429: INFO: Deleting pod "simpletest.rc-7r462" in namespace "gc-3052"
Aug 29 16:57:25.458: INFO: Deleting pod "simpletest.rc-7wck7" in namespace "gc-3052"
Aug 29 16:57:25.485: INFO: Deleting pod "simpletest.rc-7xmhr" in namespace "gc-3052"
Aug 29 16:57:25.549: INFO: Deleting pod "simpletest.rc-9bdn8" in namespace "gc-3052"
Aug 29 16:57:25.597: INFO: Deleting pod "simpletest.rc-b5dw7" in namespace "gc-3052"
Aug 29 16:57:25.639: INFO: Deleting pod "simpletest.rc-bgbxd" in namespace "gc-3052"
Aug 29 16:57:25.707: INFO: Deleting pod "simpletest.rc-bq8tg" in namespace "gc-3052"
Aug 29 16:57:25.758: INFO: Deleting pod "simpletest.rc-c46t9" in namespace "gc-3052"
Aug 29 16:57:26.216: INFO: Deleting pod "simpletest.rc-c48rc" in namespace "gc-3052"
Aug 29 16:57:26.232: INFO: Deleting pod "simpletest.rc-c5lwd" in namespace "gc-3052"
Aug 29 16:57:26.255: INFO: Deleting pod "simpletest.rc-crwn8" in namespace "gc-3052"
Aug 29 16:57:26.290: INFO: Deleting pod "simpletest.rc-czjxk" in namespace "gc-3052"
Aug 29 16:57:26.328: INFO: Deleting pod "simpletest.rc-ddsfn" in namespace "gc-3052"
Aug 29 16:57:26.366: INFO: Deleting pod "simpletest.rc-dhncv" in namespace "gc-3052"
Aug 29 16:57:26.396: INFO: Deleting pod "simpletest.rc-dzm82" in namespace "gc-3052"
Aug 29 16:57:26.467: INFO: Deleting pod "simpletest.rc-f9gf5" in namespace "gc-3052"
Aug 29 16:57:26.491: INFO: Deleting pod "simpletest.rc-fbj4m" in namespace "gc-3052"
Aug 29 16:57:26.507: INFO: Deleting pod "simpletest.rc-fg8xz" in namespace "gc-3052"
Aug 29 16:57:26.524: INFO: Deleting pod "simpletest.rc-fj86q" in namespace "gc-3052"
Aug 29 16:57:26.552: INFO: Deleting pod "simpletest.rc-fkmll" in namespace "gc-3052"
Aug 29 16:57:26.597: INFO: Deleting pod "simpletest.rc-fr2f8" in namespace "gc-3052"
Aug 29 16:57:26.626: INFO: Deleting pod "simpletest.rc-fsns6" in namespace "gc-3052"
Aug 29 16:57:26.667: INFO: Deleting pod "simpletest.rc-ftt4q" in namespace "gc-3052"
Aug 29 16:57:26.727: INFO: Deleting pod "simpletest.rc-fzx8f" in namespace "gc-3052"
Aug 29 16:57:26.765: INFO: Deleting pod "simpletest.rc-g5cxk" in namespace "gc-3052"
Aug 29 16:57:26.816: INFO: Deleting pod "simpletest.rc-g7j2s" in namespace "gc-3052"
Aug 29 16:57:26.837: INFO: Deleting pod "simpletest.rc-ghcqk" in namespace "gc-3052"
Aug 29 16:57:26.864: INFO: Deleting pod "simpletest.rc-h5frd" in namespace "gc-3052"
Aug 29 16:57:26.896: INFO: Deleting pod "simpletest.rc-jj87z" in namespace "gc-3052"
Aug 29 16:57:26.913: INFO: Deleting pod "simpletest.rc-jnktb" in namespace "gc-3052"
Aug 29 16:57:26.945: INFO: Deleting pod "simpletest.rc-jnxgm" in namespace "gc-3052"
Aug 29 16:57:26.989: INFO: Deleting pod "simpletest.rc-k4r8h" in namespace "gc-3052"
Aug 29 16:57:27.044: INFO: Deleting pod "simpletest.rc-l5f7x" in namespace "gc-3052"
Aug 29 16:57:27.084: INFO: Deleting pod "simpletest.rc-m4lgp" in namespace "gc-3052"
Aug 29 16:57:27.111: INFO: Deleting pod "simpletest.rc-m8jrs" in namespace "gc-3052"
Aug 29 16:57:27.160: INFO: Deleting pod "simpletest.rc-mbqwr" in namespace "gc-3052"
Aug 29 16:57:27.192: INFO: Deleting pod "simpletest.rc-mdsj5" in namespace "gc-3052"
Aug 29 16:57:27.239: INFO: Deleting pod "simpletest.rc-mpqtk" in namespace "gc-3052"
Aug 29 16:57:27.255: INFO: Deleting pod "simpletest.rc-n2846" in namespace "gc-3052"
Aug 29 16:57:27.274: INFO: Deleting pod "simpletest.rc-n85c5" in namespace "gc-3052"
Aug 29 16:57:27.293: INFO: Deleting pod "simpletest.rc-nkpmn" in namespace "gc-3052"
Aug 29 16:57:27.319: INFO: Deleting pod "simpletest.rc-nmdng" in namespace "gc-3052"
Aug 29 16:57:27.355: INFO: Deleting pod "simpletest.rc-nzp8h" in namespace "gc-3052"
Aug 29 16:57:27.425: INFO: Deleting pod "simpletest.rc-p8bhl" in namespace "gc-3052"
Aug 29 16:57:27.445: INFO: Deleting pod "simpletest.rc-p8rmg" in namespace "gc-3052"
Aug 29 16:57:27.497: INFO: Deleting pod "simpletest.rc-pclpz" in namespace "gc-3052"
Aug 29 16:57:27.521: INFO: Deleting pod "simpletest.rc-pkk7j" in namespace "gc-3052"
Aug 29 16:57:27.553: INFO: Deleting pod "simpletest.rc-qgz9r" in namespace "gc-3052"
Aug 29 16:57:27.574: INFO: Deleting pod "simpletest.rc-r62j4" in namespace "gc-3052"
Aug 29 16:57:27.610: INFO: Deleting pod "simpletest.rc-r7bmv" in namespace "gc-3052"
Aug 29 16:57:27.674: INFO: Deleting pod "simpletest.rc-rjkg6" in namespace "gc-3052"
Aug 29 16:57:27.692: INFO: Deleting pod "simpletest.rc-rngmq" in namespace "gc-3052"
Aug 29 16:57:27.709: INFO: Deleting pod "simpletest.rc-s2sd7" in namespace "gc-3052"
Aug 29 16:57:27.815: INFO: Deleting pod "simpletest.rc-s62kg" in namespace "gc-3052"
Aug 29 16:57:27.834: INFO: Deleting pod "simpletest.rc-sbxlj" in namespace "gc-3052"
Aug 29 16:57:27.893: INFO: Deleting pod "simpletest.rc-sggnp" in namespace "gc-3052"
Aug 29 16:57:27.983: INFO: Deleting pod "simpletest.rc-sjfv2" in namespace "gc-3052"
Aug 29 16:57:28.005: INFO: Deleting pod "simpletest.rc-t5t8r" in namespace "gc-3052"
Aug 29 16:57:28.033: INFO: Deleting pod "simpletest.rc-t7hhs" in namespace "gc-3052"
Aug 29 16:57:28.061: INFO: Deleting pod "simpletest.rc-t8blk" in namespace "gc-3052"
Aug 29 16:57:28.104: INFO: Deleting pod "simpletest.rc-tdc87" in namespace "gc-3052"
Aug 29 16:57:28.148: INFO: Deleting pod "simpletest.rc-tpkg4" in namespace "gc-3052"
Aug 29 16:57:28.193: INFO: Deleting pod "simpletest.rc-tqvb8" in namespace "gc-3052"
Aug 29 16:57:28.230: INFO: Deleting pod "simpletest.rc-v76bd" in namespace "gc-3052"
Aug 29 16:57:28.269: INFO: Deleting pod "simpletest.rc-v7jkh" in namespace "gc-3052"
Aug 29 16:57:28.289: INFO: Deleting pod "simpletest.rc-v8f6w" in namespace "gc-3052"
Aug 29 16:57:28.315: INFO: Deleting pod "simpletest.rc-vm464" in namespace "gc-3052"
Aug 29 16:57:28.341: INFO: Deleting pod "simpletest.rc-vpd8j" in namespace "gc-3052"
Aug 29 16:57:28.416: INFO: Deleting pod "simpletest.rc-vwmpk" in namespace "gc-3052"
Aug 29 16:57:28.438: INFO: Deleting pod "simpletest.rc-vxdtc" in namespace "gc-3052"
Aug 29 16:57:28.456: INFO: Deleting pod "simpletest.rc-vxxcr" in namespace "gc-3052"
Aug 29 16:57:28.472: INFO: Deleting pod "simpletest.rc-w5fp7" in namespace "gc-3052"
Aug 29 16:57:28.488: INFO: Deleting pod "simpletest.rc-w7kcm" in namespace "gc-3052"
Aug 29 16:57:28.505: INFO: Deleting pod "simpletest.rc-w9kc7" in namespace "gc-3052"
Aug 29 16:57:28.527: INFO: Deleting pod "simpletest.rc-wgf5w" in namespace "gc-3052"
Aug 29 16:57:28.563: INFO: Deleting pod "simpletest.rc-wjzgb" in namespace "gc-3052"
Aug 29 16:57:28.650: INFO: Deleting pod "simpletest.rc-wpghn" in namespace "gc-3052"
Aug 29 16:57:29.450: INFO: Deleting pod "simpletest.rc-xdddr" in namespace "gc-3052"
Aug 29 16:57:29.497: INFO: Deleting pod "simpletest.rc-xg7sz" in namespace "gc-3052"
Aug 29 16:57:29.520: INFO: Deleting pod "simpletest.rc-xkk2r" in namespace "gc-3052"
Aug 29 16:57:29.562: INFO: Deleting pod "simpletest.rc-xpr59" in namespace "gc-3052"
Aug 29 16:57:29.601: INFO: Deleting pod "simpletest.rc-xqtpk" in namespace "gc-3052"
Aug 29 16:57:29.634: INFO: Deleting pod "simpletest.rc-ztqv7" in namespace "gc-3052"
Aug 29 16:57:30.356: INFO: Deleting pod "simpletest.rc-zwj7w" in namespace "gc-3052"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:57:30.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3052" for this suite.

• [SLOW TEST:46.433 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":345,"skipped":6641,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 29 16:57:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2626036518
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
Aug 29 16:57:30.535: INFO: Creating e2e-svc-a-rhqf9
Aug 29 16:57:30.638: INFO: Creating e2e-svc-b-kp2sz
Aug 29 16:57:30.682: INFO: Creating e2e-svc-c-g4zwf
STEP: deleting service collection
Aug 29 16:57:30.782: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 29 16:57:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-164" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":346,"skipped":6656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSAug 29 16:57:30.822: INFO: Running AfterSuite actions on all nodes
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 29 16:57:30.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Aug 29 16:57:30.822: INFO: Running AfterSuite actions on node 1
Aug 29 16:57:30.822: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6704,"failed":0}

Ran 346 of 7050 Specs in 6596.618 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h50m0.078326496s
Test Suite Passed
