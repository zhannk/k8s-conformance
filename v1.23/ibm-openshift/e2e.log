I1116 01:36:25.691460      21 e2e.go:132] Starting e2e run "2763a32a-15bb-4239-bc69-fbfcd9a83e5f" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1668562585 - Will randomize all specs
Will run 346 of 7052 specs

Nov 16 01:36:28.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:36:28.772: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 16 01:36:28.835: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 16 01:36:28.919: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 16 01:36:28.919: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Nov 16 01:36:28.919: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 16 01:36:28.940: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Nov 16 01:36:28.940: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Nov 16 01:36:28.940: INFO: e2e test version: v1.23.12
Nov 16 01:36:28.946: INFO: kube-apiserver version: v1.23.12+7566c4d
Nov 16 01:36:28.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:36:28.961: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:36:28.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
W1116 01:36:29.118875      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Nov 16 01:36:29.118: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-419
Nov 16 01:36:29.248: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:36:31.262: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:36:33.263: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:36:35.259: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:36:37.262: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 16 01:36:37.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 16 01:36:37.680: INFO: rc: 7
Nov 16 01:36:37.737: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 16 01:36:37.749: INFO: Pod kube-proxy-mode-detector no longer exists
Nov 16 01:36:37.749: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-419
STEP: creating replication controller affinity-nodeport-timeout in namespace services-419
I1116 01:36:37.873046      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-419, replica count: 3
I1116 01:36:40.926131      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 01:36:43.927041      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 01:36:46.927343      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 01:36:46.976: INFO: Creating new exec pod
Nov 16 01:36:50.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Nov 16 01:36:50.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 16 01:36:50.340: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:36:50.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.176.60 80'
Nov 16 01:36:50.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.176.60 80\nConnection to 172.21.176.60 80 port [tcp/http] succeeded!\n"
Nov 16 01:36:50.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:36:50.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.151 30008'
Nov 16 01:36:50.900: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.151 30008\nConnection to 10.189.71.151 30008 port [tcp/*] succeeded!\n"
Nov 16 01:36:50.900: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:36:50.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.157 30008'
Nov 16 01:36:51.230: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.157 30008\nConnection to 10.189.71.157 30008 port [tcp/*] succeeded!\n"
Nov 16 01:36:51.230: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:36:51.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.189.71.150:30008/ ; done'
Nov 16 01:36:51.668: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n"
Nov 16 01:36:51.668: INFO: stdout: "\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq\naffinity-nodeport-timeout-gqxrq"
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Received response from host: affinity-nodeport-timeout-gqxrq
Nov 16 01:36:51.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.189.71.150:30008/'
Nov 16 01:36:52.039: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n"
Nov 16 01:36:52.039: INFO: stdout: "affinity-nodeport-timeout-gqxrq"
Nov 16 01:37:12.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-419 exec execpod-affinityg6w69 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.189.71.150:30008/'
Nov 16 01:37:12.318: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.189.71.150:30008/\n"
Nov 16 01:37:12.318: INFO: stdout: "affinity-nodeport-timeout-mmq6v"
Nov 16 01:37:12.318: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-419, will wait for the garbage collector to delete the pods
Nov 16 01:37:12.492: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 29.177364ms
Nov 16 01:37:12.693: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.469853ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:37:15.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-419" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:46.229 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":1,"skipped":14,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:37:15.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
Nov 16 01:37:15.303: INFO: Creating e2e-svc-a-rxmq6
Nov 16 01:37:15.391: INFO: Creating e2e-svc-b-6gqff
Nov 16 01:37:15.447: INFO: Creating e2e-svc-c-87pkx
STEP: deleting service collection
Nov 16 01:37:15.680: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:37:15.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6129" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":2,"skipped":25,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:37:15.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
Nov 16 01:37:15.996: INFO: Waiting up to 5m0s for pod "var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df" in namespace "var-expansion-8840" to be "Succeeded or Failed"
Nov 16 01:37:16.008: INFO: Pod "var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df": Phase="Pending", Reason="", readiness=false. Elapsed: 12.550027ms
Nov 16 01:37:18.021: INFO: Pod "var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025661415s
Nov 16 01:37:20.036: INFO: Pod "var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039857915s
Nov 16 01:37:22.063: INFO: Pod "var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067708774s
STEP: Saw pod success
Nov 16 01:37:22.063: INFO: Pod "var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df" satisfied condition "Succeeded or Failed"
Nov 16 01:37:22.075: INFO: Trying to get logs from node 10.189.71.151 pod var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df container dapi-container: <nil>
STEP: delete the pod
Nov 16 01:37:22.181: INFO: Waiting for pod var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df to disappear
Nov 16 01:37:22.191: INFO: Pod var-expansion-2a9d77c2-5389-433d-a0e3-1b2f6637d9df no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:37:22.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8840" for this suite.

• [SLOW TEST:6.503 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":3,"skipped":29,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:37:22.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 01:37:22.396: INFO: Waiting up to 5m0s for pod "downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c" in namespace "projected-7678" to be "Succeeded or Failed"
Nov 16 01:37:22.427: INFO: Pod "downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.593377ms
Nov 16 01:37:24.462: INFO: Pod "downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065788849s
Nov 16 01:37:26.484: INFO: Pod "downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087433433s
Nov 16 01:37:28.502: INFO: Pod "downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.105638331s
STEP: Saw pod success
Nov 16 01:37:28.502: INFO: Pod "downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c" satisfied condition "Succeeded or Failed"
Nov 16 01:37:28.519: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c container client-container: <nil>
STEP: delete the pod
Nov 16 01:37:28.608: INFO: Waiting for pod downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c to disappear
Nov 16 01:37:28.619: INFO: Pod downwardapi-volume-481f8d4c-b2f5-4050-8e40-5dd054791a6c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:37:28.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7678" for this suite.

• [SLOW TEST:6.414 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":60,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:37:28.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:37:56.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8187" for this suite.

• [SLOW TEST:28.367 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":5,"skipped":66,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:37:57.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 01:37:57.181: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1c4e2aab-b7cb-494c-a759-04e425c3e5a8" in namespace "security-context-test-521" to be "Succeeded or Failed"
Nov 16 01:37:57.199: INFO: Pod "busybox-user-65534-1c4e2aab-b7cb-494c-a759-04e425c3e5a8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.604364ms
Nov 16 01:37:59.213: INFO: Pod "busybox-user-65534-1c4e2aab-b7cb-494c-a759-04e425c3e5a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031388654s
Nov 16 01:38:01.224: INFO: Pod "busybox-user-65534-1c4e2aab-b7cb-494c-a759-04e425c3e5a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042356912s
Nov 16 01:38:01.224: INFO: Pod "busybox-user-65534-1c4e2aab-b7cb-494c-a759-04e425c3e5a8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:38:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-521" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":70,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:38:01.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-9fd8a875-5d31-4f6a-a4d6-3b4e20062336 in namespace container-probe-3115
Nov 16 01:38:03.502: INFO: Started pod liveness-9fd8a875-5d31-4f6a-a4d6-3b4e20062336 in namespace container-probe-3115
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 01:38:03.514: INFO: Initial restart count of pod liveness-9fd8a875-5d31-4f6a-a4d6-3b4e20062336 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3115" for this suite.

• [SLOW TEST:242.478 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":7,"skipped":83,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:03.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Nov 16 01:42:03.965: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:03.965: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:04.047: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:04.047: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:04.089: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:04.089: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:04.264: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:04.264: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 01:42:06.053: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 16 01:42:06.053: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 16 01:42:06.196: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Nov 16 01:42:06.239: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 0
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.244: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.246: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.246: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.308: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.308: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.373: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.373: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:06.393: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:06.393: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:06.522: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:06.522: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:09.202: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:09.202: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:09.256: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
STEP: listing Deployments
Nov 16 01:42:09.291: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Nov 16 01:42:09.327: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Nov 16 01:42:09.378: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:09.378: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:09.435: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:09.479: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:09.532: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:12.251: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:15.332: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:15.395: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:15.461: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 01:42:20.202: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Nov 16 01:42:20.317: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:20.318: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:20.318: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:20.320: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:20.320: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 1
Nov 16 01:42:20.320: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:20.320: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 3
Nov 16 01:42:20.320: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:20.320: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 2
Nov 16 01:42:20.322: INFO: observed Deployment test-deployment in namespace deployment-4923 with ReadyReplicas 3
STEP: deleting the Deployment
Nov 16 01:42:20.416: INFO: observed event type MODIFIED
Nov 16 01:42:20.416: INFO: observed event type MODIFIED
Nov 16 01:42:20.416: INFO: observed event type MODIFIED
Nov 16 01:42:20.416: INFO: observed event type MODIFIED
Nov 16 01:42:20.416: INFO: observed event type MODIFIED
Nov 16 01:42:20.418: INFO: observed event type MODIFIED
Nov 16 01:42:20.418: INFO: observed event type MODIFIED
Nov 16 01:42:20.418: INFO: observed event type MODIFIED
Nov 16 01:42:20.418: INFO: observed event type MODIFIED
Nov 16 01:42:20.418: INFO: observed event type MODIFIED
Nov 16 01:42:20.418: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 01:42:20.443: INFO: Log out all the ReplicaSets if there is no deployment created
Nov 16 01:42:20.466: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-4923  1fb5fad9-c134-496c-8db6-5f2139165738 74320 4 2022-11-16 01:42:06 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 00ac86d4-7764-4dc2-b5bc-914c0741f77c 0xc002652027 0xc002652028}] []  [{kube-controller-manager Update apps/v1 2022-11-16 01:42:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00ac86d4-7764-4dc2-b5bc-914c0741f77c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 01:42:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026520b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 16 01:42:20.481: INFO: pod: "test-deployment-5ddd8b47d8-r9mnh":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-r9mnh test-deployment-5ddd8b47d8- deployment-4923  27aec1cd-377b-4d62-8474-528670b7e522 74314 0 2022-11-16 01:42:09 +0000 UTC 2022-11-16 01:42:21 +0000 UTC 0xc001cab878 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b46d3c65e0ba4769e42f4817a726701020939099fd3fb14ba9a32a45859f69b cni.projectcalico.org/podIP:172.30.36.103/32 cni.projectcalico.org/podIPs:172.30.36.103/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.103"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.103"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 1fb5fad9-c134-496c-8db6-5f2139165738 0xc001cab8d7 0xc001cab8d8}] []  [{kube-controller-manager Update v1 2022-11-16 01:42:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fb5fad9-c134-496c-8db6-5f2139165738\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 01:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 01:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 01:42:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2s2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2s2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qr2nr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.103,StartTime:2022-11-16 01:42:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 01:42:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:cri-o://bd7927885de090032e9b9ced94c35abda425580bc76117389cd1c317311142be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 16 01:42:20.481: INFO: ReplicaSet "test-deployment-6d7ffcf7fb":
&ReplicaSet{ObjectMeta:{test-deployment-6d7ffcf7fb  deployment-4923  9da22819-37d9-4dd7-a9b4-5d13dde2ff33 74093 3 2022-11-16 01:42:03 +0000 UTC <nil> <nil> map[pod-template-hash:6d7ffcf7fb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 00ac86d4-7764-4dc2-b5bc-914c0741f77c 0xc002652117 0xc002652118}] []  [{kube-controller-manager Update apps/v1 2022-11-16 01:42:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00ac86d4-7764-4dc2-b5bc-914c0741f77c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 01:42:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6d7ffcf7fb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6d7ffcf7fb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026521a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 16 01:42:20.491: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-4923  6d248659-8332-4e02-996b-276c8d337951 74310 2 2022-11-16 01:42:09 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 00ac86d4-7764-4dc2-b5bc-914c0741f77c 0xc002652207 0xc002652208}] []  [{kube-controller-manager Update apps/v1 2022-11-16 01:42:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00ac86d4-7764-4dc2-b5bc-914c0741f77c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 01:42:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002652290 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Nov 16 01:42:20.500: INFO: pod: "test-deployment-854fdc678-n8zrk":
&Pod{ObjectMeta:{test-deployment-854fdc678-n8zrk test-deployment-854fdc678- deployment-4923  270ab363-9a01-4955-9baf-340e71dd67a4 74221 0 2022-11-16 01:42:09 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:2ea4a115900d818507ebc8e384d607a2dd5cd6308b3f64d5359abd265262cbc3 cni.projectcalico.org/podIP:172.30.102.247/32 cni.projectcalico.org/podIPs:172.30.102.247/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.247"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.247"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-854fdc678 6d248659-8332-4e02-996b-276c8d337951 0xc0026525a7 0xc0026525a8}] []  [{kube-controller-manager Update v1 2022-11-16 01:42:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d248659-8332-4e02-996b-276c8d337951\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 01:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 01:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 01:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vlqb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vlqb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qr2nr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.247,StartTime:2022-11-16 01:42:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 01:42:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1a5fea56478d5f82fd78f67ce324901c1c51192acca7df413105ea76c5161236,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 16 01:42:20.501: INFO: pod: "test-deployment-854fdc678-xd9wx":
&Pod{ObjectMeta:{test-deployment-854fdc678-xd9wx test-deployment-854fdc678- deployment-4923  e5458c2b-42d7-453e-b833-4449365769f9 74309 0 2022-11-16 01:42:15 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:6a6ff74204362732ae9c63b9ce7935849b07c045b66af58cca2f06274584984d cni.projectcalico.org/podIP:172.30.169.125/32 cni.projectcalico.org/podIPs:172.30.169.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.125"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-854fdc678 6d248659-8332-4e02-996b-276c8d337951 0xc002652827 0xc002652828}] []  [{kube-controller-manager Update v1 2022-11-16 01:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d248659-8332-4e02-996b-276c8d337951\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 01:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 01:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 01:42:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.169.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98h2q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98h2q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qr2nr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 01:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:172.30.169.125,StartTime:2022-11-16 01:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 01:42:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c0b8127d6eb17926bb6eb03dc478ea7aa19950c8856317b1c7f637df64beb1fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.169.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:20.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4923" for this suite.

• [SLOW TEST:16.815 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":8,"skipped":92,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:20.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-ms7m
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 01:42:20.956: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ms7m" in namespace "subpath-6743" to be "Succeeded or Failed"
Nov 16 01:42:20.974: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Pending", Reason="", readiness=false. Elapsed: 18.539281ms
Nov 16 01:42:22.995: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039747597s
Nov 16 01:42:25.011: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 4.054957494s
Nov 16 01:42:27.028: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 6.072518542s
Nov 16 01:42:29.039: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 8.083812635s
Nov 16 01:42:31.055: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 10.099647579s
Nov 16 01:42:33.068: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 12.112874586s
Nov 16 01:42:35.081: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 14.125723958s
Nov 16 01:42:37.100: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 16.144520573s
Nov 16 01:42:39.121: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 18.165345447s
Nov 16 01:42:41.134: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 20.17886407s
Nov 16 01:42:43.148: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=true. Elapsed: 22.192798615s
Nov 16 01:42:45.163: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Running", Reason="", readiness=false. Elapsed: 24.207848842s
Nov 16 01:42:47.177: INFO: Pod "pod-subpath-test-configmap-ms7m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.221162455s
STEP: Saw pod success
Nov 16 01:42:47.177: INFO: Pod "pod-subpath-test-configmap-ms7m" satisfied condition "Succeeded or Failed"
Nov 16 01:42:47.189: INFO: Trying to get logs from node 10.189.71.157 pod pod-subpath-test-configmap-ms7m container test-container-subpath-configmap-ms7m: <nil>
STEP: delete the pod
Nov 16 01:42:47.288: INFO: Waiting for pod pod-subpath-test-configmap-ms7m to disappear
Nov 16 01:42:47.302: INFO: Pod pod-subpath-test-configmap-ms7m no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ms7m
Nov 16 01:42:47.302: INFO: Deleting pod "pod-subpath-test-configmap-ms7m" in namespace "subpath-6743"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:47.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6743" for this suite.

• [SLOW TEST:26.849 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":9,"skipped":97,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:47.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 01:42:47.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2" in namespace "projected-9463" to be "Succeeded or Failed"
Nov 16 01:42:47.710: INFO: Pod "downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.943749ms
Nov 16 01:42:49.740: INFO: Pod "downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039811446s
Nov 16 01:42:51.755: INFO: Pod "downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054491087s
STEP: Saw pod success
Nov 16 01:42:51.755: INFO: Pod "downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2" satisfied condition "Succeeded or Failed"
Nov 16 01:42:51.766: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2 container client-container: <nil>
STEP: delete the pod
Nov 16 01:42:51.911: INFO: Waiting for pod downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2 to disappear
Nov 16 01:42:51.922: INFO: Pod downwardapi-volume-92ee660c-71e6-43ff-b8d6-f38c5b07f9c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:51.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9463" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 01:42:52.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592" in namespace "projected-5561" to be "Succeeded or Failed"
Nov 16 01:42:52.302: INFO: Pod "downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592": Phase="Pending", Reason="", readiness=false. Elapsed: 14.422862ms
Nov 16 01:42:54.315: INFO: Pod "downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027029979s
Nov 16 01:42:56.328: INFO: Pod "downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040661482s
STEP: Saw pod success
Nov 16 01:42:56.329: INFO: Pod "downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592" satisfied condition "Succeeded or Failed"
Nov 16 01:42:56.338: INFO: Trying to get logs from node 10.189.71.157 pod downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592 container client-container: <nil>
STEP: delete the pod
Nov 16 01:42:56.418: INFO: Waiting for pod downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592 to disappear
Nov 16 01:42:56.428: INFO: Pod downwardapi-volume-19184c72-cc85-4410-8288-4c8c4e7ab592 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:56.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5561" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":120,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:56.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:56.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5334" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":12,"skipped":127,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-3b598bb4-1164-4a34-9225-d0d835b37913
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:42:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-31" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":13,"skipped":134,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:42:56.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Nov 16 01:42:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:43:49.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1888" for this suite.

• [SLOW TEST:52.981 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":14,"skipped":138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:43:49.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 16 01:43:50.026: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 16 01:43:50.075: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 16 01:43:50.075: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 16 01:43:50.136: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 16 01:43:50.136: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 16 01:43:50.188: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 16 01:43:50.188: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 16 01:43:57.539: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:43:57.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-116" for this suite.

• [SLOW TEST:7.798 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":15,"skipped":187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:43:57.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 in namespace container-probe-8687
Nov 16 01:43:59.980: INFO: Started pod liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 in namespace container-probe-8687
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 01:43:59.991: INFO: Initial restart count of pod liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 is 0
Nov 16 01:44:20.236: INFO: Restart count of pod container-probe-8687/liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 is now 1 (20.24472824s elapsed)
Nov 16 01:44:40.469: INFO: Restart count of pod container-probe-8687/liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 is now 2 (40.477872227s elapsed)
Nov 16 01:45:00.723: INFO: Restart count of pod container-probe-8687/liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 is now 3 (1m0.731422571s elapsed)
Nov 16 01:45:20.981: INFO: Restart count of pod container-probe-8687/liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 is now 4 (1m20.989194785s elapsed)
Nov 16 01:46:27.714: INFO: Restart count of pod container-probe-8687/liveness-2cbf9e80-7e7d-40e4-8b56-60cb164e7625 is now 5 (2m27.722929494s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:46:27.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8687" for this suite.

• [SLOW TEST:150.137 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":220,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:46:27.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Nov 16 01:46:29.238: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:46:29.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1116 01:46:29.238513      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-2016" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":17,"skipped":230,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:46:29.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 01:46:29.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:46:29.629: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:30.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:46:30.669: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:31.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:31.723: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:32.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:32.683: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:33.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:33.681: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:34.671: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:34.671: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:35.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:35.663: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 01:46:36.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 01:46:36.731: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 16 01:46:36.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:36.814: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:46:37.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:37.849: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:46:38.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 01:46:38.843: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:46:39.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 01:46:39.890: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6673, will wait for the garbage collector to delete the pods
Nov 16 01:46:40.118: INFO: Deleting DaemonSet.extensions daemon-set took: 31.654587ms
Nov 16 01:46:40.319: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.05557ms
Nov 16 01:46:42.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:46:42.344: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 16 01:46:42.368: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76634"},"items":null}

Nov 16 01:46:42.379: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76634"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:46:42.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6673" for this suite.

• [SLOW TEST:13.247 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":18,"skipped":240,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:46:42.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-e78581f6-7daa-4d0f-8ec9-a099b6ca350a
STEP: Creating a pod to test consume secrets
Nov 16 01:46:42.843: INFO: Waiting up to 5m0s for pod "pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d" in namespace "secrets-6439" to be "Succeeded or Failed"
Nov 16 01:46:42.874: INFO: Pod "pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d": Phase="Pending", Reason="", readiness=false. Elapsed: 31.133648ms
Nov 16 01:46:44.891: INFO: Pod "pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047857482s
Nov 16 01:46:46.908: INFO: Pod "pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064955889s
STEP: Saw pod success
Nov 16 01:46:46.908: INFO: Pod "pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d" satisfied condition "Succeeded or Failed"
Nov 16 01:46:46.920: INFO: Trying to get logs from node 10.189.71.150 pod pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 01:46:47.013: INFO: Waiting for pod pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d to disappear
Nov 16 01:46:47.024: INFO: Pod pod-secrets-8a50907d-514d-4215-8757-cf4e7acf984d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:46:47.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6439" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":19,"skipped":253,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:46:47.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-02a0715f-53f8-41ec-b98e-e0100c1b324b
STEP: Creating a pod to test consume configMaps
Nov 16 01:46:47.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1" in namespace "configmap-7093" to be "Succeeded or Failed"
Nov 16 01:46:47.326: INFO: Pod "pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.898182ms
Nov 16 01:46:49.350: INFO: Pod "pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03561535s
Nov 16 01:46:51.366: INFO: Pod "pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052339745s
Nov 16 01:46:53.389: INFO: Pod "pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074560059s
STEP: Saw pod success
Nov 16 01:46:53.389: INFO: Pod "pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1" satisfied condition "Succeeded or Failed"
Nov 16 01:46:53.401: INFO: Trying to get logs from node 10.189.71.150 pod pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 01:46:53.459: INFO: Waiting for pod pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1 to disappear
Nov 16 01:46:53.470: INFO: Pod pod-configmaps-e21526eb-2171-4db9-87c9-4960874ae3a1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:46:53.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7093" for this suite.

• [SLOW TEST:6.443 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:46:53.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 16 01:46:53.654: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:01.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2214" for this suite.

• [SLOW TEST:8.495 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:02.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-c9183bf0-89d9-404c-ae63-2c51495b5409
STEP: Creating a pod to test consume configMaps
Nov 16 01:47:02.500: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04" in namespace "configmap-587" to be "Succeeded or Failed"
Nov 16 01:47:02.516: INFO: Pod "pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04": Phase="Pending", Reason="", readiness=false. Elapsed: 15.217483ms
Nov 16 01:47:04.534: INFO: Pod "pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033361889s
Nov 16 01:47:06.550: INFO: Pod "pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049565986s
STEP: Saw pod success
Nov 16 01:47:06.550: INFO: Pod "pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04" satisfied condition "Succeeded or Failed"
Nov 16 01:47:06.562: INFO: Trying to get logs from node 10.189.71.157 pod pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 01:47:06.640: INFO: Waiting for pod pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04 to disappear
Nov 16 01:47:06.649: INFO: Pod pod-configmaps-9a8c3c80-2da1-42f1-9c25-9b9167181e04 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:06.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-587" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":340,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:06.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-378" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":23,"skipped":345,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:07.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
Nov 16 01:47:08.035: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:47:10.054: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:11.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9319" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":24,"skipped":346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:11.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
Nov 16 01:47:11.465: INFO: Waiting up to 5m0s for pod "test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705" in namespace "svcaccounts-1451" to be "Succeeded or Failed"
Nov 16 01:47:11.504: INFO: Pod "test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705": Phase="Pending", Reason="", readiness=false. Elapsed: 38.664881ms
Nov 16 01:47:13.521: INFO: Pod "test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05567597s
Nov 16 01:47:15.541: INFO: Pod "test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075631694s
STEP: Saw pod success
Nov 16 01:47:15.541: INFO: Pod "test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705" satisfied condition "Succeeded or Failed"
Nov 16 01:47:15.553: INFO: Trying to get logs from node 10.189.71.157 pod test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 01:47:15.617: INFO: Waiting for pod test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705 to disappear
Nov 16 01:47:15.628: INFO: Pod test-pod-5aa3f479-f2cd-4a21-8e12-dbbc29077705 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:15.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1451" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":25,"skipped":385,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:15.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-4326
STEP: creating service affinity-clusterip in namespace services-4326
STEP: creating replication controller affinity-clusterip in namespace services-4326
I1116 01:47:15.856756      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4326, replica count: 3
I1116 01:47:18.908447      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 01:47:21.908753      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 01:47:21.984: INFO: Creating new exec pod
Nov 16 01:47:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4326 exec execpod-affinitytmmlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Nov 16 01:47:27.540: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 16 01:47:27.540: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:47:27.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4326 exec execpod-affinitytmmlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.33.114 80'
Nov 16 01:47:27.819: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.33.114 80\nConnection to 172.21.33.114 80 port [tcp/http] succeeded!\n"
Nov 16 01:47:27.819: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:47:27.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4326 exec execpod-affinitytmmlv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.33.114:80/ ; done'
Nov 16 01:47:28.144: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.33.114:80/\n"
Nov 16 01:47:28.145: INFO: stdout: "\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z\naffinity-clusterip-hcp5z"
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Received response from host: affinity-clusterip-hcp5z
Nov 16 01:47:28.145: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4326, will wait for the garbage collector to delete the pods
Nov 16 01:47:28.311: INFO: Deleting ReplicationController affinity-clusterip took: 37.937494ms
Nov 16 01:47:28.414: INFO: Terminating ReplicationController affinity-clusterip pods took: 102.564468ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:31.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4326" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:15.675 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":26,"skipped":394,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:31.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-wgpnf in namespace proxy-71
I1116 01:47:31.544815      21 runners.go:193] Created replication controller with name: proxy-service-wgpnf, namespace: proxy-71, replica count: 1
I1116 01:47:32.602403      21 runners.go:193] proxy-service-wgpnf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 01:47:33.603146      21 runners.go:193] proxy-service-wgpnf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1116 01:47:34.603554      21 runners.go:193] proxy-service-wgpnf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 01:47:34.632: INFO: setup took 3.153627487s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 16 01:47:34.660: INFO: (0) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 27.380818ms)
Nov 16 01:47:34.666: INFO: (0) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 32.785391ms)
Nov 16 01:47:34.667: INFO: (0) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 33.829468ms)
Nov 16 01:47:34.671: INFO: (0) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 37.189607ms)
Nov 16 01:47:34.671: INFO: (0) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 38.29886ms)
Nov 16 01:47:34.671: INFO: (0) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 37.702894ms)
Nov 16 01:47:34.673: INFO: (0) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 39.987242ms)
Nov 16 01:47:34.679: INFO: (0) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 45.552021ms)
Nov 16 01:47:34.679: INFO: (0) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 45.413977ms)
Nov 16 01:47:34.679: INFO: (0) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 45.739538ms)
Nov 16 01:47:34.679: INFO: (0) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 45.714395ms)
Nov 16 01:47:34.686: INFO: (0) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 52.286843ms)
Nov 16 01:47:34.686: INFO: (0) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 52.776007ms)
Nov 16 01:47:34.693: INFO: (0) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 59.771135ms)
Nov 16 01:47:34.693: INFO: (0) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 59.91991ms)
Nov 16 01:47:34.693: INFO: (0) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 59.839649ms)
Nov 16 01:47:34.723: INFO: (1) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 29.102011ms)
Nov 16 01:47:34.724: INFO: (1) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 28.901681ms)
Nov 16 01:47:34.724: INFO: (1) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 30.327646ms)
Nov 16 01:47:34.724: INFO: (1) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 28.866326ms)
Nov 16 01:47:34.724: INFO: (1) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 30.287488ms)
Nov 16 01:47:34.724: INFO: (1) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 28.818663ms)
Nov 16 01:47:34.725: INFO: (1) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 29.894533ms)
Nov 16 01:47:34.725: INFO: (1) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 29.797249ms)
Nov 16 01:47:34.725: INFO: (1) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 31.419613ms)
Nov 16 01:47:34.725: INFO: (1) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 30.955051ms)
Nov 16 01:47:34.725: INFO: (1) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 30.474443ms)
Nov 16 01:47:34.726: INFO: (1) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 30.839747ms)
Nov 16 01:47:34.728: INFO: (1) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 33.911663ms)
Nov 16 01:47:34.729: INFO: (1) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 35.692129ms)
Nov 16 01:47:34.730: INFO: (1) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 34.989234ms)
Nov 16 01:47:34.731: INFO: (1) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 36.547332ms)
Nov 16 01:47:34.758: INFO: (2) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 26.971302ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 27.576439ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 27.759887ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 27.818852ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 28.539162ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 27.90144ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 28.3137ms)
Nov 16 01:47:34.759: INFO: (2) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 27.931985ms)
Nov 16 01:47:34.760: INFO: (2) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 28.083688ms)
Nov 16 01:47:34.760: INFO: (2) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 28.698406ms)
Nov 16 01:47:34.764: INFO: (2) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 32.683673ms)
Nov 16 01:47:34.765: INFO: (2) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 33.918432ms)
Nov 16 01:47:34.768: INFO: (2) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 36.983517ms)
Nov 16 01:47:34.771: INFO: (2) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 39.714546ms)
Nov 16 01:47:34.771: INFO: (2) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 39.780595ms)
Nov 16 01:47:34.771: INFO: (2) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 40.311243ms)
Nov 16 01:47:34.788: INFO: (3) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 16.062412ms)
Nov 16 01:47:34.796: INFO: (3) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 24.314465ms)
Nov 16 01:47:34.796: INFO: (3) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 24.680333ms)
Nov 16 01:47:34.797: INFO: (3) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 24.721757ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 25.838795ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 26.132754ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 25.859474ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 25.839421ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 25.752252ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 26.114041ms)
Nov 16 01:47:34.798: INFO: (3) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 25.735749ms)
Nov 16 01:47:34.799: INFO: (3) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 27.333094ms)
Nov 16 01:47:34.804: INFO: (3) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 31.234357ms)
Nov 16 01:47:34.804: INFO: (3) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 31.744757ms)
Nov 16 01:47:34.804: INFO: (3) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 32.213737ms)
Nov 16 01:47:34.804: INFO: (3) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 32.177359ms)
Nov 16 01:47:34.823: INFO: (4) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 18.916969ms)
Nov 16 01:47:34.844: INFO: (4) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 39.394623ms)
Nov 16 01:47:34.844: INFO: (4) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 39.155988ms)
Nov 16 01:47:34.844: INFO: (4) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 40.183404ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 39.956567ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 39.216124ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 39.752872ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 38.57317ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 39.582601ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 39.17539ms)
Nov 16 01:47:34.845: INFO: (4) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 38.706628ms)
Nov 16 01:47:34.847: INFO: (4) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 41.695706ms)
Nov 16 01:47:34.847: INFO: (4) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 41.806863ms)
Nov 16 01:47:34.848: INFO: (4) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 42.624335ms)
Nov 16 01:47:34.848: INFO: (4) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 43.194148ms)
Nov 16 01:47:34.848: INFO: (4) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 42.250716ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 31.385724ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 31.850799ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 30.108379ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 31.217062ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 30.569549ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 30.929121ms)
Nov 16 01:47:34.880: INFO: (5) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 30.396546ms)
Nov 16 01:47:34.881: INFO: (5) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 30.119954ms)
Nov 16 01:47:34.881: INFO: (5) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 29.983951ms)
Nov 16 01:47:34.881: INFO: (5) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 18.743808ms)
Nov 16 01:47:34.881: INFO: (5) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 29.921251ms)
Nov 16 01:47:34.881: INFO: (5) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 32.020831ms)
Nov 16 01:47:34.882: INFO: (5) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 19.510246ms)
Nov 16 01:47:34.884: INFO: (5) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 21.960736ms)
Nov 16 01:47:34.888: INFO: (5) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 26.14555ms)
Nov 16 01:47:34.889: INFO: (5) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 26.925443ms)
Nov 16 01:47:34.906: INFO: (6) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 17.115041ms)
Nov 16 01:47:34.909: INFO: (6) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 19.384922ms)
Nov 16 01:47:34.911: INFO: (6) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 21.927502ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 22.30144ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 22.246875ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 22.476077ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 22.755304ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 22.564005ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 22.747714ms)
Nov 16 01:47:34.912: INFO: (6) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 22.681735ms)
Nov 16 01:47:34.916: INFO: (6) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 26.534128ms)
Nov 16 01:47:34.917: INFO: (6) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 27.015522ms)
Nov 16 01:47:34.918: INFO: (6) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 28.245226ms)
Nov 16 01:47:34.918: INFO: (6) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 28.639432ms)
Nov 16 01:47:34.918: INFO: (6) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 28.389716ms)
Nov 16 01:47:34.918: INFO: (6) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 28.65695ms)
Nov 16 01:47:34.932: INFO: (7) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 14.247089ms)
Nov 16 01:47:34.934: INFO: (7) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 15.378148ms)
Nov 16 01:47:34.938: INFO: (7) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 18.198227ms)
Nov 16 01:47:34.940: INFO: (7) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 21.406125ms)
Nov 16 01:47:34.940: INFO: (7) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 21.200076ms)
Nov 16 01:47:34.941: INFO: (7) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 20.488896ms)
Nov 16 01:47:34.942: INFO: (7) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 21.952065ms)
Nov 16 01:47:34.943: INFO: (7) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 24.239025ms)
Nov 16 01:47:34.943: INFO: (7) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 23.001336ms)
Nov 16 01:47:34.943: INFO: (7) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 23.486046ms)
Nov 16 01:47:34.943: INFO: (7) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 24.087227ms)
Nov 16 01:47:34.946: INFO: (7) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 25.840407ms)
Nov 16 01:47:34.946: INFO: (7) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 25.527043ms)
Nov 16 01:47:34.946: INFO: (7) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 26.26756ms)
Nov 16 01:47:34.947: INFO: (7) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 27.608873ms)
Nov 16 01:47:34.947: INFO: (7) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 28.571459ms)
Nov 16 01:47:34.962: INFO: (8) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 14.816016ms)
Nov 16 01:47:34.966: INFO: (8) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 18.471105ms)
Nov 16 01:47:34.968: INFO: (8) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 19.622822ms)
Nov 16 01:47:34.968: INFO: (8) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 19.707513ms)
Nov 16 01:47:34.969: INFO: (8) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 20.170044ms)
Nov 16 01:47:34.970: INFO: (8) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 20.711737ms)
Nov 16 01:47:34.970: INFO: (8) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 21.757922ms)
Nov 16 01:47:34.970: INFO: (8) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 21.484061ms)
Nov 16 01:47:34.970: INFO: (8) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 21.896569ms)
Nov 16 01:47:34.970: INFO: (8) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 21.018587ms)
Nov 16 01:47:34.972: INFO: (8) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 24.711199ms)
Nov 16 01:47:34.975: INFO: (8) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 26.231333ms)
Nov 16 01:47:34.975: INFO: (8) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 26.400071ms)
Nov 16 01:47:34.976: INFO: (8) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 26.422443ms)
Nov 16 01:47:34.976: INFO: (8) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 26.882731ms)
Nov 16 01:47:34.978: INFO: (8) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 29.208484ms)
Nov 16 01:47:34.995: INFO: (9) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 15.925838ms)
Nov 16 01:47:34.996: INFO: (9) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 17.786632ms)
Nov 16 01:47:34.997: INFO: (9) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 18.388618ms)
Nov 16 01:47:34.997: INFO: (9) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 17.644775ms)
Nov 16 01:47:34.997: INFO: (9) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 17.555797ms)
Nov 16 01:47:35.004: INFO: (9) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 24.809023ms)
Nov 16 01:47:35.005: INFO: (9) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 24.425348ms)
Nov 16 01:47:35.005: INFO: (9) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 24.830509ms)
Nov 16 01:47:35.005: INFO: (9) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 25.828099ms)
Nov 16 01:47:35.005: INFO: (9) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 25.278697ms)
Nov 16 01:47:35.005: INFO: (9) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 25.361803ms)
Nov 16 01:47:35.006: INFO: (9) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 26.856681ms)
Nov 16 01:47:35.007: INFO: (9) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 28.692163ms)
Nov 16 01:47:35.008: INFO: (9) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 28.696755ms)
Nov 16 01:47:35.009: INFO: (9) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 28.938047ms)
Nov 16 01:47:35.010: INFO: (9) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 31.003721ms)
Nov 16 01:47:35.029: INFO: (10) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 18.283648ms)
Nov 16 01:47:35.032: INFO: (10) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 21.140505ms)
Nov 16 01:47:35.033: INFO: (10) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 21.470141ms)
Nov 16 01:47:35.033: INFO: (10) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 21.419056ms)
Nov 16 01:47:35.034: INFO: (10) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 22.875751ms)
Nov 16 01:47:35.034: INFO: (10) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 22.919521ms)
Nov 16 01:47:35.035: INFO: (10) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 25.104072ms)
Nov 16 01:47:35.036: INFO: (10) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 26.057573ms)
Nov 16 01:47:35.036: INFO: (10) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 25.910083ms)
Nov 16 01:47:35.037: INFO: (10) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 25.857967ms)
Nov 16 01:47:35.039: INFO: (10) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 29.393677ms)
Nov 16 01:47:35.040: INFO: (10) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 29.6132ms)
Nov 16 01:47:35.040: INFO: (10) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 28.700086ms)
Nov 16 01:47:35.042: INFO: (10) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 32.155248ms)
Nov 16 01:47:35.042: INFO: (10) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 31.544453ms)
Nov 16 01:47:35.044: INFO: (10) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 32.268087ms)
Nov 16 01:47:35.058: INFO: (11) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 14.06164ms)
Nov 16 01:47:35.058: INFO: (11) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 14.575866ms)
Nov 16 01:47:35.061: INFO: (11) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 17.035105ms)
Nov 16 01:47:35.061: INFO: (11) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 17.029381ms)
Nov 16 01:47:35.061: INFO: (11) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 16.83538ms)
Nov 16 01:47:35.062: INFO: (11) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 18.085026ms)
Nov 16 01:47:35.063: INFO: (11) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 18.182029ms)
Nov 16 01:47:35.063: INFO: (11) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 19.045319ms)
Nov 16 01:47:35.064: INFO: (11) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 19.398454ms)
Nov 16 01:47:35.065: INFO: (11) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 20.79985ms)
Nov 16 01:47:35.090: INFO: (11) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 46.014757ms)
Nov 16 01:47:35.095: INFO: (11) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 50.741833ms)
Nov 16 01:47:35.096: INFO: (11) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 52.574364ms)
Nov 16 01:47:35.096: INFO: (11) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 51.909322ms)
Nov 16 01:47:35.097: INFO: (11) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 52.976872ms)
Nov 16 01:47:35.097: INFO: (11) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 52.33346ms)
Nov 16 01:47:35.115: INFO: (12) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 18.332441ms)
Nov 16 01:47:35.117: INFO: (12) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 20.42853ms)
Nov 16 01:47:35.121: INFO: (12) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 23.755051ms)
Nov 16 01:47:35.121: INFO: (12) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 24.144925ms)
Nov 16 01:47:35.122: INFO: (12) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 25.237292ms)
Nov 16 01:47:35.122: INFO: (12) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 25.087834ms)
Nov 16 01:47:35.122: INFO: (12) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 25.532774ms)
Nov 16 01:47:35.123: INFO: (12) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 25.632397ms)
Nov 16 01:47:35.124: INFO: (12) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 26.550225ms)
Nov 16 01:47:35.125: INFO: (12) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 28.258131ms)
Nov 16 01:47:35.125: INFO: (12) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 28.34308ms)
Nov 16 01:47:35.127: INFO: (12) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 29.662297ms)
Nov 16 01:47:35.130: INFO: (12) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 32.750641ms)
Nov 16 01:47:35.131: INFO: (12) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 34.119972ms)
Nov 16 01:47:35.131: INFO: (12) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 34.159283ms)
Nov 16 01:47:35.131: INFO: (12) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 34.401349ms)
Nov 16 01:47:35.176: INFO: (13) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 43.756724ms)
Nov 16 01:47:35.177: INFO: (13) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 43.763087ms)
Nov 16 01:47:35.181: INFO: (13) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 47.52294ms)
Nov 16 01:47:35.181: INFO: (13) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 48.831667ms)
Nov 16 01:47:35.181: INFO: (13) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 49.209908ms)
Nov 16 01:47:35.181: INFO: (13) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 49.099206ms)
Nov 16 01:47:35.183: INFO: (13) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 49.975928ms)
Nov 16 01:47:35.183: INFO: (13) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 50.444892ms)
Nov 16 01:47:35.184: INFO: (13) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 51.58663ms)
Nov 16 01:47:35.184: INFO: (13) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 49.86343ms)
Nov 16 01:47:35.185: INFO: (13) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 51.834101ms)
Nov 16 01:47:35.185: INFO: (13) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 51.464644ms)
Nov 16 01:47:35.185: INFO: (13) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 52.02409ms)
Nov 16 01:47:35.186: INFO: (13) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 54.002219ms)
Nov 16 01:47:35.187: INFO: (13) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 53.270273ms)
Nov 16 01:47:35.189: INFO: (13) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 55.699743ms)
Nov 16 01:47:35.205: INFO: (14) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 15.810779ms)
Nov 16 01:47:35.209: INFO: (14) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 19.781452ms)
Nov 16 01:47:35.210: INFO: (14) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 20.834158ms)
Nov 16 01:47:35.211: INFO: (14) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 21.727307ms)
Nov 16 01:47:35.211: INFO: (14) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 21.636803ms)
Nov 16 01:47:35.212: INFO: (14) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 22.606202ms)
Nov 16 01:47:35.212: INFO: (14) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 22.262948ms)
Nov 16 01:47:35.212: INFO: (14) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 22.469388ms)
Nov 16 01:47:35.212: INFO: (14) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 22.30147ms)
Nov 16 01:47:35.212: INFO: (14) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 22.744308ms)
Nov 16 01:47:35.213: INFO: (14) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 23.680833ms)
Nov 16 01:47:35.214: INFO: (14) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 25.077379ms)
Nov 16 01:47:35.255: INFO: (14) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 65.218167ms)
Nov 16 01:47:35.255: INFO: (14) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 65.322994ms)
Nov 16 01:47:35.255: INFO: (14) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 65.167136ms)
Nov 16 01:47:35.255: INFO: (14) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 65.369792ms)
Nov 16 01:47:35.272: INFO: (15) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 16.481105ms)
Nov 16 01:47:35.274: INFO: (15) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 19.091429ms)
Nov 16 01:47:35.275: INFO: (15) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 19.534559ms)
Nov 16 01:47:35.275: INFO: (15) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 20.297221ms)
Nov 16 01:47:35.276: INFO: (15) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 20.231669ms)
Nov 16 01:47:35.276: INFO: (15) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 21.01342ms)
Nov 16 01:47:35.310: INFO: (15) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 54.624498ms)
Nov 16 01:47:35.312: INFO: (15) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 57.077492ms)
Nov 16 01:47:35.313: INFO: (15) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 57.723921ms)
Nov 16 01:47:35.313: INFO: (15) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 57.614221ms)
Nov 16 01:47:35.315: INFO: (15) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 60.39364ms)
Nov 16 01:47:35.316: INFO: (15) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 61.039019ms)
Nov 16 01:47:35.317: INFO: (15) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 62.339525ms)
Nov 16 01:47:35.317: INFO: (15) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 62.046949ms)
Nov 16 01:47:35.318: INFO: (15) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 62.205583ms)
Nov 16 01:47:35.318: INFO: (15) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 62.306879ms)
Nov 16 01:47:35.362: INFO: (16) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 43.906095ms)
Nov 16 01:47:35.369: INFO: (16) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 51.21338ms)
Nov 16 01:47:35.369: INFO: (16) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 51.21061ms)
Nov 16 01:47:35.369: INFO: (16) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 51.299535ms)
Nov 16 01:47:35.369: INFO: (16) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 51.189918ms)
Nov 16 01:47:35.369: INFO: (16) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 51.196854ms)
Nov 16 01:47:35.370: INFO: (16) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 51.644153ms)
Nov 16 01:47:35.371: INFO: (16) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 53.2696ms)
Nov 16 01:47:35.371: INFO: (16) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 53.038876ms)
Nov 16 01:47:35.374: INFO: (16) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 56.443173ms)
Nov 16 01:47:35.380: INFO: (16) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 62.393757ms)
Nov 16 01:47:35.383: INFO: (16) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 65.242635ms)
Nov 16 01:47:35.384: INFO: (16) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 66.059286ms)
Nov 16 01:47:35.384: INFO: (16) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 66.582136ms)
Nov 16 01:47:35.384: INFO: (16) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 66.350136ms)
Nov 16 01:47:35.384: INFO: (16) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 66.49061ms)
Nov 16 01:47:35.401: INFO: (17) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 16.064464ms)
Nov 16 01:47:35.401: INFO: (17) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 16.163254ms)
Nov 16 01:47:35.405: INFO: (17) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 20.224397ms)
Nov 16 01:47:35.405: INFO: (17) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 19.930456ms)
Nov 16 01:47:35.406: INFO: (17) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 20.576705ms)
Nov 16 01:47:35.406: INFO: (17) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 20.928456ms)
Nov 16 01:47:35.406: INFO: (17) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 21.700726ms)
Nov 16 01:47:35.407: INFO: (17) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 22.384134ms)
Nov 16 01:47:35.407: INFO: (17) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 22.470553ms)
Nov 16 01:47:35.409: INFO: (17) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 24.630706ms)
Nov 16 01:47:35.410: INFO: (17) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 25.6044ms)
Nov 16 01:47:35.415: INFO: (17) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 29.868045ms)
Nov 16 01:47:35.418: INFO: (17) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 33.64736ms)
Nov 16 01:47:35.419: INFO: (17) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 33.508215ms)
Nov 16 01:47:35.419: INFO: (17) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 33.940346ms)
Nov 16 01:47:35.419: INFO: (17) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 34.349583ms)
Nov 16 01:47:35.434: INFO: (18) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 14.629862ms)
Nov 16 01:47:35.440: INFO: (18) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 20.272212ms)
Nov 16 01:47:35.441: INFO: (18) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 21.844762ms)
Nov 16 01:47:35.442: INFO: (18) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 21.925663ms)
Nov 16 01:47:35.442: INFO: (18) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 22.206545ms)
Nov 16 01:47:35.442: INFO: (18) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 22.174005ms)
Nov 16 01:47:35.442: INFO: (18) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 22.498762ms)
Nov 16 01:47:35.442: INFO: (18) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 22.086271ms)
Nov 16 01:47:35.442: INFO: (18) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 22.431272ms)
Nov 16 01:47:35.443: INFO: (18) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 23.800642ms)
Nov 16 01:47:35.447: INFO: (18) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 27.070453ms)
Nov 16 01:47:35.450: INFO: (18) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 30.386833ms)
Nov 16 01:47:35.453: INFO: (18) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 33.648266ms)
Nov 16 01:47:35.455: INFO: (18) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 34.829694ms)
Nov 16 01:47:35.455: INFO: (18) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 35.25595ms)
Nov 16 01:47:35.455: INFO: (18) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 35.323757ms)
Nov 16 01:47:35.471: INFO: (19) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 15.655789ms)
Nov 16 01:47:35.483: INFO: (19) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">test</a... (200; 27.924996ms)
Nov 16 01:47:35.484: INFO: (19) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:460/proxy/: tls baz (200; 28.711735ms)
Nov 16 01:47:35.485: INFO: (19) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 29.593624ms)
Nov 16 01:47:35.485: INFO: (19) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:443/proxy/tlsrewriteme"... (200; 29.980135ms)
Nov 16 01:47:35.486: INFO: (19) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:162/proxy/: bar (200; 30.250256ms)
Nov 16 01:47:35.488: INFO: (19) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs/proxy/rewriteme">test</a> (200; 33.115959ms)
Nov 16 01:47:35.491: INFO: (19) /api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-71/pods/http:proxy-service-wgpnf-7xdvs:1080/proxy/rewriteme">te... (200; 36.133113ms)
Nov 16 01:47:35.491: INFO: (19) /api/v1/namespaces/proxy-71/pods/proxy-service-wgpnf-7xdvs:160/proxy/: foo (200; 36.119328ms)
Nov 16 01:47:35.493: INFO: (19) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname1/proxy/: foo (200; 37.500218ms)
Nov 16 01:47:35.493: INFO: (19) /api/v1/namespaces/proxy-71/pods/https:proxy-service-wgpnf-7xdvs:462/proxy/: tls qux (200; 37.34753ms)
Nov 16 01:47:35.493: INFO: (19) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname2/proxy/: bar (200; 37.753669ms)
Nov 16 01:47:35.493: INFO: (19) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname1/proxy/: tls baz (200; 37.795216ms)
Nov 16 01:47:35.493: INFO: (19) /api/v1/namespaces/proxy-71/services/https:proxy-service-wgpnf:tlsportname2/proxy/: tls qux (200; 38.307541ms)
Nov 16 01:47:35.495: INFO: (19) /api/v1/namespaces/proxy-71/services/proxy-service-wgpnf:portname2/proxy/: bar (200; 39.377991ms)
Nov 16 01:47:35.498: INFO: (19) /api/v1/namespaces/proxy-71/services/http:proxy-service-wgpnf:portname1/proxy/: foo (200; 42.8294ms)
STEP: deleting ReplicationController proxy-service-wgpnf in namespace proxy-71, will wait for the garbage collector to delete the pods
Nov 16 01:47:35.593: INFO: Deleting ReplicationController proxy-service-wgpnf took: 28.147684ms
Nov 16 01:47:35.695: INFO: Terminating ReplicationController proxy-service-wgpnf pods took: 101.580047ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:37.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-71" for this suite.

• [SLOW TEST:6.633 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":27,"skipped":395,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:37.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 01:47:38.265: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 16 01:47:38.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:38.302: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Nov 16 01:47:38.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:38.376: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:39.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:39.410: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:40.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:40.393: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:41.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 16 01:47:41.395: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 16 01:47:41.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 16 01:47:41.469: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Nov 16 01:47:42.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:42.485: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 16 01:47:42.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:42.561: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:43.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:43.582: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:44.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:44.586: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:45.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:45.574: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:46.579: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:46.579: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 01:47:47.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 16 01:47:47.616: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9074, will wait for the garbage collector to delete the pods
Nov 16 01:47:47.815: INFO: Deleting DaemonSet.extensions daemon-set took: 81.540388ms
Nov 16 01:47:48.016: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.977112ms
Nov 16 01:47:50.926: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 01:47:50.926: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 16 01:47:50.938: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"78200"},"items":null}

Nov 16 01:47:50.948: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"78200"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:47:51.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9074" for this suite.

• [SLOW TEST:13.123 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":28,"skipped":397,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:47:51.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 16 01:47:51.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 16 01:48:30.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:48:41.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:21.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3043" for this suite.

• [SLOW TEST:90.104 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":29,"skipped":405,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:49:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Nov 16 01:49:21.463: INFO: The status of Pod labelsupdate06529f2f-822f-4b82-bbb7-abab0df9f28d is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:49:23.488: INFO: The status of Pod labelsupdate06529f2f-822f-4b82-bbb7-abab0df9f28d is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:49:25.479: INFO: The status of Pod labelsupdate06529f2f-822f-4b82-bbb7-abab0df9f28d is Running (Ready = true)
Nov 16 01:49:26.135: INFO: Successfully updated pod "labelsupdate06529f2f-822f-4b82-bbb7-abab0df9f28d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:28.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7980" for this suite.

• [SLOW TEST:7.068 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:49:28.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-2011
STEP: creating service affinity-nodeport-transition in namespace services-2011
STEP: creating replication controller affinity-nodeport-transition in namespace services-2011
I1116 01:49:28.559710      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2011, replica count: 3
I1116 01:49:31.613486      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 01:49:31.674: INFO: Creating new exec pod
Nov 16 01:49:36.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-2011 exec execpod-affinitygk8kn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Nov 16 01:49:37.218: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 16 01:49:37.218: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:49:37.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-2011 exec execpod-affinitygk8kn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.131.113 80'
Nov 16 01:49:37.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.131.113 80\nConnection to 172.21.131.113 80 port [tcp/http] succeeded!\n"
Nov 16 01:49:37.552: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:49:37.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-2011 exec execpod-affinitygk8kn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.157 32700'
Nov 16 01:49:37.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.157 32700\nConnection to 10.189.71.157 32700 port [tcp/*] succeeded!\n"
Nov 16 01:49:37.903: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:49:37.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-2011 exec execpod-affinitygk8kn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.151 32700'
Nov 16 01:49:38.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.151 32700\nConnection to 10.189.71.151 32700 port [tcp/*] succeeded!\n"
Nov 16 01:49:38.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:49:38.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-2011 exec execpod-affinitygk8kn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.189.71.150:32700/ ; done'
Nov 16 01:49:38.730: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n"
Nov 16 01:49:38.730: INFO: stdout: "\naffinity-nodeport-transition-mlrzf\naffinity-nodeport-transition-zncjd\naffinity-nodeport-transition-zncjd\naffinity-nodeport-transition-zncjd\naffinity-nodeport-transition-mlrzf\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-zncjd\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-mlrzf\naffinity-nodeport-transition-mlrzf\naffinity-nodeport-transition-mlrzf\naffinity-nodeport-transition-zncjd\naffinity-nodeport-transition-mlrzf\naffinity-nodeport-transition-zncjd\naffinity-nodeport-transition-68p25"
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-mlrzf
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-zncjd
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-zncjd
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-zncjd
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-mlrzf
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-zncjd
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-mlrzf
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-mlrzf
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-mlrzf
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-zncjd
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-mlrzf
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-zncjd
Nov 16 01:49:38.730: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:38.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-2011 exec execpod-affinitygk8kn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.189.71.150:32700/ ; done'
Nov 16 01:49:39.471: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32700/\n"
Nov 16 01:49:39.471: INFO: stdout: "\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25\naffinity-nodeport-transition-68p25"
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Received response from host: affinity-nodeport-transition-68p25
Nov 16 01:49:39.471: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2011, will wait for the garbage collector to delete the pods
Nov 16 01:49:39.630: INFO: Deleting ReplicationController affinity-nodeport-transition took: 30.385124ms
Nov 16 01:49:39.831: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.149327ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:43.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2011" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:14.850 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":31,"skipped":465,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:49:43.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Nov 16 01:49:43.396: INFO: Waiting up to 5m0s for pod "downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5" in namespace "downward-api-6455" to be "Succeeded or Failed"
Nov 16 01:49:43.408: INFO: Pod "downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.886465ms
Nov 16 01:49:45.428: INFO: Pod "downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031890353s
Nov 16 01:49:47.442: INFO: Pod "downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045486209s
Nov 16 01:49:49.462: INFO: Pod "downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065505809s
STEP: Saw pod success
Nov 16 01:49:49.462: INFO: Pod "downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5" satisfied condition "Succeeded or Failed"
Nov 16 01:49:49.474: INFO: Trying to get logs from node 10.189.71.151 pod downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5 container dapi-container: <nil>
STEP: delete the pod
Nov 16 01:49:49.582: INFO: Waiting for pod downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5 to disappear
Nov 16 01:49:49.593: INFO: Pod downward-api-2c6021fb-f2ea-41fd-907b-82365b3236c5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:49.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6455" for this suite.

• [SLOW TEST:6.517 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":470,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:49:49.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 01:49:49.825: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 01:49:49.879: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 01:49:49.904: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.150 before test
Nov 16 01:49:49.996: INFO: calico-node-wwftz from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 01:49:49.996: INFO: calico-typha-6c9689f9f9-7lrnr from calico-system started at 2022-11-15 22:55:50 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 01:49:49.996: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-qbrrz from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 01:49:49.996: INFO: ibm-keepalived-watcher-fvdpr from kube-system started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 01:49:49.996: INFO: ibm-master-proxy-static-10.189.71.150 from kube-system started at 2022-11-15 22:55:22 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 01:49:49.996: INFO: 	Container pause ready: true, restart count 0
Nov 16 01:49:49.996: INFO: ibmcloud-block-storage-driver-8skv4 from kube-system started at 2022-11-15 22:55:31 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 01:49:49.996: INFO: tuned-pwjtn from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container tuned ready: true, restart count 0
Nov 16 01:49:49.996: INFO: csi-snapshot-controller-67b9554c7c-2j5st from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 01:49:49.996: INFO: csi-snapshot-webhook-59f45645b4-jvp9f from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container webhook ready: true, restart count 0
Nov 16 01:49:49.996: INFO: console-7b55c47f46-m72dl from openshift-console started at 2022-11-15 22:58:16 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container console ready: true, restart count 0
Nov 16 01:49:49.996: INFO: downloads-7bb5c774c4-9j76w from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container download-server ready: true, restart count 0
Nov 16 01:49:49.996: INFO: dns-default-hsr72 from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.996: INFO: 	Container dns ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: node-resolver-n4drz from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 01:49:49.997: INFO: image-pruner-27809280-w5zhl from openshift-image-registry started at 2022-11-16 00:00:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container image-pruner ready: false, restart count 0
Nov 16 01:49:49.997: INFO: node-ca-7fr4t from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 01:49:49.997: INFO: registry-pvc-permissions-jnr42 from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 01:49:49.997: INFO: ingress-canary-c4qbp from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 01:49:49.997: INFO: router-default-585ff5cb57-5mh8f from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container router ready: true, restart count 0
Nov 16 01:49:49.997: INFO: openshift-kube-proxy-g54k6 from openshift-kube-proxy started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: node-exporter-rc4tp from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 01:49:49.997: INFO: prometheus-adapter-6dfb554d7c-ld9xx from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 01:49:49.997: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 01:49:49.997: INFO: thanos-querier-964664765-v92dp from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 01:49:49.997: INFO: multus-additional-cni-plugins-lg6ck from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 01:49:49.997: INFO: multus-admission-controller-72x8x from openshift-multus started at 2022-11-15 22:56:30 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 01:49:49.997: INFO: multus-hsbff from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 01:49:49.997: INFO: network-metrics-daemon-z2nx6 from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 01:49:49.997: INFO: network-check-target-tbk2k from openshift-network-diagnostics started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 01:49:49.997: INFO: sonobuoy from sonobuoy started at 2022-11-16 01:36:05 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 01:49:49.997: INFO: sonobuoy-e2e-job-d1033a110ac84614 from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container e2e ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:49:49.997: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-zzb4n from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:49.997: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 01:49:49.997: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.151 before test
Nov 16 01:49:50.073: INFO: calico-node-k6ktc from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 01:49:50.073: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-11-15 23:00:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 01:49:50.073: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-fgdkx from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 01:49:50.073: INFO: ibm-keepalived-watcher-hxcww from kube-system started at 2022-11-15 22:54:31 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 01:49:50.073: INFO: ibm-master-proxy-static-10.189.71.151 from kube-system started at 2022-11-15 22:54:22 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container pause ready: true, restart count 0
Nov 16 01:49:50.073: INFO: ibmcloud-block-storage-driver-jmfzn from kube-system started at 2022-11-15 22:54:37 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 01:49:50.073: INFO: vpn-bcc48b544-7bsch from kube-system started at 2022-11-15 22:59:50 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container vpn ready: true, restart count 0
Nov 16 01:49:50.073: INFO: tuned-j4zhw from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container tuned ready: true, restart count 0
Nov 16 01:49:50.073: INFO: csi-snapshot-controller-67b9554c7c-h9nb4 from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 01:49:50.073: INFO: csi-snapshot-webhook-59f45645b4-6v8x8 from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container webhook ready: true, restart count 0
Nov 16 01:49:50.073: INFO: downloads-7bb5c774c4-2wfgp from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container download-server ready: true, restart count 0
Nov 16 01:49:50.073: INFO: dns-default-lv86r from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container dns ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: node-resolver-45b2v from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 01:49:50.073: INFO: image-registry-665b8bc74-tzf8g from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container registry ready: true, restart count 0
Nov 16 01:49:50.073: INFO: node-ca-6xvc9 from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 01:49:50.073: INFO: ingress-canary-wd4rg from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 01:49:50.073: INFO: router-default-585ff5cb57-hvdfj from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container router ready: true, restart count 0
Nov 16 01:49:50.073: INFO: openshift-kube-proxy-fttpn from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: migrator-84db9689c8-sd7pr from openshift-kube-storage-version-migrator started at 2022-11-15 22:56:36 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container migrator ready: true, restart count 0
Nov 16 01:49:50.073: INFO: certified-operators-6p9ch from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:49:50.073: INFO: community-operators-djvrf from openshift-marketplace started at 2022-11-16 00:48:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:49:50.073: INFO: redhat-marketplace-mnw2d from openshift-marketplace started at 2022-11-15 22:57:21 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:49:50.073: INFO: redhat-operators-t5v8m from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:49:50.073: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: grafana-d8cb7d96c-zbvqh from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (3 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container grafana ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 01:49:50.073: INFO: kube-state-metrics-6545ff684-9k7k4 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 01:49:50.073: INFO: node-exporter-2qsf6 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 01:49:50.073: INFO: openshift-state-metrics-6f59cd77b7-gd25m from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 01:49:50.073: INFO: prometheus-adapter-6dfb554d7c-8ks56 from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 01:49:50.073: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 01:49:50.073: INFO: prometheus-operator-78c776cffb-vvd4z from openshift-monitoring started at 2022-11-15 22:57:51 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 01:49:50.073: INFO: telemeter-client-867498bdb-2sc6r from openshift-monitoring started at 2022-11-15 23:01:05 +0000 UTC (3 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container reload ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 01:49:50.073: INFO: thanos-querier-964664765-tpwrf from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:49:50.073: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 01:49:50.074: INFO: multus-additional-cni-plugins-lkmkq from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 01:49:50.074: INFO: multus-admission-controller-52p7r from openshift-multus started at 2022-11-15 22:56:01 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.074: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 01:49:50.074: INFO: multus-b7qhm from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 01:49:50.074: INFO: network-metrics-daemon-lmb6c from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.074: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 01:49:50.074: INFO: network-check-target-fknwj from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 01:49:50.074: INFO: collect-profiles-27809355-fz8z5 from openshift-operator-lifecycle-manager started at 2022-11-16 01:15:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 01:49:50.074: INFO: collect-profiles-27809370-64l7h from openshift-operator-lifecycle-manager started at 2022-11-16 01:30:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 01:49:50.074: INFO: collect-profiles-27809385-dcxn4 from openshift-operator-lifecycle-manager started at 2022-11-16 01:45:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 01:49:50.074: INFO: packageserver-74b6688945-wbm7x from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 01:49:50.074: INFO: service-ca-c6c7f64d9-kkrmn from openshift-service-ca started at 2022-11-15 22:56:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 16 01:49:50.074: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-glhsh from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:49:50.074: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 01:49:50.074: INFO: tigera-operator-56bfd47f4b-t9q5p from tigera-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.074: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 01:49:50.074: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.157 before test
Nov 16 01:49:50.130: INFO: calico-kube-controllers-7558694cbb-7jmn8 from calico-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 01:49:50.130: INFO: calico-node-m6bg7 from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 01:49:50.130: INFO: calico-typha-6c9689f9f9-2zwbk from calico-system started at 2022-11-15 22:55:41 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 01:49:50.130: INFO: managed-storage-validation-webhooks-577f9fb75-6h9ln from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 01:49:50.130: INFO: managed-storage-validation-webhooks-577f9fb75-fnp2h from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 16 01:49:50.130: INFO: managed-storage-validation-webhooks-577f9fb75-w69vd from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 01:49:50.130: INFO: ibm-file-plugin-9b7d4b5b8-4bnbc from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ibm-keepalived-watcher-rrpgn from kube-system started at 2022-11-15 22:54:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ibm-master-proxy-static-10.189.71.157 from kube-system started at 2022-11-15 22:54:20 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container pause ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ibm-storage-metrics-agent-5fdfb985fb-xk9pl from kube-system started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ibm-storage-watcher-b65cff46-l8xr7 from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ibmcloud-block-storage-driver-prkm8 from kube-system started at 2022-11-15 22:54:34 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ibmcloud-block-storage-plugin-54cb6d9d56-9228p from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 01:49:50.130: INFO: cluster-node-tuning-operator-58499f758f-tdqh6 from openshift-cluster-node-tuning-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: tuned-jqlkl from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container tuned ready: true, restart count 0
Nov 16 01:49:50.130: INFO: cluster-samples-operator-786cd9cc49-f96c8 from openshift-cluster-samples-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 01:49:50.130: INFO: cluster-storage-operator-7bffdd9f8b-h2ws8 from openshift-cluster-storage-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 16 01:49:50.130: INFO: csi-snapshot-controller-operator-687bbd75c7-28mzk from openshift-cluster-storage-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: console-operator-fc6655cfb-l5vkm from openshift-console-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 01:49:50.130: INFO: console-7b55c47f46-vcg9t from openshift-console started at 2022-11-15 22:57:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container console ready: true, restart count 0
Nov 16 01:49:50.130: INFO: dns-operator-86cb86fff-qfhb7 from openshift-dns-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.130: INFO: dns-default-2w2wr from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container dns ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.130: INFO: node-resolver-lvbbq from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 01:49:50.130: INFO: cluster-image-registry-operator-6b6ddc84cf-gltvh from openshift-image-registry started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: node-ca-pvrpl from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ingress-canary-njn8v from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 01:49:50.130: INFO: ingress-operator-659f7f8c4c-hmtfd from openshift-ingress-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.130: INFO: openshift-kube-proxy-lzfjb from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.130: INFO: kube-storage-version-migrator-operator-5bb6b9d4df-t7xsn from openshift-kube-storage-version-migrator-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 16 01:49:50.130: INFO: marketplace-operator-7b5856958-2qbfp from openshift-marketplace started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: cluster-monitoring-operator-6d9c66b9d-hrwvw from openshift-monitoring started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.130: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 01:49:50.130: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.131: INFO: node-exporter-c942p from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.131: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 01:49:50.131: INFO: multus-additional-cni-plugins-4g7rl from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 01:49:50.131: INFO: multus-admission-controller-594c2 from openshift-multus started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.131: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 01:49:50.131: INFO: multus-zq5lk from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 01:49:50.131: INFO: network-metrics-daemon-bw5dw from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:49:50.131: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 01:49:50.131: INFO: network-check-source-585b669bfd-r8rkl from openshift-network-diagnostics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 16 01:49:50.131: INFO: network-check-target-nq57m from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 01:49:50.131: INFO: network-operator-585b458dd4-dzhkh from openshift-network-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 01:49:50.131: INFO: catalog-operator-5f469dc4d6-plpn4 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 01:49:50.131: INFO: olm-operator-69ddc4ffc7-v85g5 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 01:49:50.131: INFO: package-server-manager-6b45b7b9cb-qmvk7 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 16 01:49:50.131: INFO: packageserver-74b6688945-wx6bb from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 01:49:50.131: INFO: metrics-6d5d4dd59c-txgqh from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container metrics ready: true, restart count 3
Nov 16 01:49:50.131: INFO: push-gateway-8565b96d89-q54bc from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container push-gateway ready: true, restart count 0
Nov 16 01:49:50.131: INFO: service-ca-operator-5f77cff646-4mbcp from openshift-service-ca-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 16 01:49:50.131: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-68x2f from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:49:50.131: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:49:50.131: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-385e7da8-feb4-4b57-8494-d033530c49dc 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-385e7da8-feb4-4b57-8494-d033530c49dc off the node 10.189.71.151
STEP: verifying the node doesn't have the label kubernetes.io/e2e-385e7da8-feb4-4b57-8494-d033530c49dc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:58.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7446" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.896 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":33,"skipped":491,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:49:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:58.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6705" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":34,"skipped":496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:49:58.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 01:49:58.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:49:59.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8336" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":35,"skipped":544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:50:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 16 01:50:00.373: INFO: Waiting up to 5m0s for pod "pod-abec3b38-9dcb-4383-81be-21de2855c2b1" in namespace "emptydir-6186" to be "Succeeded or Failed"
Nov 16 01:50:00.383: INFO: Pod "pod-abec3b38-9dcb-4383-81be-21de2855c2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.539961ms
Nov 16 01:50:02.402: INFO: Pod "pod-abec3b38-9dcb-4383-81be-21de2855c2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028830586s
Nov 16 01:50:04.413: INFO: Pod "pod-abec3b38-9dcb-4383-81be-21de2855c2b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039950569s
STEP: Saw pod success
Nov 16 01:50:04.413: INFO: Pod "pod-abec3b38-9dcb-4383-81be-21de2855c2b1" satisfied condition "Succeeded or Failed"
Nov 16 01:50:04.424: INFO: Trying to get logs from node 10.189.71.157 pod pod-abec3b38-9dcb-4383-81be-21de2855c2b1 container test-container: <nil>
STEP: delete the pod
Nov 16 01:50:04.532: INFO: Waiting for pod pod-abec3b38-9dcb-4383-81be-21de2855c2b1 to disappear
Nov 16 01:50:04.630: INFO: Pod pod-abec3b38-9dcb-4383-81be-21de2855c2b1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:50:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6186" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:50:04.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:50:04.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2415" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":37,"skipped":624,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:50:04.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:50:09.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7865" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":640,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:50:09.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9740
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-9740
Nov 16 01:50:09.443: INFO: Found 0 stateful pods, waiting for 1
Nov 16 01:50:19.466: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Nov 16 01:50:19.532: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Nov 16 01:50:19.563: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Nov 16 01:50:19.568: INFO: Observed &StatefulSet event: ADDED
Nov 16 01:50:19.568: INFO: Found Statefulset ss in namespace statefulset-9740 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 01:50:19.568: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Nov 16 01:50:19.568: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 16 01:50:19.592: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Nov 16 01:50:19.597: INFO: Observed &StatefulSet event: ADDED
Nov 16 01:50:19.597: INFO: Observed Statefulset ss in namespace statefulset-9740 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 01:50:19.597: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 01:50:19.597: INFO: Deleting all statefulset in ns statefulset-9740
Nov 16 01:50:19.610: INFO: Scaling statefulset ss to 0
Nov 16 01:50:29.673: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 01:50:29.684: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:50:29.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9740" for this suite.

• [SLOW TEST:20.570 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":39,"skipped":651,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:50:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 01:50:30.797: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 01:50:32.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 1, 50, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 1, 50, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 1, 50, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 1, 50, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 01:50:35.901: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:50:36.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6178" for this suite.
STEP: Destroying namespace "webhook-6178-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.632 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":40,"skipped":660,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:50:36.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:52:00.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5519" for this suite.

• [SLOW TEST:84.331 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":41,"skipped":665,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:52:00.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 01:52:01.026: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-54baed31-63d3-4fd0-8e08-c2dbd5f0f9d5
STEP: Creating configMap with name cm-test-opt-upd-ab4e173f-2022-41a1-b054-d1bea8ae5b4a
STEP: Creating the pod
Nov 16 01:52:01.144: INFO: The status of Pod pod-configmaps-fc7dbdc9-4545-461b-aba2-5b025fee4909 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:52:03.161: INFO: The status of Pod pod-configmaps-fc7dbdc9-4545-461b-aba2-5b025fee4909 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:52:05.169: INFO: The status of Pod pod-configmaps-fc7dbdc9-4545-461b-aba2-5b025fee4909 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-54baed31-63d3-4fd0-8e08-c2dbd5f0f9d5
STEP: Updating configmap cm-test-opt-upd-ab4e173f-2022-41a1-b054-d1bea8ae5b4a
STEP: Creating configMap with name cm-test-opt-create-4170e256-c7aa-47dc-886e-9a2b55ab85aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:53:34.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7530" for this suite.

• [SLOW TEST:94.248 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":668,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:53:34.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 16 01:53:35.177: INFO: Waiting up to 5m0s for pod "pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d" in namespace "emptydir-3168" to be "Succeeded or Failed"
Nov 16 01:53:35.188: INFO: Pod "pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.366454ms
Nov 16 01:53:37.213: INFO: Pod "pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036437281s
Nov 16 01:53:39.236: INFO: Pod "pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058711493s
STEP: Saw pod success
Nov 16 01:53:39.236: INFO: Pod "pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d" satisfied condition "Succeeded or Failed"
Nov 16 01:53:39.249: INFO: Trying to get logs from node 10.189.71.150 pod pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d container test-container: <nil>
STEP: delete the pod
Nov 16 01:53:39.361: INFO: Waiting for pod pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d to disappear
Nov 16 01:53:39.373: INFO: Pod pod-5e7e01b7-1cbf-4aed-8620-1b875db0ac4d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:53:39.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3168" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":684,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:53:39.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-6910
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 01:53:39.551: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 01:53:39.813: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:53:41.830: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:43.832: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:45.833: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:47.834: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:49.837: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:51.830: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:53.838: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:55.838: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:57.831: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:53:59.843: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:54:01.834: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 01:54:01.866: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 01:54:01.901: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 01:54:04.036: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 01:54:04.036: INFO: Breadth first check of 172.30.36.117 on host 10.189.71.150...
Nov 16 01:54:04.053: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.36.118:9080/dial?request=hostname&protocol=udp&host=172.30.36.117&port=8081&tries=1'] Namespace:pod-network-test-6910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 01:54:04.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:54:04.060: INFO: ExecWithOptions: Clientset creation
Nov 16 01:54:04.060: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.36.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.36.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Nov 16 01:54:04.275: INFO: Waiting for responses: map[]
Nov 16 01:54:04.275: INFO: reached 172.30.36.117 after 0/1 tries
Nov 16 01:54:04.275: INFO: Breadth first check of 172.30.102.229 on host 10.189.71.151...
Nov 16 01:54:04.290: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.36.118:9080/dial?request=hostname&protocol=udp&host=172.30.102.229&port=8081&tries=1'] Namespace:pod-network-test-6910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 01:54:04.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:54:04.290: INFO: ExecWithOptions: Clientset creation
Nov 16 01:54:04.291: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.36.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.102.229%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Nov 16 01:54:04.480: INFO: Waiting for responses: map[]
Nov 16 01:54:04.480: INFO: reached 172.30.102.229 after 0/1 tries
Nov 16 01:54:04.480: INFO: Breadth first check of 172.30.169.106 on host 10.189.71.157...
Nov 16 01:54:04.494: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.36.118:9080/dial?request=hostname&protocol=udp&host=172.30.169.106&port=8081&tries=1'] Namespace:pod-network-test-6910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 01:54:04.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:54:04.494: INFO: ExecWithOptions: Clientset creation
Nov 16 01:54:04.495: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.36.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.169.106%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Nov 16 01:54:04.682: INFO: Waiting for responses: map[]
Nov 16 01:54:04.682: INFO: reached 172.30.169.106 after 0/1 tries
Nov 16 01:54:04.682: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:04.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6910" for this suite.

• [SLOW TEST:25.290 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":699,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 16 01:54:04.951: INFO: Waiting up to 5m0s for pod "pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef" in namespace "emptydir-9123" to be "Succeeded or Failed"
Nov 16 01:54:04.970: INFO: Pod "pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef": Phase="Pending", Reason="", readiness=false. Elapsed: 18.475684ms
Nov 16 01:54:06.995: INFO: Pod "pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043621717s
Nov 16 01:54:09.014: INFO: Pod "pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062142055s
Nov 16 01:54:11.039: INFO: Pod "pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087789889s
STEP: Saw pod success
Nov 16 01:54:11.039: INFO: Pod "pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef" satisfied condition "Succeeded or Failed"
Nov 16 01:54:11.084: INFO: Trying to get logs from node 10.189.71.157 pod pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef container test-container: <nil>
STEP: delete the pod
Nov 16 01:54:11.236: INFO: Waiting for pod pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef to disappear
Nov 16 01:54:11.395: INFO: Pod pod-3fee79c9-2b99-47e1-9b52-ea33c95a25ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:11.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9123" for this suite.

• [SLOW TEST:6.735 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":700,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:11.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 01:54:12.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 01:54:14.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 1, 54, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 1, 54, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 1, 54, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 1, 54, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 01:54:17.394: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:17.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5644" for this suite.
STEP: Destroying namespace "webhook-5644-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.260 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":46,"skipped":708,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:17.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 01:54:18.166: INFO: The status of Pod busybox-host-aliases386ad24a-6956-4ed7-a438-ff1cf189c401 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:54:20.182: INFO: The status of Pod busybox-host-aliases386ad24a-6956-4ed7-a438-ff1cf189c401 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:20.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5337" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":719,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:20.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 01:54:21.519: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 16 01:54:23.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 1, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 1, 54, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 1, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 1, 54, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 01:54:26.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 16 01:54:26.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:26.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5411" for this suite.
STEP: Destroying namespace "webhook-5411-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.729 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":48,"skipped":728,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:27.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 16 01:54:27.330: INFO: Waiting up to 5m0s for pod "pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc" in namespace "emptydir-3255" to be "Succeeded or Failed"
Nov 16 01:54:27.347: INFO: Pod "pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.061639ms
Nov 16 01:54:29.373: INFO: Pod "pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042955326s
Nov 16 01:54:31.393: INFO: Pod "pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062784331s
Nov 16 01:54:33.412: INFO: Pod "pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08215184s
STEP: Saw pod success
Nov 16 01:54:33.412: INFO: Pod "pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc" satisfied condition "Succeeded or Failed"
Nov 16 01:54:33.455: INFO: Trying to get logs from node 10.189.71.150 pod pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc container test-container: <nil>
STEP: delete the pod
Nov 16 01:54:33.546: INFO: Waiting for pod pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc to disappear
Nov 16 01:54:33.562: INFO: Pod pod-b00a71a0-0e87-48c9-96d6-02d1b3dacdfc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:33.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3255" for this suite.

• [SLOW TEST:6.521 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":731,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:33.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Nov 16 01:54:33.836: INFO: The status of Pod labelsupdate2fae4e8d-eed5-4cf0-984a-824013dac7b3 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:54:35.859: INFO: The status of Pod labelsupdate2fae4e8d-eed5-4cf0-984a-824013dac7b3 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:54:37.863: INFO: The status of Pod labelsupdate2fae4e8d-eed5-4cf0-984a-824013dac7b3 is Running (Ready = true)
Nov 16 01:54:38.504: INFO: Successfully updated pod "labelsupdate2fae4e8d-eed5-4cf0-984a-824013dac7b3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:54:40.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5622" for this suite.

• [SLOW TEST:7.085 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":737,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:54:40.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-763
Nov 16 01:54:40.880: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:54:42.899: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 16 01:54:42.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 16 01:54:43.248: INFO: rc: 7
Nov 16 01:54:43.299: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 16 01:54:43.312: INFO: Pod kube-proxy-mode-detector no longer exists
Nov 16 01:54:43.312: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-763
STEP: creating replication controller affinity-clusterip-timeout in namespace services-763
I1116 01:54:43.377765      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-763, replica count: 3
I1116 01:54:46.429436      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 01:54:46.468: INFO: Creating new exec pod
Nov 16 01:54:51.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec execpod-affinitycd4kg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Nov 16 01:54:51.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 16 01:54:51.884: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:54:51.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec execpod-affinitycd4kg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.167.66 80'
Nov 16 01:54:52.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.167.66 80\nConnection to 172.21.167.66 80 port [tcp/http] succeeded!\n"
Nov 16 01:54:52.194: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 01:54:52.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec execpod-affinitycd4kg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.167.66:80/ ; done'
Nov 16 01:54:52.650: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n"
Nov 16 01:54:52.650: INFO: stdout: "\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f\naffinity-clusterip-timeout-mj44f"
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.650: INFO: Received response from host: affinity-clusterip-timeout-mj44f
Nov 16 01:54:52.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec execpod-affinitycd4kg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.167.66:80/'
Nov 16 01:54:53.019: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n"
Nov 16 01:54:53.019: INFO: stdout: "affinity-clusterip-timeout-mj44f"
Nov 16 01:55:13.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-763 exec execpod-affinitycd4kg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.167.66:80/'
Nov 16 01:55:13.376: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.167.66:80/\n"
Nov 16 01:55:13.376: INFO: stdout: "affinity-clusterip-timeout-m4nnh"
Nov 16 01:55:13.376: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-763, will wait for the garbage collector to delete the pods
Nov 16 01:55:13.518: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 26.147307ms
Nov 16 01:55:13.719: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.630385ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:55:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-763" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:35.946 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":51,"skipped":748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:55:16.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-fa650db2-6547-40d0-94f1-c2db9e8d3591
STEP: Creating a pod to test consume secrets
Nov 16 01:55:16.853: INFO: Waiting up to 5m0s for pod "pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11" in namespace "secrets-5268" to be "Succeeded or Failed"
Nov 16 01:55:16.866: INFO: Pod "pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11": Phase="Pending", Reason="", readiness=false. Elapsed: 13.070669ms
Nov 16 01:55:18.882: INFO: Pod "pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028736896s
Nov 16 01:55:20.900: INFO: Pod "pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047350225s
STEP: Saw pod success
Nov 16 01:55:20.900: INFO: Pod "pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11" satisfied condition "Succeeded or Failed"
Nov 16 01:55:20.912: INFO: Trying to get logs from node 10.189.71.151 pod pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11 container secret-env-test: <nil>
STEP: delete the pod
Nov 16 01:55:21.002: INFO: Waiting for pod pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11 to disappear
Nov 16 01:55:21.010: INFO: Pod pod-secrets-862c5f10-36eb-4aa4-9a30-e16ad5569b11 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:55:21.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5268" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":772,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:55:21.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-921
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 01:55:21.185: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 01:55:21.390: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:55:23.408: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:55:25.411: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:27.411: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:29.411: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:31.408: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:33.410: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:35.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:37.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:39.418: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:41.419: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 01:55:43.406: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 01:55:43.428: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 01:55:43.452: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 01:55:45.639: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 01:55:45.639: INFO: Going to poll 172.30.36.123 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 16 01:55:45.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.36.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-921 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 01:55:45.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:55:45.650: INFO: ExecWithOptions: Clientset creation
Nov 16 01:55:45.650: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-921/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.36.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 01:55:45.854: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 16 01:55:45.854: INFO: Going to poll 172.30.102.236 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 16 01:55:45.869: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.102.236:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-921 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 01:55:45.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:55:45.870: INFO: ExecWithOptions: Clientset creation
Nov 16 01:55:45.870: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-921/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.102.236%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 01:55:46.051: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 16 01:55:46.052: INFO: Going to poll 172.30.169.119 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 16 01:55:46.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.169.119:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-921 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 01:55:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 01:55:46.065: INFO: ExecWithOptions: Clientset creation
Nov 16 01:55:46.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-921/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.169.119%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 01:55:46.225: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:55:46.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-921" for this suite.

• [SLOW TEST:25.173 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":774,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:55:46.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Nov 16 01:55:46.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1832 create -f -'
Nov 16 01:55:49.592: INFO: stderr: ""
Nov 16 01:55:49.592: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 16 01:55:50.608: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 01:55:50.608: INFO: Found 0 / 1
Nov 16 01:55:51.617: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 01:55:51.617: INFO: Found 1 / 1
Nov 16 01:55:51.617: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 16 01:55:51.642: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 01:55:51.642: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 01:55:51.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1832 patch pod agnhost-primary-7c9gq -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 16 01:55:51.815: INFO: stderr: ""
Nov 16 01:55:51.815: INFO: stdout: "pod/agnhost-primary-7c9gq patched\n"
STEP: checking annotations
Nov 16 01:55:51.827: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 01:55:51.827: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:55:51.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1832" for this suite.

• [SLOW TEST:5.604 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1485
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":54,"skipped":795,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:55:51.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2781.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2781.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2781.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2781.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 01:56:02.311: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.343: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.368: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.385: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.401: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.458: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.476: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.495: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2781.svc.cluster.local from pod dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc: the server could not find the requested resource (get pods dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc)
Nov 16 01:56:02.495: INFO: Lookups using dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2781.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2781.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2781.svc.cluster.local jessie_udp@dns-test-service-2.dns-2781.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2781.svc.cluster.local]

Nov 16 01:56:07.758: INFO: DNS probes using dns-2781/dns-test-9688cc6f-5741-40ec-9619-257a31ae89bc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:56:07.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2781" for this suite.

• [SLOW TEST:16.056 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":55,"skipped":801,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:56:07.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Nov 16 01:56:08.149: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:10.169: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:12.171: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Nov 16 01:56:12.239: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:14.251: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:16.255: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 16 01:56:16.351: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 01:56:16.367: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 16 01:56:18.368: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 01:56:18.388: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 16 01:56:20.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 01:56:20.395: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:56:20.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3354" for this suite.

• [SLOW TEST:12.504 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":804,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:56:20.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Nov 16 01:56:20.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-4117 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 16 01:56:20.678: INFO: stderr: ""
Nov 16 01:56:20.678: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 16 01:56:25.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-4117 get pod e2e-test-httpd-pod -o json'
Nov 16 01:56:25.820: INFO: stderr: ""
Nov 16 01:56:25.820: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"3b024d7f99d3f8554a76fd26b091184b926128a2efade5a27c2d494b15ed11a2\",\n            \"cni.projectcalico.org/podIP\": \"172.30.36.125/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.36.125/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.36.125\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.36.125\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2022-11-16T01:56:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4117\",\n        \"resourceVersion\": \"83991\",\n        \"uid\": \"ceb551bd-810d-4063-ab70-c1cf15a67387\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qrfph\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.189.71.150\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c38,c2\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qrfph\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-16T01:56:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-16T01:56:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-16T01:56:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-16T01:56:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://d0fb3deb68bd5ca095bb1c8c0c2be620a062a35645df968075e0599935d210b1\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-11-16T01:56:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.189.71.150\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.36.125\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.36.125\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-11-16T01:56:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 16 01:56:25.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-4117 replace -f -'
Nov 16 01:56:26.336: INFO: stderr: ""
Nov 16 01:56:26.336: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
Nov 16 01:56:26.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-4117 delete pods e2e-test-httpd-pod'
Nov 16 01:56:28.762: INFO: stderr: ""
Nov 16 01:56:28.762: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:56:28.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4117" for this suite.

• [SLOW TEST:8.414 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":57,"skipped":815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:56:28.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Nov 16 01:56:29.149: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:31.171: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Nov 16 01:56:31.245: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:33.282: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 01:56:35.262: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Nov 16 01:56:35.298: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 01:56:35.310: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 01:56:37.311: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 01:56:37.334: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 01:56:39.311: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 01:56:39.336: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:56:39.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7226" for this suite.

• [SLOW TEST:10.555 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:56:39.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
Nov 16 01:56:39.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2942 cluster-info'
Nov 16 01:56:39.624: INFO: stderr: ""
Nov 16 01:56:39.624: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:56:39.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2942" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":59,"skipped":890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:56:39.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 16 01:56:41.014: INFO: Pod name wrapped-volume-race-53bfb4bc-04ca-4ac7-bbe1-cd13c1bd7ced: Found 0 pods out of 5
Nov 16 01:56:46.051: INFO: Pod name wrapped-volume-race-53bfb4bc-04ca-4ac7-bbe1-cd13c1bd7ced: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-53bfb4bc-04ca-4ac7-bbe1-cd13c1bd7ced in namespace emptydir-wrapper-5435, will wait for the garbage collector to delete the pods
Nov 16 01:56:46.219: INFO: Deleting ReplicationController wrapped-volume-race-53bfb4bc-04ca-4ac7-bbe1-cd13c1bd7ced took: 28.179952ms
Nov 16 01:56:46.321: INFO: Terminating ReplicationController wrapped-volume-race-53bfb4bc-04ca-4ac7-bbe1-cd13c1bd7ced pods took: 101.216955ms
STEP: Creating RC which spawns configmap-volume pods
Nov 16 01:56:50.291: INFO: Pod name wrapped-volume-race-aa7bc97e-1c2d-4fc0-b4be-142302779c95: Found 0 pods out of 5
Nov 16 01:56:55.326: INFO: Pod name wrapped-volume-race-aa7bc97e-1c2d-4fc0-b4be-142302779c95: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-aa7bc97e-1c2d-4fc0-b4be-142302779c95 in namespace emptydir-wrapper-5435, will wait for the garbage collector to delete the pods
Nov 16 01:56:55.472: INFO: Deleting ReplicationController wrapped-volume-race-aa7bc97e-1c2d-4fc0-b4be-142302779c95 took: 28.162046ms
Nov 16 01:56:55.574: INFO: Terminating ReplicationController wrapped-volume-race-aa7bc97e-1c2d-4fc0-b4be-142302779c95 pods took: 101.097236ms
STEP: Creating RC which spawns configmap-volume pods
Nov 16 01:56:59.563: INFO: Pod name wrapped-volume-race-5eca57f7-4425-40a0-a58c-78510699459f: Found 0 pods out of 5
Nov 16 01:57:04.589: INFO: Pod name wrapped-volume-race-5eca57f7-4425-40a0-a58c-78510699459f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5eca57f7-4425-40a0-a58c-78510699459f in namespace emptydir-wrapper-5435, will wait for the garbage collector to delete the pods
Nov 16 01:57:04.752: INFO: Deleting ReplicationController wrapped-volume-race-5eca57f7-4425-40a0-a58c-78510699459f took: 25.751425ms
Nov 16 01:57:04.854: INFO: Terminating ReplicationController wrapped-volume-race-5eca57f7-4425-40a0-a58c-78510699459f pods took: 102.066595ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:57:10.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5435" for this suite.

• [SLOW TEST:30.807 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":60,"skipped":922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:57:10.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
Nov 16 01:57:10.663: INFO: Waiting up to 5m0s for pod "var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d" in namespace "var-expansion-2075" to be "Succeeded or Failed"
Nov 16 01:57:10.677: INFO: Pod "var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.410365ms
Nov 16 01:57:12.697: INFO: Pod "var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034266517s
Nov 16 01:57:14.711: INFO: Pod "var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048298575s
Nov 16 01:57:16.729: INFO: Pod "var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065855121s
STEP: Saw pod success
Nov 16 01:57:16.729: INFO: Pod "var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d" satisfied condition "Succeeded or Failed"
Nov 16 01:57:16.739: INFO: Trying to get logs from node 10.189.71.151 pod var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d container dapi-container: <nil>
STEP: delete the pod
Nov 16 01:57:16.805: INFO: Waiting for pod var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d to disappear
Nov 16 01:57:16.816: INFO: Pod var-expansion-58fb1c9a-7037-45a1-8d9e-41f952e5b01d no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 01:57:16.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2075" for this suite.

• [SLOW TEST:6.369 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 01:57:16.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 01:57:16.961: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 01:57:17.003: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 01:57:17.038: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.150 before test
Nov 16 01:57:17.120: INFO: calico-node-wwftz from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 01:57:17.120: INFO: calico-typha-6c9689f9f9-7lrnr from calico-system started at 2022-11-15 22:55:50 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 01:57:17.120: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-qbrrz from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 01:57:17.120: INFO: ibm-keepalived-watcher-fvdpr from kube-system started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 01:57:17.120: INFO: ibm-master-proxy-static-10.189.71.150 from kube-system started at 2022-11-15 22:55:22 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container pause ready: true, restart count 0
Nov 16 01:57:17.120: INFO: ibmcloud-block-storage-driver-8skv4 from kube-system started at 2022-11-15 22:55:31 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 01:57:17.120: INFO: tuned-pwjtn from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container tuned ready: true, restart count 0
Nov 16 01:57:17.120: INFO: csi-snapshot-controller-67b9554c7c-2j5st from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 01:57:17.120: INFO: csi-snapshot-webhook-59f45645b4-jvp9f from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container webhook ready: true, restart count 0
Nov 16 01:57:17.120: INFO: console-7b55c47f46-m72dl from openshift-console started at 2022-11-15 22:58:16 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container console ready: true, restart count 0
Nov 16 01:57:17.120: INFO: downloads-7bb5c774c4-9j76w from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container download-server ready: true, restart count 0
Nov 16 01:57:17.120: INFO: dns-default-hsr72 from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container dns ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: node-resolver-n4drz from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 01:57:17.120: INFO: image-pruner-27809280-w5zhl from openshift-image-registry started at 2022-11-16 00:00:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container image-pruner ready: false, restart count 0
Nov 16 01:57:17.120: INFO: node-ca-7fr4t from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 01:57:17.120: INFO: registry-pvc-permissions-jnr42 from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 01:57:17.120: INFO: ingress-canary-c4qbp from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 01:57:17.120: INFO: router-default-585ff5cb57-5mh8f from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container router ready: true, restart count 0
Nov 16 01:57:17.120: INFO: openshift-kube-proxy-g54k6 from openshift-kube-proxy started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: node-exporter-rc4tp from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 01:57:17.120: INFO: prometheus-adapter-6dfb554d7c-ld9xx from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 01:57:17.120: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 01:57:17.120: INFO: thanos-querier-964664765-v92dp from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 01:57:17.120: INFO: multus-additional-cni-plugins-lg6ck from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 01:57:17.120: INFO: multus-admission-controller-72x8x from openshift-multus started at 2022-11-15 22:56:30 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 01:57:17.120: INFO: multus-hsbff from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 01:57:17.120: INFO: network-metrics-daemon-z2nx6 from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 01:57:17.120: INFO: network-check-target-tbk2k from openshift-network-diagnostics started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 01:57:17.120: INFO: sonobuoy from sonobuoy started at 2022-11-16 01:36:05 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 01:57:17.120: INFO: sonobuoy-e2e-job-d1033a110ac84614 from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container e2e ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:57:17.120: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-zzb4n from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 01:57:17.120: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.151 before test
Nov 16 01:57:17.188: INFO: calico-node-k6ktc from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.188: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 01:57:17.188: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-11-15 23:00:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.188: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 01:57:17.188: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-fgdkx from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.188: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 01:57:17.188: INFO: ibm-keepalived-watcher-hxcww from kube-system started at 2022-11-15 22:54:31 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.188: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 01:57:17.188: INFO: ibm-master-proxy-static-10.189.71.151 from kube-system started at 2022-11-15 22:54:22 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.188: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 01:57:17.188: INFO: 	Container pause ready: true, restart count 0
Nov 16 01:57:17.188: INFO: ibmcloud-block-storage-driver-jmfzn from kube-system started at 2022-11-15 22:54:37 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 01:57:17.189: INFO: vpn-bcc48b544-7bsch from kube-system started at 2022-11-15 22:59:50 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container vpn ready: true, restart count 0
Nov 16 01:57:17.189: INFO: tuned-j4zhw from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container tuned ready: true, restart count 0
Nov 16 01:57:17.189: INFO: csi-snapshot-controller-67b9554c7c-h9nb4 from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 01:57:17.189: INFO: csi-snapshot-webhook-59f45645b4-6v8x8 from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container webhook ready: true, restart count 0
Nov 16 01:57:17.189: INFO: downloads-7bb5c774c4-2wfgp from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container download-server ready: true, restart count 0
Nov 16 01:57:17.189: INFO: dns-default-lv86r from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container dns ready: true, restart count 0
Nov 16 01:57:17.189: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.189: INFO: node-resolver-45b2v from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.189: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 01:57:17.190: INFO: image-registry-665b8bc74-tzf8g from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container registry ready: true, restart count 0
Nov 16 01:57:17.190: INFO: node-ca-6xvc9 from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 01:57:17.190: INFO: ingress-canary-wd4rg from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 01:57:17.190: INFO: router-default-585ff5cb57-hvdfj from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container router ready: true, restart count 0
Nov 16 01:57:17.190: INFO: openshift-kube-proxy-fttpn from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 01:57:17.190: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.190: INFO: migrator-84db9689c8-sd7pr from openshift-kube-storage-version-migrator started at 2022-11-15 22:56:36 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container migrator ready: true, restart count 0
Nov 16 01:57:17.190: INFO: certified-operators-6p9ch from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.190: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:57:17.190: INFO: community-operators-djvrf from openshift-marketplace started at 2022-11-16 00:48:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.191: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:57:17.191: INFO: redhat-marketplace-mnw2d from openshift-marketplace started at 2022-11-15 22:57:21 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.191: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:57:17.191: INFO: redhat-operators-p74fx from openshift-marketplace started at 2022-11-16 01:57:11 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.191: INFO: 	Container registry-server ready: false, restart count 0
Nov 16 01:57:17.191: INFO: redhat-operators-t5v8m from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.191: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 01:57:17.191: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 01:57:17.191: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 01:57:17.191: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 01:57:17.191: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:57:17.191: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.191: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 01:57:17.191: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:57:17.191: INFO: grafana-d8cb7d96c-zbvqh from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (3 container statuses recorded)
Nov 16 01:57:17.191: INFO: 	Container grafana ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 01:57:17.192: INFO: kube-state-metrics-6545ff684-9k7k4 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 01:57:17.192: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 01:57:17.192: INFO: node-exporter-2qsf6 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 01:57:17.192: INFO: openshift-state-metrics-6f59cd77b7-gd25m from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 01:57:17.192: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 01:57:17.192: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 01:57:17.192: INFO: prometheus-adapter-6dfb554d7c-8ks56 from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.192: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 01:57:17.192: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 01:57:17.192: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 01:57:17.193: INFO: prometheus-operator-78c776cffb-vvd4z from openshift-monitoring started at 2022-11-15 22:57:51 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 01:57:17.193: INFO: telemeter-client-867498bdb-2sc6r from openshift-monitoring started at 2022-11-15 23:01:05 +0000 UTC (3 container statuses recorded)
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container reload ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 01:57:17.193: INFO: thanos-querier-964664765-tpwrf from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 01:57:17.193: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 01:57:17.194: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 01:57:17.194: INFO: multus-additional-cni-plugins-lkmkq from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.194: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 01:57:17.194: INFO: multus-admission-controller-52p7r from openshift-multus started at 2022-11-15 22:56:01 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.194: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 01:57:17.194: INFO: multus-b7qhm from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.194: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 01:57:17.194: INFO: network-metrics-daemon-lmb6c from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.194: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.194: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 01:57:17.194: INFO: network-check-target-fknwj from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.194: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 01:57:17.194: INFO: collect-profiles-27809355-fz8z5 from openshift-operator-lifecycle-manager started at 2022-11-16 01:15:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.194: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 01:57:17.194: INFO: collect-profiles-27809370-64l7h from openshift-operator-lifecycle-manager started at 2022-11-16 01:30:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.195: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 01:57:17.195: INFO: collect-profiles-27809385-dcxn4 from openshift-operator-lifecycle-manager started at 2022-11-16 01:45:00 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.195: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 01:57:17.195: INFO: packageserver-74b6688945-wbm7x from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.195: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 01:57:17.195: INFO: service-ca-c6c7f64d9-kkrmn from openshift-service-ca started at 2022-11-15 22:56:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.195: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 16 01:57:17.195: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-glhsh from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:57:17.195: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 01:57:17.195: INFO: tigera-operator-56bfd47f4b-t9q5p from tigera-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.195: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 01:57:17.195: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.157 before test
Nov 16 01:57:17.255: INFO: calico-kube-controllers-7558694cbb-7jmn8 from calico-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.255: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 01:57:17.255: INFO: calico-node-m6bg7 from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.255: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 01:57:17.255: INFO: calico-typha-6c9689f9f9-2zwbk from calico-system started at 2022-11-15 22:55:41 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.255: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 01:57:17.255: INFO: managed-storage-validation-webhooks-577f9fb75-6h9ln from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.255: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 01:57:17.255: INFO: managed-storage-validation-webhooks-577f9fb75-fnp2h from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 16 01:57:17.256: INFO: managed-storage-validation-webhooks-577f9fb75-w69vd from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 01:57:17.256: INFO: ibm-file-plugin-9b7d4b5b8-4bnbc from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 01:57:17.256: INFO: ibm-keepalived-watcher-rrpgn from kube-system started at 2022-11-15 22:54:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 01:57:17.256: INFO: ibm-master-proxy-static-10.189.71.157 from kube-system started at 2022-11-15 22:54:20 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 01:57:17.256: INFO: 	Container pause ready: true, restart count 0
Nov 16 01:57:17.256: INFO: ibm-storage-metrics-agent-5fdfb985fb-xk9pl from kube-system started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 16 01:57:17.256: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 16 01:57:17.256: INFO: ibm-storage-watcher-b65cff46-l8xr7 from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 01:57:17.256: INFO: ibmcloud-block-storage-driver-prkm8 from kube-system started at 2022-11-15 22:54:34 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.256: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 01:57:17.256: INFO: ibmcloud-block-storage-plugin-54cb6d9d56-9228p from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 01:57:17.257: INFO: cluster-node-tuning-operator-58499f758f-tdqh6 from openshift-cluster-node-tuning-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 01:57:17.257: INFO: tuned-jqlkl from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container tuned ready: true, restart count 0
Nov 16 01:57:17.257: INFO: cluster-samples-operator-786cd9cc49-f96c8 from openshift-cluster-samples-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 01:57:17.257: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 01:57:17.257: INFO: cluster-storage-operator-7bffdd9f8b-h2ws8 from openshift-cluster-storage-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 16 01:57:17.257: INFO: csi-snapshot-controller-operator-687bbd75c7-28mzk from openshift-cluster-storage-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 16 01:57:17.257: INFO: console-operator-fc6655cfb-l5vkm from openshift-console-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 01:57:17.257: INFO: console-7b55c47f46-vcg9t from openshift-console started at 2022-11-15 22:57:42 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.257: INFO: 	Container console ready: true, restart count 0
Nov 16 01:57:17.257: INFO: dns-operator-86cb86fff-qfhb7 from openshift-dns-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 01:57:17.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.258: INFO: dns-default-2w2wr from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container dns ready: true, restart count 0
Nov 16 01:57:17.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.258: INFO: node-resolver-lvbbq from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 01:57:17.258: INFO: cluster-image-registry-operator-6b6ddc84cf-gltvh from openshift-image-registry started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 01:57:17.258: INFO: node-ca-pvrpl from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 01:57:17.258: INFO: ingress-canary-njn8v from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 01:57:17.258: INFO: ingress-operator-659f7f8c4c-hmtfd from openshift-ingress-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.258: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 01:57:17.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.258: INFO: openshift-kube-proxy-lzfjb from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 01:57:17.259: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.259: INFO: kube-storage-version-migrator-operator-5bb6b9d4df-t7xsn from openshift-kube-storage-version-migrator-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 16 01:57:17.259: INFO: marketplace-operator-7b5856958-2qbfp from openshift-marketplace started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 01:57:17.259: INFO: cluster-monitoring-operator-6d9c66b9d-hrwvw from openshift-monitoring started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 01:57:17.259: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.259: INFO: node-exporter-c942p from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.259: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 01:57:17.259: INFO: multus-additional-cni-plugins-4g7rl from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 01:57:17.259: INFO: multus-admission-controller-594c2 from openshift-multus started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.259: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.259: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 01:57:17.260: INFO: multus-zq5lk from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 01:57:17.260: INFO: network-metrics-daemon-bw5dw from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 01:57:17.260: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 01:57:17.260: INFO: network-check-source-585b669bfd-r8rkl from openshift-network-diagnostics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 16 01:57:17.260: INFO: network-check-target-nq57m from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 01:57:17.260: INFO: network-operator-585b458dd4-dzhkh from openshift-network-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 01:57:17.260: INFO: catalog-operator-5f469dc4d6-plpn4 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 01:57:17.260: INFO: olm-operator-69ddc4ffc7-v85g5 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.260: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 01:57:17.260: INFO: package-server-manager-6b45b7b9cb-qmvk7 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.261: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 16 01:57:17.261: INFO: packageserver-74b6688945-wx6bb from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.261: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 01:57:17.261: INFO: metrics-6d5d4dd59c-txgqh from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.261: INFO: 	Container metrics ready: true, restart count 3
Nov 16 01:57:17.261: INFO: push-gateway-8565b96d89-q54bc from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.261: INFO: 	Container push-gateway ready: true, restart count 0
Nov 16 01:57:17.261: INFO: service-ca-operator-5f77cff646-4mbcp from openshift-service-ca-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 01:57:17.261: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 16 01:57:17.261: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-68x2f from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 01:57:17.261: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 01:57:17.261: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6cee4981-7a32-4439-8fdc-e4764eb069fb 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.189.71.151 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6cee4981-7a32-4439-8fdc-e4764eb069fb off the node 10.189.71.151
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6cee4981-7a32-4439-8fdc-e4764eb069fb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:02:21.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3174" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.952 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":62,"skipped":1123,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:02:21.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-06db9f04-ca06-4a4b-87af-11269017d55e in namespace container-probe-5226
Nov 16 02:02:25.998: INFO: Started pod test-webserver-06db9f04-ca06-4a4b-87af-11269017d55e in namespace container-probe-5226
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 02:02:26.012: INFO: Initial restart count of pod test-webserver-06db9f04-ca06-4a4b-87af-11269017d55e is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:06:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5226" for this suite.

• [SLOW TEST:245.036 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:06:26.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:06:36.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2749" for this suite.
STEP: Destroying namespace "nsdeletetest-6525" for this suite.
Nov 16 02:06:36.570: INFO: Namespace nsdeletetest-6525 was already deleted
STEP: Destroying namespace "nsdeletetest-8839" for this suite.

• [SLOW TEST:9.758 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":64,"skipped":1152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:06:36.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:06:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7364" for this suite.
STEP: Destroying namespace "nsdeletetest-8629" for this suite.
Nov 16 02:06:55.407: INFO: Namespace nsdeletetest-8629 was already deleted
STEP: Destroying namespace "nsdeletetest-6156" for this suite.

• [SLOW TEST:18.815 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":65,"skipped":1189,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:06:55.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:06:55.657: INFO: The status of Pod server-envvars-3256c9ae-9cb1-4e64-81d4-3878a77779a4 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:06:57.685: INFO: The status of Pod server-envvars-3256c9ae-9cb1-4e64-81d4-3878a77779a4 is Running (Ready = true)
Nov 16 02:06:57.782: INFO: Waiting up to 5m0s for pod "client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd" in namespace "pods-3005" to be "Succeeded or Failed"
Nov 16 02:06:57.796: INFO: Pod "client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.833161ms
Nov 16 02:06:59.822: INFO: Pod "client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03972797s
Nov 16 02:07:01.848: INFO: Pod "client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065631278s
Nov 16 02:07:03.861: INFO: Pod "client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078530418s
STEP: Saw pod success
Nov 16 02:07:03.861: INFO: Pod "client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd" satisfied condition "Succeeded or Failed"
Nov 16 02:07:03.871: INFO: Trying to get logs from node 10.189.71.151 pod client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd container env3cont: <nil>
STEP: delete the pod
Nov 16 02:07:03.966: INFO: Waiting for pod client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd to disappear
Nov 16 02:07:03.978: INFO: Pod client-envvars-e77f64e0-5c1b-4615-a0f4-ea861dab31fd no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:07:03.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3005" for this suite.

• [SLOW TEST:8.587 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1192,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:07:04.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-7241
STEP: creating service affinity-nodeport in namespace services-7241
STEP: creating replication controller affinity-nodeport in namespace services-7241
I1116 02:07:04.266842      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7241, replica count: 3
I1116 02:07:07.318405      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 02:07:07.422: INFO: Creating new exec pod
Nov 16 02:07:12.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-7241 exec execpod-affinitym4wxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Nov 16 02:07:13.011: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 16 02:07:13.011: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:07:13.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-7241 exec execpod-affinitym4wxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.75.31 80'
Nov 16 02:07:13.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.75.31 80\nConnection to 172.21.75.31 80 port [tcp/http] succeeded!\n"
Nov 16 02:07:13.318: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:07:13.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-7241 exec execpod-affinitym4wxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.157 32673'
Nov 16 02:07:13.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.157 32673\nConnection to 10.189.71.157 32673 port [tcp/*] succeeded!\n"
Nov 16 02:07:13.672: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:07:13.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-7241 exec execpod-affinitym4wxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.151 32673'
Nov 16 02:07:13.974: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.151 32673\nConnection to 10.189.71.151 32673 port [tcp/*] succeeded!\n"
Nov 16 02:07:13.974: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:07:13.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-7241 exec execpod-affinitym4wxq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.189.71.150:32673/ ; done'
Nov 16 02:07:14.409: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.189.71.150:32673/\n"
Nov 16 02:07:14.409: INFO: stdout: "\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2\naffinity-nodeport-vqcb2"
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Received response from host: affinity-nodeport-vqcb2
Nov 16 02:07:14.409: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7241, will wait for the garbage collector to delete the pods
Nov 16 02:07:14.585: INFO: Deleting ReplicationController affinity-nodeport took: 48.277526ms
Nov 16 02:07:14.691: INFO: Terminating ReplicationController affinity-nodeport pods took: 105.767165ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:07:17.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7241" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:13.847 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":67,"skipped":1202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:07:17.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 02:07:18.088: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 02:08:18.330: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Nov 16 02:08:18.447: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 16 02:08:18.487: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 16 02:08:18.573: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 16 02:08:18.626: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 16 02:08:18.711: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 16 02:08:18.760: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:08:37.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6250" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:79.437 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":68,"skipped":1230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:08:37.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 16 02:10:38.126: INFO: Successfully updated pod "var-expansion-8d97287b-4360-46e9-b46b-31c7a4530b52"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 16 02:10:40.157: INFO: Deleting pod "var-expansion-8d97287b-4360-46e9-b46b-31c7a4530b52" in namespace "var-expansion-4012"
Nov 16 02:10:40.181: INFO: Wait up to 5m0s for pod "var-expansion-8d97287b-4360-46e9-b46b-31c7a4530b52" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:12.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4012" for this suite.

• [SLOW TEST:154.960 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":69,"skipped":1253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:12.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:11:12.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6558" for this suite.

• [SLOW TEST:7.913 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":70,"skipped":1277,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:20.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 02:11:20.374: INFO: The status of Pod pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:11:22.390: INFO: The status of Pod pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:11:24.389: INFO: The status of Pod pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 16 02:11:25.000: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412"
Nov 16 02:11:25.000: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412" in namespace "pods-2285" to be "terminated due to deadline exceeded"
Nov 16 02:11:25.014: INFO: Pod "pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412": Phase="Running", Reason="", readiness=true. Elapsed: 13.669936ms
Nov 16 02:11:27.038: INFO: Pod "pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.037458244s
Nov 16 02:11:27.038: INFO: Pod "pod-update-activedeadlineseconds-c24c129a-a307-4c82-814d-17ee8a3df412" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:27.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2285" for this suite.

• [SLOW TEST:6.903 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:27.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:11:27.190: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 16 02:11:29.365: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9827" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":72,"skipped":1317,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:30.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Nov 16 02:11:40.665: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W1116 02:11:40.665402      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 02:11:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4548" for this suite.

• [SLOW TEST:10.271 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":73,"skipped":1328,"failed":0}
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:40.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Nov 16 02:11:40.906: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:11:42.933: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Nov 16 02:11:43.002: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:11:45.020: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 16 02:11:45.063: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:45.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:45.063: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:45.063: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:45.393: INFO: Exec stderr: ""
Nov 16 02:11:45.393: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:45.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:45.394: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:45.394: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:45.606: INFO: Exec stderr: ""
Nov 16 02:11:45.606: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:45.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:45.607: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:45.607: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:45.780: INFO: Exec stderr: ""
Nov 16 02:11:45.780: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:45.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:45.781: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:45.781: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:46.149: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 16 02:11:46.149: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:46.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:46.150: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:46.150: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:46.807: INFO: Exec stderr: ""
Nov 16 02:11:46.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:46.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:46.809: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:46.809: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:47.041: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 16 02:11:47.041: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:47.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:47.042: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:47.043: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:47.292: INFO: Exec stderr: ""
Nov 16 02:11:47.293: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:47.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:47.294: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:47.294: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:47.505: INFO: Exec stderr: ""
Nov 16 02:11:47.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:47.506: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:47.506: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:47.701: INFO: Exec stderr: ""
Nov 16 02:11:47.701: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3408 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:11:47.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:11:47.702: INFO: ExecWithOptions: Clientset creation
Nov 16 02:11:47.702: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3408/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:11:47.869: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:47.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3408" for this suite.

• [SLOW TEST:7.226 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:47.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:11:48.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:51.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5421" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":75,"skipped":1362,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:51.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-68fb8992-a6ab-4565-ab63-a3b09a02fae3
STEP: Creating a pod to test consume secrets
Nov 16 02:11:51.768: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242" in namespace "projected-3203" to be "Succeeded or Failed"
Nov 16 02:11:51.786: INFO: Pod "pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242": Phase="Pending", Reason="", readiness=false. Elapsed: 18.129744ms
Nov 16 02:11:53.811: INFO: Pod "pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043206116s
Nov 16 02:11:55.859: INFO: Pod "pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091199687s
STEP: Saw pod success
Nov 16 02:11:55.859: INFO: Pod "pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242" satisfied condition "Succeeded or Failed"
Nov 16 02:11:55.900: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:11:55.987: INFO: Waiting for pod pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242 to disappear
Nov 16 02:11:56.001: INFO: Pod pod-projected-secrets-b7f1f600-5967-4f80-9aea-2fdba547b242 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:11:56.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3203" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1362,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:11:56.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:12:03.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7029" for this suite.

• [SLOW TEST:7.221 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":77,"skipped":1372,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:12:03.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:12:03.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385" in namespace "projected-6796" to be "Succeeded or Failed"
Nov 16 02:12:03.630: INFO: Pod "downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385": Phase="Pending", Reason="", readiness=false. Elapsed: 45.58806ms
Nov 16 02:12:05.658: INFO: Pod "downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073506291s
Nov 16 02:12:07.684: INFO: Pod "downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099265326s
Nov 16 02:12:09.696: INFO: Pod "downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.111668071s
STEP: Saw pod success
Nov 16 02:12:09.696: INFO: Pod "downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385" satisfied condition "Succeeded or Failed"
Nov 16 02:12:09.707: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385 container client-container: <nil>
STEP: delete the pod
Nov 16 02:12:09.823: INFO: Waiting for pod downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385 to disappear
Nov 16 02:12:09.832: INFO: Pod downwardapi-volume-4e645632-db96-4800-885e-384a2c48e385 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:12:09.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6796" for this suite.

• [SLOW TEST:6.603 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1392,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:12:09.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-5273
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5273 to expose endpoints map[]
Nov 16 02:12:10.153: INFO: successfully validated that service endpoint-test2 in namespace services-5273 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5273
Nov 16 02:12:10.219: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:12:12.230: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5273 to expose endpoints map[pod1:[80]]
Nov 16 02:12:12.275: INFO: successfully validated that service endpoint-test2 in namespace services-5273 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Nov 16 02:12:12.275: INFO: Creating new exec pod
Nov 16 02:12:17.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5273 exec execpod8z787 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 16 02:12:17.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 16 02:12:17.786: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:12:17.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5273 exec execpod8z787 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.206.28 80'
Nov 16 02:12:18.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.206.28 80\nConnection to 172.21.206.28 80 port [tcp/http] succeeded!\n"
Nov 16 02:12:18.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5273
Nov 16 02:12:18.341: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:12:20.365: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5273 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 16 02:12:20.434: INFO: successfully validated that service endpoint-test2 in namespace services-5273 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Nov 16 02:12:21.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5273 exec execpod8z787 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 16 02:12:21.717: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 16 02:12:21.717: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:12:21.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5273 exec execpod8z787 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.206.28 80'
Nov 16 02:12:21.987: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.206.28 80\nConnection to 172.21.206.28 80 port [tcp/http] succeeded!\n"
Nov 16 02:12:21.987: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5273
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5273 to expose endpoints map[pod2:[80]]
Nov 16 02:12:22.192: INFO: successfully validated that service endpoint-test2 in namespace services-5273 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Nov 16 02:12:23.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5273 exec execpod8z787 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 16 02:12:23.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 16 02:12:23.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:12:23.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5273 exec execpod8z787 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.206.28 80'
Nov 16 02:12:23.749: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.206.28 80\nConnection to 172.21.206.28 80 port [tcp/http] succeeded!\n"
Nov 16 02:12:23.749: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5273
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5273 to expose endpoints map[]
Nov 16 02:12:24.873: INFO: successfully validated that service endpoint-test2 in namespace services-5273 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:12:24.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5273" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:15.112 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":79,"skipped":1397,"failed":0}
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:12:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-6a9ef074-bc44-4550-b21d-fb5095150c97
STEP: Creating a pod to test consume configMaps
Nov 16 02:12:25.211: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b" in namespace "configmap-4762" to be "Succeeded or Failed"
Nov 16 02:12:25.228: INFO: Pod "pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.79053ms
Nov 16 02:12:27.240: INFO: Pod "pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029335088s
Nov 16 02:12:29.260: INFO: Pod "pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049099898s
STEP: Saw pod success
Nov 16 02:12:29.260: INFO: Pod "pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b" satisfied condition "Succeeded or Failed"
Nov 16 02:12:29.270: INFO: Trying to get logs from node 10.189.71.157 pod pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 02:12:29.389: INFO: Waiting for pod pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b to disappear
Nov 16 02:12:29.400: INFO: Pod pod-configmaps-7e8750cb-c980-40a1-b2c5-711d08c4f76b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:12:29.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4762" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":80,"skipped":1397,"failed":0}

------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:12:29.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:18:01.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9012" for this suite.

• [SLOW TEST:332.365 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":81,"skipped":1397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:18:01.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:18:02.514: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 16 02:18:04.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 2, 18, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 18, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 18, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 18, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:18:07.664: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:18:07.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3352-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:18:11.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9661" for this suite.
STEP: Destroying namespace "webhook-9661-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.723 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":82,"skipped":1441,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:18:11.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Nov 16 02:18:11.829: INFO: Waiting up to 5m0s for pod "downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f" in namespace "downward-api-2294" to be "Succeeded or Failed"
Nov 16 02:18:11.845: INFO: Pod "downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.122185ms
Nov 16 02:18:13.886: INFO: Pod "downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056843985s
Nov 16 02:18:15.906: INFO: Pod "downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07707493s
STEP: Saw pod success
Nov 16 02:18:15.906: INFO: Pod "downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f" satisfied condition "Succeeded or Failed"
Nov 16 02:18:15.916: INFO: Trying to get logs from node 10.189.71.151 pod downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f container dapi-container: <nil>
STEP: delete the pod
Nov 16 02:18:16.028: INFO: Waiting for pod downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f to disappear
Nov 16 02:18:16.041: INFO: Pod downward-api-3e13d1bf-de24-4f45-be7e-256ff163638f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:18:16.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2294" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:18:16.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Nov 16 02:18:16.297: INFO: Waiting up to 5m0s for pod "downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a" in namespace "downward-api-3449" to be "Succeeded or Failed"
Nov 16 02:18:16.310: INFO: Pod "downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.268534ms
Nov 16 02:18:18.325: INFO: Pod "downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027850257s
Nov 16 02:18:20.346: INFO: Pod "downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048484489s
Nov 16 02:18:22.382: INFO: Pod "downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084846456s
STEP: Saw pod success
Nov 16 02:18:22.382: INFO: Pod "downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a" satisfied condition "Succeeded or Failed"
Nov 16 02:18:22.394: INFO: Trying to get logs from node 10.189.71.151 pod downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a container dapi-container: <nil>
STEP: delete the pod
Nov 16 02:18:22.478: INFO: Waiting for pod downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a to disappear
Nov 16 02:18:22.492: INFO: Pod downward-api-f42dbc52-d5ce-4acc-9943-e681f13f289a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:18:22.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3449" for this suite.

• [SLOW TEST:6.462 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1482,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:18:22.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:20:00.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6024" for this suite.

• [SLOW TEST:98.315 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":85,"skipped":1493,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:20:00.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:20:18.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4309" for this suite.

• [SLOW TEST:17.470 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":86,"skipped":1505,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:20:18.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:20:18.538: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552" in namespace "downward-api-2995" to be "Succeeded or Failed"
Nov 16 02:20:18.547: INFO: Pod "downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552": Phase="Pending", Reason="", readiness=false. Elapsed: 9.62158ms
Nov 16 02:20:20.561: INFO: Pod "downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023704947s
Nov 16 02:20:22.596: INFO: Pod "downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058672413s
Nov 16 02:20:24.618: INFO: Pod "downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080269679s
STEP: Saw pod success
Nov 16 02:20:24.618: INFO: Pod "downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552" satisfied condition "Succeeded or Failed"
Nov 16 02:20:24.629: INFO: Trying to get logs from node 10.189.71.157 pod downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552 container client-container: <nil>
STEP: delete the pod
Nov 16 02:20:24.803: INFO: Waiting for pod downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552 to disappear
Nov 16 02:20:24.814: INFO: Pod downwardapi-volume-c4b98a70-d802-4d67-92ef-d88b7a658552 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:20:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2995" for this suite.

• [SLOW TEST:6.504 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:20:24.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 16 02:20:29.631: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3541 pod-service-account-b9fa9d18-a95a-47e8-87fc-6bc3b890eeaf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 16 02:20:30.112: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3541 pod-service-account-b9fa9d18-a95a-47e8-87fc-6bc3b890eeaf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 16 02:20:30.378: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3541 pod-service-account-b9fa9d18-a95a-47e8-87fc-6bc3b890eeaf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:20:30.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3541" for this suite.

• [SLOW TEST:5.940 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":88,"skipped":1562,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:20:30.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-b4d58cff-27c3-4292-b4e1-76e2d004f83c
STEP: Creating a pod to test consume configMaps
Nov 16 02:20:31.063: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb" in namespace "configmap-897" to be "Succeeded or Failed"
Nov 16 02:20:31.073: INFO: Pod "pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.418594ms
Nov 16 02:20:33.088: INFO: Pod "pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024772881s
Nov 16 02:20:35.125: INFO: Pod "pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062240836s
Nov 16 02:20:37.150: INFO: Pod "pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086854077s
STEP: Saw pod success
Nov 16 02:20:37.150: INFO: Pod "pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb" satisfied condition "Succeeded or Failed"
Nov 16 02:20:37.161: INFO: Trying to get logs from node 10.189.71.150 pod pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:20:37.285: INFO: Waiting for pod pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb to disappear
Nov 16 02:20:37.294: INFO: Pod pod-configmaps-bdc67903-80c3-4f1d-bc4b-561078252ccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:20:37.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-897" for this suite.

• [SLOW TEST:6.536 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:20:37.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-058a7470-2de7-485e-a97d-7ec22b8a9dba
STEP: Creating a pod to test consume secrets
Nov 16 02:20:37.546: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4" in namespace "projected-1784" to be "Succeeded or Failed"
Nov 16 02:20:37.559: INFO: Pod "pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.972179ms
Nov 16 02:20:39.580: INFO: Pod "pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033587999s
Nov 16 02:20:41.597: INFO: Pod "pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050531597s
STEP: Saw pod success
Nov 16 02:20:41.597: INFO: Pod "pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4" satisfied condition "Succeeded or Failed"
Nov 16 02:20:41.624: INFO: Trying to get logs from node 10.189.71.150 pod pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:20:41.693: INFO: Waiting for pod pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4 to disappear
Nov 16 02:20:41.703: INFO: Pod pod-projected-secrets-5b94f0c4-e3f0-400c-ba83-b58c82482bc4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:20:41.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1784" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":90,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:20:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Nov 16 02:21:02.360: INFO: EndpointSlice for Service endpointslice-9164/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:12.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9164" for this suite.

• [SLOW TEST:30.710 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":91,"skipped":1616,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:12.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 16 02:21:12.657: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 16 02:21:17.697: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:18.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1914" for this suite.

• [SLOW TEST:6.442 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":92,"skipped":1618,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:18.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4779.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4779.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4779.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4779.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:21:31.215: INFO: DNS probes using dns-4779/dns-test-c2df691f-f14d-460a-aef7-9f604d6de67f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:31.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4779" for this suite.

• [SLOW TEST:12.588 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":93,"skipped":1621,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:31.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-722ba3a3-51f1-413d-bd28-35828e158bee
STEP: Creating a pod to test consume configMaps
Nov 16 02:21:31.791: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad" in namespace "projected-3925" to be "Succeeded or Failed"
Nov 16 02:21:31.806: INFO: Pod "pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad": Phase="Pending", Reason="", readiness=false. Elapsed: 15.034464ms
Nov 16 02:21:33.820: INFO: Pod "pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029385841s
Nov 16 02:21:35.837: INFO: Pod "pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046320271s
Nov 16 02:21:37.879: INFO: Pod "pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087793222s
STEP: Saw pod success
Nov 16 02:21:37.879: INFO: Pod "pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad" satisfied condition "Succeeded or Failed"
Nov 16 02:21:37.912: INFO: Trying to get logs from node 10.189.71.157 pod pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:21:38.075: INFO: Waiting for pod pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad to disappear
Nov 16 02:21:38.088: INFO: Pod pod-projected-configmaps-5d52757e-8cdc-41c8-930a-e198189a98ad no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:38.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3925" for this suite.

• [SLOW TEST:6.673 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:38.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
Nov 16 02:21:38.970: INFO: created pod pod-service-account-defaultsa
Nov 16 02:21:38.970: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 16 02:21:39.035: INFO: created pod pod-service-account-mountsa
Nov 16 02:21:39.035: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 16 02:21:39.110: INFO: created pod pod-service-account-nomountsa
Nov 16 02:21:39.110: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 16 02:21:39.174: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 16 02:21:39.174: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 16 02:21:39.222: INFO: created pod pod-service-account-mountsa-mountspec
Nov 16 02:21:39.222: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 16 02:21:39.346: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 16 02:21:39.346: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 16 02:21:39.385: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 16 02:21:39.385: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 16 02:21:39.434: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 16 02:21:39.434: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 16 02:21:39.497: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 16 02:21:39.497: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:39.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9690" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":95,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:39.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9065
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9065
STEP: creating replication controller externalsvc in namespace services-9065
I1116 02:21:39.845006      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9065, replica count: 2
I1116 02:21:42.901627      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 16 02:21:42.964: INFO: Creating new exec pod
Nov 16 02:21:47.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-9065 exec execpodzfwsg -- /bin/sh -x -c nslookup clusterip-service.services-9065.svc.cluster.local'
Nov 16 02:21:47.527: INFO: stderr: "+ nslookup clusterip-service.services-9065.svc.cluster.local\n"
Nov 16 02:21:47.527: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-9065.svc.cluster.local\tcanonical name = externalsvc.services-9065.svc.cluster.local.\nName:\texternalsvc.services-9065.svc.cluster.local\nAddress: 172.21.247.48\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9065, will wait for the garbage collector to delete the pods
Nov 16 02:21:47.746: INFO: Deleting ReplicationController externalsvc took: 85.984718ms
Nov 16 02:21:47.847: INFO: Terminating ReplicationController externalsvc pods took: 101.58366ms
Nov 16 02:21:50.708: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9065" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:11.319 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":96,"skipped":1727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:50.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 16 02:21:57.435: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:21:57.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1116 02:21:57.435018      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-6364" for this suite.

• [SLOW TEST:6.607 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":97,"skipped":1777,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:21:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 16 02:21:57.752: INFO: Waiting up to 5m0s for pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b" in namespace "emptydir-2519" to be "Succeeded or Failed"
Nov 16 02:21:57.765: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.319619ms
Nov 16 02:21:59.813: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061263585s
Nov 16 02:22:01.831: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078749114s
Nov 16 02:22:03.844: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092596799s
Nov 16 02:22:05.861: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109226626s
Nov 16 02:22:07.884: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.132021528s
Nov 16 02:22:09.899: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.146973678s
STEP: Saw pod success
Nov 16 02:22:09.899: INFO: Pod "pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b" satisfied condition "Succeeded or Failed"
Nov 16 02:22:09.908: INFO: Trying to get logs from node 10.189.71.150 pod pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b container test-container: <nil>
STEP: delete the pod
Nov 16 02:22:10.004: INFO: Waiting for pod pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b to disappear
Nov 16 02:22:10.040: INFO: Pod pod-d0634dcd-eaa4-4ef5-8171-7ac778285a1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:22:10.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2519" for this suite.

• [SLOW TEST:12.621 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1788,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:22:10.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-5fae2fbd-c597-4f1b-8a68-7e4223f6cafe
STEP: Creating a pod to test consume configMaps
Nov 16 02:22:10.342: INFO: Waiting up to 5m0s for pod "pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd" in namespace "configmap-3370" to be "Succeeded or Failed"
Nov 16 02:22:10.353: INFO: Pod "pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.662562ms
Nov 16 02:22:12.372: INFO: Pod "pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030358612s
Nov 16 02:22:14.387: INFO: Pod "pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045213661s
Nov 16 02:22:16.403: INFO: Pod "pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061143223s
STEP: Saw pod success
Nov 16 02:22:16.403: INFO: Pod "pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd" satisfied condition "Succeeded or Failed"
Nov 16 02:22:16.412: INFO: Trying to get logs from node 10.189.71.157 pod pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:22:16.470: INFO: Waiting for pod pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd to disappear
Nov 16 02:22:16.480: INFO: Pod pod-configmaps-dafdfd9b-0af1-4f7c-acb1-83726cb5abdd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:22:16.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3370" for this suite.

• [SLOW TEST:6.402 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1800,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:22:16.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3618, will wait for the garbage collector to delete the pods
Nov 16 02:22:20.816: INFO: Deleting Job.batch foo took: 29.808282ms
Nov 16 02:22:20.917: INFO: Terminating Job.batch foo pods took: 100.952928ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:22:53.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3618" for this suite.

• [SLOW TEST:37.046 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":100,"skipped":1816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:22:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:22:53.688: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-fefd161d-f0d1-4104-aa68-510dc075e9dc
STEP: Creating configMap with name cm-test-opt-upd-40d5cd07-891e-43df-97ac-6dbf7cee6cfc
STEP: Creating the pod
Nov 16 02:22:53.828: INFO: The status of Pod pod-projected-configmaps-83532a0c-bc36-4999-b14a-567af7c09ebf is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:22:55.841: INFO: The status of Pod pod-projected-configmaps-83532a0c-bc36-4999-b14a-567af7c09ebf is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:22:57.844: INFO: The status of Pod pod-projected-configmaps-83532a0c-bc36-4999-b14a-567af7c09ebf is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-fefd161d-f0d1-4104-aa68-510dc075e9dc
STEP: Updating configmap cm-test-opt-upd-40d5cd07-891e-43df-97ac-6dbf7cee6cfc
STEP: Creating configMap with name cm-test-opt-create-53a53a8c-978e-4681-825a-7d6e75fa7cc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:24:11.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4448" for this suite.

• [SLOW TEST:77.878 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:24:11.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-b511b98f-447e-48d9-9c56-ca6781ec2d98
STEP: Creating a pod to test consume configMaps
Nov 16 02:24:11.658: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117" in namespace "projected-7709" to be "Succeeded or Failed"
Nov 16 02:24:11.670: INFO: Pod "pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117": Phase="Pending", Reason="", readiness=false. Elapsed: 12.355689ms
Nov 16 02:24:13.689: INFO: Pod "pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031547294s
Nov 16 02:24:15.707: INFO: Pod "pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049454374s
Nov 16 02:24:17.740: INFO: Pod "pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082655971s
STEP: Saw pod success
Nov 16 02:24:17.740: INFO: Pod "pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117" satisfied condition "Succeeded or Failed"
Nov 16 02:24:17.789: INFO: Trying to get logs from node 10.189.71.150 pod pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:24:17.924: INFO: Waiting for pod pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117 to disappear
Nov 16 02:24:17.945: INFO: Pod pod-projected-configmaps-e60a04ec-24fa-4f6a-b264-bcb22816f117 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:24:17.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7709" for this suite.

• [SLOW TEST:6.550 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":1919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:24:18.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Nov 16 02:24:18.231: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 02:24:23.260: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Nov 16 02:24:23.273: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Nov 16 02:24:23.320: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Nov 16 02:24:23.326: INFO: Observed &ReplicaSet event: ADDED
Nov 16 02:24:23.327: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.327: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.329: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.329: INFO: Found replicaset test-rs in namespace replicaset-3762 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 02:24:23.329: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Nov 16 02:24:23.329: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 16 02:24:23.354: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Nov 16 02:24:23.362: INFO: Observed &ReplicaSet event: ADDED
Nov 16 02:24:23.363: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.363: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.364: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.364: INFO: Observed replicaset test-rs in namespace replicaset-3762 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 02:24:23.364: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 02:24:23.364: INFO: Found replicaset test-rs in namespace replicaset-3762 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 16 02:24:23.364: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:24:23.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3762" for this suite.

• [SLOW TEST:5.418 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":103,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:24:23.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:24:23.525: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-630d8c99-a713-4732-b7ce-09e193e13454
STEP: Creating secret with name s-test-opt-upd-1c0374c5-56d7-4c49-aea7-f71497d13cb1
STEP: Creating the pod
Nov 16 02:24:23.679: INFO: The status of Pod pod-secrets-56a2e4b0-f9a8-46e7-8f96-5686b3cc6120 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:24:25.698: INFO: The status of Pod pod-secrets-56a2e4b0-f9a8-46e7-8f96-5686b3cc6120 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:24:27.706: INFO: The status of Pod pod-secrets-56a2e4b0-f9a8-46e7-8f96-5686b3cc6120 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-630d8c99-a713-4732-b7ce-09e193e13454
STEP: Updating secret s-test-opt-upd-1c0374c5-56d7-4c49-aea7-f71497d13cb1
STEP: Creating secret with name s-test-opt-create-42c266dd-ce89-4422-b35f-30698bc013b8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:25:37.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7630" for this suite.

• [SLOW TEST:73.957 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1995,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:25:37.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-7560/secret-test-12f3c769-9feb-4b4f-bbd1-7ede632a5ae2
STEP: Creating a pod to test consume secrets
Nov 16 02:25:37.600: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0" in namespace "secrets-7560" to be "Succeeded or Failed"
Nov 16 02:25:37.612: INFO: Pod "pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.870465ms
Nov 16 02:25:39.627: INFO: Pod "pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027123243s
Nov 16 02:25:41.645: INFO: Pod "pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045452401s
Nov 16 02:25:43.659: INFO: Pod "pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059360947s
STEP: Saw pod success
Nov 16 02:25:43.659: INFO: Pod "pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0" satisfied condition "Succeeded or Failed"
Nov 16 02:25:43.671: INFO: Trying to get logs from node 10.189.71.151 pod pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0 container env-test: <nil>
STEP: delete the pod
Nov 16 02:25:43.780: INFO: Waiting for pod pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0 to disappear
Nov 16 02:25:43.793: INFO: Pod pod-configmaps-d5e4d040-12e8-46ac-91da-bea9b96663d0 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:25:43.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7560" for this suite.

• [SLOW TEST:6.458 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":2013,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:25:43.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-zqg7
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 02:25:44.081: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zqg7" in namespace "subpath-5743" to be "Succeeded or Failed"
Nov 16 02:25:44.096: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.68061ms
Nov 16 02:25:46.124: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043863086s
Nov 16 02:25:48.148: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 4.067780668s
Nov 16 02:25:50.171: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 6.090096331s
Nov 16 02:25:52.211: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 8.130605206s
Nov 16 02:25:54.233: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 10.152351362s
Nov 16 02:25:56.252: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 12.171822676s
Nov 16 02:25:58.267: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 14.186729838s
Nov 16 02:26:00.291: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 16.210187013s
Nov 16 02:26:02.316: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 18.235366055s
Nov 16 02:26:04.333: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 20.252872905s
Nov 16 02:26:06.361: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=true. Elapsed: 22.279987801s
Nov 16 02:26:08.383: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Running", Reason="", readiness=false. Elapsed: 24.30259836s
Nov 16 02:26:10.399: INFO: Pod "pod-subpath-test-configmap-zqg7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.318920729s
STEP: Saw pod success
Nov 16 02:26:10.400: INFO: Pod "pod-subpath-test-configmap-zqg7" satisfied condition "Succeeded or Failed"
Nov 16 02:26:10.412: INFO: Trying to get logs from node 10.189.71.150 pod pod-subpath-test-configmap-zqg7 container test-container-subpath-configmap-zqg7: <nil>
STEP: delete the pod
Nov 16 02:26:10.693: INFO: Waiting for pod pod-subpath-test-configmap-zqg7 to disappear
Nov 16 02:26:10.705: INFO: Pod pod-subpath-test-configmap-zqg7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zqg7
Nov 16 02:26:10.705: INFO: Deleting pod "pod-subpath-test-configmap-zqg7" in namespace "subpath-5743"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:26:10.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5743" for this suite.

• [SLOW TEST:26.997 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":106,"skipped":2017,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:26:10.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:26:11.783: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:26:14.868: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:26:15.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-373" for this suite.
STEP: Destroying namespace "webhook-373-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.007 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":107,"skipped":2019,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:26:15.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 02:26:16.143: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 16 02:26:16.177: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 02:26:16.228: INFO: waiting for watch events with expected annotations
Nov 16 02:26:16.228: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:26:16.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-719" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":108,"skipped":2033,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:26:16.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 16 02:26:16.550: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101105 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:26:16.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101105 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 16 02:26:16.599: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101114 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:26:16.599: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101114 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 16 02:26:16.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101122 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:26:16.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101122 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 16 02:26:16.721: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101125 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:26:16.721: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9573  35e51fae-2f66-49c4-ba19-4aa5e795ee22 101125 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 16 02:26:16.759: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9573  e1559d78-aa22-4d2f-b487-4509def357bd 101128 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:26:16.760: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9573  e1559d78-aa22-4d2f-b487-4509def357bd 101128 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 16 02:26:26.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9573  e1559d78-aa22-4d2f-b487-4509def357bd 101280 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:26:26.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9573  e1559d78-aa22-4d2f-b487-4509def357bd 101280 0 2022-11-16 02:26:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-11-16 02:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:26:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9573" for this suite.

• [SLOW TEST:20.428 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":109,"skipped":2042,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:26:36.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
Nov 16 02:26:37.006: INFO: Waiting up to 5m0s for pod "var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9" in namespace "var-expansion-3870" to be "Succeeded or Failed"
Nov 16 02:26:37.019: INFO: Pod "var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.428109ms
Nov 16 02:26:39.048: INFO: Pod "var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042726115s
Nov 16 02:26:41.063: INFO: Pod "var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057705979s
STEP: Saw pod success
Nov 16 02:26:41.063: INFO: Pod "var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9" satisfied condition "Succeeded or Failed"
Nov 16 02:26:41.072: INFO: Trying to get logs from node 10.189.71.151 pod var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9 container dapi-container: <nil>
STEP: delete the pod
Nov 16 02:26:41.166: INFO: Waiting for pod var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9 to disappear
Nov 16 02:26:41.178: INFO: Pod var-expansion-8c5d9ce0-b5d7-4a04-b9c4-0867129905f9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:26:41.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3870" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2057,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:26:41.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-dcdc2f5a-3678-44ca-8df4-c3f105b32789
STEP: Creating a pod to test consume configMaps
Nov 16 02:26:41.556: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e" in namespace "projected-9406" to be "Succeeded or Failed"
Nov 16 02:26:41.622: INFO: Pod "pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e": Phase="Pending", Reason="", readiness=false. Elapsed: 65.73237ms
Nov 16 02:26:43.663: INFO: Pod "pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107324057s
Nov 16 02:26:45.683: INFO: Pod "pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127019724s
Nov 16 02:26:47.700: INFO: Pod "pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.144104737s
STEP: Saw pod success
Nov 16 02:26:47.700: INFO: Pod "pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e" satisfied condition "Succeeded or Failed"
Nov 16 02:26:47.713: INFO: Trying to get logs from node 10.189.71.150 pod pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:26:47.780: INFO: Waiting for pod pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e to disappear
Nov 16 02:26:47.793: INFO: Pod pod-projected-configmaps-8a4acfb2-6c0b-43f7-a1df-6fd8a03a038e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:26:47.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9406" for this suite.

• [SLOW TEST:6.625 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2060,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:26:47.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Nov 16 02:26:47.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 create -f -'
Nov 16 02:26:48.428: INFO: stderr: ""
Nov 16 02:26:48.428: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 02:26:48.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 02:26:48.556: INFO: stderr: ""
Nov 16 02:26:48.557: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Nov 16 02:26:53.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 02:26:53.670: INFO: stderr: ""
Nov 16 02:26:53.670: INFO: stdout: "update-demo-nautilus-kn67x update-demo-nautilus-kwnkv "
Nov 16 02:26:53.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kn67x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 02:26:53.814: INFO: stderr: ""
Nov 16 02:26:53.814: INFO: stdout: "true"
Nov 16 02:26:53.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kn67x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 02:26:53.934: INFO: stderr: ""
Nov 16 02:26:53.934: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 02:26:53.934: INFO: validating pod update-demo-nautilus-kn67x
Nov 16 02:26:53.998: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 02:26:53.998: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 02:26:53.998: INFO: update-demo-nautilus-kn67x is verified up and running
Nov 16 02:26:53.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kwnkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 02:26:54.147: INFO: stderr: ""
Nov 16 02:26:54.147: INFO: stdout: ""
Nov 16 02:26:54.147: INFO: update-demo-nautilus-kwnkv is created but not running
Nov 16 02:26:59.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 02:26:59.254: INFO: stderr: ""
Nov 16 02:26:59.255: INFO: stdout: "update-demo-nautilus-kn67x update-demo-nautilus-kwnkv "
Nov 16 02:26:59.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kn67x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 02:26:59.389: INFO: stderr: ""
Nov 16 02:26:59.389: INFO: stdout: "true"
Nov 16 02:26:59.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kn67x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 02:26:59.489: INFO: stderr: ""
Nov 16 02:26:59.489: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 02:26:59.489: INFO: validating pod update-demo-nautilus-kn67x
Nov 16 02:26:59.505: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 02:26:59.505: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 02:26:59.505: INFO: update-demo-nautilus-kn67x is verified up and running
Nov 16 02:26:59.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kwnkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 02:26:59.596: INFO: stderr: ""
Nov 16 02:26:59.596: INFO: stdout: "true"
Nov 16 02:26:59.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods update-demo-nautilus-kwnkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 02:26:59.708: INFO: stderr: ""
Nov 16 02:26:59.708: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 02:26:59.708: INFO: validating pod update-demo-nautilus-kwnkv
Nov 16 02:26:59.731: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 02:26:59.731: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 02:26:59.731: INFO: update-demo-nautilus-kwnkv is verified up and running
STEP: using delete to clean up resources
Nov 16 02:26:59.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 delete --grace-period=0 --force -f -'
Nov 16 02:26:59.906: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:26:59.906: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 16 02:26:59.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get rc,svc -l name=update-demo --no-headers'
Nov 16 02:27:00.060: INFO: stderr: "No resources found in kubectl-1490 namespace.\n"
Nov 16 02:27:00.060: INFO: stdout: ""
Nov 16 02:27:00.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-1490 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 02:27:00.164: INFO: stderr: ""
Nov 16 02:27:00.164: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1490" for this suite.

• [SLOW TEST:12.381 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":112,"skipped":2068,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:00.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-e3326c7a-c25b-4e07-8e60-3330c1b34ca0
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:00.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8112" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":113,"skipped":2084,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:27:00.597: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 16 02:27:05.621: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 02:27:05.621: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 02:27:05.751: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5359  6d81d749-9e91-4ba8-a632-612f66542a34 101822 1 2022-11-16 02:27:05 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-11-16 02:27:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a566f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 16 02:27:05.767: INFO: New ReplicaSet "test-cleanup-deployment-5dbdbf94dc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5dbdbf94dc  deployment-5359  ea76b64b-bcd9-48b7-bcfe-78f3d6391f87 101825 1 2022-11-16 02:27:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6d81d749-9e91-4ba8-a632-612f66542a34 0xc002a56f27 0xc002a56f28}] []  [{kube-controller-manager Update apps/v1 2022-11-16 02:27:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d81d749-9e91-4ba8-a632-612f66542a34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5dbdbf94dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a56fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:27:05.767: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 16 02:27:05.767: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5359  d811bf92-9d5b-46ca-832d-7d3379e9cb40 101824 1 2022-11-16 02:27:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 6d81d749-9e91-4ba8-a632-612f66542a34 0xc002a56df7 0xc002a56df8}] []  [{e2e.test Update apps/v1 2022-11-16 02:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:27:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-16 02:27:05 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"6d81d749-9e91-4ba8-a632-612f66542a34\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a56eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:27:05.778: INFO: Pod "test-cleanup-controller-m9qp6" is available:
&Pod{ObjectMeta:{test-cleanup-controller-m9qp6 test-cleanup-controller- deployment-5359  9ba2e764-fe3a-4303-81aa-7120d78f8cab 101797 0 2022-11-16 02:27:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:02002cba9b3ad257b90c231b608f144b2f8b26d7c824de07b147409c67ed6043 cni.projectcalico.org/podIP:172.30.36.127/32 cni.projectcalico.org/podIPs:172.30.36.127/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.127"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.127"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller d811bf92-9d5b-46ca-832d-7d3379e9cb40 0xc002a57417 0xc002a57418}] []  [{kube-controller-manager Update v1 2022-11-16 02:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d811bf92-9d5b-46ca-832d-7d3379e9cb40\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 02:27:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-11-16 02:27:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-16 02:27:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gffpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gffpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:27:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:27:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:27:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:27:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.127,StartTime:2022-11-16 02:27:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 02:27:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9444afc07a3ff02bbab59ab3b5aba245b22a3054ffcb9e622640ba5c61959e9c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:05.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5359" for this suite.

• [SLOW TEST:5.444 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":114,"skipped":2105,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:05.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3408
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3408
I1116 02:27:06.111993      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3408, replica count: 2
Nov 16 02:27:09.163: INFO: Creating new exec pod
I1116 02:27:09.163380      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 02:27:14.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-3408 exec execpodjdsdt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 16 02:27:14.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 02:27:14.573: INFO: stdout: "externalname-service-xr9pm"
Nov 16 02:27:14.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-3408 exec execpodjdsdt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.102.27 80'
Nov 16 02:27:14.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.102.27 80\nConnection to 172.21.102.27 80 port [tcp/http] succeeded!\n"
Nov 16 02:27:14.833: INFO: stdout: "externalname-service-xr9pm"
Nov 16 02:27:14.833: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:14.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3408" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.065 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":115,"skipped":2105,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:14.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:15.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5307" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":116,"skipped":2113,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:15.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2361.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2361.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2361.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2361.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:27:19.654: INFO: DNS probes using dns-test-f945f644-8aa8-474a-82a5-975d7ca290cb succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2361.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2361.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2361.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2361.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:27:23.891: INFO: DNS probes using dns-test-7415ea7f-691c-42ff-8166-20b7e8a2649d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2361.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2361.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2361.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2361.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:27:28.246: INFO: DNS probes using dns-test-21f2633f-9de6-49b7-a4b4-760f1146c7b9 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:28.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2361" for this suite.

• [SLOW TEST:13.118 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":117,"skipped":2118,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:28.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:27:28.739: INFO: The status of Pod pod-secrets-9004e0a6-ee3e-44b9-a868-0e008640b907 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:27:30.756: INFO: The status of Pod pod-secrets-9004e0a6-ee3e-44b9-a868-0e008640b907 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:27:32.765: INFO: The status of Pod pod-secrets-9004e0a6-ee3e-44b9-a868-0e008640b907 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:32.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":118,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:32.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
Nov 16 02:27:33.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8164 api-versions'
Nov 16 02:27:33.388: INFO: stderr: ""
Nov 16 02:27:33.388: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:33.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8164" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":119,"skipped":2150,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:33.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Nov 16 02:27:37.849: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7591 PodName:pod-sharedvolume-22badfa0-f73e-471d-ac06-ab2157e9d043 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:27:37.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:27:37.850: INFO: ExecWithOptions: Clientset creation
Nov 16 02:27:37.850: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-7591/pods/pod-sharedvolume-22badfa0-f73e-471d-ac06-ab2157e9d043/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:27:38.049: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:27:38.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7591" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":120,"skipped":2153,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:27:38.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:27:38.249: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-8bc7b0b7-1659-42a2-aa33-3fe8ba70750c
STEP: Creating the pod
Nov 16 02:27:38.460: INFO: The status of Pod pod-configmaps-2c083920-096b-4bd0-b678-4ea368934430 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:27:40.481: INFO: The status of Pod pod-configmaps-2c083920-096b-4bd0-b678-4ea368934430 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:27:42.480: INFO: The status of Pod pod-configmaps-2c083920-096b-4bd0-b678-4ea368934430 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-8bc7b0b7-1659-42a2-aa33-3fe8ba70750c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:04.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6536" for this suite.

• [SLOW TEST:86.105 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:04.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-9523/configmap-test-535a2a48-2fd2-4ac3-a905-2bcbb75cbcc8
STEP: Creating a pod to test consume configMaps
Nov 16 02:29:04.426: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491" in namespace "configmap-9523" to be "Succeeded or Failed"
Nov 16 02:29:04.440: INFO: Pod "pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023796ms
Nov 16 02:29:06.458: INFO: Pod "pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031322995s
Nov 16 02:29:08.473: INFO: Pod "pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046420657s
STEP: Saw pod success
Nov 16 02:29:08.473: INFO: Pod "pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491" satisfied condition "Succeeded or Failed"
Nov 16 02:29:08.484: INFO: Trying to get logs from node 10.189.71.151 pod pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491 container env-test: <nil>
STEP: delete the pod
Nov 16 02:29:08.584: INFO: Waiting for pod pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491 to disappear
Nov 16 02:29:08.594: INFO: Pod pod-configmaps-bc95c9be-16bd-45fb-aef3-dbc65e8d8491 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:08.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9523" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:08.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Nov 16 02:29:11.071: INFO: running pods: 0 < 3
Nov 16 02:29:13.093: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2104" for this suite.

• [SLOW TEST:6.542 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":123,"skipped":2277,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:15.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4215 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4215;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4215 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4215;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4215.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4215.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4215.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4215.svc;check="$$(dig +notcp +noall +answer +search 158.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.158_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4215 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4215;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4215 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4215;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4215.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4215.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4215.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4215.svc;check="$$(dig +notcp +noall +answer +search 158.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.116.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.116.158_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:29:19.596: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.614: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-4215 from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.675: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4215 from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-4215.svc from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.717: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4215.svc from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4215.svc from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.854: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.874: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.893: INFO: Unable to read jessie_udp@dns-test-service.dns-4215 from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-4215 from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.939: INFO: Unable to read jessie_udp@dns-test-service.dns-4215.svc from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.963: INFO: Unable to read jessie_tcp@dns-test-service.dns-4215.svc from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:19.980: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4215.svc from pod dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b: the server could not find the requested resource (get pods dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b)
Nov 16 02:29:20.118: INFO: Lookups using dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4215 wheezy_tcp@dns-test-service.dns-4215 wheezy_udp@dns-test-service.dns-4215.svc wheezy_tcp@dns-test-service.dns-4215.svc wheezy_udp@_http._tcp.dns-test-service.dns-4215.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4215 jessie_tcp@dns-test-service.dns-4215 jessie_udp@dns-test-service.dns-4215.svc jessie_tcp@dns-test-service.dns-4215.svc jessie_udp@_http._tcp.dns-test-service.dns-4215.svc]

Nov 16 02:29:25.610: INFO: DNS probes using dns-4215/dns-test-61274a39-6bd3-4459-9d2f-cac877a9388b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:25.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4215" for this suite.

• [SLOW TEST:10.709 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":124,"skipped":2279,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:25.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 02:29:26.035: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 02:29:26.121: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 02:29:26.176: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.150 before test
Nov 16 02:29:26.263: INFO: calico-node-wwftz from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 02:29:26.263: INFO: calico-typha-6c9689f9f9-7lrnr from calico-system started at 2022-11-15 22:55:50 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 02:29:26.263: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-qbrrz from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 02:29:26.263: INFO: ibm-keepalived-watcher-fvdpr from kube-system started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 02:29:26.263: INFO: ibm-master-proxy-static-10.189.71.150 from kube-system started at 2022-11-15 22:55:22 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container pause ready: true, restart count 0
Nov 16 02:29:26.263: INFO: ibmcloud-block-storage-driver-8skv4 from kube-system started at 2022-11-15 22:55:31 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 02:29:26.263: INFO: tuned-pwjtn from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container tuned ready: true, restart count 0
Nov 16 02:29:26.263: INFO: csi-snapshot-controller-67b9554c7c-2j5st from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 02:29:26.263: INFO: csi-snapshot-webhook-59f45645b4-jvp9f from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container webhook ready: true, restart count 0
Nov 16 02:29:26.263: INFO: console-7b55c47f46-m72dl from openshift-console started at 2022-11-15 22:58:16 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container console ready: true, restart count 0
Nov 16 02:29:26.263: INFO: downloads-7bb5c774c4-9j76w from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container download-server ready: true, restart count 0
Nov 16 02:29:26.263: INFO: dns-default-hsr72 from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container dns ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: node-resolver-n4drz from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 02:29:26.263: INFO: image-pruner-27809280-w5zhl from openshift-image-registry started at 2022-11-16 00:00:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container image-pruner ready: false, restart count 0
Nov 16 02:29:26.263: INFO: node-ca-7fr4t from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 02:29:26.263: INFO: registry-pvc-permissions-jnr42 from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 02:29:26.263: INFO: ingress-canary-c4qbp from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 02:29:26.263: INFO: router-default-585ff5cb57-5mh8f from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container router ready: true, restart count 0
Nov 16 02:29:26.263: INFO: openshift-kube-proxy-g54k6 from openshift-kube-proxy started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: node-exporter-rc4tp from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 02:29:26.263: INFO: prometheus-adapter-6dfb554d7c-ld9xx from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 02:29:26.263: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 02:29:26.263: INFO: thanos-querier-964664765-v92dp from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 02:29:26.263: INFO: multus-additional-cni-plugins-lg6ck from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 02:29:26.263: INFO: multus-admission-controller-72x8x from openshift-multus started at 2022-11-15 22:56:30 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 02:29:26.263: INFO: multus-hsbff from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 02:29:26.263: INFO: network-metrics-daemon-z2nx6 from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 02:29:26.263: INFO: network-check-target-tbk2k from openshift-network-diagnostics started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 02:29:26.263: INFO: sonobuoy from sonobuoy started at 2022-11-16 01:36:05 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 02:29:26.263: INFO: sonobuoy-e2e-job-d1033a110ac84614 from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container e2e ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:29:26.263: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-zzb4n from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.263: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 02:29:26.263: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.151 before test
Nov 16 02:29:26.344: INFO: calico-node-k6ktc from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 02:29:26.344: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-11-15 23:00:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 02:29:26.344: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-fgdkx from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 02:29:26.344: INFO: ibm-keepalived-watcher-hxcww from kube-system started at 2022-11-15 22:54:31 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 02:29:26.344: INFO: ibm-master-proxy-static-10.189.71.151 from kube-system started at 2022-11-15 22:54:22 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container pause ready: true, restart count 0
Nov 16 02:29:26.344: INFO: ibmcloud-block-storage-driver-jmfzn from kube-system started at 2022-11-15 22:54:37 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 02:29:26.344: INFO: vpn-bcc48b544-7bsch from kube-system started at 2022-11-15 22:59:50 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container vpn ready: true, restart count 0
Nov 16 02:29:26.344: INFO: tuned-j4zhw from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container tuned ready: true, restart count 0
Nov 16 02:29:26.344: INFO: csi-snapshot-controller-67b9554c7c-h9nb4 from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 02:29:26.344: INFO: csi-snapshot-webhook-59f45645b4-6v8x8 from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container webhook ready: true, restart count 0
Nov 16 02:29:26.344: INFO: downloads-7bb5c774c4-2wfgp from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container download-server ready: true, restart count 0
Nov 16 02:29:26.344: INFO: dns-default-lv86r from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container dns ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: node-resolver-45b2v from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 02:29:26.344: INFO: image-registry-665b8bc74-tzf8g from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container registry ready: true, restart count 0
Nov 16 02:29:26.344: INFO: node-ca-6xvc9 from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 02:29:26.344: INFO: ingress-canary-wd4rg from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 02:29:26.344: INFO: router-default-585ff5cb57-hvdfj from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container router ready: true, restart count 0
Nov 16 02:29:26.344: INFO: openshift-kube-proxy-fttpn from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: migrator-84db9689c8-sd7pr from openshift-kube-storage-version-migrator started at 2022-11-15 22:56:36 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container migrator ready: true, restart count 0
Nov 16 02:29:26.344: INFO: certified-operators-6p9ch from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:29:26.344: INFO: community-operators-djvrf from openshift-marketplace started at 2022-11-16 00:48:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:29:26.344: INFO: redhat-marketplace-mnw2d from openshift-marketplace started at 2022-11-15 22:57:21 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:29:26.344: INFO: redhat-operators-t5v8m from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:29:26.344: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: grafana-d8cb7d96c-zbvqh from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (3 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container grafana ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 02:29:26.344: INFO: kube-state-metrics-6545ff684-9k7k4 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 02:29:26.344: INFO: node-exporter-2qsf6 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 02:29:26.344: INFO: openshift-state-metrics-6f59cd77b7-gd25m from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 02:29:26.344: INFO: prometheus-adapter-6dfb554d7c-8ks56 from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 02:29:26.344: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 02:29:26.344: INFO: prometheus-operator-78c776cffb-vvd4z from openshift-monitoring started at 2022-11-15 22:57:51 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.344: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 02:29:26.345: INFO: telemeter-client-867498bdb-2sc6r from openshift-monitoring started at 2022-11-15 23:01:05 +0000 UTC (3 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container reload ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 02:29:26.345: INFO: thanos-querier-964664765-tpwrf from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 02:29:26.345: INFO: multus-additional-cni-plugins-lkmkq from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 02:29:26.345: INFO: multus-admission-controller-52p7r from openshift-multus started at 2022-11-15 22:56:01 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 02:29:26.345: INFO: multus-b7qhm from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 02:29:26.345: INFO: network-metrics-daemon-lmb6c from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 02:29:26.345: INFO: network-check-target-fknwj from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 02:29:26.345: INFO: collect-profiles-27809385-dcxn4 from openshift-operator-lifecycle-manager started at 2022-11-16 01:45:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 02:29:26.345: INFO: collect-profiles-27809400-qwlr9 from openshift-operator-lifecycle-manager started at 2022-11-16 02:00:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 02:29:26.345: INFO: collect-profiles-27809415-sbm2g from openshift-operator-lifecycle-manager started at 2022-11-16 02:15:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 02:29:26.345: INFO: packageserver-74b6688945-wbm7x from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 02:29:26.345: INFO: service-ca-c6c7f64d9-kkrmn from openshift-service-ca started at 2022-11-15 22:56:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 16 02:29:26.345: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-glhsh from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:29:26.345: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 02:29:26.345: INFO: tigera-operator-56bfd47f4b-t9q5p from tigera-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.345: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 02:29:26.345: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.157 before test
Nov 16 02:29:26.431: INFO: calico-kube-controllers-7558694cbb-7jmn8 from calico-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 02:29:26.431: INFO: calico-node-m6bg7 from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 02:29:26.431: INFO: calico-typha-6c9689f9f9-2zwbk from calico-system started at 2022-11-15 22:55:41 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 02:29:26.431: INFO: managed-storage-validation-webhooks-577f9fb75-6h9ln from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 02:29:26.431: INFO: managed-storage-validation-webhooks-577f9fb75-fnp2h from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 16 02:29:26.431: INFO: managed-storage-validation-webhooks-577f9fb75-w69vd from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 02:29:26.431: INFO: ibm-file-plugin-9b7d4b5b8-4bnbc from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ibm-keepalived-watcher-rrpgn from kube-system started at 2022-11-15 22:54:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ibm-master-proxy-static-10.189.71.157 from kube-system started at 2022-11-15 22:54:20 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container pause ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ibm-storage-metrics-agent-5fdfb985fb-xk9pl from kube-system started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ibm-storage-watcher-b65cff46-l8xr7 from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ibmcloud-block-storage-driver-prkm8 from kube-system started at 2022-11-15 22:54:34 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ibmcloud-block-storage-plugin-54cb6d9d56-9228p from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 02:29:26.431: INFO: cluster-node-tuning-operator-58499f758f-tdqh6 from openshift-cluster-node-tuning-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: tuned-jqlkl from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container tuned ready: true, restart count 0
Nov 16 02:29:26.431: INFO: cluster-samples-operator-786cd9cc49-f96c8 from openshift-cluster-samples-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 02:29:26.431: INFO: cluster-storage-operator-7bffdd9f8b-h2ws8 from openshift-cluster-storage-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 16 02:29:26.431: INFO: csi-snapshot-controller-operator-687bbd75c7-28mzk from openshift-cluster-storage-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: console-operator-fc6655cfb-l5vkm from openshift-console-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 02:29:26.431: INFO: console-7b55c47f46-vcg9t from openshift-console started at 2022-11-15 22:57:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container console ready: true, restart count 0
Nov 16 02:29:26.431: INFO: dns-operator-86cb86fff-qfhb7 from openshift-dns-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: dns-default-2w2wr from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container dns ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: node-resolver-lvbbq from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 02:29:26.431: INFO: cluster-image-registry-operator-6b6ddc84cf-gltvh from openshift-image-registry started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: node-ca-pvrpl from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ingress-canary-njn8v from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 02:29:26.431: INFO: ingress-operator-659f7f8c4c-hmtfd from openshift-ingress-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: openshift-kube-proxy-lzfjb from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: kube-storage-version-migrator-operator-5bb6b9d4df-t7xsn from openshift-kube-storage-version-migrator-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 16 02:29:26.431: INFO: marketplace-operator-7b5856958-2qbfp from openshift-marketplace started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: cluster-monitoring-operator-6d9c66b9d-hrwvw from openshift-monitoring started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: node-exporter-c942p from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 02:29:26.431: INFO: multus-additional-cni-plugins-4g7rl from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 02:29:26.431: INFO: multus-admission-controller-594c2 from openshift-multus started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 02:29:26.431: INFO: multus-zq5lk from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 02:29:26.431: INFO: network-metrics-daemon-bw5dw from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:29:26.431: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 02:29:26.431: INFO: network-check-source-585b669bfd-r8rkl from openshift-network-diagnostics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 16 02:29:26.431: INFO: network-check-target-nq57m from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 02:29:26.431: INFO: network-operator-585b458dd4-dzhkh from openshift-network-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: catalog-operator-5f469dc4d6-plpn4 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: olm-operator-69ddc4ffc7-v85g5 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 02:29:26.431: INFO: package-server-manager-6b45b7b9cb-qmvk7 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 16 02:29:26.431: INFO: packageserver-74b6688945-wx6bb from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 02:29:26.431: INFO: metrics-6d5d4dd59c-txgqh from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container metrics ready: true, restart count 3
Nov 16 02:29:26.431: INFO: push-gateway-8565b96d89-q54bc from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.431: INFO: 	Container push-gateway ready: true, restart count 0
Nov 16 02:29:26.431: INFO: service-ca-operator-5f77cff646-4mbcp from openshift-service-ca-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:29:26.432: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 16 02:29:26.432: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-68x2f from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:29:26.432: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:29:26.432: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node 10.189.71.150
STEP: verifying the node has the label node 10.189.71.151
STEP: verifying the node has the label node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod calico-kube-controllers-7558694cbb-7jmn8 requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod calico-node-k6ktc requesting resource cpu=250m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod calico-node-m6bg7 requesting resource cpu=250m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod calico-node-wwftz requesting resource cpu=250m on Node 10.189.71.150
Nov 16 02:29:26.751: INFO: Pod calico-typha-6c9689f9f9-2zwbk requesting resource cpu=250m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod calico-typha-6c9689f9f9-7lrnr requesting resource cpu=250m on Node 10.189.71.150
Nov 16 02:29:26.751: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod managed-storage-validation-webhooks-577f9fb75-6h9ln requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod managed-storage-validation-webhooks-577f9fb75-fnp2h requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod managed-storage-validation-webhooks-577f9fb75-w69vd requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-fgdkx requesting resource cpu=5m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-qbrrz requesting resource cpu=5m on Node 10.189.71.150
Nov 16 02:29:26.751: INFO: Pod ibm-file-plugin-9b7d4b5b8-4bnbc requesting resource cpu=50m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibm-keepalived-watcher-fvdpr requesting resource cpu=5m on Node 10.189.71.150
Nov 16 02:29:26.751: INFO: Pod ibm-keepalived-watcher-hxcww requesting resource cpu=5m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod ibm-keepalived-watcher-rrpgn requesting resource cpu=5m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibm-master-proxy-static-10.189.71.150 requesting resource cpu=26m on Node 10.189.71.150
Nov 16 02:29:26.751: INFO: Pod ibm-master-proxy-static-10.189.71.151 requesting resource cpu=26m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod ibm-master-proxy-static-10.189.71.157 requesting resource cpu=26m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibm-storage-metrics-agent-5fdfb985fb-xk9pl requesting resource cpu=60m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibm-storage-watcher-b65cff46-l8xr7 requesting resource cpu=50m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibmcloud-block-storage-driver-8skv4 requesting resource cpu=50m on Node 10.189.71.150
Nov 16 02:29:26.751: INFO: Pod ibmcloud-block-storage-driver-jmfzn requesting resource cpu=50m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod ibmcloud-block-storage-driver-prkm8 requesting resource cpu=50m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod ibmcloud-block-storage-plugin-54cb6d9d56-9228p requesting resource cpu=50m on Node 10.189.71.157
Nov 16 02:29:26.751: INFO: Pod vpn-bcc48b544-7bsch requesting resource cpu=5m on Node 10.189.71.151
Nov 16 02:29:26.751: INFO: Pod cluster-node-tuning-operator-58499f758f-tdqh6 requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod tuned-j4zhw requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod tuned-jqlkl requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod tuned-pwjtn requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod cluster-samples-operator-786cd9cc49-f96c8 requesting resource cpu=20m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod cluster-storage-operator-7bffdd9f8b-h2ws8 requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod csi-snapshot-controller-67b9554c7c-2j5st requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod csi-snapshot-controller-67b9554c7c-h9nb4 requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod csi-snapshot-controller-operator-687bbd75c7-28mzk requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod csi-snapshot-webhook-59f45645b4-6v8x8 requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod csi-snapshot-webhook-59f45645b4-jvp9f requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod console-operator-fc6655cfb-l5vkm requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod console-7b55c47f46-m72dl requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod console-7b55c47f46-vcg9t requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod downloads-7bb5c774c4-2wfgp requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod downloads-7bb5c774c4-9j76w requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod dns-operator-86cb86fff-qfhb7 requesting resource cpu=20m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod dns-default-2w2wr requesting resource cpu=60m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod dns-default-hsr72 requesting resource cpu=60m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod dns-default-lv86r requesting resource cpu=60m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod node-resolver-45b2v requesting resource cpu=5m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod node-resolver-lvbbq requesting resource cpu=5m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod node-resolver-n4drz requesting resource cpu=5m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod cluster-image-registry-operator-6b6ddc84cf-gltvh requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod image-registry-665b8bc74-tzf8g requesting resource cpu=100m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod node-ca-6xvc9 requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod node-ca-7fr4t requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod node-ca-pvrpl requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod ingress-canary-c4qbp requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod ingress-canary-njn8v requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod ingress-canary-wd4rg requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod ingress-operator-659f7f8c4c-hmtfd requesting resource cpu=20m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod router-default-585ff5cb57-5mh8f requesting resource cpu=100m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod router-default-585ff5cb57-hvdfj requesting resource cpu=100m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod openshift-kube-proxy-fttpn requesting resource cpu=110m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod openshift-kube-proxy-g54k6 requesting resource cpu=110m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod openshift-kube-proxy-lzfjb requesting resource cpu=110m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod kube-storage-version-migrator-operator-5bb6b9d4df-t7xsn requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod migrator-84db9689c8-sd7pr requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod certified-operators-6p9ch requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod community-operators-djvrf requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod marketplace-operator-7b5856958-2qbfp requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod redhat-marketplace-mnw2d requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod redhat-operators-t5v8m requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod cluster-monitoring-operator-6d9c66b9d-hrwvw requesting resource cpu=11m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod grafana-d8cb7d96c-zbvqh requesting resource cpu=6m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod kube-state-metrics-6545ff684-9k7k4 requesting resource cpu=4m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod node-exporter-2qsf6 requesting resource cpu=9m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod node-exporter-c942p requesting resource cpu=9m on Node 10.189.71.157
Nov 16 02:29:26.752: INFO: Pod node-exporter-rc4tp requesting resource cpu=9m on Node 10.189.71.150
Nov 16 02:29:26.752: INFO: Pod openshift-state-metrics-6f59cd77b7-gd25m requesting resource cpu=3m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod prometheus-adapter-6dfb554d7c-8ks56 requesting resource cpu=1m on Node 10.189.71.151
Nov 16 02:29:26.752: INFO: Pod prometheus-adapter-6dfb554d7c-ld9xx requesting resource cpu=1m on Node 10.189.71.150
Nov 16 02:29:26.753: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.189.71.150
Nov 16 02:29:26.753: INFO: Pod prometheus-operator-78c776cffb-vvd4z requesting resource cpu=6m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod telemeter-client-867498bdb-2sc6r requesting resource cpu=3m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod thanos-querier-964664765-tpwrf requesting resource cpu=15m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod thanos-querier-964664765-v92dp requesting resource cpu=15m on Node 10.189.71.150
Nov 16 02:29:26.753: INFO: Pod multus-additional-cni-plugins-4g7rl requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.753: INFO: Pod multus-additional-cni-plugins-lg6ck requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.753: INFO: Pod multus-additional-cni-plugins-lkmkq requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod multus-admission-controller-52p7r requesting resource cpu=20m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod multus-admission-controller-594c2 requesting resource cpu=20m on Node 10.189.71.157
Nov 16 02:29:26.753: INFO: Pod multus-admission-controller-72x8x requesting resource cpu=20m on Node 10.189.71.150
Nov 16 02:29:26.753: INFO: Pod multus-b7qhm requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.753: INFO: Pod multus-hsbff requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.753: INFO: Pod multus-zq5lk requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod network-metrics-daemon-bw5dw requesting resource cpu=20m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod network-metrics-daemon-lmb6c requesting resource cpu=20m on Node 10.189.71.151
Nov 16 02:29:26.754: INFO: Pod network-metrics-daemon-z2nx6 requesting resource cpu=20m on Node 10.189.71.150
Nov 16 02:29:26.754: INFO: Pod network-check-source-585b669bfd-r8rkl requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod network-check-target-fknwj requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.754: INFO: Pod network-check-target-nq57m requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod network-check-target-tbk2k requesting resource cpu=10m on Node 10.189.71.150
Nov 16 02:29:26.754: INFO: Pod network-operator-585b458dd4-dzhkh requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod catalog-operator-5f469dc4d6-plpn4 requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod olm-operator-69ddc4ffc7-v85g5 requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod package-server-manager-6b45b7b9cb-qmvk7 requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod packageserver-74b6688945-wbm7x requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.754: INFO: Pod packageserver-74b6688945-wx6bb requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod metrics-6d5d4dd59c-txgqh requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod push-gateway-8565b96d89-q54bc requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod service-ca-operator-5f77cff646-4mbcp requesting resource cpu=10m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod service-ca-c6c7f64d9-kkrmn requesting resource cpu=10m on Node 10.189.71.151
Nov 16 02:29:26.754: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.189.71.150
Nov 16 02:29:26.754: INFO: Pod sonobuoy-e2e-job-d1033a110ac84614 requesting resource cpu=0m on Node 10.189.71.150
Nov 16 02:29:26.754: INFO: Pod sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-68x2f requesting resource cpu=0m on Node 10.189.71.157
Nov 16 02:29:26.754: INFO: Pod sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-glhsh requesting resource cpu=0m on Node 10.189.71.151
Nov 16 02:29:26.754: INFO: Pod sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-zzb4n requesting resource cpu=0m on Node 10.189.71.150
Nov 16 02:29:26.754: INFO: Pod tigera-operator-56bfd47f4b-t9q5p requesting resource cpu=100m on Node 10.189.71.151
STEP: Starting Pods to consume most of the cluster CPU.
Nov 16 02:29:26.754: INFO: Creating a pod which consumes cpu=1960m on Node 10.189.71.150
Nov 16 02:29:26.826: INFO: Creating a pod which consumes cpu=1934m on Node 10.189.71.151
Nov 16 02:29:26.881: INFO: Creating a pod which consumes cpu=1787m on Node 10.189.71.157
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee.1727efb978ff94eb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9876/filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee to 10.189.71.150]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee.1727efb9b949b0b3], Reason = [AddedInterface], Message = [Add eth0 [172.30.36.81/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee.1727efb9ca2777bb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee.1727efb9d659b317], Reason = [Created], Message = [Created container filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee.1727efb9d85a7a9f], Reason = [Started], Message = [Started container filler-pod-5bfe0590-d909-421e-ac40-351a89b595ee]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8face916-1827-42c5-8f33-054964847a58.1727efb97f11a813], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9876/filler-pod-8face916-1827-42c5-8f33-054964847a58 to 10.189.71.157]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8face916-1827-42c5-8f33-054964847a58.1727efb9d098024f], Reason = [AddedInterface], Message = [Add eth0 [172.30.169.115/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8face916-1827-42c5-8f33-054964847a58.1727efb9e0be8b2d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8face916-1827-42c5-8f33-054964847a58.1727efb9ebab5cef], Reason = [Created], Message = [Created container filler-pod-8face916-1827-42c5-8f33-054964847a58]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8face916-1827-42c5-8f33-054964847a58.1727efb9ed14a836], Reason = [Started], Message = [Started container filler-pod-8face916-1827-42c5-8f33-054964847a58]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834.1727efb97c93bb43], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9876/filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834 to 10.189.71.151]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834.1727efb9b9327179], Reason = [AddedInterface], Message = [Add eth0 [172.30.102.239/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834.1727efb9c9a936a4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834.1727efb9d678ab6a], Reason = [Created], Message = [Created container filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834.1727efb9d87a68d1], Reason = [Started], Message = [Started container filler-pod-f083869c-71ad-485d-9d5d-f98e906c0834]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1727efba73d83016], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.189.71.150
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.189.71.151
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.189.71.157
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:32.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9876" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.385 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":125,"skipped":2282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:32.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:29:32.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449" in namespace "downward-api-245" to be "Succeeded or Failed"
Nov 16 02:29:32.472: INFO: Pod "downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449": Phase="Pending", Reason="", readiness=false. Elapsed: 22.897359ms
Nov 16 02:29:34.488: INFO: Pod "downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03930692s
Nov 16 02:29:36.514: INFO: Pod "downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064717322s
STEP: Saw pod success
Nov 16 02:29:36.514: INFO: Pod "downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449" satisfied condition "Succeeded or Failed"
Nov 16 02:29:36.524: INFO: Trying to get logs from node 10.189.71.157 pod downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449 container client-container: <nil>
STEP: delete the pod
Nov 16 02:29:36.612: INFO: Waiting for pod downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449 to disappear
Nov 16 02:29:36.626: INFO: Pod downwardapi-volume-2878f590-d3c2-48d2-9c74-74f345d77449 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:36.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-245" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2317,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:36.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:29:36.971: INFO: The status of Pod busybox-readonly-fsbcf4e773-b703-44e3-a301-c83204763111 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:29:38.999: INFO: The status of Pod busybox-readonly-fsbcf4e773-b703-44e3-a301-c83204763111 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:39.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2135" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2334,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-5784c62c-4c72-437c-bd65-14e50c99ce1c
STEP: Creating a pod to test consume configMaps
Nov 16 02:29:39.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e" in namespace "configmap-7592" to be "Succeeded or Failed"
Nov 16 02:29:39.299: INFO: Pod "pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.64079ms
Nov 16 02:29:41.315: INFO: Pod "pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026030526s
Nov 16 02:29:43.332: INFO: Pod "pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042239157s
Nov 16 02:29:45.345: INFO: Pod "pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055332776s
STEP: Saw pod success
Nov 16 02:29:45.345: INFO: Pod "pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e" satisfied condition "Succeeded or Failed"
Nov 16 02:29:45.356: INFO: Trying to get logs from node 10.189.71.157 pod pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:29:45.496: INFO: Waiting for pod pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e to disappear
Nov 16 02:29:45.507: INFO: Pod pod-configmaps-71c1734c-ea65-40ce-8362-ba918dc6a92e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:45.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7592" for this suite.

• [SLOW TEST:6.468 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:45.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 02:29:45.772: INFO: The status of Pod pod-update-3b3eec56-b6b7-4888-bcfa-a66ce775d4fd is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:29:47.795: INFO: The status of Pod pod-update-3b3eec56-b6b7-4888-bcfa-a66ce775d4fd is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:29:49.787: INFO: The status of Pod pod-update-3b3eec56-b6b7-4888-bcfa-a66ce775d4fd is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 16 02:29:50.383: INFO: Successfully updated pod "pod-update-3b3eec56-b6b7-4888-bcfa-a66ce775d4fd"
STEP: verifying the updated pod is in kubernetes
Nov 16 02:29:50.406: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:50.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2534" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2383,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:50.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 16 02:29:50.677: INFO: Waiting up to 5m0s for pod "pod-bc138e11-946b-455f-8493-bbb81890df1f" in namespace "emptydir-5932" to be "Succeeded or Failed"
Nov 16 02:29:50.690: INFO: Pod "pod-bc138e11-946b-455f-8493-bbb81890df1f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.383439ms
Nov 16 02:29:52.713: INFO: Pod "pod-bc138e11-946b-455f-8493-bbb81890df1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036562524s
Nov 16 02:29:54.731: INFO: Pod "pod-bc138e11-946b-455f-8493-bbb81890df1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054530509s
Nov 16 02:29:56.748: INFO: Pod "pod-bc138e11-946b-455f-8493-bbb81890df1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07141653s
STEP: Saw pod success
Nov 16 02:29:56.748: INFO: Pod "pod-bc138e11-946b-455f-8493-bbb81890df1f" satisfied condition "Succeeded or Failed"
Nov 16 02:29:56.758: INFO: Trying to get logs from node 10.189.71.150 pod pod-bc138e11-946b-455f-8493-bbb81890df1f container test-container: <nil>
STEP: delete the pod
Nov 16 02:29:56.872: INFO: Waiting for pod pod-bc138e11-946b-455f-8493-bbb81890df1f to disappear
Nov 16 02:29:56.885: INFO: Pod pod-bc138e11-946b-455f-8493-bbb81890df1f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:56.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5932" for this suite.

• [SLOW TEST:6.490 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2403,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:56.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
Nov 16 02:29:57.062: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-5963 proxy --unix-socket=/tmp/kubectl-proxy-unix726993568/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:29:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5963" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":131,"skipped":2419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:29:57.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1116 02:30:09.256015      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 02:30:09.256: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 16 02:30:09.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-2h5w7" in namespace "gc-7804"
Nov 16 02:30:09.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nnmj" in namespace "gc-7804"
Nov 16 02:30:09.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c9hq" in namespace "gc-7804"
Nov 16 02:30:09.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cvc7" in namespace "gc-7804"
Nov 16 02:30:09.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fxmr" in namespace "gc-7804"
Nov 16 02:30:09.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q8px" in namespace "gc-7804"
Nov 16 02:30:09.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-57nc8" in namespace "gc-7804"
Nov 16 02:30:09.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-57zww" in namespace "gc-7804"
Nov 16 02:30:09.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hkdb" in namespace "gc-7804"
Nov 16 02:30:09.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l4rt" in namespace "gc-7804"
Nov 16 02:30:09.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l5lt" in namespace "gc-7804"
Nov 16 02:30:09.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jhsg" in namespace "gc-7804"
Nov 16 02:30:09.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tn44" in namespace "gc-7804"
Nov 16 02:30:09.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-76mn5" in namespace "gc-7804"
Nov 16 02:30:09.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dttr" in namespace "gc-7804"
Nov 16 02:30:10.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hxtc" in namespace "gc-7804"
Nov 16 02:30:10.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-7trfz" in namespace "gc-7804"
Nov 16 02:30:10.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l4cg" in namespace "gc-7804"
Nov 16 02:30:10.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-94pq8" in namespace "gc-7804"
Nov 16 02:30:10.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-9d7tn" in namespace "gc-7804"
Nov 16 02:30:10.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-b74n9" in namespace "gc-7804"
Nov 16 02:30:10.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh5j5" in namespace "gc-7804"
Nov 16 02:30:10.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhtxq" in namespace "gc-7804"
Nov 16 02:30:10.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsfcz" in namespace "gc-7804"
Nov 16 02:30:10.447: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbfds" in namespace "gc-7804"
Nov 16 02:30:10.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr6ts" in namespace "gc-7804"
Nov 16 02:30:10.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7kkc" in namespace "gc-7804"
Nov 16 02:30:10.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlfcp" in namespace "gc-7804"
Nov 16 02:30:10.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-dntmv" in namespace "gc-7804"
Nov 16 02:30:10.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsqpj" in namespace "gc-7804"
Nov 16 02:30:10.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtxjf" in namespace "gc-7804"
Nov 16 02:30:10.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff7gr" in namespace "gc-7804"
Nov 16 02:30:10.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-g277x" in namespace "gc-7804"
Nov 16 02:30:10.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4h9r" in namespace "gc-7804"
Nov 16 02:30:10.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-g74jh" in namespace "gc-7804"
Nov 16 02:30:10.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-gclcf" in namespace "gc-7804"
Nov 16 02:30:10.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-glhsd" in namespace "gc-7804"
Nov 16 02:30:10.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtllf" in namespace "gc-7804"
Nov 16 02:30:11.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7pdq" in namespace "gc-7804"
Nov 16 02:30:11.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfbwj" in namespace "gc-7804"
Nov 16 02:30:11.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlrnm" in namespace "gc-7804"
Nov 16 02:30:11.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpz47" in namespace "gc-7804"
Nov 16 02:30:11.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-htmx9" in namespace "gc-7804"
Nov 16 02:30:11.328: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvgll" in namespace "gc-7804"
Nov 16 02:30:11.370: INFO: Deleting pod "simpletest-rc-to-be-deleted-hw9zc" in namespace "gc-7804"
Nov 16 02:30:11.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxtsv" in namespace "gc-7804"
Nov 16 02:30:11.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzhb2" in namespace "gc-7804"
Nov 16 02:30:11.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6xvh" in namespace "gc-7804"
Nov 16 02:30:11.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqszk" in namespace "gc-7804"
Nov 16 02:30:11.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7phd" in namespace "gc-7804"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:30:11.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7804" for this suite.

• [SLOW TEST:14.757 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":132,"skipped":2507,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:30:11.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-z25r
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 02:30:12.328: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-z25r" in namespace "subpath-9688" to be "Succeeded or Failed"
Nov 16 02:30:12.361: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Pending", Reason="", readiness=false. Elapsed: 32.968411ms
Nov 16 02:30:14.386: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057518085s
Nov 16 02:30:16.406: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078223292s
Nov 16 02:30:18.433: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 6.105228646s
Nov 16 02:30:20.452: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 8.124030355s
Nov 16 02:30:22.466: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 10.137892298s
Nov 16 02:30:24.479: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 12.151378256s
Nov 16 02:30:26.506: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 14.177985434s
Nov 16 02:30:28.532: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 16.20401546s
Nov 16 02:30:30.543: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 18.215089232s
Nov 16 02:30:32.557: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 20.229258131s
Nov 16 02:30:34.577: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=true. Elapsed: 22.249365193s
Nov 16 02:30:36.594: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Running", Reason="", readiness=false. Elapsed: 24.265444332s
Nov 16 02:30:38.612: INFO: Pod "pod-subpath-test-secret-z25r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.28401605s
STEP: Saw pod success
Nov 16 02:30:38.612: INFO: Pod "pod-subpath-test-secret-z25r" satisfied condition "Succeeded or Failed"
Nov 16 02:30:38.625: INFO: Trying to get logs from node 10.189.71.150 pod pod-subpath-test-secret-z25r container test-container-subpath-secret-z25r: <nil>
STEP: delete the pod
Nov 16 02:30:38.733: INFO: Waiting for pod pod-subpath-test-secret-z25r to disappear
Nov 16 02:30:38.748: INFO: Pod pod-subpath-test-secret-z25r no longer exists
STEP: Deleting pod pod-subpath-test-secret-z25r
Nov 16 02:30:38.748: INFO: Deleting pod "pod-subpath-test-secret-z25r" in namespace "subpath-9688"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:30:38.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9688" for this suite.

• [SLOW TEST:26.869 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":133,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:30:38.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
Nov 16 02:30:38.927: INFO: created test-event-1
Nov 16 02:30:38.945: INFO: created test-event-2
Nov 16 02:30:38.980: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 16 02:30:38.991: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 16 02:30:39.089: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:30:39.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8472" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":134,"skipped":2539,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:30:39.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:30:39.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8" in namespace "downward-api-8073" to be "Succeeded or Failed"
Nov 16 02:30:39.372: INFO: Pod "downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.302579ms
Nov 16 02:30:41.394: INFO: Pod "downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034851507s
Nov 16 02:30:43.420: INFO: Pod "downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060311046s
STEP: Saw pod success
Nov 16 02:30:43.420: INFO: Pod "downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8" satisfied condition "Succeeded or Failed"
Nov 16 02:30:43.431: INFO: Trying to get logs from node 10.189.71.157 pod downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8 container client-container: <nil>
STEP: delete the pod
Nov 16 02:30:43.519: INFO: Waiting for pod downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8 to disappear
Nov 16 02:30:43.529: INFO: Pod downwardapi-volume-7f2f8565-3918-437b-bce7-71fb3e7d57e8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:30:43.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8073" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:30:43.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b
Nov 16 02:30:43.717: INFO: Pod name my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b: Found 0 pods out of 1
Nov 16 02:30:48.739: INFO: Pod name my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b: Found 1 pods out of 1
Nov 16 02:30:48.739: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b" are running
Nov 16 02:30:48.748: INFO: Pod "my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b-sfjrs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 02:30:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 02:30:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 02:30:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 02:30:43 +0000 UTC Reason: Message:}])
Nov 16 02:30:48.748: INFO: Trying to dial the pod
Nov 16 02:30:53.826: INFO: Controller my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b: Got expected result from replica 1 [my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b-sfjrs]: "my-hostname-basic-f3bd3bcf-3865-4e78-8a3c-6adcd3d8e53b-sfjrs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:30:53.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3031" for this suite.

• [SLOW TEST:10.302 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":136,"skipped":2592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:30:53.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-5ce6683f-a943-4474-9f54-6afde7029250
STEP: Creating a pod to test consume secrets
Nov 16 02:30:54.046: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba" in namespace "projected-1819" to be "Succeeded or Failed"
Nov 16 02:30:54.058: INFO: Pod "pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040249ms
Nov 16 02:30:56.090: INFO: Pod "pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044802602s
Nov 16 02:30:58.112: INFO: Pod "pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066152528s
STEP: Saw pod success
Nov 16 02:30:58.112: INFO: Pod "pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba" satisfied condition "Succeeded or Failed"
Nov 16 02:30:58.123: INFO: Trying to get logs from node 10.189.71.157 pod pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:30:58.186: INFO: Waiting for pod pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba to disappear
Nov 16 02:30:58.198: INFO: Pod pod-projected-secrets-1798e1f8-ecd8-450d-8e39-51d6800b68ba no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:30:58.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1819" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2618,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:30:58.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:31:03.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4880" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":138,"skipped":2635,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:31:03.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 02:31:03.410: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 02:32:03.691: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Nov 16 02:32:03.806: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 16 02:32:03.860: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 16 02:32:03.972: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 16 02:32:04.008: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 16 02:32:04.082: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 16 02:32:04.162: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:12.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5441" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:69.584 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":139,"skipped":2656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:12.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Nov 16 02:32:12.993: INFO: Waiting up to 5m0s for pod "downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f" in namespace "downward-api-8443" to be "Succeeded or Failed"
Nov 16 02:32:13.006: INFO: Pod "downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.530363ms
Nov 16 02:32:15.020: INFO: Pod "downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02747048s
Nov 16 02:32:17.044: INFO: Pod "downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051799059s
STEP: Saw pod success
Nov 16 02:32:17.044: INFO: Pod "downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f" satisfied condition "Succeeded or Failed"
Nov 16 02:32:17.057: INFO: Trying to get logs from node 10.189.71.151 pod downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f container dapi-container: <nil>
STEP: delete the pod
Nov 16 02:32:17.184: INFO: Waiting for pod downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f to disappear
Nov 16 02:32:17.195: INFO: Pod downward-api-b6a2ddf9-428c-4031-a6c0-fa718a9f7d2f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:17.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8443" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 02:32:17.453: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 16 02:32:17.487: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 02:32:17.541: INFO: waiting for watch events with expected annotations
Nov 16 02:32:17.542: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:17.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2540" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":141,"skipped":2758,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:17.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 16 02:32:17.964: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9339  61f5c94c-5f39-4552-a59e-77fd9aa208bf 108058 0 2022-11-16 02:32:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-11-16 02:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:32:17.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9339  61f5c94c-5f39-4552-a59e-77fd9aa208bf 108065 0 2022-11-16 02:32:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-11-16 02:32:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:32:17.965: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9339  61f5c94c-5f39-4552-a59e-77fd9aa208bf 108075 0 2022-11-16 02:32:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-11-16 02:32:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 16 02:32:28.125: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9339  61f5c94c-5f39-4552-a59e-77fd9aa208bf 108340 0 2022-11-16 02:32:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-11-16 02:32:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:32:28.126: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9339  61f5c94c-5f39-4552-a59e-77fd9aa208bf 108341 0 2022-11-16 02:32:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-11-16 02:32:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:32:28.126: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9339  61f5c94c-5f39-4552-a59e-77fd9aa208bf 108342 0 2022-11-16 02:32:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-11-16 02:32:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:28.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9339" for this suite.

• [SLOW TEST:10.490 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":142,"skipped":2765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:28.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.85.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.85.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.85.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.85.82_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5932.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5932.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5932.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5932.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.85.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.85.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.85.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.85.82_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:32:32.580: INFO: Unable to read wheezy_udp@dns-test-service.dns-5932.svc.cluster.local from pod dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019: the server could not find the requested resource (get pods dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019)
Nov 16 02:32:32.621: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local from pod dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019: the server could not find the requested resource (get pods dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019)
Nov 16 02:32:32.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local from pod dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019: the server could not find the requested resource (get pods dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019)
Nov 16 02:32:32.770: INFO: Unable to read jessie_udp@dns-test-service.dns-5932.svc.cluster.local from pod dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019: the server could not find the requested resource (get pods dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019)
Nov 16 02:32:32.845: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local from pod dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019: the server could not find the requested resource (get pods dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019)
Nov 16 02:32:32.895: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local from pod dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019: the server could not find the requested resource (get pods dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019)
Nov 16 02:32:32.981: INFO: Lookups using dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019 failed for: [wheezy_udp@dns-test-service.dns-5932.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local jessie_udp@dns-test-service.dns-5932.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5932.svc.cluster.local]

Nov 16 02:32:38.345: INFO: DNS probes using dns-5932/dns-test-0142bfb2-e147-47f7-8d62-ea7c70799019 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:38.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5932" for this suite.

• [SLOW TEST:10.367 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":143,"skipped":2795,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
Nov 16 02:32:38.660: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-7099 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:38.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7099" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":144,"skipped":2804,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:38.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
Nov 16 02:32:39.011: INFO: Major version: 1
STEP: Confirm minor version
Nov 16 02:32:39.011: INFO: cleanMinorVersion: 23
Nov 16 02:32:39.011: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:39.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4274" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":145,"skipped":2814,"failed":0}
SSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:39.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-1719
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1719
STEP: Deleting pre-stop pod
Nov 16 02:32:50.485: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:32:50.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1719" for this suite.

• [SLOW TEST:11.470 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":146,"skipped":2817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:32:50.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:32:51.873: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 16 02:32:53.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 2, 32, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 32, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 32, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 32, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:32:56.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:09.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3321" for this suite.
STEP: Destroying namespace "webhook-3321-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":147,"skipped":2850,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:09.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
Nov 16 02:33:09.889: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 16 02:33:09.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 create -f -'
Nov 16 02:33:10.702: INFO: stderr: ""
Nov 16 02:33:10.702: INFO: stdout: "service/agnhost-replica created\n"
Nov 16 02:33:10.702: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 16 02:33:10.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 create -f -'
Nov 16 02:33:11.229: INFO: stderr: ""
Nov 16 02:33:11.229: INFO: stdout: "service/agnhost-primary created\n"
Nov 16 02:33:11.229: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 16 02:33:11.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 create -f -'
Nov 16 02:33:11.619: INFO: stderr: ""
Nov 16 02:33:11.619: INFO: stdout: "service/frontend created\n"
Nov 16 02:33:11.619: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 16 02:33:11.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 create -f -'
Nov 16 02:33:12.163: INFO: stderr: ""
Nov 16 02:33:12.163: INFO: stdout: "deployment.apps/frontend created\n"
Nov 16 02:33:12.163: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 16 02:33:12.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 create -f -'
Nov 16 02:33:12.548: INFO: stderr: ""
Nov 16 02:33:12.548: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 16 02:33:12.548: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 16 02:33:12.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 create -f -'
Nov 16 02:33:13.073: INFO: stderr: ""
Nov 16 02:33:13.073: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 16 02:33:13.073: INFO: Waiting for all frontend pods to be Running.
Nov 16 02:33:18.124: INFO: Waiting for frontend to serve content.
Nov 16 02:33:18.158: INFO: Trying to add a new entry to the guestbook.
Nov 16 02:33:18.187: INFO: Verifying that added entry can be retrieved.
Nov 16 02:33:18.220: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Nov 16 02:33:23.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 delete --grace-period=0 --force -f -'
Nov 16 02:33:23.500: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:33:23.500: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 02:33:23.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 delete --grace-period=0 --force -f -'
Nov 16 02:33:23.699: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:33:23.699: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 02:33:23.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 delete --grace-period=0 --force -f -'
Nov 16 02:33:23.936: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:33:23.936: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 02:33:23.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 delete --grace-period=0 --force -f -'
Nov 16 02:33:24.061: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:33:24.061: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 02:33:24.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 delete --grace-period=0 --force -f -'
Nov 16 02:33:24.192: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:33:24.192: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 02:33:24.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3747 delete --grace-period=0 --force -f -'
Nov 16 02:33:24.323: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 02:33:24.323: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:24.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3747" for this suite.

• [SLOW TEST:14.600 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":148,"skipped":2853,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:24.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:24.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1915" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":149,"skipped":2856,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:25.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:33:25.274: INFO: The status of Pod busybox-scheduling-0485d9f2-e5f4-41d0-9d8a-20d2a738fce5 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:33:27.304: INFO: The status of Pod busybox-scheduling-0485d9f2-e5f4-41d0-9d8a-20d2a738fce5 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:27.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-708" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2872,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:27.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:33:27.514: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c95458b4-92b6-4cfe-a713-bc866dae0371
STEP: Creating the pod
Nov 16 02:33:27.658: INFO: The status of Pod pod-projected-configmaps-292008da-b261-4658-8331-de331170ce12 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:33:29.675: INFO: The status of Pod pod-projected-configmaps-292008da-b261-4658-8331-de331170ce12 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-c95458b4-92b6-4cfe-a713-bc866dae0371
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-116" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2892,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:31.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 16 02:33:32.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8787  c44201cb-0f74-4281-b519-167d0eeec2ad 109700 0 2022-11-16 02:33:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-11-16 02:33:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:33:32.152: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8787  c44201cb-0f74-4281-b519-167d0eeec2ad 109704 0 2022-11-16 02:33:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-11-16 02:33:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:32.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8787" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":152,"skipped":2905,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:32.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:33:32.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced" in namespace "downward-api-1944" to be "Succeeded or Failed"
Nov 16 02:33:32.413: INFO: Pod "downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced": Phase="Pending", Reason="", readiness=false. Elapsed: 13.392728ms
Nov 16 02:33:34.435: INFO: Pod "downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035323454s
Nov 16 02:33:36.458: INFO: Pod "downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058068163s
STEP: Saw pod success
Nov 16 02:33:36.458: INFO: Pod "downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced" satisfied condition "Succeeded or Failed"
Nov 16 02:33:36.469: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced container client-container: <nil>
STEP: delete the pod
Nov 16 02:33:36.616: INFO: Waiting for pod downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced to disappear
Nov 16 02:33:36.642: INFO: Pod downwardapi-volume-813a37b7-02eb-4b0d-865a-e42c7e533ced no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:33:36.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1944" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2908,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:33:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 02:33:36.872: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 02:34:37.122: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:34:37.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:34:37.322: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Nov 16 02:34:37.364: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:34:37.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-67" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:34:37.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9082" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.150 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":154,"skipped":2915,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:34:37.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 16 02:34:37.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:34:49.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:35:30.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-132" for this suite.

• [SLOW TEST:53.004 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":155,"skipped":2927,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:35:30.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:35:30.936: INFO: Creating simple deployment test-new-deployment
Nov 16 02:35:31.047: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 02:35:33.281: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8012  bab48e4f-1c55-4118-8973-c69f0db64094 110738 3 2022-11-16 02:35:30 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-11-16 02:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a7f5368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-16 02:35:33 +0000 UTC,LastTransitionTime:2022-11-16 02:35:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-11-16 02:35:33 +0000 UTC,LastTransitionTime:2022-11-16 02:35:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 02:35:33.304: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-8012  0b6614dd-566d-43ab-aa9b-0f538ac39ddf 110740 3 2022-11-16 02:35:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment bab48e4f-1c55-4118-8973-c69f0db64094 0xc00a81c007 0xc00a81c008}] []  [{kube-controller-manager Update apps/v1 2022-11-16 02:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bab48e4f-1c55-4118-8973-c69f0db64094\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:35:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a81c098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:35:33.320: INFO: Pod "test-new-deployment-5d9fdcc779-clhgw" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-clhgw test-new-deployment-5d9fdcc779- deployment-8012  81a7bdfa-b8fd-424b-812e-f8c6595bfdb5 110742 0 2022-11-16 02:35:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 0b6614dd-566d-43ab-aa9b-0f538ac39ddf 0xc00a81c437 0xc00a81c438}] []  [{kube-controller-manager Update v1 2022-11-16 02:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b6614dd-566d-43ab-aa9b-0f538ac39ddf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8j6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8j6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6vgwl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:35:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 02:35:33.320: INFO: Pod "test-new-deployment-5d9fdcc779-dv2db" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-dv2db test-new-deployment-5d9fdcc779- deployment-8012  bfd00990-2359-4ba3-bde9-2191ee24cc00 110732 0 2022-11-16 02:35:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:9b5e57548ce52298525f51e47b8e25a977455b3f5752e99e1f5c52e3e270448f cni.projectcalico.org/podIP:172.30.36.83/32 cni.projectcalico.org/podIPs:172.30.36.83/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.83"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.83"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 0b6614dd-566d-43ab-aa9b-0f538ac39ddf 0xc00a81c657 0xc00a81c658}] []  [{kube-controller-manager Update v1 2022-11-16 02:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b6614dd-566d-43ab-aa9b-0f538ac39ddf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 02:35:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-11-16 02:35:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-16 02:35:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8rhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8rhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6vgwl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:35:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:35:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:35:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:35:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.83,StartTime:2022-11-16 02:35:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 02:35:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5ac3fde7a7a3440ddec7b16eff36b1fbdc63861f3a08e68b9870e3bb7315f94d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:35:33.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8012" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":156,"skipped":2945,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:35:33.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-6200/configmap-test-ad11bddb-2b8b-4c94-a3af-a1e97ff59091
STEP: Creating a pod to test consume configMaps
Nov 16 02:35:33.711: INFO: Waiting up to 5m0s for pod "pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048" in namespace "configmap-6200" to be "Succeeded or Failed"
Nov 16 02:35:33.724: INFO: Pod "pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048": Phase="Pending", Reason="", readiness=false. Elapsed: 12.548033ms
Nov 16 02:35:35.739: INFO: Pod "pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027056518s
Nov 16 02:35:37.752: INFO: Pod "pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040583015s
STEP: Saw pod success
Nov 16 02:35:37.752: INFO: Pod "pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048" satisfied condition "Succeeded or Failed"
Nov 16 02:35:37.764: INFO: Trying to get logs from node 10.189.71.151 pod pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048 container env-test: <nil>
STEP: delete the pod
Nov 16 02:35:37.858: INFO: Waiting for pod pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048 to disappear
Nov 16 02:35:37.865: INFO: Pod pod-configmaps-93f2c593-c54c-4f2e-8476-796acbff6048 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:35:37.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6200" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:35:37.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-8363
STEP: creating replication controller nodeport-test in namespace services-8363
I1116 02:35:38.141409      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8363, replica count: 2
I1116 02:35:41.192859      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 02:35:41.192: INFO: Creating new exec pod
Nov 16 02:35:44.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-8363 exec execpodgt7cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 16 02:35:44.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 16 02:35:44.577: INFO: stdout: "nodeport-test-n2dg9"
Nov 16 02:35:44.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-8363 exec execpodgt7cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.135.105 80'
Nov 16 02:35:44.874: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.135.105 80\nConnection to 172.21.135.105 80 port [tcp/http] succeeded!\n"
Nov 16 02:35:44.875: INFO: stdout: ""
Nov 16 02:35:45.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-8363 exec execpodgt7cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.135.105 80'
Nov 16 02:35:46.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.135.105 80\nConnection to 172.21.135.105 80 port [tcp/http] succeeded!\n"
Nov 16 02:35:46.171: INFO: stdout: "nodeport-test-f5hnn"
Nov 16 02:35:46.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-8363 exec execpodgt7cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.150 30458'
Nov 16 02:35:46.434: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.150 30458\nConnection to 10.189.71.150 30458 port [tcp/*] succeeded!\n"
Nov 16 02:35:46.434: INFO: stdout: "nodeport-test-n2dg9"
Nov 16 02:35:46.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-8363 exec execpodgt7cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.151 30458'
Nov 16 02:35:46.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.151 30458\nConnection to 10.189.71.151 30458 port [tcp/*] succeeded!\n"
Nov 16 02:35:46.983: INFO: stdout: "nodeport-test-n2dg9"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:35:46.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8363" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.121 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":158,"skipped":2969,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:35:47.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:35:47.243: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4" in namespace "downward-api-5722" to be "Succeeded or Failed"
Nov 16 02:35:47.257: INFO: Pod "downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.696411ms
Nov 16 02:35:49.272: INFO: Pod "downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029211956s
Nov 16 02:35:51.307: INFO: Pod "downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063787556s
STEP: Saw pod success
Nov 16 02:35:51.307: INFO: Pod "downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4" satisfied condition "Succeeded or Failed"
Nov 16 02:35:51.321: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4 container client-container: <nil>
STEP: delete the pod
Nov 16 02:35:51.416: INFO: Waiting for pod downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4 to disappear
Nov 16 02:35:51.424: INFO: Pod downwardapi-volume-6e31a460-7182-4f95-bd8c-a61f3e18a5b4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:35:51.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5722" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":2978,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:35:51.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:35:51.705: INFO: Create a RollingUpdate DaemonSet
Nov 16 02:35:51.735: INFO: Check that daemon pods launch on every node of the cluster
Nov 16 02:35:51.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:35:51.772: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:35:52.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:35:52.808: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:35:53.799: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 16 02:35:53.799: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:35:54.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 02:35:54.850: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Nov 16 02:35:54.850: INFO: Update the DaemonSet to trigger a rollout
Nov 16 02:35:55.039: INFO: Updating DaemonSet daemon-set
Nov 16 02:35:58.111: INFO: Roll back the DaemonSet before rollout is complete
Nov 16 02:35:58.144: INFO: Updating DaemonSet daemon-set
Nov 16 02:35:58.144: INFO: Make sure DaemonSet rollback is complete
Nov 16 02:35:58.158: INFO: Wrong image for pod: daemon-set-xczv2. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Nov 16 02:35:58.158: INFO: Pod daemon-set-xczv2 is not available
Nov 16 02:36:01.216: INFO: Pod daemon-set-nxlnn is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1953, will wait for the garbage collector to delete the pods
Nov 16 02:36:01.343: INFO: Deleting DaemonSet.extensions daemon-set took: 22.08386ms
Nov 16 02:36:01.444: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.82701ms
Nov 16 02:36:05.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:36:05.265: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 16 02:36:05.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111487"},"items":null}

Nov 16 02:36:05.287: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111487"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:36:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1953" for this suite.

• [SLOW TEST:13.921 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":160,"skipped":2978,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:36:05.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-217
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-217
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-217
Nov 16 02:36:05.607: INFO: Found 0 stateful pods, waiting for 1
Nov 16 02:36:15.627: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 16 02:36:15.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:36:15.967: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:36:15.967: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:36:15.967: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:36:15.978: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 16 02:36:25.998: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 02:36:25.998: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:36:26.107: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998557s
Nov 16 02:36:27.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983084205s
Nov 16 02:36:28.155: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.967401122s
Nov 16 02:36:29.165: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.935373912s
Nov 16 02:36:30.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.923720167s
Nov 16 02:36:31.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.908278444s
Nov 16 02:36:32.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.89414221s
Nov 16 02:36:33.225: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.878730371s
Nov 16 02:36:34.262: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.865423579s
Nov 16 02:36:35.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 827.320767ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-217
Nov 16 02:36:36.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:36:36.576: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 02:36:36.576: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:36:36.576: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:36:36.591: INFO: Found 1 stateful pods, waiting for 3
Nov 16 02:36:46.605: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 02:36:46.605: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 02:36:46.605: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 16 02:36:46.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:36:46.925: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:36:46.925: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:36:46.925: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:36:46.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:36:47.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:36:47.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:36:47.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:36:47.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:36:47.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:36:47.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:36:47.529: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:36:47.529: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:36:47.545: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 16 02:36:57.578: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 02:36:57.578: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 02:36:57.578: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 02:36:57.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998825s
Nov 16 02:36:58.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98459675s
Nov 16 02:36:59.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97185517s
Nov 16 02:37:00.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9556114s
Nov 16 02:37:01.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.91947567s
Nov 16 02:37:02.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.904774126s
Nov 16 02:37:03.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.883512473s
Nov 16 02:37:04.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.867581712s
Nov 16 02:37:05.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.852870898s
Nov 16 02:37:06.786: INFO: Verifying statefulset ss doesn't scale past 3 for another 838.238359ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-217
Nov 16 02:37:07.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:37:08.166: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 02:37:08.166: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:37:08.166: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:37:08.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:37:08.472: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 02:37:08.472: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:37:08.472: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:37:08.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-217 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:37:08.788: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 02:37:08.788: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:37:08.788: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:37:08.788: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 02:37:18.872: INFO: Deleting all statefulset in ns statefulset-217
Nov 16 02:37:18.886: INFO: Scaling statefulset ss to 0
Nov 16 02:37:18.933: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:37:18.946: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:37:19.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-217" for this suite.

• [SLOW TEST:73.677 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":161,"skipped":2991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:37:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:37:22.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3398" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":162,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:37:22.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 02:37:22.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:37:22.648: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:37:23.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:37:23.730: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:37:24.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 02:37:24.710: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 02:37:25.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 02:37:25.707: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Nov 16 02:37:25.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"112482"},"items":null}

Nov 16 02:37:25.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"112483"},"items":[{"metadata":{"name":"daemon-set-54mwq","generateName":"daemon-set-","namespace":"daemonsets-5980","uid":"2cb2c71b-dc55-4b3b-9b22-b6ee51552a8a","resourceVersion":"112483","creationTimestamp":"2022-11-16T02:37:22Z","deletionTimestamp":"2022-11-16T02:37:55Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4f0eb6af95d4fb5eafb61562b55f70746375e7fe1025814bbf34ecaa4e23f04e","cni.projectcalico.org/podIP":"172.30.102.236/32","cni.projectcalico.org/podIPs":"172.30.102.236/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.102.236\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.102.236\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7bd656b3-e81a-40c6-93e4-8c73344124a0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd656b3-e81a-40c6-93e4-8c73344124a0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hr46r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hr46r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.189.71.151","securityContext":{"seLinuxOptions":{"level":"s0:c51,c0"}},"imagePullSecrets":[{"name":"default-dockercfg-nclhp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.189.71.151"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:22Z"}],"hostIP":"10.189.71.151","podIP":"172.30.102.236","podIPs":[{"ip":"172.30.102.236"}],"startTime":"2022-11-16T02:37:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-16T02:37:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://d96aef3459a104ee6e83008d2946b7f04947adbb9db7604caa729222087940bc","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-krjhs","generateName":"daemon-set-","namespace":"daemonsets-5980","uid":"037d6664-1cf3-4508-b19e-43dbcc41c2d2","resourceVersion":"112456","creationTimestamp":"2022-11-16T02:37:22Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a0f4c5dd105fa2b4adbd195510405c13a954348f4e229161e7cee22e7b6f5ffe","cni.projectcalico.org/podIP":"172.30.169.119/32","cni.projectcalico.org/podIPs":"172.30.169.119/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.169.119\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.169.119\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7bd656b3-e81a-40c6-93e4-8c73344124a0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd656b3-e81a-40c6-93e4-8c73344124a0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.169.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zlc96","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zlc96","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.189.71.157","securityContext":{"seLinuxOptions":{"level":"s0:c51,c0"}},"imagePullSecrets":[{"name":"default-dockercfg-nclhp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.189.71.157"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:22Z"}],"hostIP":"10.189.71.157","podIP":"172.30.169.119","podIPs":[{"ip":"172.30.169.119"}],"startTime":"2022-11-16T02:37:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-16T02:37:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://cc6df4b7fabce9d20b7e8f020928a05754f9829f51faf29e5319ca276fb72044","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z8mhr","generateName":"daemon-set-","namespace":"daemonsets-5980","uid":"b8d7d285-c90d-4435-acd1-8b65ef44aff1","resourceVersion":"112482","creationTimestamp":"2022-11-16T02:37:22Z","deletionTimestamp":"2022-11-16T02:37:55Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b0ae41389fd9e24acd9adc30dad1873fac78e5e2418c846f12937ace4940cf7c","cni.projectcalico.org/podIP":"172.30.36.105/32","cni.projectcalico.org/podIPs":"172.30.36.105/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.36.105\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.36.105\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7bd656b3-e81a-40c6-93e4-8c73344124a0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bd656b3-e81a-40c6-93e4-8c73344124a0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-16T02:37:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-44fvh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-44fvh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.189.71.150","securityContext":{"seLinuxOptions":{"level":"s0:c51,c0"}},"imagePullSecrets":[{"name":"default-dockercfg-nclhp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.189.71.150"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-16T02:37:22Z"}],"hostIP":"10.189.71.150","podIP":"172.30.36.105","podIPs":[{"ip":"172.30.36.105"}],"startTime":"2022-11-16T02:37:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-16T02:37:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://e4c6d625691fef10f2b709afc2a49049bb038dfa18caf10ffdb418ee64902095","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:37:25.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5980" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":163,"skipped":3137,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:37:25.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 16 02:37:26.148: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4658  d582cbc9-847e-4474-8081-cda788828824 112507 0 2022-11-16 02:37:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-11-16 02:37:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:37:26.148: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4658  d582cbc9-847e-4474-8081-cda788828824 112512 0 2022-11-16 02:37:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-11-16 02:37:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 16 02:37:26.253: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4658  d582cbc9-847e-4474-8081-cda788828824 112521 0 2022-11-16 02:37:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-11-16 02:37:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 02:37:26.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4658  d582cbc9-847e-4474-8081-cda788828824 112524 0 2022-11-16 02:37:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-11-16 02:37:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:37:26.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4658" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":164,"skipped":3137,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:37:26.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 02:37:26.432: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 02:37:26.506: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 02:37:26.601: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.150 before test
Nov 16 02:37:26.704: INFO: calico-node-wwftz from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 02:37:26.710: INFO: calico-typha-6c9689f9f9-7lrnr from calico-system started at 2022-11-15 22:55:50 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 02:37:26.710: INFO: daemon-set-z8mhr from daemonsets-5980 started at 2022-11-16 02:37:22 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container app ready: true, restart count 0
Nov 16 02:37:26.710: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-qbrrz from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 02:37:26.710: INFO: ibm-keepalived-watcher-fvdpr from kube-system started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 02:37:26.710: INFO: ibm-master-proxy-static-10.189.71.150 from kube-system started at 2022-11-15 22:55:22 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container pause ready: true, restart count 0
Nov 16 02:37:26.710: INFO: ibmcloud-block-storage-driver-8skv4 from kube-system started at 2022-11-15 22:55:31 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 02:37:26.710: INFO: tuned-pwjtn from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container tuned ready: true, restart count 0
Nov 16 02:37:26.710: INFO: csi-snapshot-controller-67b9554c7c-2j5st from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 02:37:26.710: INFO: csi-snapshot-webhook-59f45645b4-jvp9f from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container webhook ready: true, restart count 0
Nov 16 02:37:26.710: INFO: console-7b55c47f46-m72dl from openshift-console started at 2022-11-15 22:58:16 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container console ready: true, restart count 0
Nov 16 02:37:26.710: INFO: downloads-7bb5c774c4-9j76w from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container download-server ready: true, restart count 0
Nov 16 02:37:26.710: INFO: dns-default-hsr72 from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container dns ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: node-resolver-n4drz from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 02:37:26.710: INFO: image-pruner-27809280-w5zhl from openshift-image-registry started at 2022-11-16 00:00:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container image-pruner ready: false, restart count 0
Nov 16 02:37:26.710: INFO: node-ca-7fr4t from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 02:37:26.710: INFO: registry-pvc-permissions-jnr42 from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 02:37:26.710: INFO: ingress-canary-c4qbp from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 02:37:26.710: INFO: router-default-585ff5cb57-5mh8f from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container router ready: true, restart count 0
Nov 16 02:37:26.710: INFO: openshift-kube-proxy-g54k6 from openshift-kube-proxy started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: node-exporter-rc4tp from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 02:37:26.710: INFO: prometheus-adapter-6dfb554d7c-ld9xx from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 02:37:26.710: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 02:37:26.710: INFO: thanos-querier-964664765-v92dp from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 02:37:26.710: INFO: multus-additional-cni-plugins-lg6ck from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 02:37:26.710: INFO: multus-admission-controller-72x8x from openshift-multus started at 2022-11-15 22:56:30 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.710: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.710: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 02:37:26.710: INFO: multus-hsbff from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.711: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 02:37:26.711: INFO: network-metrics-daemon-z2nx6 from openshift-multus started at 2022-11-15 22:55:30 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.711: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 02:37:26.711: INFO: network-check-target-tbk2k from openshift-network-diagnostics started at 2022-11-15 22:55:30 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.711: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 02:37:26.711: INFO: sonobuoy from sonobuoy started at 2022-11-16 01:36:05 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.711: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 02:37:26.711: INFO: sonobuoy-e2e-job-d1033a110ac84614 from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.711: INFO: 	Container e2e ready: true, restart count 0
Nov 16 02:37:26.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:37:26.711: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-zzb4n from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:37:26.711: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 02:37:26.711: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.151 before test
Nov 16 02:37:26.762: INFO: calico-node-k6ktc from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 02:37:26.762: INFO: daemon-set-54mwq from daemonsets-5980 started at 2022-11-16 02:37:22 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container app ready: true, restart count 0
Nov 16 02:37:26.762: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-11-15 23:00:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 02:37:26.762: INFO: ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-fgdkx from ibm-system started at 2022-11-15 22:57:24 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container ibm-cloud-provider-ip-169-60-77-124 ready: true, restart count 0
Nov 16 02:37:26.762: INFO: ibm-keepalived-watcher-hxcww from kube-system started at 2022-11-15 22:54:31 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 02:37:26.762: INFO: ibm-master-proxy-static-10.189.71.151 from kube-system started at 2022-11-15 22:54:22 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 02:37:26.762: INFO: 	Container pause ready: true, restart count 0
Nov 16 02:37:26.762: INFO: ibmcloud-block-storage-driver-jmfzn from kube-system started at 2022-11-15 22:54:37 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 02:37:26.762: INFO: vpn-bcc48b544-7bsch from kube-system started at 2022-11-15 22:59:50 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container vpn ready: true, restart count 0
Nov 16 02:37:26.762: INFO: tuned-j4zhw from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container tuned ready: true, restart count 0
Nov 16 02:37:26.762: INFO: csi-snapshot-controller-67b9554c7c-h9nb4 from openshift-cluster-storage-operator started at 2022-11-15 22:56:41 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 16 02:37:26.762: INFO: csi-snapshot-webhook-59f45645b4-6v8x8 from openshift-cluster-storage-operator started at 2022-11-15 22:56:38 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container webhook ready: true, restart count 0
Nov 16 02:37:26.762: INFO: downloads-7bb5c774c4-2wfgp from openshift-console started at 2022-11-15 22:56:34 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.762: INFO: 	Container download-server ready: true, restart count 0
Nov 16 02:37:26.763: INFO: dns-default-lv86r from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container dns ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: node-resolver-45b2v from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 02:37:26.763: INFO: image-registry-665b8bc74-tzf8g from openshift-image-registry started at 2022-11-15 23:06:10 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container registry ready: true, restart count 0
Nov 16 02:37:26.763: INFO: node-ca-6xvc9 from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 02:37:26.763: INFO: ingress-canary-wd4rg from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 02:37:26.763: INFO: router-default-585ff5cb57-hvdfj from openshift-ingress started at 2022-11-15 22:59:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container router ready: true, restart count 0
Nov 16 02:37:26.763: INFO: openshift-kube-proxy-fttpn from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: migrator-84db9689c8-sd7pr from openshift-kube-storage-version-migrator started at 2022-11-15 22:56:36 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container migrator ready: true, restart count 0
Nov 16 02:37:26.763: INFO: certified-operators-6p9ch from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:37:26.763: INFO: community-operators-djvrf from openshift-marketplace started at 2022-11-16 00:48:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:37:26.763: INFO: redhat-marketplace-mnw2d from openshift-marketplace started at 2022-11-15 22:57:21 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:37:26.763: INFO: redhat-operators-t5v8m from openshift-marketplace started at 2022-11-15 22:57:20 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container registry-server ready: true, restart count 0
Nov 16 02:37:26.763: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-11-15 23:00:06 +0000 UTC (6 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: grafana-d8cb7d96c-zbvqh from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (3 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container grafana ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 02:37:26.763: INFO: kube-state-metrics-6545ff684-9k7k4 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 02:37:26.763: INFO: node-exporter-2qsf6 from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 02:37:26.763: INFO: openshift-state-metrics-6f59cd77b7-gd25m from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (3 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 02:37:26.763: INFO: prometheus-adapter-6dfb554d7c-8ks56 from openshift-monitoring started at 2022-11-15 23:01:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 02:37:26.763: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-11-15 23:00:22 +0000 UTC (6 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container prometheus ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 02:37:26.763: INFO: prometheus-operator-78c776cffb-vvd4z from openshift-monitoring started at 2022-11-15 22:57:51 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 02:37:26.763: INFO: telemeter-client-867498bdb-2sc6r from openshift-monitoring started at 2022-11-15 23:01:05 +0000 UTC (3 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container reload ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 02:37:26.763: INFO: thanos-querier-964664765-tpwrf from openshift-monitoring started at 2022-11-15 23:00:07 +0000 UTC (6 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container thanos-query ready: true, restart count 0
Nov 16 02:37:26.763: INFO: multus-additional-cni-plugins-lkmkq from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 02:37:26.763: INFO: multus-admission-controller-52p7r from openshift-multus started at 2022-11-15 22:56:01 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 02:37:26.763: INFO: multus-b7qhm from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 02:37:26.763: INFO: network-metrics-daemon-lmb6c from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 02:37:26.763: INFO: network-check-target-fknwj from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 02:37:26.763: INFO: collect-profiles-27809400-qwlr9 from openshift-operator-lifecycle-manager started at 2022-11-16 02:00:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 02:37:26.763: INFO: collect-profiles-27809415-sbm2g from openshift-operator-lifecycle-manager started at 2022-11-16 02:15:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 02:37:26.763: INFO: collect-profiles-27809430-gnp26 from openshift-operator-lifecycle-manager started at 2022-11-16 02:30:00 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 16 02:37:26.763: INFO: packageserver-74b6688945-wbm7x from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 02:37:26.763: INFO: service-ca-c6c7f64d9-kkrmn from openshift-service-ca started at 2022-11-15 22:56:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 16 02:37:26.763: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-glhsh from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:37:26.763: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 02:37:26.763: INFO: tigera-operator-56bfd47f4b-t9q5p from tigera-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.763: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 02:37:26.763: INFO: 
Logging pods the apiserver thinks is on node 10.189.71.157 before test
Nov 16 02:37:26.809: INFO: calico-kube-controllers-7558694cbb-7jmn8 from calico-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 02:37:26.809: INFO: calico-node-m6bg7 from calico-system started at 2022-11-15 22:55:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 02:37:26.809: INFO: calico-typha-6c9689f9f9-2zwbk from calico-system started at 2022-11-15 22:55:41 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 02:37:26.809: INFO: daemon-set-krjhs from daemonsets-5980 started at 2022-11-16 02:37:22 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container app ready: true, restart count 0
Nov 16 02:37:26.809: INFO: managed-storage-validation-webhooks-577f9fb75-6h9ln from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 02:37:26.809: INFO: managed-storage-validation-webhooks-577f9fb75-fnp2h from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 16 02:37:26.809: INFO: managed-storage-validation-webhooks-577f9fb75-w69vd from ibm-odf-validation-webhook started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Nov 16 02:37:26.809: INFO: ibm-file-plugin-9b7d4b5b8-4bnbc from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ibm-keepalived-watcher-rrpgn from kube-system started at 2022-11-15 22:54:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ibm-master-proxy-static-10.189.71.157 from kube-system started at 2022-11-15 22:54:20 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container pause ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ibm-storage-metrics-agent-5fdfb985fb-xk9pl from kube-system started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ibm-storage-watcher-b65cff46-l8xr7 from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ibmcloud-block-storage-driver-prkm8 from kube-system started at 2022-11-15 22:54:34 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ibmcloud-block-storage-plugin-54cb6d9d56-9228p from kube-system started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 02:37:26.809: INFO: cluster-node-tuning-operator-58499f758f-tdqh6 from openshift-cluster-node-tuning-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: tuned-jqlkl from openshift-cluster-node-tuning-operator started at 2022-11-15 22:57:29 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container tuned ready: true, restart count 0
Nov 16 02:37:26.809: INFO: cluster-samples-operator-786cd9cc49-f96c8 from openshift-cluster-samples-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 02:37:26.809: INFO: cluster-storage-operator-7bffdd9f8b-h2ws8 from openshift-cluster-storage-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 16 02:37:26.809: INFO: csi-snapshot-controller-operator-687bbd75c7-28mzk from openshift-cluster-storage-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: console-operator-fc6655cfb-l5vkm from openshift-console-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 02:37:26.809: INFO: console-7b55c47f46-vcg9t from openshift-console started at 2022-11-15 22:57:42 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container console ready: true, restart count 0
Nov 16 02:37:26.809: INFO: dns-operator-86cb86fff-qfhb7 from openshift-dns-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: dns-default-2w2wr from openshift-dns started at 2022-11-15 22:57:16 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container dns ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: node-resolver-lvbbq from openshift-dns started at 2022-11-15 22:57:17 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 02:37:26.809: INFO: cluster-image-registry-operator-6b6ddc84cf-gltvh from openshift-image-registry started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: node-ca-pvrpl from openshift-image-registry started at 2022-11-15 22:57:28 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ingress-canary-njn8v from openshift-ingress-canary started at 2022-11-15 22:57:23 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 16 02:37:26.809: INFO: ingress-operator-659f7f8c4c-hmtfd from openshift-ingress-operator started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: openshift-kube-proxy-lzfjb from openshift-kube-proxy started at 2022-11-15 22:55:00 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: kube-storage-version-migrator-operator-5bb6b9d4df-t7xsn from openshift-kube-storage-version-migrator-operator started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 16 02:37:26.809: INFO: marketplace-operator-7b5856958-2qbfp from openshift-marketplace started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: cluster-monitoring-operator-6d9c66b9d-hrwvw from openshift-monitoring started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: node-exporter-c942p from openshift-monitoring started at 2022-11-15 22:59:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 02:37:26.809: INFO: multus-additional-cni-plugins-4g7rl from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 16 02:37:26.809: INFO: multus-admission-controller-594c2 from openshift-multus started at 2022-11-15 22:55:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 02:37:26.809: INFO: multus-zq5lk from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 02:37:26.809: INFO: network-metrics-daemon-bw5dw from openshift-multus started at 2022-11-15 22:54:59 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 16 02:37:26.809: INFO: network-check-source-585b669bfd-r8rkl from openshift-network-diagnostics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 16 02:37:26.809: INFO: network-check-target-nq57m from openshift-network-diagnostics started at 2022-11-15 22:55:01 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 16 02:37:26.809: INFO: network-operator-585b458dd4-dzhkh from openshift-network-operator started at 2022-11-15 22:54:39 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: catalog-operator-5f469dc4d6-plpn4 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:58 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: olm-operator-69ddc4ffc7-v85g5 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 02:37:26.809: INFO: package-server-manager-6b45b7b9cb-qmvk7 from openshift-operator-lifecycle-manager started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 16 02:37:26.809: INFO: packageserver-74b6688945-wx6bb from openshift-operator-lifecycle-manager started at 2022-11-15 22:57:08 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 02:37:26.809: INFO: metrics-6d5d4dd59c-txgqh from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container metrics ready: true, restart count 3
Nov 16 02:37:26.809: INFO: push-gateway-8565b96d89-q54bc from openshift-roks-metrics started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container push-gateway ready: true, restart count 0
Nov 16 02:37:26.809: INFO: service-ca-operator-5f77cff646-4mbcp from openshift-service-ca-operator started at 2022-11-15 22:55:59 +0000 UTC (1 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 16 02:37:26.809: INFO: sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-68x2f from sonobuoy started at 2022-11-16 01:36:09 +0000 UTC (2 container statuses recorded)
Nov 16 02:37:26.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 02:37:26.809: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1727f02943baaaca], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:37:27.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5931" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":165,"skipped":3148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:37:28.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-7d0c74b0-5338-4b96-953f-171a1aa22b12
STEP: Creating a pod to test consume secrets
Nov 16 02:37:28.288: INFO: Waiting up to 5m0s for pod "pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428" in namespace "secrets-4933" to be "Succeeded or Failed"
Nov 16 02:37:28.319: INFO: Pod "pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428": Phase="Pending", Reason="", readiness=false. Elapsed: 31.213603ms
Nov 16 02:37:30.344: INFO: Pod "pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056142759s
Nov 16 02:37:32.382: INFO: Pod "pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093969763s
Nov 16 02:37:34.406: INFO: Pod "pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.118179717s
STEP: Saw pod success
Nov 16 02:37:34.406: INFO: Pod "pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428" satisfied condition "Succeeded or Failed"
Nov 16 02:37:34.417: INFO: Trying to get logs from node 10.189.71.157 pod pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:37:34.510: INFO: Waiting for pod pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428 to disappear
Nov 16 02:37:34.535: INFO: Pod pod-secrets-854cf7fa-3cfb-48fd-97f6-71478c9da428 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:37:34.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4933" for this suite.

• [SLOW TEST:6.514 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":3293,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:37:34.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Nov 16 02:37:34.705: INFO: PodSpec: initContainers in spec.initContainers
Nov 16 02:38:17.554: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6da546e6-4003-4354-bd8f-3e9ea8bc9a33", GenerateName:"", Namespace:"init-container-9234", SelfLink:"", UID:"154edf42-454d-416f-b7e3-9311dc58236a", ResourceVersion:"113220", Generation:0, CreationTimestamp:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"705481636"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"480cec26c99d139966f80720fc8f91442e0d5e1d8d7d261ac2a4aeb0192ebfeb", "cni.projectcalico.org/podIP":"172.30.102.206/32", "cni.projectcalico.org/podIPs":"172.30.102.206/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.102.206\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.102.206\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005a7f9c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 16, 2, 37, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005a7f9f8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 16, 2, 37, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005a7fa28), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 16, 2, 37, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005a7fa58), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-kmjv9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003d9d440), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kmjv9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001738d20), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kmjv9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001738d80), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kmjv9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001738cc0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00677f5b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.189.71.151", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0024cc690), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00677f670)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00677f690)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00677f6ac), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00677f6b0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00655a7a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.189.71.151", PodIP:"172.30.102.206", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.102.206"}}, StartTime:time.Date(2022, time.November, 16, 2, 37, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0024cc7e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0024cc8c0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://f1e6549b9d96ea501c07e252838f40b923e460f7a0b52bb89802f5649128df44", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003d9d4c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003d9d4a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00677f724)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:38:17.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9234" for this suite.

• [SLOW TEST:43.036 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":167,"skipped":3309,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:38:17.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-5781
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5781 to expose endpoints map[]
Nov 16 02:38:17.860: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 16 02:38:18.895: INFO: successfully validated that service multi-endpoint-test in namespace services-5781 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5781
Nov 16 02:38:18.973: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:38:20.986: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:38:22.990: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5781 to expose endpoints map[pod1:[100]]
Nov 16 02:38:23.042: INFO: successfully validated that service multi-endpoint-test in namespace services-5781 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5781
Nov 16 02:38:23.099: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:38:25.114: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5781 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 16 02:38:25.176: INFO: successfully validated that service multi-endpoint-test in namespace services-5781 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Nov 16 02:38:25.176: INFO: Creating new exec pod
Nov 16 02:38:28.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5781 exec execpod9t57m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Nov 16 02:38:28.617: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 16 02:38:28.617: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:38:28.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5781 exec execpod9t57m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.112.29 80'
Nov 16 02:38:28.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.112.29 80\nConnection to 172.21.112.29 80 port [tcp/http] succeeded!\n"
Nov 16 02:38:28.938: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:38:28.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5781 exec execpod9t57m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Nov 16 02:38:29.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 16 02:38:29.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 02:38:29.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-5781 exec execpod9t57m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.112.29 81'
Nov 16 02:38:29.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.112.29 81\nConnection to 172.21.112.29 81 port [tcp/*] succeeded!\n"
Nov 16 02:38:29.577: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5781
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5781 to expose endpoints map[pod2:[101]]
Nov 16 02:38:30.660: INFO: successfully validated that service multi-endpoint-test in namespace services-5781 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5781
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5781 to expose endpoints map[]
Nov 16 02:38:30.754: INFO: successfully validated that service multi-endpoint-test in namespace services-5781 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:38:30.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5781" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:13.284 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":168,"skipped":3330,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:38:30.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
Nov 16 02:38:31.054: INFO: created test-podtemplate-1
Nov 16 02:38:31.085: INFO: created test-podtemplate-2
Nov 16 02:38:31.109: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 16 02:38:31.127: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 16 02:38:31.199: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:38:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4539" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":169,"skipped":3336,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:38:31.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:38:42.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-171" for this suite.

• [SLOW TEST:11.466 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":170,"skipped":3343,"failed":0}
S
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:38:42.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:39:42.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9836" for this suite.

• [SLOW TEST:60.276 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3344,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:39:42.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Nov 16 02:39:43.243: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:39:45.260: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:39:47.267: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 16 02:39:48.361: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:39:49.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9167" for this suite.

• [SLOW TEST:6.507 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":172,"skipped":3348,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:39:49.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:39:49.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:39:50.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4926" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":173,"skipped":3348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:39:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 16 02:39:50.499: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:39:50.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-552" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":174,"skipped":3406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:39:50.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-01a0f843-a443-4f24-b906-4e6141d4d51c
STEP: Creating a pod to test consume secrets
Nov 16 02:39:51.253: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01" in namespace "projected-5104" to be "Succeeded or Failed"
Nov 16 02:39:51.268: INFO: Pod "pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01": Phase="Pending", Reason="", readiness=false. Elapsed: 14.212635ms
Nov 16 02:39:53.282: INFO: Pod "pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028919587s
Nov 16 02:39:55.302: INFO: Pod "pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048410878s
STEP: Saw pod success
Nov 16 02:39:55.302: INFO: Pod "pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01" satisfied condition "Succeeded or Failed"
Nov 16 02:39:55.323: INFO: Trying to get logs from node 10.189.71.150 pod pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:39:55.429: INFO: Waiting for pod pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01 to disappear
Nov 16 02:39:55.439: INFO: Pod pod-projected-secrets-981b7828-b2a5-4c47-b06f-21ba19f8de01 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:39:55.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5104" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3441,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:39:55.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Nov 16 02:39:55.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:40:53.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4067" for this suite.

• [SLOW TEST:57.588 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":176,"skipped":3444,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:40:53.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
Nov 16 02:40:53.307: INFO: Waiting up to 5m0s for pod "client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65" in namespace "containers-5588" to be "Succeeded or Failed"
Nov 16 02:40:53.331: INFO: Pod "client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65": Phase="Pending", Reason="", readiness=false. Elapsed: 24.466808ms
Nov 16 02:40:55.346: INFO: Pod "client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039348817s
Nov 16 02:40:57.370: INFO: Pod "client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062678113s
Nov 16 02:40:59.383: INFO: Pod "client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07632473s
STEP: Saw pod success
Nov 16 02:40:59.383: INFO: Pod "client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65" satisfied condition "Succeeded or Failed"
Nov 16 02:40:59.395: INFO: Trying to get logs from node 10.189.71.150 pod client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:40:59.495: INFO: Waiting for pod client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65 to disappear
Nov 16 02:40:59.508: INFO: Pod client-containers-af7e57e8-b102-46e0-9f11-17cf1ab4dd65 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:40:59.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5588" for this suite.

• [SLOW TEST:6.456 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3462,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:40:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:41:00.731: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 16 02:41:02.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 2, 41, 0, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 41, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 41, 0, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 41, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-67c86bcf4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:41:05.895: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:41:05.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:09.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7450" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.057 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":178,"skipped":3469,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:09.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Nov 16 02:41:11.771: INFO: pods: 0 < 3
Nov 16 02:41:13.788: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Nov 16 02:41:20.102: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:22.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8129" for this suite.

• [SLOW TEST:12.698 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":179,"skipped":3487,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:22.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4763" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":180,"skipped":3499,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:22.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:41:22.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 create -f -'
Nov 16 02:41:25.237: INFO: stderr: ""
Nov 16 02:41:25.237: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 16 02:41:25.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 create -f -'
Nov 16 02:41:27.433: INFO: stderr: ""
Nov 16 02:41:27.433: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 16 02:41:28.486: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 02:41:28.486: INFO: Found 1 / 1
Nov 16 02:41:28.486: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 16 02:41:28.520: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 02:41:28.520: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 02:41:28.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 describe pod agnhost-primary-fpgmd'
Nov 16 02:41:28.685: INFO: stderr: ""
Nov 16 02:41:28.685: INFO: stdout: "Name:         agnhost-primary-fpgmd\nNamespace:    kubectl-2670\nPriority:     0\nNode:         10.189.71.157/10.189.71.157\nStart Time:   Wed, 16 Nov 2022 02:41:25 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: bb06cde193de43cf7982c3dee6ae616f38b334f4457042d70dcc4edb5a722e85\n              cni.projectcalico.org/podIP: 172.30.169.114/32\n              cni.projectcalico.org/podIPs: 172.30.169.114/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.169.114\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.169.114\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.169.114\nIPs:\n  IP:           172.30.169.114\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://ba76fb6b9dc9aac0ad0156e63eb1401d376e9a74b7ca2829378395a85ded750a\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 16 Nov 2022 02:41:26 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-47dbw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-47dbw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-2670/agnhost-primary-fpgmd to 10.189.71.157\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.169.114/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
Nov 16 02:41:28.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 describe rc agnhost-primary'
Nov 16 02:41:28.852: INFO: stderr: ""
Nov 16 02:41:28.852: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2670\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-fpgmd\n"
Nov 16 02:41:28.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 describe service agnhost-primary'
Nov 16 02:41:29.041: INFO: stderr: ""
Nov 16 02:41:29.041: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2670\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.83.190\nIPs:               172.21.83.190\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.169.114:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 16 02:41:29.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 describe node 10.189.71.150'
Nov 16 02:41:29.467: INFO: stderr: ""
Nov 16 02:41:29.467: INFO: stdout: "Name:               10.189.71.150\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=wdc06\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.63.138.30\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.189.71.150\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=us-east\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cdq18a6w0nfmebhhl9sg-kubee2epvgs-default-000001e5\n                    ibm-cloud.kubernetes.io/worker-pool-id=cdq18a6w0nfmebhhl9sg-042216a\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.10.39_1544_openshift\n                    ibm-cloud.kubernetes.io/zone=wdc06\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.189.71.150\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722950\n                    publicVLAN=2722948\n                    topology.kubernetes.io/region=us-east\n                    topology.kubernetes.io/zone=wdc06\nAnnotations:        projectcalico.org/IPv4Address: 10.189.71.150/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.36.64\nCreationTimestamp:  Tue, 15 Nov 2022 22:55:28 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.189.71.150\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 16 Nov 2022 02:41:23 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 15 Nov 2022 22:56:30 +0000   Tue, 15 Nov 2022 22:56:30 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 16 Nov 2022 02:37:25 +0000   Tue, 15 Nov 2022 22:55:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 16 Nov 2022 02:37:25 +0000   Tue, 15 Nov 2022 22:55:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 16 Nov 2022 02:37:25 +0000   Tue, 15 Nov 2022 22:55:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 16 Nov 2022 02:37:25 +0000   Tue, 15 Nov 2022 22:56:30 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.189.71.150\n  ExternalIP:  169.63.138.30\n  Hostname:    10.189.71.150\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386536Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13597160Ki\n  pods:               110\nSystem Info:\n  Machine ID:                             74f845bd71a84a03babf6bd52c9088f5\n  System UUID:                            e737ef07-32c7-c352-ebb4-f44ab44a0275\n  Boot ID:                                539b6546-ce56-4774-a333-8477ce314ef4\n  Kernel Version:                         4.18.0-425.3.1.el8.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.23.3-20.rhaos4.10.git89344de.el8\n  Kubelet Version:                        v1.23.12+6b34f32\n  Kube-Proxy Version:                     v1.23.12+6b34f32\nPodCIDR:                                  172.30.2.0/24\nPodCIDRs:                                 172.30.2.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///cdq18a6w0nfmebhhl9sg/kube-cdq18a6w0nfmebhhl9sg-kubee2epvgs-default-000001e5\nNon-terminated Pods:                      (30 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-wwftz                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h45m\n  calico-system                           calico-typha-6c9689f9f9-7lrnr                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h45m\n  ibm-system                              ibm-cloud-provider-ip-169-60-77-124-5f54b878c4-qbrrz       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h44m\n  kube-system                             ibm-keepalived-watcher-fvdpr                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h46m\n  kube-system                             ibm-master-proxy-static-10.189.71.150                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      3h44m\n  kube-system                             ibmcloud-block-storage-driver-8skv4                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     3h45m\n  openshift-cluster-node-tuning-operator  tuned-pwjtn                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h44m\n  openshift-cluster-storage-operator      csi-snapshot-controller-67b9554c7c-2j5st                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h44m\n  openshift-cluster-storage-operator      csi-snapshot-webhook-59f45645b4-jvp9f                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h44m\n  openshift-console                       console-7b55c47f46-m72dl                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h43m\n  openshift-console                       downloads-7bb5c774c4-9j76w                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h44m\n  openshift-dns                           dns-default-hsr72                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         3h44m\n  openshift-dns                           node-resolver-n4drz                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h44m\n  openshift-image-registry                node-ca-7fr4t                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h44m\n  openshift-ingress-canary                ingress-canary-c4qbp                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h44m\n  openshift-ingress                       router-default-585ff5cb57-5mh8f                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         3h41m\n  openshift-kube-proxy                    openshift-kube-proxy-g54k6                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         3h46m\n  openshift-monitoring                    alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         3h41m\n  openshift-monitoring                    node-exporter-rc4tp                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h41m\n  openshift-monitoring                    prometheus-adapter-6dfb554d7c-ld9xx                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         3h40m\n  openshift-monitoring                    prometheus-k8s-1                                           100m (2%)     0 (0%)      1104Mi (8%)      0 (0%)         3h41m\n  openshift-monitoring                    thanos-querier-964664765-v92dp                             15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         3h41m\n  openshift-multus                        multus-additional-cni-plugins-lg6ck                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h46m\n  openshift-multus                        multus-admission-controller-72x8x                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h44m\n  openshift-multus                        multus-hsbff                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h46m\n  openshift-multus                        network-metrics-daemon-z2nx6                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h46m\n  openshift-network-diagnostics           network-check-target-tbk2k                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         3h46m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  sonobuoy                                sonobuoy-e2e-job-d1033a110ac84614                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-d129b1b29779480f-zzb4n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1135m (29%)      600m (15%)\n  memory             2970131Ki (21%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type     Reason                                            Age    From        Message\n  ----     ------                                            ----   ----        -------\n  Warning  listen tcp4 :32296: bind: address already in use  158m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-1:port-1\" (:32296/tcp4), skipping it\n  Warning  listen tcp4 :31039: bind: address already in use  154m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-12:port-1\" (:31039/tcp4), skipping it\n  Warning  listen tcp4 :31902: bind: address already in use  153m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-2:port-1\" (:31902/tcp4), skipping it\n  Warning  listen tcp4 :31823: bind: address already in use  151m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-2:port-1\" (:31823/tcp4), skipping it\n  Warning  listen tcp4 :32603: bind: address already in use  144m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-3:port-1\" (:32603/tcp4), skipping it\n  Warning  listen udp4 :32061: bind: address already in use  144m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-5:port-1\" (:32061/udp4), skipping it\n  Warning  listen tcp4 :30132: bind: address already in use  141m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-7:port-1\" (:30132/tcp4), skipping it\n  Warning  listen tcp4 :30825: bind: address already in use  139m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-8:port-1\" (:30825/tcp4), skipping it\n  Warning  listen tcp4 :32095: bind: address already in use  138m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-9:port-1\" (:32095/tcp4), skipping it\n  Warning  listen tcp4 :32514: bind: address already in use  131m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-1:port-1\" (:32514/tcp4), skipping it\n  Warning  listen tcp4 :31943: bind: address already in use  128m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-12:port-1\" (:31943/tcp4), skipping it\n  Warning  listen tcp4 :32447: bind: address already in use  127m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-2:port-1\" (:32447/tcp4), skipping it\n  Warning  listen tcp4 :31504: bind: address already in use  124m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-2:port-1\" (:31504/tcp4), skipping it\n  Warning  listen tcp4 :32659: bind: address already in use  114m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-3:port-1\" (:32659/tcp4), skipping it\n  Warning  listen udp4 :31665: bind: address already in use  113m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-5:port-1\" (:31665/udp4), skipping it\n  Warning  listen tcp4 :31135: bind: address already in use  110m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-7:port-1\" (:31135/tcp4), skipping it\n  Warning  listen tcp4 :30615: bind: address already in use  107m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-8:port-1\" (:30615/tcp4), skipping it\n  Warning  listen tcp4 :30398: bind: address already in use  105m   kube-proxy  can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-9:port-1\" (:30398/tcp4), skipping it\n  Warning  listen tcp4 :30079: bind: address already in use  98m    kube-proxy  can't open port \"nodePort for default/rolling-deploy-b389278bd4514160ae7155e420faf4f8-svc\" (:30079/tcp4), skipping it\n  Warning  listen tcp4 :30956: bind: address already in use  97m    kube-proxy  can't open port \"nodePort for default/ha-apps-across-workers-f791ff676fac4cdcb253832d3897dccd-svc\" (:30956/tcp4), skipping it\n  Warning  listen tcp4 :30933: bind: address already in use  97m    kube-proxy  can't open port \"nodePort for default/scale-318b0267dcbc46ef87728892f738391d-svc\" (:30933/tcp4), skipping it\n  Warning  listen tcp4 :31541: bind: address already in use  96m    kube-proxy  can't open port \"nodePort for default/load-tester-8a19df8218f2485f88b8ba13d0503b5d-svc\" (:31541/tcp4), skipping it\n  Warning  listen tcp4 :30008: bind: address already in use  64m    kube-proxy  can't open port \"nodePort for services-419/affinity-nodeport-timeout\" (:30008/tcp4), skipping it\n  Warning  listen tcp4 :32700: bind: address already in use  52m    kube-proxy  can't open port \"nodePort for services-2011/affinity-nodeport-transition\" (:32700/tcp4), skipping it\n  Warning  listen tcp4 :32673: bind: address already in use  34m    kube-proxy  can't open port \"nodePort for services-7241/affinity-nodeport\" (:32673/tcp4), skipping it\n  Warning  listen tcp4 :30458: bind: address already in use  5m51s  kube-proxy  can't open port \"nodePort for services-8363/nodeport-test:http\" (:30458/tcp4), skipping it\n"
Nov 16 02:41:29.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2670 describe namespace kubectl-2670'
Nov 16 02:41:29.608: INFO: stderr: ""
Nov 16 02:41:29.608: INFO: stdout: "Name:         kubectl-2670\nLabels:       e2e-framework=kubectl\n              e2e-run=2763a32a-15bb-4239-bc69-fbfcd9a83e5f\n              kubernetes.io/metadata.name=kubectl-2670\nAnnotations:  openshift.io/sa.scc.mcs: s0:c52,c39\n              openshift.io/sa.scc.supplemental-groups: 1002730000/10000\n              openshift.io/sa.scc.uid-range: 1002730000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:29.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2670" for this suite.

• [SLOW TEST:6.945 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1109
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":181,"skipped":3506,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:29.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:46.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4596" for this suite.

• [SLOW TEST:16.605 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":182,"skipped":3516,"failed":0}
SSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:46.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 02:41:46.443: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 02:41:46.477: INFO: waiting for watch events with expected annotations
Nov 16 02:41:46.477: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:46.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4072" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":183,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Nov 16 02:41:46.833: INFO: Waiting up to 5m0s for pod "downward-api-2297a3d8-110b-4198-894f-519c703921b8" in namespace "downward-api-9007" to be "Succeeded or Failed"
Nov 16 02:41:46.848: INFO: Pod "downward-api-2297a3d8-110b-4198-894f-519c703921b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.730959ms
Nov 16 02:41:48.879: INFO: Pod "downward-api-2297a3d8-110b-4198-894f-519c703921b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045666032s
Nov 16 02:41:50.896: INFO: Pod "downward-api-2297a3d8-110b-4198-894f-519c703921b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063317988s
STEP: Saw pod success
Nov 16 02:41:50.897: INFO: Pod "downward-api-2297a3d8-110b-4198-894f-519c703921b8" satisfied condition "Succeeded or Failed"
Nov 16 02:41:50.911: INFO: Trying to get logs from node 10.189.71.151 pod downward-api-2297a3d8-110b-4198-894f-519c703921b8 container dapi-container: <nil>
STEP: delete the pod
Nov 16 02:41:51.022: INFO: Waiting for pod downward-api-2297a3d8-110b-4198-894f-519c703921b8 to disappear
Nov 16 02:41:51.040: INFO: Pod downward-api-2297a3d8-110b-4198-894f-519c703921b8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:51.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9007" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3547,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:51.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:41:51.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d" in namespace "projected-6568" to be "Succeeded or Failed"
Nov 16 02:41:51.287: INFO: Pod "downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d": Phase="Pending", Reason="", readiness=false. Elapsed: 32.439713ms
Nov 16 02:41:53.307: INFO: Pod "downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052418466s
Nov 16 02:41:55.326: INFO: Pod "downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071056289s
STEP: Saw pod success
Nov 16 02:41:55.326: INFO: Pod "downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d" satisfied condition "Succeeded or Failed"
Nov 16 02:41:55.338: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d container client-container: <nil>
STEP: delete the pod
Nov 16 02:41:55.402: INFO: Waiting for pod downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d to disappear
Nov 16 02:41:55.415: INFO: Pod downwardapi-volume-7889f391-9b1d-447a-9b3b-a7c15e20aa5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:41:55.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6568" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3561,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:41:55.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-1678
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-1678
Nov 16 02:41:55.626: INFO: Found 0 stateful pods, waiting for 1
Nov 16 02:42:05.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 02:42:05.743: INFO: Deleting all statefulset in ns statefulset-1678
Nov 16 02:42:05.753: INFO: Scaling statefulset ss to 0
Nov 16 02:42:15.843: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:42:15.856: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:42:15.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1678" for this suite.

• [SLOW TEST:20.483 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":186,"skipped":3574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:42:15.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:42:16.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2682" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":187,"skipped":3596,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:42:16.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8649
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8649
STEP: Waiting until pod test-pod will start running in namespace statefulset-8649
STEP: Creating statefulset with conflicting port in namespace statefulset-8649
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8649
Nov 16 02:42:18.608: INFO: Observed stateful pod in namespace: statefulset-8649, name: ss-0, uid: fbae027f-254f-4c91-920c-b23c2b447461, status phase: Pending. Waiting for statefulset controller to delete.
Nov 16 02:42:18.640: INFO: Observed stateful pod in namespace: statefulset-8649, name: ss-0, uid: fbae027f-254f-4c91-920c-b23c2b447461, status phase: Failed. Waiting for statefulset controller to delete.
Nov 16 02:42:18.663: INFO: Observed stateful pod in namespace: statefulset-8649, name: ss-0, uid: fbae027f-254f-4c91-920c-b23c2b447461, status phase: Failed. Waiting for statefulset controller to delete.
Nov 16 02:42:18.679: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8649
STEP: Removing pod with conflicting port in namespace statefulset-8649
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8649 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 02:42:24.809: INFO: Deleting all statefulset in ns statefulset-8649
Nov 16 02:42:24.823: INFO: Scaling statefulset ss to 0
Nov 16 02:42:34.903: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:42:34.913: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:42:34.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8649" for this suite.

• [SLOW TEST:18.743 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":188,"skipped":3605,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:42:35.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
Nov 16 02:42:35.197: INFO: Waiting up to 5m0s for pod "var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9" in namespace "var-expansion-3112" to be "Succeeded or Failed"
Nov 16 02:42:35.226: INFO: Pod "var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.977427ms
Nov 16 02:42:37.244: INFO: Pod "var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046973082s
Nov 16 02:42:39.281: INFO: Pod "var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083934248s
STEP: Saw pod success
Nov 16 02:42:39.281: INFO: Pod "var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9" satisfied condition "Succeeded or Failed"
Nov 16 02:42:39.308: INFO: Trying to get logs from node 10.189.71.151 pod var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9 container dapi-container: <nil>
STEP: delete the pod
Nov 16 02:42:39.382: INFO: Waiting for pod var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9 to disappear
Nov 16 02:42:39.395: INFO: Pod var-expansion-6f44591b-f72f-41bf-b61d-b95f0833d7c9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:42:39.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3112" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":189,"skipped":3626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:42:39.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-576490c4-6fef-4a91-ad0e-f38f0bb9d55e
STEP: Creating a pod to test consume secrets
Nov 16 02:42:39.674: INFO: Waiting up to 5m0s for pod "pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829" in namespace "secrets-2102" to be "Succeeded or Failed"
Nov 16 02:42:39.692: INFO: Pod "pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829": Phase="Pending", Reason="", readiness=false. Elapsed: 18.354398ms
Nov 16 02:42:41.710: INFO: Pod "pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035475552s
Nov 16 02:42:43.724: INFO: Pod "pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049971079s
Nov 16 02:42:45.745: INFO: Pod "pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070684096s
STEP: Saw pod success
Nov 16 02:42:45.745: INFO: Pod "pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829" satisfied condition "Succeeded or Failed"
Nov 16 02:42:45.757: INFO: Trying to get logs from node 10.189.71.150 pod pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:42:45.848: INFO: Waiting for pod pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829 to disappear
Nov 16 02:42:45.859: INFO: Pod pod-secrets-bbf0e723-4c07-4048-87ff-c4cd089b1829 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:42:45.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2102" for this suite.

• [SLOW TEST:6.432 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:42:45.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:42:46.091: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 02:42:46.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:42:46.153: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:42:47.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:42:47.219: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:42:48.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 16 02:42:48.181: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:42:49.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 02:42:49.182: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 16 02:42:49.285: INFO: Wrong image for pod: daemon-set-56nw9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:49.285: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:49.285: INFO: Wrong image for pod: daemon-set-mstpj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:50.323: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:50.323: INFO: Wrong image for pod: daemon-set-mstpj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:51.342: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:51.342: INFO: Wrong image for pod: daemon-set-mstpj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:52.313: INFO: Pod daemon-set-hgdzl is not available
Nov 16 02:42:52.313: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:52.313: INFO: Wrong image for pod: daemon-set-mstpj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:53.317: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:54.312: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:55.314: INFO: Pod daemon-set-8jck4 is not available
Nov 16 02:42:55.314: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:56.314: INFO: Pod daemon-set-8jck4 is not available
Nov 16 02:42:56.314: INFO: Wrong image for pod: daemon-set-mr9xb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Nov 16 02:42:59.314: INFO: Pod daemon-set-92m89 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 16 02:42:59.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 02:42:59.401: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:43:00.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 02:43:00.436: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8024, will wait for the garbage collector to delete the pods
Nov 16 02:43:00.581: INFO: Deleting DaemonSet.extensions daemon-set took: 24.673332ms
Nov 16 02:43:00.682: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.605172ms
Nov 16 02:43:04.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:43:04.498: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 16 02:43:04.514: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"117198"},"items":null}

Nov 16 02:43:04.525: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"117198"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:43:04.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8024" for this suite.

• [SLOW TEST:18.712 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":191,"skipped":3675,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:43:04.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:43:04.777: INFO: created pod
Nov 16 02:43:04.777: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7652" to be "Succeeded or Failed"
Nov 16 02:43:04.790: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.929208ms
Nov 16 02:43:06.807: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029864132s
Nov 16 02:43:08.843: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065883112s
STEP: Saw pod success
Nov 16 02:43:08.843: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 16 02:43:38.845: INFO: polling logs
Nov 16 02:43:38.878: INFO: Pod logs: 
I1116 02:43:06.243318       1 log.go:195] OK: Got token
I1116 02:43:06.243573       1 log.go:195] validating with in-cluster discovery
I1116 02:43:06.244314       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I1116 02:43:06.244396       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7652:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1668567185, NotBefore:1668566585, IssuedAt:1668566585, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7652", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c2978667-3bab-4c9a-b265-5e4a1f87c66c"}}}
I1116 02:43:06.268904       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I1116 02:43:06.284686       1 log.go:195] OK: Validated signature on JWT
I1116 02:43:06.284817       1 log.go:195] OK: Got valid claims from token!
I1116 02:43:06.284875       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7652:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1668567185, NotBefore:1668566585, IssuedAt:1668566585, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7652", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c2978667-3bab-4c9a-b265-5e4a1f87c66c"}}}

Nov 16 02:43:38.878: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:43:38.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7652" for this suite.

• [SLOW TEST:34.330 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":192,"skipped":3687,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:43:38.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:43:43.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6671" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":193,"skipped":3688,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:43:43.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:43:54.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1502" for this suite.

• [SLOW TEST:11.566 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":194,"skipped":3708,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:43:54.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7470" for this suite.

• [SLOW TEST:11.426 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":195,"skipped":3713,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:06.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
Nov 16 02:44:06.456: INFO: Waiting up to 5m0s for pod "client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298" in namespace "containers-7567" to be "Succeeded or Failed"
Nov 16 02:44:06.473: INFO: Pod "client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298": Phase="Pending", Reason="", readiness=false. Elapsed: 17.374847ms
Nov 16 02:44:08.511: INFO: Pod "client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055477446s
Nov 16 02:44:10.531: INFO: Pod "client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075392039s
Nov 16 02:44:12.549: INFO: Pod "client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093679902s
STEP: Saw pod success
Nov 16 02:44:12.549: INFO: Pod "client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298" satisfied condition "Succeeded or Failed"
Nov 16 02:44:12.562: INFO: Trying to get logs from node 10.189.71.150 pod client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:44:12.704: INFO: Waiting for pod client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298 to disappear
Nov 16 02:44:12.739: INFO: Pod client-containers-2e718bb7-a838-4dc1-a2c2-f3171740a298 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:12.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7567" for this suite.

• [SLOW TEST:6.482 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3732,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:12.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
Nov 16 02:44:13.046: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Nov 16 02:44:15.150: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Nov 16 02:44:17.195: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:19.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8322" for this suite.

• [SLOW TEST:6.499 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":197,"skipped":3751,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:19.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Nov 16 02:44:19.461: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 16 02:44:24.476: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Nov 16 02:44:24.486: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-863" for this suite.

• [SLOW TEST:5.343 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":198,"skipped":3768,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:24.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:44:24.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073" in namespace "downward-api-774" to be "Succeeded or Failed"
Nov 16 02:44:24.789: INFO: Pod "downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073": Phase="Pending", Reason="", readiness=false. Elapsed: 11.563299ms
Nov 16 02:44:26.805: INFO: Pod "downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028021519s
Nov 16 02:44:28.821: INFO: Pod "downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043350316s
Nov 16 02:44:30.840: INFO: Pod "downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062519802s
STEP: Saw pod success
Nov 16 02:44:30.840: INFO: Pod "downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073" satisfied condition "Succeeded or Failed"
Nov 16 02:44:30.855: INFO: Trying to get logs from node 10.189.71.150 pod downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073 container client-container: <nil>
STEP: delete the pod
Nov 16 02:44:30.921: INFO: Waiting for pod downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073 to disappear
Nov 16 02:44:30.933: INFO: Pod downwardapi-volume-5b9da76d-4c82-49ac-aae9-b8af867f9073 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:30.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-774" for this suite.

• [SLOW TEST:6.362 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3774,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:30.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:44:31.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 02:44:31.219: INFO: The status of Pod pod-exec-websocket-f5943459-d4a5-4bf0-bf7c-5d1d4e61d4fa is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:44:33.241: INFO: The status of Pod pod-exec-websocket-f5943459-d4a5-4bf0-bf7c-5d1d4e61d4fa is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:44:35.235: INFO: The status of Pod pod-exec-websocket-f5943459-d4a5-4bf0-bf7c-5d1d4e61d4fa is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:35.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4175" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3782,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:35.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Nov 16 02:44:35.589: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:44:37.605: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:44:39.610: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Nov 16 02:44:39.681: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:44:41.698: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 16 02:44:41.844: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 02:44:41.856: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 02:44:43.857: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 02:44:43.873: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 02:44:45.857: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 02:44:45.879: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:45.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9083" for this suite.

• [SLOW TEST:10.494 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3803,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:45.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:44:46.799: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 02:44:48.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 2, 44, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 44, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 44, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 44, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:44:51.894: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:44:51.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3949-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:44:55.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8481" for this suite.
STEP: Destroying namespace "webhook-8481-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.529 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":202,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:44:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Nov 16 02:44:55.649: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 02:45:00.664: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:45:00.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1362" for this suite.

• [SLOW TEST:5.317 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":203,"skipped":3844,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:45:00.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:45:01.831: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 16 02:45:01.837: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 16 02:45:01.837: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 16 02:45:01.837: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 16 02:45:01.837: INFO: Checking APIGroup: apps
Nov 16 02:45:01.841: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 16 02:45:01.841: INFO: Versions found [{apps/v1 v1}]
Nov 16 02:45:01.841: INFO: apps/v1 matches apps/v1
Nov 16 02:45:01.841: INFO: Checking APIGroup: events.k8s.io
Nov 16 02:45:01.846: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 16 02:45:01.846: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:01.846: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 16 02:45:01.846: INFO: Checking APIGroup: authentication.k8s.io
Nov 16 02:45:01.851: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 16 02:45:01.851: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 16 02:45:01.852: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 16 02:45:01.852: INFO: Checking APIGroup: authorization.k8s.io
Nov 16 02:45:01.882: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 16 02:45:01.882: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 16 02:45:01.882: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 16 02:45:01.882: INFO: Checking APIGroup: autoscaling
Nov 16 02:45:01.893: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Nov 16 02:45:01.893: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 16 02:45:01.893: INFO: autoscaling/v2 matches autoscaling/v2
Nov 16 02:45:01.893: INFO: Checking APIGroup: batch
Nov 16 02:45:01.898: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 16 02:45:01.898: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 16 02:45:01.898: INFO: batch/v1 matches batch/v1
Nov 16 02:45:01.898: INFO: Checking APIGroup: certificates.k8s.io
Nov 16 02:45:01.903: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 16 02:45:01.903: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 16 02:45:01.903: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 16 02:45:01.903: INFO: Checking APIGroup: networking.k8s.io
Nov 16 02:45:01.923: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 16 02:45:01.923: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 16 02:45:01.923: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 16 02:45:01.923: INFO: Checking APIGroup: policy
Nov 16 02:45:01.948: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 16 02:45:01.948: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Nov 16 02:45:01.948: INFO: policy/v1 matches policy/v1
Nov 16 02:45:01.948: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 16 02:45:01.957: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 16 02:45:01.957: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 16 02:45:01.957: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 16 02:45:01.957: INFO: Checking APIGroup: storage.k8s.io
Nov 16 02:45:01.968: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 16 02:45:01.968: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:01.968: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 16 02:45:01.968: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 16 02:45:01.978: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 16 02:45:01.978: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 16 02:45:01.979: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 16 02:45:01.979: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 16 02:45:01.991: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 16 02:45:01.991: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 16 02:45:01.991: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 16 02:45:01.991: INFO: Checking APIGroup: scheduling.k8s.io
Nov 16 02:45:01.995: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 16 02:45:01.995: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 16 02:45:01.995: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 16 02:45:01.995: INFO: Checking APIGroup: coordination.k8s.io
Nov 16 02:45:01.999: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 16 02:45:01.999: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 16 02:45:01.999: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 16 02:45:01.999: INFO: Checking APIGroup: node.k8s.io
Nov 16 02:45:02.004: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 16 02:45:02.004: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:02.004: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 16 02:45:02.004: INFO: Checking APIGroup: discovery.k8s.io
Nov 16 02:45:02.016: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 16 02:45:02.016: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:02.016: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 16 02:45:02.016: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 16 02:45:02.031: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Nov 16 02:45:02.031: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:02.031: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Nov 16 02:45:02.031: INFO: Checking APIGroup: apps.openshift.io
Nov 16 02:45:02.042: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Nov 16 02:45:02.042: INFO: Versions found [{apps.openshift.io/v1 v1}]
Nov 16 02:45:02.042: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Nov 16 02:45:02.042: INFO: Checking APIGroup: authorization.openshift.io
Nov 16 02:45:02.046: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Nov 16 02:45:02.046: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Nov 16 02:45:02.046: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Nov 16 02:45:02.046: INFO: Checking APIGroup: build.openshift.io
Nov 16 02:45:02.051: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Nov 16 02:45:02.051: INFO: Versions found [{build.openshift.io/v1 v1}]
Nov 16 02:45:02.051: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Nov 16 02:45:02.051: INFO: Checking APIGroup: image.openshift.io
Nov 16 02:45:02.055: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Nov 16 02:45:02.055: INFO: Versions found [{image.openshift.io/v1 v1}]
Nov 16 02:45:02.055: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Nov 16 02:45:02.055: INFO: Checking APIGroup: oauth.openshift.io
Nov 16 02:45:02.059: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Nov 16 02:45:02.059: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Nov 16 02:45:02.059: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Nov 16 02:45:02.059: INFO: Checking APIGroup: project.openshift.io
Nov 16 02:45:02.071: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Nov 16 02:45:02.071: INFO: Versions found [{project.openshift.io/v1 v1}]
Nov 16 02:45:02.071: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Nov 16 02:45:02.071: INFO: Checking APIGroup: quota.openshift.io
Nov 16 02:45:02.076: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Nov 16 02:45:02.076: INFO: Versions found [{quota.openshift.io/v1 v1}]
Nov 16 02:45:02.076: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Nov 16 02:45:02.076: INFO: Checking APIGroup: route.openshift.io
Nov 16 02:45:02.082: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Nov 16 02:45:02.082: INFO: Versions found [{route.openshift.io/v1 v1}]
Nov 16 02:45:02.082: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Nov 16 02:45:02.082: INFO: Checking APIGroup: security.openshift.io
Nov 16 02:45:02.086: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Nov 16 02:45:02.086: INFO: Versions found [{security.openshift.io/v1 v1}]
Nov 16 02:45:02.086: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Nov 16 02:45:02.086: INFO: Checking APIGroup: template.openshift.io
Nov 16 02:45:02.091: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Nov 16 02:45:02.091: INFO: Versions found [{template.openshift.io/v1 v1}]
Nov 16 02:45:02.091: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Nov 16 02:45:02.091: INFO: Checking APIGroup: user.openshift.io
Nov 16 02:45:02.095: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Nov 16 02:45:02.095: INFO: Versions found [{user.openshift.io/v1 v1}]
Nov 16 02:45:02.095: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Nov 16 02:45:02.095: INFO: Checking APIGroup: packages.operators.coreos.com
Nov 16 02:45:02.101: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Nov 16 02:45:02.101: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Nov 16 02:45:02.101: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Nov 16 02:45:02.101: INFO: Checking APIGroup: config.openshift.io
Nov 16 02:45:02.117: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Nov 16 02:45:02.117: INFO: Versions found [{config.openshift.io/v1 v1}]
Nov 16 02:45:02.117: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Nov 16 02:45:02.117: INFO: Checking APIGroup: operator.openshift.io
Nov 16 02:45:02.123: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Nov 16 02:45:02.123: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Nov 16 02:45:02.123: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Nov 16 02:45:02.123: INFO: Checking APIGroup: apiserver.openshift.io
Nov 16 02:45:02.127: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Nov 16 02:45:02.127: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Nov 16 02:45:02.127: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Nov 16 02:45:02.127: INFO: Checking APIGroup: cloudcredential.openshift.io
Nov 16 02:45:02.131: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Nov 16 02:45:02.131: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Nov 16 02:45:02.131: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Nov 16 02:45:02.131: INFO: Checking APIGroup: console.openshift.io
Nov 16 02:45:02.136: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Nov 16 02:45:02.136: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Nov 16 02:45:02.136: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Nov 16 02:45:02.136: INFO: Checking APIGroup: crd.projectcalico.org
Nov 16 02:45:02.142: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 16 02:45:02.142: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 16 02:45:02.142: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 16 02:45:02.142: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Nov 16 02:45:02.148: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Nov 16 02:45:02.148: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Nov 16 02:45:02.148: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Nov 16 02:45:02.148: INFO: Checking APIGroup: ingress.operator.openshift.io
Nov 16 02:45:02.164: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Nov 16 02:45:02.164: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Nov 16 02:45:02.164: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Nov 16 02:45:02.164: INFO: Checking APIGroup: k8s.cni.cncf.io
Nov 16 02:45:02.195: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Nov 16 02:45:02.195: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Nov 16 02:45:02.195: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Nov 16 02:45:02.195: INFO: Checking APIGroup: machineconfiguration.openshift.io
Nov 16 02:45:02.211: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Nov 16 02:45:02.211: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Nov 16 02:45:02.211: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Nov 16 02:45:02.211: INFO: Checking APIGroup: monitoring.coreos.com
Nov 16 02:45:02.215: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Nov 16 02:45:02.215: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Nov 16 02:45:02.215: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Nov 16 02:45:02.215: INFO: Checking APIGroup: network.operator.openshift.io
Nov 16 02:45:02.220: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Nov 16 02:45:02.220: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Nov 16 02:45:02.220: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Nov 16 02:45:02.220: INFO: Checking APIGroup: operator.tigera.io
Nov 16 02:45:02.233: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Nov 16 02:45:02.233: INFO: Versions found [{operator.tigera.io/v1 v1}]
Nov 16 02:45:02.233: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Nov 16 02:45:02.233: INFO: Checking APIGroup: operators.coreos.com
Nov 16 02:45:02.246: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Nov 16 02:45:02.246: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Nov 16 02:45:02.246: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Nov 16 02:45:02.246: INFO: Checking APIGroup: samples.operator.openshift.io
Nov 16 02:45:02.258: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Nov 16 02:45:02.258: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Nov 16 02:45:02.258: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Nov 16 02:45:02.258: INFO: Checking APIGroup: security.internal.openshift.io
Nov 16 02:45:02.294: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Nov 16 02:45:02.294: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Nov 16 02:45:02.294: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Nov 16 02:45:02.295: INFO: Checking APIGroup: snapshot.storage.k8s.io
Nov 16 02:45:02.330: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Nov 16 02:45:02.330: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:02.330: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Nov 16 02:45:02.330: INFO: Checking APIGroup: tuned.openshift.io
Nov 16 02:45:02.336: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Nov 16 02:45:02.336: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Nov 16 02:45:02.336: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Nov 16 02:45:02.336: INFO: Checking APIGroup: controlplane.operator.openshift.io
Nov 16 02:45:02.358: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Nov 16 02:45:02.358: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Nov 16 02:45:02.358: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Nov 16 02:45:02.358: INFO: Checking APIGroup: ibm.com
Nov 16 02:45:02.370: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Nov 16 02:45:02.370: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Nov 16 02:45:02.370: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Nov 16 02:45:02.370: INFO: Checking APIGroup: migration.k8s.io
Nov 16 02:45:02.390: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Nov 16 02:45:02.390: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Nov 16 02:45:02.390: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Nov 16 02:45:02.390: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Nov 16 02:45:02.395: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Nov 16 02:45:02.396: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Nov 16 02:45:02.396: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Nov 16 02:45:02.396: INFO: Checking APIGroup: helm.openshift.io
Nov 16 02:45:02.401: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Nov 16 02:45:02.401: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Nov 16 02:45:02.401: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Nov 16 02:45:02.401: INFO: Checking APIGroup: metrics.k8s.io
Nov 16 02:45:02.436: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 16 02:45:02.436: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 16 02:45:02.436: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:45:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-426" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":204,"skipped":3852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:45:02.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 02:45:02.745: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 16 02:45:02.807: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 02:45:02.891: INFO: waiting for watch events with expected annotations
Nov 16 02:45:02.891: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:45:03.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4307" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":205,"skipped":3908,"failed":0}
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:45:03.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 16 02:45:03.338: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 02:46:03.508: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:46:03.534: INFO: Starting informer...
STEP: Starting pods...
Nov 16 02:46:03.810: INFO: Pod1 is running on 10.189.71.151. Tainting Node
Nov 16 02:46:06.127: INFO: Pod2 is running on 10.189.71.151. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 16 02:46:15.243: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 16 02:46:32.449: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:46:32.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8688" for this suite.

• [SLOW TEST:89.369 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":206,"skipped":3909,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:46:32.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9393
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:46:32.824: INFO: Found 0 stateful pods, waiting for 1
Nov 16 02:46:42.852: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Nov 16 02:46:42.931: INFO: Found 1 stateful pods, waiting for 2
Nov 16 02:46:52.947: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Nov 16 02:47:02.949: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 02:47:02.949: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 02:47:03.017: INFO: Deleting all statefulset in ns statefulset-9393
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:47:03.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9393" for this suite.

• [SLOW TEST:30.515 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":207,"skipped":3930,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:47:03.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:47:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8083
I1116 02:47:03.254355      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8083, replica count: 1
I1116 02:47:04.309777      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 02:47:05.310338      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 02:47:06.311028      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 02:47:06.448: INFO: Created: latency-svc-xsmm5
Nov 16 02:47:06.505: INFO: Got endpoints: latency-svc-xsmm5 [93.77912ms]
Nov 16 02:47:06.543: INFO: Created: latency-svc-7tsc6
Nov 16 02:47:06.570: INFO: Got endpoints: latency-svc-7tsc6 [64.944791ms]
Nov 16 02:47:06.571: INFO: Created: latency-svc-sdmsw
Nov 16 02:47:06.600: INFO: Created: latency-svc-r7n2n
Nov 16 02:47:06.624: INFO: Got endpoints: latency-svc-r7n2n [118.506865ms]
Nov 16 02:47:06.624: INFO: Got endpoints: latency-svc-sdmsw [118.756754ms]
Nov 16 02:47:06.633: INFO: Created: latency-svc-x6dpx
Nov 16 02:47:06.660: INFO: Created: latency-svc-zrt5m
Nov 16 02:47:06.667: INFO: Got endpoints: latency-svc-x6dpx [161.405382ms]
Nov 16 02:47:06.685: INFO: Got endpoints: latency-svc-zrt5m [179.68026ms]
Nov 16 02:47:06.702: INFO: Created: latency-svc-kkpqk
Nov 16 02:47:06.711: INFO: Created: latency-svc-k7c6f
Nov 16 02:47:06.721: INFO: Got endpoints: latency-svc-kkpqk [215.080214ms]
Nov 16 02:47:06.731: INFO: Created: latency-svc-t46t9
Nov 16 02:47:06.738: INFO: Got endpoints: latency-svc-k7c6f [231.55258ms]
Nov 16 02:47:06.758: INFO: Got endpoints: latency-svc-t46t9 [252.111763ms]
Nov 16 02:47:06.764: INFO: Created: latency-svc-96ltk
Nov 16 02:47:06.783: INFO: Created: latency-svc-fbqq5
Nov 16 02:47:06.801: INFO: Created: latency-svc-hg7dz
Nov 16 02:47:06.812: INFO: Got endpoints: latency-svc-96ltk [305.201356ms]
Nov 16 02:47:06.815: INFO: Got endpoints: latency-svc-fbqq5 [308.863468ms]
Nov 16 02:47:06.823: INFO: Created: latency-svc-788qf
Nov 16 02:47:06.846: INFO: Created: latency-svc-jc5zk
Nov 16 02:47:06.860: INFO: Got endpoints: latency-svc-788qf [353.229023ms]
Nov 16 02:47:06.863: INFO: Got endpoints: latency-svc-hg7dz [356.665712ms]
Nov 16 02:47:06.870: INFO: Got endpoints: latency-svc-jc5zk [363.393439ms]
Nov 16 02:47:06.871: INFO: Created: latency-svc-crfn2
Nov 16 02:47:06.927: INFO: Created: latency-svc-swh7m
Nov 16 02:47:06.930: INFO: Got endpoints: latency-svc-crfn2 [423.250856ms]
Nov 16 02:47:06.950: INFO: Created: latency-svc-rk82l
Nov 16 02:47:06.957: INFO: Got endpoints: latency-svc-swh7m [450.174473ms]
Nov 16 02:47:06.973: INFO: Created: latency-svc-sjrgn
Nov 16 02:47:06.975: INFO: Got endpoints: latency-svc-rk82l [404.82186ms]
Nov 16 02:47:06.995: INFO: Got endpoints: latency-svc-sjrgn [369.733768ms]
Nov 16 02:47:07.341: INFO: Created: latency-svc-kbgjv
Nov 16 02:47:07.345: INFO: Created: latency-svc-mdlsq
Nov 16 02:47:07.346: INFO: Created: latency-svc-628cm
Nov 16 02:47:07.347: INFO: Created: latency-svc-mpt2s
Nov 16 02:47:07.347: INFO: Created: latency-svc-2fbkq
Nov 16 02:47:07.349: INFO: Created: latency-svc-2mtmv
Nov 16 02:47:07.349: INFO: Created: latency-svc-726zx
Nov 16 02:47:07.350: INFO: Created: latency-svc-tjwrm
Nov 16 02:47:07.350: INFO: Created: latency-svc-qwdfk
Nov 16 02:47:07.350: INFO: Created: latency-svc-mpmwt
Nov 16 02:47:07.350: INFO: Created: latency-svc-nm45c
Nov 16 02:47:07.350: INFO: Created: latency-svc-98bcg
Nov 16 02:47:07.351: INFO: Created: latency-svc-5t9bq
Nov 16 02:47:07.351: INFO: Created: latency-svc-8cqzd
Nov 16 02:47:07.351: INFO: Created: latency-svc-gq7nt
Nov 16 02:47:07.381: INFO: Got endpoints: latency-svc-kbgjv [643.20078ms]
Nov 16 02:47:07.387: INFO: Got endpoints: latency-svc-mdlsq [524.039252ms]
Nov 16 02:47:07.387: INFO: Got endpoints: latency-svc-628cm [527.54953ms]
Nov 16 02:47:07.394: INFO: Got endpoints: latency-svc-mpt2s [726.140205ms]
Nov 16 02:47:07.433: INFO: Got endpoints: latency-svc-98bcg [674.938938ms]
Nov 16 02:47:07.434: INFO: Got endpoints: latency-svc-2fbkq [458.458808ms]
Nov 16 02:47:07.439: INFO: Got endpoints: latency-svc-tjwrm [569.105817ms]
Nov 16 02:47:07.439: INFO: Got endpoints: latency-svc-2mtmv [624.210294ms]
Nov 16 02:47:07.439: INFO: Got endpoints: latency-svc-nm45c [509.301543ms]
Nov 16 02:47:07.464: INFO: Got endpoints: latency-svc-5t9bq [778.794797ms]
Nov 16 02:47:07.467: INFO: Got endpoints: latency-svc-mpmwt [745.496854ms]
Nov 16 02:47:07.467: INFO: Got endpoints: latency-svc-gq7nt [842.639491ms]
Nov 16 02:47:07.468: INFO: Got endpoints: latency-svc-8cqzd [655.976459ms]
Nov 16 02:47:07.500: INFO: Got endpoints: latency-svc-726zx [543.298126ms]
Nov 16 02:47:07.501: INFO: Got endpoints: latency-svc-qwdfk [506.464095ms]
Nov 16 02:47:07.508: INFO: Created: latency-svc-rxx7k
Nov 16 02:47:07.522: INFO: Created: latency-svc-knksl
Nov 16 02:47:07.524: INFO: Got endpoints: latency-svc-rxx7k [143.12269ms]
Nov 16 02:47:07.536: INFO: Created: latency-svc-bql25
Nov 16 02:47:07.547: INFO: Got endpoints: latency-svc-knksl [159.466679ms]
Nov 16 02:47:07.588: INFO: Got endpoints: latency-svc-bql25 [200.570343ms]
Nov 16 02:47:07.604: INFO: Created: latency-svc-mtzkb
Nov 16 02:47:07.639: INFO: Created: latency-svc-lgcvl
Nov 16 02:47:07.650: INFO: Got endpoints: latency-svc-mtzkb [256.529587ms]
Nov 16 02:47:07.702: INFO: Got endpoints: latency-svc-lgcvl [268.736465ms]
Nov 16 02:47:07.719: INFO: Created: latency-svc-8xxlv
Nov 16 02:47:07.733: INFO: Created: latency-svc-zhqk4
Nov 16 02:47:07.756: INFO: Created: latency-svc-7w9jw
Nov 16 02:47:07.765: INFO: Got endpoints: latency-svc-zhqk4 [325.037528ms]
Nov 16 02:47:07.766: INFO: Got endpoints: latency-svc-8xxlv [332.519353ms]
Nov 16 02:47:07.778: INFO: Got endpoints: latency-svc-7w9jw [338.710273ms]
Nov 16 02:47:07.782: INFO: Created: latency-svc-bsbf9
Nov 16 02:47:07.800: INFO: Got endpoints: latency-svc-bsbf9 [360.922805ms]
Nov 16 02:47:07.801: INFO: Created: latency-svc-s5mmg
Nov 16 02:47:07.820: INFO: Created: latency-svc-xl4nc
Nov 16 02:47:07.828: INFO: Got endpoints: latency-svc-s5mmg [363.356888ms]
Nov 16 02:47:07.846: INFO: Created: latency-svc-97tqh
Nov 16 02:47:07.846: INFO: Got endpoints: latency-svc-xl4nc [379.492123ms]
Nov 16 02:47:07.873: INFO: Got endpoints: latency-svc-97tqh [404.757547ms]
Nov 16 02:47:07.892: INFO: Created: latency-svc-b4rkt
Nov 16 02:47:07.903: INFO: Created: latency-svc-88hzm
Nov 16 02:47:07.905: INFO: Got endpoints: latency-svc-b4rkt [437.842277ms]
Nov 16 02:47:07.972: INFO: Got endpoints: latency-svc-88hzm [470.790865ms]
Nov 16 02:47:07.982: INFO: Created: latency-svc-rwdjh
Nov 16 02:47:08.022: INFO: Created: latency-svc-xsmvz
Nov 16 02:47:08.024: INFO: Got endpoints: latency-svc-rwdjh [523.862027ms]
Nov 16 02:47:08.054: INFO: Got endpoints: latency-svc-xsmvz [530.20715ms]
Nov 16 02:47:08.055: INFO: Created: latency-svc-4lvcw
Nov 16 02:47:08.074: INFO: Created: latency-svc-hq2cz
Nov 16 02:47:08.075: INFO: Got endpoints: latency-svc-4lvcw [528.3766ms]
Nov 16 02:47:08.126: INFO: Got endpoints: latency-svc-hq2cz [538.207693ms]
Nov 16 02:47:08.132: INFO: Created: latency-svc-n6rxr
Nov 16 02:47:08.151: INFO: Created: latency-svc-zfd7c
Nov 16 02:47:08.162: INFO: Got endpoints: latency-svc-n6rxr [511.842552ms]
Nov 16 02:47:08.189: INFO: Got endpoints: latency-svc-zfd7c [486.796382ms]
Nov 16 02:47:08.198: INFO: Created: latency-svc-wdwvc
Nov 16 02:47:08.289: INFO: Created: latency-svc-zv4jg
Nov 16 02:47:08.294: INFO: Created: latency-svc-48zj4
Nov 16 02:47:08.294: INFO: Got endpoints: latency-svc-zv4jg [527.275487ms]
Nov 16 02:47:08.294: INFO: Got endpoints: latency-svc-wdwvc [528.296836ms]
Nov 16 02:47:08.326: INFO: Created: latency-svc-47mvv
Nov 16 02:47:08.350: INFO: Created: latency-svc-mj5d5
Nov 16 02:47:08.365: INFO: Got endpoints: latency-svc-47mvv [564.183071ms]
Nov 16 02:47:08.369: INFO: Got endpoints: latency-svc-48zj4 [590.945955ms]
Nov 16 02:47:08.372: INFO: Got endpoints: latency-svc-mj5d5 [544.433712ms]
Nov 16 02:47:08.372: INFO: Created: latency-svc-p8xpf
Nov 16 02:47:08.390: INFO: Got endpoints: latency-svc-p8xpf [543.4275ms]
Nov 16 02:47:08.392: INFO: Created: latency-svc-tqrx4
Nov 16 02:47:08.420: INFO: Got endpoints: latency-svc-tqrx4 [546.967402ms]
Nov 16 02:47:08.421: INFO: Created: latency-svc-njdrk
Nov 16 02:47:08.444: INFO: Got endpoints: latency-svc-njdrk [538.478606ms]
Nov 16 02:47:08.445: INFO: Created: latency-svc-962vz
Nov 16 02:47:08.462: INFO: Created: latency-svc-g5fsp
Nov 16 02:47:08.487: INFO: Got endpoints: latency-svc-g5fsp [462.710353ms]
Nov 16 02:47:08.487: INFO: Got endpoints: latency-svc-962vz [515.333677ms]
Nov 16 02:47:08.495: INFO: Created: latency-svc-wf4bx
Nov 16 02:47:08.545: INFO: Got endpoints: latency-svc-wf4bx [490.115691ms]
Nov 16 02:47:08.551: INFO: Created: latency-svc-wklcv
Nov 16 02:47:08.571: INFO: Got endpoints: latency-svc-wklcv [495.465405ms]
Nov 16 02:47:08.579: INFO: Created: latency-svc-9cs2g
Nov 16 02:47:08.661: INFO: Got endpoints: latency-svc-9cs2g [535.000259ms]
Nov 16 02:47:08.689: INFO: Created: latency-svc-j2vkt
Nov 16 02:47:08.699: INFO: Got endpoints: latency-svc-j2vkt [536.780111ms]
Nov 16 02:47:08.733: INFO: Created: latency-svc-25wvt
Nov 16 02:47:08.740: INFO: Got endpoints: latency-svc-25wvt [551.215721ms]
Nov 16 02:47:08.750: INFO: Created: latency-svc-g5h28
Nov 16 02:47:08.772: INFO: Created: latency-svc-dzxl2
Nov 16 02:47:08.774: INFO: Got endpoints: latency-svc-g5h28 [480.50509ms]
Nov 16 02:47:08.785: INFO: Created: latency-svc-fxqh2
Nov 16 02:47:08.787: INFO: Got endpoints: latency-svc-dzxl2 [493.123108ms]
Nov 16 02:47:08.808: INFO: Created: latency-svc-xm46t
Nov 16 02:47:08.812: INFO: Got endpoints: latency-svc-fxqh2 [447.352936ms]
Nov 16 02:47:08.830: INFO: Created: latency-svc-bfjtn
Nov 16 02:47:08.830: INFO: Got endpoints: latency-svc-xm46t [460.604195ms]
Nov 16 02:47:08.845: INFO: Got endpoints: latency-svc-bfjtn [472.300228ms]
Nov 16 02:47:08.856: INFO: Created: latency-svc-xf85c
Nov 16 02:47:08.876: INFO: Created: latency-svc-jk77g
Nov 16 02:47:08.877: INFO: Got endpoints: latency-svc-xf85c [487.192872ms]
Nov 16 02:47:08.891: INFO: Got endpoints: latency-svc-jk77g [471.714293ms]
Nov 16 02:47:08.892: INFO: Created: latency-svc-c5wqp
Nov 16 02:47:08.913: INFO: Created: latency-svc-526br
Nov 16 02:47:08.925: INFO: Got endpoints: latency-svc-c5wqp [480.973726ms]
Nov 16 02:47:08.934: INFO: Got endpoints: latency-svc-526br [447.020047ms]
Nov 16 02:47:08.935: INFO: Created: latency-svc-lf6gf
Nov 16 02:47:08.959: INFO: Created: latency-svc-krhg9
Nov 16 02:47:08.979: INFO: Got endpoints: latency-svc-lf6gf [491.695582ms]
Nov 16 02:47:08.980: INFO: Got endpoints: latency-svc-krhg9 [435.315427ms]
Nov 16 02:47:08.984: INFO: Created: latency-svc-xlgpn
Nov 16 02:47:09.003: INFO: Created: latency-svc-wmplj
Nov 16 02:47:09.047: INFO: Got endpoints: latency-svc-wmplj [385.443387ms]
Nov 16 02:47:09.047: INFO: Got endpoints: latency-svc-xlgpn [475.947639ms]
Nov 16 02:47:09.050: INFO: Created: latency-svc-4jndt
Nov 16 02:47:09.071: INFO: Got endpoints: latency-svc-4jndt [370.4264ms]
Nov 16 02:47:09.084: INFO: Created: latency-svc-kqq77
Nov 16 02:47:09.113: INFO: Got endpoints: latency-svc-kqq77 [372.303297ms]
Nov 16 02:47:09.152: INFO: Created: latency-svc-gglxc
Nov 16 02:47:09.152: INFO: Created: latency-svc-lnxcg
Nov 16 02:47:09.184: INFO: Created: latency-svc-wk4mk
Nov 16 02:47:09.191: INFO: Got endpoints: latency-svc-gglxc [403.559847ms]
Nov 16 02:47:09.197: INFO: Got endpoints: latency-svc-lnxcg [422.365911ms]
Nov 16 02:47:09.212: INFO: Got endpoints: latency-svc-wk4mk [399.791623ms]
Nov 16 02:47:09.229: INFO: Created: latency-svc-swdh5
Nov 16 02:47:09.264: INFO: Created: latency-svc-nm68p
Nov 16 02:47:09.283: INFO: Created: latency-svc-mtppg
Nov 16 02:47:09.284: INFO: Got endpoints: latency-svc-swdh5 [453.940279ms]
Nov 16 02:47:09.287: INFO: Got endpoints: latency-svc-nm68p [442.138249ms]
Nov 16 02:47:09.304: INFO: Created: latency-svc-pxclm
Nov 16 02:47:09.309: INFO: Got endpoints: latency-svc-mtppg [431.842045ms]
Nov 16 02:47:09.341: INFO: Created: latency-svc-w6d9p
Nov 16 02:47:09.350: INFO: Got endpoints: latency-svc-pxclm [458.512908ms]
Nov 16 02:47:09.366: INFO: Got endpoints: latency-svc-w6d9p [440.913951ms]
Nov 16 02:47:09.416: INFO: Created: latency-svc-kfxkg
Nov 16 02:47:09.425: INFO: Got endpoints: latency-svc-kfxkg [491.103308ms]
Nov 16 02:47:09.472: INFO: Created: latency-svc-6ghs4
Nov 16 02:47:09.495: INFO: Created: latency-svc-s7j85
Nov 16 02:47:09.505: INFO: Got endpoints: latency-svc-6ghs4 [525.722702ms]
Nov 16 02:47:09.513: INFO: Created: latency-svc-mkkjl
Nov 16 02:47:09.545: INFO: Got endpoints: latency-svc-s7j85 [564.955071ms]
Nov 16 02:47:09.548: INFO: Got endpoints: latency-svc-mkkjl [501.520224ms]
Nov 16 02:47:09.551: INFO: Created: latency-svc-lhhrk
Nov 16 02:47:09.573: INFO: Created: latency-svc-4v8nh
Nov 16 02:47:09.594: INFO: Got endpoints: latency-svc-lhhrk [546.667495ms]
Nov 16 02:47:09.596: INFO: Got endpoints: latency-svc-4v8nh [525.129466ms]
Nov 16 02:47:09.597: INFO: Created: latency-svc-6sld7
Nov 16 02:47:09.624: INFO: Created: latency-svc-rscvt
Nov 16 02:47:09.641: INFO: Got endpoints: latency-svc-6sld7 [528.410097ms]
Nov 16 02:47:09.645: INFO: Created: latency-svc-qvqcz
Nov 16 02:47:09.645: INFO: Got endpoints: latency-svc-rscvt [454.511118ms]
Nov 16 02:47:09.682: INFO: Got endpoints: latency-svc-qvqcz [485.302527ms]
Nov 16 02:47:09.692: INFO: Created: latency-svc-cgl42
Nov 16 02:47:09.713: INFO: Got endpoints: latency-svc-cgl42 [501.029023ms]
Nov 16 02:47:09.715: INFO: Created: latency-svc-dwtbl
Nov 16 02:47:09.784: INFO: Got endpoints: latency-svc-dwtbl [500.04057ms]
Nov 16 02:47:09.795: INFO: Created: latency-svc-md8js
Nov 16 02:47:09.819: INFO: Created: latency-svc-5r7rp
Nov 16 02:47:09.862: INFO: Created: latency-svc-zdf52
Nov 16 02:47:09.863: INFO: Got endpoints: latency-svc-5r7rp [553.81873ms]
Nov 16 02:47:09.865: INFO: Got endpoints: latency-svc-md8js [577.791417ms]
Nov 16 02:47:09.890: INFO: Got endpoints: latency-svc-zdf52 [539.662712ms]
Nov 16 02:47:09.906: INFO: Created: latency-svc-bdhdb
Nov 16 02:47:09.950: INFO: Got endpoints: latency-svc-bdhdb [584.616592ms]
Nov 16 02:47:09.954: INFO: Created: latency-svc-nhzpv
Nov 16 02:47:09.966: INFO: Created: latency-svc-xvkbt
Nov 16 02:47:09.989: INFO: Got endpoints: latency-svc-nhzpv [563.18485ms]
Nov 16 02:47:10.005: INFO: Created: latency-svc-6289f
Nov 16 02:47:10.054: INFO: Created: latency-svc-bcmjz
Nov 16 02:47:10.055: INFO: Got endpoints: latency-svc-6289f [509.409233ms]
Nov 16 02:47:10.075: INFO: Got endpoints: latency-svc-xvkbt [569.88963ms]
Nov 16 02:47:10.140: INFO: Got endpoints: latency-svc-bcmjz [591.117218ms]
Nov 16 02:47:10.167: INFO: Created: latency-svc-7465n
Nov 16 02:47:10.167: INFO: Created: latency-svc-cnxcs
Nov 16 02:47:10.167: INFO: Created: latency-svc-8x79p
Nov 16 02:47:10.237: INFO: Created: latency-svc-c8h9s
Nov 16 02:47:10.240: INFO: Got endpoints: latency-svc-cnxcs [643.943974ms]
Nov 16 02:47:10.243: INFO: Got endpoints: latency-svc-8x79p [601.953488ms]
Nov 16 02:47:10.244: INFO: Got endpoints: latency-svc-7465n [650.291864ms]
Nov 16 02:47:10.246: INFO: Created: latency-svc-gghwl
Nov 16 02:47:10.252: INFO: Got endpoints: latency-svc-c8h9s [570.513825ms]
Nov 16 02:47:10.279: INFO: Created: latency-svc-dsdbj
Nov 16 02:47:10.286: INFO: Got endpoints: latency-svc-gghwl [572.922866ms]
Nov 16 02:47:10.310: INFO: Got endpoints: latency-svc-dsdbj [525.728218ms]
Nov 16 02:47:10.311: INFO: Created: latency-svc-2zs8m
Nov 16 02:47:10.316: INFO: Created: latency-svc-s4qlg
Nov 16 02:47:10.316: INFO: Got endpoints: latency-svc-s4qlg [670.64337ms]
Nov 16 02:47:10.331: INFO: Created: latency-svc-q4l2l
Nov 16 02:47:10.331: INFO: Got endpoints: latency-svc-2zs8m [468.354724ms]
Nov 16 02:47:10.358: INFO: Got endpoints: latency-svc-q4l2l [493.160385ms]
Nov 16 02:47:10.417: INFO: Created: latency-svc-n2db8
Nov 16 02:47:10.449: INFO: Created: latency-svc-wtcd9
Nov 16 02:47:10.449: INFO: Got endpoints: latency-svc-wtcd9 [559.015394ms]
Nov 16 02:47:10.452: INFO: Created: latency-svc-mhkwq
Nov 16 02:47:10.453: INFO: Got endpoints: latency-svc-n2db8 [498.535576ms]
Nov 16 02:47:10.455: INFO: Got endpoints: latency-svc-mhkwq [466.161684ms]
Nov 16 02:47:10.460: INFO: Created: latency-svc-8hfrc
Nov 16 02:47:10.460: INFO: Got endpoints: latency-svc-8hfrc [405.577587ms]
Nov 16 02:47:10.462: INFO: Created: latency-svc-l8jl9
Nov 16 02:47:10.527: INFO: Created: latency-svc-h7phb
Nov 16 02:47:10.554: INFO: Created: latency-svc-hxcqf
Nov 16 02:47:10.554: INFO: Created: latency-svc-qgcwz
Nov 16 02:47:10.554: INFO: Got endpoints: latency-svc-h7phb [413.934367ms]
Nov 16 02:47:10.554: INFO: Got endpoints: latency-svc-l8jl9 [452.922537ms]
Nov 16 02:47:10.554: INFO: Created: latency-svc-p7pf7
Nov 16 02:47:10.581: INFO: Created: latency-svc-gjgcl
Nov 16 02:47:10.605: INFO: Got endpoints: latency-svc-hxcqf [361.341126ms]
Nov 16 02:47:10.610: INFO: Got endpoints: latency-svc-qgcwz [367.031641ms]
Nov 16 02:47:10.611: INFO: Got endpoints: latency-svc-p7pf7 [370.29923ms]
Nov 16 02:47:10.619: INFO: Got endpoints: latency-svc-gjgcl [366.795023ms]
Nov 16 02:47:10.770: INFO: Created: latency-svc-vjnbm
Nov 16 02:47:10.770: INFO: Created: latency-svc-frdvw
Nov 16 02:47:10.770: INFO: Got endpoints: latency-svc-vjnbm [460.189397ms]
Nov 16 02:47:10.770: INFO: Created: latency-svc-bgbgq
Nov 16 02:47:10.770: INFO: Got endpoints: latency-svc-bgbgq [454.066551ms]
Nov 16 02:47:10.770: INFO: Created: latency-svc-w2qkv
Nov 16 02:47:10.771: INFO: Got endpoints: latency-svc-w2qkv [439.196822ms]
Nov 16 02:47:10.770: INFO: Created: latency-svc-lh99w
Nov 16 02:47:10.771: INFO: Got endpoints: latency-svc-lh99w [485.149707ms]
Nov 16 02:47:10.842: INFO: Created: latency-svc-lk248
Nov 16 02:47:10.842: INFO: Got endpoints: latency-svc-frdvw [484.001331ms]
Nov 16 02:47:10.846: INFO: Created: latency-svc-dwx5t
Nov 16 02:47:10.846: INFO: Created: latency-svc-mvfpk
Nov 16 02:47:10.846: INFO: Got endpoints: latency-svc-mvfpk [397.483471ms]
Nov 16 02:47:10.867: INFO: Created: latency-svc-dxssd
Nov 16 02:47:10.867: INFO: Got endpoints: latency-svc-lk248 [412.460316ms]
Nov 16 02:47:10.882: INFO: Got endpoints: latency-svc-dwx5t [429.39233ms]
Nov 16 02:47:10.896: INFO: Created: latency-svc-9pjn7
Nov 16 02:47:10.914: INFO: Got endpoints: latency-svc-dxssd [453.772578ms]
Nov 16 02:47:10.946: INFO: Got endpoints: latency-svc-9pjn7 [392.381065ms]
Nov 16 02:47:10.947: INFO: Created: latency-svc-kjvqf
Nov 16 02:47:10.994: INFO: Got endpoints: latency-svc-kjvqf [439.812162ms]
Nov 16 02:47:10.994: INFO: Created: latency-svc-qqgbt
Nov 16 02:47:11.041: INFO: Created: latency-svc-6z52v
Nov 16 02:47:11.049: INFO: Got endpoints: latency-svc-qqgbt [444.056157ms]
Nov 16 02:47:11.100: INFO: Got endpoints: latency-svc-6z52v [480.600162ms]
Nov 16 02:47:11.134: INFO: Created: latency-svc-r92w8
Nov 16 02:47:11.134: INFO: Got endpoints: latency-svc-r92w8 [519.90766ms]
Nov 16 02:47:11.142: INFO: Created: latency-svc-7kz4c
Nov 16 02:47:11.166: INFO: Got endpoints: latency-svc-7kz4c [555.691343ms]
Nov 16 02:47:11.176: INFO: Created: latency-svc-2xdz8
Nov 16 02:47:11.178: INFO: Created: latency-svc-42zds
Nov 16 02:47:11.182: INFO: Got endpoints: latency-svc-2xdz8 [411.374252ms]
Nov 16 02:47:11.238: INFO: Got endpoints: latency-svc-42zds [467.035786ms]
Nov 16 02:47:11.259: INFO: Created: latency-svc-c8pzq
Nov 16 02:47:11.301: INFO: Got endpoints: latency-svc-c8pzq [530.521645ms]
Nov 16 02:47:11.302: INFO: Created: latency-svc-r5ggj
Nov 16 02:47:11.360: INFO: Got endpoints: latency-svc-r5ggj [582.557176ms]
Nov 16 02:47:11.361: INFO: Created: latency-svc-7f9dx
Nov 16 02:47:11.412: INFO: Got endpoints: latency-svc-7f9dx [570.170189ms]
Nov 16 02:47:11.442: INFO: Created: latency-svc-p7wfh
Nov 16 02:47:11.442: INFO: Created: latency-svc-kjk56
Nov 16 02:47:11.469: INFO: Created: latency-svc-nfv9h
Nov 16 02:47:11.498: INFO: Got endpoints: latency-svc-kjk56 [651.502657ms]
Nov 16 02:47:11.521: INFO: Got endpoints: latency-svc-p7wfh [640.385062ms]
Nov 16 02:47:11.522: INFO: Created: latency-svc-frj8m
Nov 16 02:47:11.551: INFO: Created: latency-svc-89vsb
Nov 16 02:47:11.594: INFO: Created: latency-svc-t9vb4
Nov 16 02:47:11.594: INFO: Got endpoints: latency-svc-89vsb [646.651275ms]
Nov 16 02:47:11.598: INFO: Got endpoints: latency-svc-frj8m [684.483997ms]
Nov 16 02:47:11.599: INFO: Got endpoints: latency-svc-nfv9h [716.586986ms]
Nov 16 02:47:11.633: INFO: Created: latency-svc-jq794
Nov 16 02:47:11.736: INFO: Got endpoints: latency-svc-jq794 [686.03812ms]
Nov 16 02:47:11.634: INFO: Created: latency-svc-5qphp
Nov 16 02:47:11.651: INFO: Got endpoints: latency-svc-t9vb4 [656.661406ms]
Nov 16 02:47:11.758: INFO: Created: latency-svc-gg7x5
Nov 16 02:47:11.758: INFO: Created: latency-svc-dvnmr
Nov 16 02:47:11.761: INFO: Got endpoints: latency-svc-5qphp [627.464428ms]
Nov 16 02:47:11.762: INFO: Got endpoints: latency-svc-gg7x5 [619.863648ms]
Nov 16 02:47:11.802: INFO: Got endpoints: latency-svc-dvnmr [635.907166ms]
Nov 16 02:47:11.803: INFO: Created: latency-svc-kfb9l
Nov 16 02:47:11.803: INFO: Got endpoints: latency-svc-kfb9l [621.501265ms]
Nov 16 02:47:11.833: INFO: Created: latency-svc-5w8j2
Nov 16 02:47:11.851: INFO: Created: latency-svc-pvdfj
Nov 16 02:47:11.884: INFO: Got endpoints: latency-svc-5w8j2 [645.967719ms]
Nov 16 02:47:11.941: INFO: Created: latency-svc-smp9l
Nov 16 02:47:11.941: INFO: Got endpoints: latency-svc-pvdfj [639.997681ms]
Nov 16 02:47:11.980: INFO: Got endpoints: latency-svc-smp9l [619.701421ms]
Nov 16 02:47:11.982: INFO: Created: latency-svc-ckg5g
Nov 16 02:47:12.007: INFO: Got endpoints: latency-svc-ckg5g [556.447638ms]
Nov 16 02:47:12.009: INFO: Created: latency-svc-7sq9b
Nov 16 02:47:12.039: INFO: Created: latency-svc-dsjgs
Nov 16 02:47:12.046: INFO: Got endpoints: latency-svc-7sq9b [548.439745ms]
Nov 16 02:47:12.081: INFO: Got endpoints: latency-svc-dsjgs [559.597977ms]
Nov 16 02:47:12.086: INFO: Created: latency-svc-xxwz5
Nov 16 02:47:12.108: INFO: Created: latency-svc-xvj5z
Nov 16 02:47:12.129: INFO: Got endpoints: latency-svc-xxwz5 [530.639333ms]
Nov 16 02:47:12.130: INFO: Got endpoints: latency-svc-xvj5z [536.341761ms]
Nov 16 02:47:12.135: INFO: Created: latency-svc-qql4w
Nov 16 02:47:12.184: INFO: Created: latency-svc-qr6mj
Nov 16 02:47:12.186: INFO: Got endpoints: latency-svc-qql4w [587.698789ms]
Nov 16 02:47:12.230: INFO: Got endpoints: latency-svc-qr6mj [479.63157ms]
Nov 16 02:47:12.232: INFO: Created: latency-svc-qfv2p
Nov 16 02:47:12.275: INFO: Got endpoints: latency-svc-qfv2p [523.728438ms]
Nov 16 02:47:12.291: INFO: Created: latency-svc-87xt4
Nov 16 02:47:12.316: INFO: Got endpoints: latency-svc-87xt4 [554.703322ms]
Nov 16 02:47:12.327: INFO: Created: latency-svc-z7ckg
Nov 16 02:47:12.350: INFO: Created: latency-svc-26mq2
Nov 16 02:47:12.352: INFO: Got endpoints: latency-svc-z7ckg [589.657834ms]
Nov 16 02:47:12.371: INFO: Got endpoints: latency-svc-26mq2 [569.165261ms]
Nov 16 02:47:12.375: INFO: Created: latency-svc-fgdf9
Nov 16 02:47:12.410: INFO: Created: latency-svc-rvqbv
Nov 16 02:47:12.415: INFO: Got endpoints: latency-svc-fgdf9 [611.431386ms]
Nov 16 02:47:12.433: INFO: Created: latency-svc-xbrp9
Nov 16 02:47:12.434: INFO: Got endpoints: latency-svc-rvqbv [550.188698ms]
Nov 16 02:47:12.453: INFO: Got endpoints: latency-svc-xbrp9 [511.142092ms]
Nov 16 02:47:12.463: INFO: Created: latency-svc-khpn4
Nov 16 02:47:12.485: INFO: Got endpoints: latency-svc-khpn4 [505.077064ms]
Nov 16 02:47:12.486: INFO: Created: latency-svc-4xntc
Nov 16 02:47:12.552: INFO: Got endpoints: latency-svc-4xntc [545.677815ms]
Nov 16 02:47:12.553: INFO: Created: latency-svc-9zf2r
Nov 16 02:47:12.575: INFO: Created: latency-svc-bwt6x
Nov 16 02:47:12.580: INFO: Got endpoints: latency-svc-9zf2r [533.850662ms]
Nov 16 02:47:12.601: INFO: Got endpoints: latency-svc-bwt6x [520.520514ms]
Nov 16 02:47:12.606: INFO: Created: latency-svc-5s8lj
Nov 16 02:47:12.634: INFO: Got endpoints: latency-svc-5s8lj [505.126194ms]
Nov 16 02:47:12.643: INFO: Created: latency-svc-7xflb
Nov 16 02:47:12.708: INFO: Created: latency-svc-xxps5
Nov 16 02:47:12.708: INFO: Created: latency-svc-22cbq
Nov 16 02:47:12.709: INFO: Got endpoints: latency-svc-22cbq [522.287677ms]
Nov 16 02:47:12.709: INFO: Got endpoints: latency-svc-7xflb [579.372926ms]
Nov 16 02:47:12.722: INFO: Got endpoints: latency-svc-xxps5 [492.450324ms]
Nov 16 02:47:12.778: INFO: Created: latency-svc-2dwz9
Nov 16 02:47:12.778: INFO: Created: latency-svc-rn5jw
Nov 16 02:47:12.783: INFO: Got endpoints: latency-svc-2dwz9 [508.568124ms]
Nov 16 02:47:12.800: INFO: Created: latency-svc-rr9tn
Nov 16 02:47:12.813: INFO: Got endpoints: latency-svc-rn5jw [496.781387ms]
Nov 16 02:47:12.826: INFO: Got endpoints: latency-svc-rr9tn [474.07125ms]
Nov 16 02:47:12.846: INFO: Created: latency-svc-ld68g
Nov 16 02:47:12.871: INFO: Got endpoints: latency-svc-ld68g [499.07814ms]
Nov 16 02:47:12.876: INFO: Created: latency-svc-hx2pq
Nov 16 02:47:12.897: INFO: Got endpoints: latency-svc-hx2pq [482.189633ms]
Nov 16 02:47:12.897: INFO: Created: latency-svc-5q2rw
Nov 16 02:47:12.933: INFO: Created: latency-svc-5cxn4
Nov 16 02:47:12.958: INFO: Got endpoints: latency-svc-5q2rw [523.74605ms]
Nov 16 02:47:12.964: INFO: Got endpoints: latency-svc-5cxn4 [510.925294ms]
Nov 16 02:47:12.966: INFO: Created: latency-svc-ns694
Nov 16 02:47:12.987: INFO: Got endpoints: latency-svc-ns694 [501.395046ms]
Nov 16 02:47:12.988: INFO: Created: latency-svc-trcct
Nov 16 02:47:13.006: INFO: Got endpoints: latency-svc-trcct [453.987428ms]
Nov 16 02:47:13.007: INFO: Created: latency-svc-dnwgj
Nov 16 02:47:13.040: INFO: Created: latency-svc-xx267
Nov 16 02:47:13.045: INFO: Got endpoints: latency-svc-dnwgj [464.942449ms]
Nov 16 02:47:13.061: INFO: Got endpoints: latency-svc-xx267 [459.837708ms]
Nov 16 02:47:13.061: INFO: Created: latency-svc-t5gb6
Nov 16 02:47:13.069: INFO: Created: latency-svc-rj7nm
Nov 16 02:47:13.092: INFO: Got endpoints: latency-svc-t5gb6 [457.321129ms]
Nov 16 02:47:13.093: INFO: Created: latency-svc-7tsjd
Nov 16 02:47:13.097: INFO: Got endpoints: latency-svc-rj7nm [387.686596ms]
Nov 16 02:47:13.120: INFO: Got endpoints: latency-svc-7tsjd [410.381085ms]
Nov 16 02:47:13.134: INFO: Created: latency-svc-jnbkf
Nov 16 02:47:13.156: INFO: Created: latency-svc-psjbb
Nov 16 02:47:13.180: INFO: Got endpoints: latency-svc-jnbkf [457.468981ms]
Nov 16 02:47:13.185: INFO: Got endpoints: latency-svc-psjbb [402.207352ms]
Nov 16 02:47:13.186: INFO: Latencies: [64.944791ms 118.506865ms 118.756754ms 143.12269ms 159.466679ms 161.405382ms 179.68026ms 200.570343ms 215.080214ms 231.55258ms 252.111763ms 256.529587ms 268.736465ms 305.201356ms 308.863468ms 325.037528ms 332.519353ms 338.710273ms 353.229023ms 356.665712ms 360.922805ms 361.341126ms 363.356888ms 363.393439ms 366.795023ms 367.031641ms 369.733768ms 370.29923ms 370.4264ms 372.303297ms 379.492123ms 385.443387ms 387.686596ms 392.381065ms 397.483471ms 399.791623ms 402.207352ms 403.559847ms 404.757547ms 404.82186ms 405.577587ms 410.381085ms 411.374252ms 412.460316ms 413.934367ms 422.365911ms 423.250856ms 429.39233ms 431.842045ms 435.315427ms 437.842277ms 439.196822ms 439.812162ms 440.913951ms 442.138249ms 444.056157ms 447.020047ms 447.352936ms 450.174473ms 452.922537ms 453.772578ms 453.940279ms 453.987428ms 454.066551ms 454.511118ms 457.321129ms 457.468981ms 458.458808ms 458.512908ms 459.837708ms 460.189397ms 460.604195ms 462.710353ms 464.942449ms 466.161684ms 467.035786ms 468.354724ms 470.790865ms 471.714293ms 472.300228ms 474.07125ms 475.947639ms 479.63157ms 480.50509ms 480.600162ms 480.973726ms 482.189633ms 484.001331ms 485.149707ms 485.302527ms 486.796382ms 487.192872ms 490.115691ms 491.103308ms 491.695582ms 492.450324ms 493.123108ms 493.160385ms 495.465405ms 496.781387ms 498.535576ms 499.07814ms 500.04057ms 501.029023ms 501.395046ms 501.520224ms 505.077064ms 505.126194ms 506.464095ms 508.568124ms 509.301543ms 509.409233ms 510.925294ms 511.142092ms 511.842552ms 515.333677ms 519.90766ms 520.520514ms 522.287677ms 523.728438ms 523.74605ms 523.862027ms 524.039252ms 525.129466ms 525.722702ms 525.728218ms 527.275487ms 527.54953ms 528.296836ms 528.3766ms 528.410097ms 530.20715ms 530.521645ms 530.639333ms 533.850662ms 535.000259ms 536.341761ms 536.780111ms 538.207693ms 538.478606ms 539.662712ms 543.298126ms 543.4275ms 544.433712ms 545.677815ms 546.667495ms 546.967402ms 548.439745ms 550.188698ms 551.215721ms 553.81873ms 554.703322ms 555.691343ms 556.447638ms 559.015394ms 559.597977ms 563.18485ms 564.183071ms 564.955071ms 569.105817ms 569.165261ms 569.88963ms 570.170189ms 570.513825ms 572.922866ms 577.791417ms 579.372926ms 582.557176ms 584.616592ms 587.698789ms 589.657834ms 590.945955ms 591.117218ms 601.953488ms 611.431386ms 619.701421ms 619.863648ms 621.501265ms 624.210294ms 627.464428ms 635.907166ms 639.997681ms 640.385062ms 643.20078ms 643.943974ms 645.967719ms 646.651275ms 650.291864ms 651.502657ms 655.976459ms 656.661406ms 670.64337ms 674.938938ms 684.483997ms 686.03812ms 716.586986ms 726.140205ms 745.496854ms 778.794797ms 842.639491ms]
Nov 16 02:47:13.186: INFO: 50 %ile: 498.535576ms
Nov 16 02:47:13.186: INFO: 90 %ile: 635.907166ms
Nov 16 02:47:13.186: INFO: 99 %ile: 778.794797ms
Nov 16 02:47:13.186: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:47:13.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8083" for this suite.

• [SLOW TEST:10.134 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":208,"skipped":3942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:47:13.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:47:13.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 16 02:47:25.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 create -f -'
Nov 16 02:47:27.562: INFO: stderr: ""
Nov 16 02:47:27.562: INFO: stdout: "e2e-test-crd-publish-openapi-8004-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 16 02:47:27.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 delete e2e-test-crd-publish-openapi-8004-crds test-foo'
Nov 16 02:47:27.753: INFO: stderr: ""
Nov 16 02:47:27.753: INFO: stdout: "e2e-test-crd-publish-openapi-8004-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 16 02:47:27.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 apply -f -'
Nov 16 02:47:29.453: INFO: stderr: ""
Nov 16 02:47:29.453: INFO: stdout: "e2e-test-crd-publish-openapi-8004-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 16 02:47:29.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 delete e2e-test-crd-publish-openapi-8004-crds test-foo'
Nov 16 02:47:29.611: INFO: stderr: ""
Nov 16 02:47:29.611: INFO: stdout: "e2e-test-crd-publish-openapi-8004-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Nov 16 02:47:29.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 create -f -'
Nov 16 02:47:30.106: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 16 02:47:30.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 create -f -'
Nov 16 02:47:31.863: INFO: rc: 1
Nov 16 02:47:31.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 apply -f -'
Nov 16 02:47:32.249: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 16 02:47:32.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 create -f -'
Nov 16 02:47:32.606: INFO: rc: 1
Nov 16 02:47:32.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 --namespace=crd-publish-openapi-5397 apply -f -'
Nov 16 02:47:33.080: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 16 02:47:33.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 explain e2e-test-crd-publish-openapi-8004-crds'
Nov 16 02:47:33.458: INFO: stderr: ""
Nov 16 02:47:33.458: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 16 02:47:33.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 explain e2e-test-crd-publish-openapi-8004-crds.metadata'
Nov 16 02:47:34.170: INFO: stderr: ""
Nov 16 02:47:34.170: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 16 02:47:34.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 explain e2e-test-crd-publish-openapi-8004-crds.spec'
Nov 16 02:47:34.591: INFO: stderr: ""
Nov 16 02:47:34.591: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 16 02:47:34.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 explain e2e-test-crd-publish-openapi-8004-crds.spec.bars'
Nov 16 02:47:35.175: INFO: stderr: ""
Nov 16 02:47:35.175: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 16 02:47:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5397 explain e2e-test-crd-publish-openapi-8004-crds.spec.bars2'
Nov 16 02:47:35.550: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:47:47.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5397" for this suite.

• [SLOW TEST:34.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":209,"skipped":3964,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:47:47.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:52:48.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5315" for this suite.

• [SLOW TEST:300.273 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":210,"skipped":3971,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:52:48.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:52:48.320: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 16 02:52:48.396: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 02:52:53.410: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 02:52:53.411: INFO: Creating deployment "test-rolling-update-deployment"
Nov 16 02:52:53.430: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 16 02:52:53.454: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 16 02:52:55.482: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 16 02:52:55.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 52, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 52, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 52, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 52, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-8656fc4b57\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 02:52:57.511: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 02:52:57.557: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6394  930763fd-ff97-4e88-832d-f33b3636b42e 125205 1 2022-11-16 02:52:53 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-11-16 02:52:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:52:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008b15d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-16 02:52:53 +0000 UTC,LastTransitionTime:2022-11-16 02:52:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-8656fc4b57" has successfully progressed.,LastUpdateTime:2022-11-16 02:52:55 +0000 UTC,LastTransitionTime:2022-11-16 02:52:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 02:52:57.588: INFO: New ReplicaSet "test-rolling-update-deployment-8656fc4b57" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-8656fc4b57  deployment-6394  58383989-e32f-4c96-b4d5-9be29d326409 125195 1 2022-11-16 02:52:53 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 930763fd-ff97-4e88-832d-f33b3636b42e 0xc00cb0c417 0xc00cb0c418}] []  [{kube-controller-manager Update apps/v1 2022-11-16 02:52:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"930763fd-ff97-4e88-832d-f33b3636b42e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:52:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 8656fc4b57,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cb0c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:52:57.588: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 16 02:52:57.589: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6394  88b07cd7-5822-4de2-b8d3-8b0c1feb2214 125204 2 2022-11-16 02:52:48 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 930763fd-ff97-4e88-832d-f33b3636b42e 0xc00cb0c2e7 0xc00cb0c2e8}] []  [{e2e.test Update apps/v1 2022-11-16 02:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:52:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"930763fd-ff97-4e88-832d-f33b3636b42e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:52:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00cb0c3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:52:57.609: INFO: Pod "test-rolling-update-deployment-8656fc4b57-mpxpg" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-8656fc4b57-mpxpg test-rolling-update-deployment-8656fc4b57- deployment-6394  72adcc14-dd39-4122-86af-509fc82c1d63 125194 0 2022-11-16 02:52:53 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[cni.projectcalico.org/containerID:adcd59eb1aee20f3cbf756f8fd25d76f61beab18d952c364b43821004d768a1a cni.projectcalico.org/podIP:172.30.102.249/32 cni.projectcalico.org/podIPs:172.30.102.249/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.249"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.249"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-8656fc4b57 58383989-e32f-4c96-b4d5-9be29d326409 0xc005a7d817 0xc005a7d818}] []  [{kube-controller-manager Update v1 2022-11-16 02:52:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58383989-e32f-4c96-b4d5-9be29d326409\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 02:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 02:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 02:52:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lms2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lms2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b7vvf,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:52:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:52:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.249,StartTime:2022-11-16 02:52:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 02:52:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:cri-o://dc8b14d85655f7ad08148e00305fc016443b01c80cb1ee23a49dceb9e2750db5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:52:57.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6394" for this suite.

• [SLOW TEST:9.460 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":211,"skipped":4015,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:52:57.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 02:53:00.046: INFO: DNS probes using dns-4291/dns-test-d0a5a144-5c19-49ea-a271-813262c52b22 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:00.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4291" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":212,"skipped":4033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:00.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
Nov 16 02:53:00.310: INFO: The status of Pod pod-hostip-87f4162c-6194-43ab-bfa4-6a2a3a53ce66 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:53:02.326: INFO: The status of Pod pod-hostip-87f4162c-6194-43ab-bfa4-6a2a3a53ce66 is Running (Ready = true)
Nov 16 02:53:02.353: INFO: Pod pod-hostip-87f4162c-6194-43ab-bfa4-6a2a3a53ce66 has hostIP: 10.189.71.151
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:02.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1083" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":4067,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:02.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 16 02:53:02.644: INFO: Waiting up to 5m0s for pod "pod-48a99b28-f99f-474c-a98f-0695e9e24095" in namespace "emptydir-4452" to be "Succeeded or Failed"
Nov 16 02:53:02.683: INFO: Pod "pod-48a99b28-f99f-474c-a98f-0695e9e24095": Phase="Pending", Reason="", readiness=false. Elapsed: 38.986759ms
Nov 16 02:53:04.701: INFO: Pod "pod-48a99b28-f99f-474c-a98f-0695e9e24095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056371766s
Nov 16 02:53:06.716: INFO: Pod "pod-48a99b28-f99f-474c-a98f-0695e9e24095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071705543s
STEP: Saw pod success
Nov 16 02:53:06.716: INFO: Pod "pod-48a99b28-f99f-474c-a98f-0695e9e24095" satisfied condition "Succeeded or Failed"
Nov 16 02:53:06.726: INFO: Trying to get logs from node 10.189.71.151 pod pod-48a99b28-f99f-474c-a98f-0695e9e24095 container test-container: <nil>
STEP: delete the pod
Nov 16 02:53:06.823: INFO: Waiting for pod pod-48a99b28-f99f-474c-a98f-0695e9e24095 to disappear
Nov 16 02:53:06.836: INFO: Pod pod-48a99b28-f99f-474c-a98f-0695e9e24095 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:06.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4452" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:06.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Nov 16 02:53:08.222: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1116 02:53:08.222767      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:08.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4905" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":215,"skipped":4102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5542
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 02:53:08.339: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 02:53:08.579: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:53:10.625: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:12.597: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:14.597: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:16.603: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:18.594: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:20.601: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:22.598: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:24.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:26.593: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:28.601: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 02:53:30.618: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 02:53:30.642: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 02:53:30.667: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 02:53:34.823: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 02:53:34.823: INFO: Going to poll 172.30.36.95 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 16 02:53:34.841: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.36.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5542 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:53:34.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:53:34.842: INFO: ExecWithOptions: Clientset creation
Nov 16 02:53:34.842: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5542/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.36.95+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:53:36.058: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 16 02:53:36.058: INFO: Going to poll 172.30.102.237 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 16 02:53:36.072: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.102.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5542 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:53:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:53:36.073: INFO: ExecWithOptions: Clientset creation
Nov 16 02:53:36.073: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5542/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.102.237+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:53:37.246: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 16 02:53:37.246: INFO: Going to poll 172.30.169.109 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 16 02:53:37.261: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.169.109 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5542 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 02:53:37.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 02:53:37.262: INFO: ExecWithOptions: Clientset creation
Nov 16 02:53:37.262: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5542/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.169.109+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 02:53:38.437: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:38.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5542" for this suite.

• [SLOW TEST:30.219 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":4133,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:38.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:53:39.277: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:53:42.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 16 02:53:44.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=webhook-638 attach --namespace=webhook-638 to-be-attached-pod -i -c=container1'
Nov 16 02:53:44.761: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:44.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-638" for this suite.
STEP: Destroying namespace "webhook-638-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.521 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":217,"skipped":4146,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:53:45.855: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Nov 16 02:53:47.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 2, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 53, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 53, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-67c86bcf4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:53:50.965: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:53:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:53:54.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3264" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.616 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":218,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:53:54.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:53:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 02:54:06.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5852 --namespace=crd-publish-openapi-5852 create -f -'
Nov 16 02:54:08.532: INFO: stderr: ""
Nov 16 02:54:08.532: INFO: stdout: "e2e-test-crd-publish-openapi-9664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 16 02:54:08.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5852 --namespace=crd-publish-openapi-5852 delete e2e-test-crd-publish-openapi-9664-crds test-cr'
Nov 16 02:54:08.704: INFO: stderr: ""
Nov 16 02:54:08.704: INFO: stdout: "e2e-test-crd-publish-openapi-9664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 16 02:54:08.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5852 --namespace=crd-publish-openapi-5852 apply -f -'
Nov 16 02:54:10.428: INFO: stderr: ""
Nov 16 02:54:10.428: INFO: stdout: "e2e-test-crd-publish-openapi-9664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 16 02:54:10.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5852 --namespace=crd-publish-openapi-5852 delete e2e-test-crd-publish-openapi-9664-crds test-cr'
Nov 16 02:54:10.640: INFO: stderr: ""
Nov 16 02:54:10.640: INFO: stdout: "e2e-test-crd-publish-openapi-9664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 16 02:54:10.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-5852 explain e2e-test-crd-publish-openapi-9664-crds'
Nov 16 02:54:11.093: INFO: stderr: ""
Nov 16 02:54:11.093: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9664-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:54:22.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5852" for this suite.

• [SLOW TEST:27.780 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":219,"skipped":4171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:54:22.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:54:22.484: INFO: Creating deployment "test-recreate-deployment"
Nov 16 02:54:22.506: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 16 02:54:22.533: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Nov 16 02:54:24.558: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 16 02:54:24.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 2, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 54, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 2, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 2, 54, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-594f666cd9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 02:54:26.578: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 16 02:54:26.613: INFO: Updating deployment test-recreate-deployment
Nov 16 02:54:26.614: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 02:54:26.959: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4285  ec140fe5-164d-4f7d-9414-989d6de6a2f4 126706 2 2022-11-16 02:54:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b4bbd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-11-16 02:54:26 +0000 UTC,LastTransitionTime:2022-11-16 02:54:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-11-16 02:54:26 +0000 UTC,LastTransitionTime:2022-11-16 02:54:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 16 02:54:26.969: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-4285  2bb03814-6992-4a1f-bf9a-8ac61ee4dce9 126705 1 2022-11-16 02:54:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ec140fe5-164d-4f7d-9414-989d6de6a2f4 0xc00a7de2b7 0xc00a7de2b8}] []  [{kube-controller-manager Update apps/v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec140fe5-164d-4f7d-9414-989d6de6a2f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a7de358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:54:26.969: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 16 02:54:26.970: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-594f666cd9  deployment-4285  03053cbf-3e6d-4603-9fb3-066099937a0e 126693 2 2022-11-16 02:54:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:594f666cd9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ec140fe5-164d-4f7d-9414-989d6de6a2f4 0xc00a7de197 0xc00a7de198}] []  [{kube-controller-manager Update apps/v1 2022-11-16 02:54:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec140fe5-164d-4f7d-9414-989d6de6a2f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 594f666cd9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:594f666cd9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a7de248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 02:54:26.982: INFO: Pod "test-recreate-deployment-5b99bd5487-pbwg2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-pbwg2 test-recreate-deployment-5b99bd5487- deployment-4285  c622f3b8-b26e-4cd7-9fad-6dd5ef951888 126703 0 2022-11-16 02:54:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 2bb03814-6992-4a1f-bf9a-8ac61ee4dce9 0xc00a730d97 0xc00a730d98}] []  [{kube-controller-manager Update v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bb03814-6992-4a1f-bf9a-8ac61ee4dce9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 02:54:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-br9qv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-br9qv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c56,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xx22j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:54:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:54:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:54:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 02:54:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 02:54:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:54:26.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4285" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":220,"skipped":4226,"failed":0}

------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:54:27.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
Nov 16 02:54:27.207: INFO: Waiting up to 5m0s for pod "client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b" in namespace "containers-7821" to be "Succeeded or Failed"
Nov 16 02:54:27.218: INFO: Pod "client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.731868ms
Nov 16 02:54:29.237: INFO: Pod "client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030636359s
Nov 16 02:54:31.270: INFO: Pod "client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063945229s
STEP: Saw pod success
Nov 16 02:54:31.271: INFO: Pod "client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b" satisfied condition "Succeeded or Failed"
Nov 16 02:54:31.304: INFO: Trying to get logs from node 10.189.71.151 pod client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:54:31.384: INFO: Waiting for pod client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b to disappear
Nov 16 02:54:31.394: INFO: Pod client-containers-5e7dd4db-e0f4-4d37-a0bf-f6e02695741b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:54:31.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7821" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4226,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:54:31.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-fe905ffb-c98f-4c10-829c-d624027f3d9e
STEP: Creating a pod to test consume configMaps
Nov 16 02:54:31.800: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0" in namespace "projected-4295" to be "Succeeded or Failed"
Nov 16 02:54:31.826: INFO: Pod "pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031184ms
Nov 16 02:54:33.840: INFO: Pod "pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040210972s
Nov 16 02:54:35.854: INFO: Pod "pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053439496s
Nov 16 02:54:37.872: INFO: Pod "pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071659278s
STEP: Saw pod success
Nov 16 02:54:37.872: INFO: Pod "pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0" satisfied condition "Succeeded or Failed"
Nov 16 02:54:37.883: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 02:54:37.957: INFO: Waiting for pod pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0 to disappear
Nov 16 02:54:37.970: INFO: Pod pod-projected-configmaps-9d9c118a-92e7-49e2-aa88-8caea5fa69e0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:54:37.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4295" for this suite.

• [SLOW TEST:6.572 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":4247,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:54:38.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-e3238d3c-7b1f-4933-893a-658792ec73e2
STEP: Creating a pod to test consume secrets
Nov 16 02:54:38.178: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4" in namespace "projected-1618" to be "Succeeded or Failed"
Nov 16 02:54:38.199: INFO: Pod "pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.02585ms
Nov 16 02:54:40.215: INFO: Pod "pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036757111s
Nov 16 02:54:42.229: INFO: Pod "pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050909201s
STEP: Saw pod success
Nov 16 02:54:42.229: INFO: Pod "pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4" satisfied condition "Succeeded or Failed"
Nov 16 02:54:42.242: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 02:54:42.310: INFO: Waiting for pod pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4 to disappear
Nov 16 02:54:42.321: INFO: Pod pod-projected-secrets-be20cc5c-e60e-4e2f-915d-c793d9e8cca4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:54:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1618" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4253,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:54:42.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:54:42.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Creating first CR 
Nov 16 02:54:45.086: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-16T02:54:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-16T02:54:45Z]] name:name1 resourceVersion:127131 uid:dc8f7e1f-b085-434a-8b4d-54455e2e8991] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 16 02:54:55.141: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-16T02:54:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-16T02:54:55Z]] name:name2 resourceVersion:127213 uid:5c77b86f-8994-4baa-962e-b881d8287e52] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 16 02:55:05.168: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-16T02:54:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-16T02:55:05Z]] name:name1 resourceVersion:127274 uid:dc8f7e1f-b085-434a-8b4d-54455e2e8991] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 16 02:55:15.196: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-16T02:54:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-16T02:55:15Z]] name:name2 resourceVersion:127309 uid:5c77b86f-8994-4baa-962e-b881d8287e52] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 16 02:55:25.218: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-16T02:54:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-16T02:55:05Z]] name:name1 resourceVersion:127358 uid:dc8f7e1f-b085-434a-8b4d-54455e2e8991] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 16 02:55:35.266: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-16T02:54:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-16T02:55:15Z]] name:name2 resourceVersion:127405 uid:5c77b86f-8994-4baa-962e-b881d8287e52] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:55:45.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5474" for this suite.

• [SLOW TEST:63.517 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":224,"skipped":4256,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:55:45.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:55:46.136: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:55:48.157: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:55:50.158: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:55:52.152: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:55:54.157: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:55:56.149: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:55:58.151: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:56:00.152: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:56:02.168: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:56:04.156: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:56:06.186: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = false)
Nov 16 02:56:08.157: INFO: The status of Pod test-webserver-70be36b2-c096-4da2-b6c0-ec1e65d16bfc is Running (Ready = true)
Nov 16 02:56:08.168: INFO: Container started at 2022-11-16 02:55:47 +0000 UTC, pod became ready at 2022-11-16 02:56:06 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:08.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7127" for this suite.

• [SLOW TEST:22.332 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4273,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:08.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:24.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2078" for this suite.

• [SLOW TEST:16.471 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":226,"skipped":4287,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:24.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Nov 16 02:56:24.849: INFO: The status of Pod annotationupdate418dc34a-1337-4786-baa5-f50a01b2d459 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 02:56:26.865: INFO: The status of Pod annotationupdate418dc34a-1337-4786-baa5-f50a01b2d459 is Running (Ready = true)
Nov 16 02:56:27.488: INFO: Successfully updated pod "annotationupdate418dc34a-1337-4786-baa5-f50a01b2d459"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:31.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9246" for this suite.

• [SLOW TEST:6.963 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4298,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:31.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 02:56:32.470: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 02:56:35.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:35.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6243" for this suite.
STEP: Destroying namespace "webhook-6243-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":228,"skipped":4301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:35.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:56:36.212: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1f3faec9-4aa1-44ed-9518-9d7d32e19658" in namespace "security-context-test-4085" to be "Succeeded or Failed"
Nov 16 02:56:36.223: INFO: Pod "alpine-nnp-false-1f3faec9-4aa1-44ed-9518-9d7d32e19658": Phase="Pending", Reason="", readiness=false. Elapsed: 10.900029ms
Nov 16 02:56:38.255: INFO: Pod "alpine-nnp-false-1f3faec9-4aa1-44ed-9518-9d7d32e19658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042887923s
Nov 16 02:56:40.268: INFO: Pod "alpine-nnp-false-1f3faec9-4aa1-44ed-9518-9d7d32e19658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056360899s
Nov 16 02:56:42.293: INFO: Pod "alpine-nnp-false-1f3faec9-4aa1-44ed-9518-9d7d32e19658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081110828s
Nov 16 02:56:42.293: INFO: Pod "alpine-nnp-false-1f3faec9-4aa1-44ed-9518-9d7d32e19658" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4085" for this suite.

• [SLOW TEST:6.388 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4328,"failed":0}
SSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:42.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:44.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1685" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 16 02:56:44.798: INFO: Waiting up to 5m0s for pod "pod-6f76e232-fd0b-4345-9f0c-028c12003955" in namespace "emptydir-4595" to be "Succeeded or Failed"
Nov 16 02:56:44.809: INFO: Pod "pod-6f76e232-fd0b-4345-9f0c-028c12003955": Phase="Pending", Reason="", readiness=false. Elapsed: 10.722892ms
Nov 16 02:56:46.824: INFO: Pod "pod-6f76e232-fd0b-4345-9f0c-028c12003955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025985654s
Nov 16 02:56:48.839: INFO: Pod "pod-6f76e232-fd0b-4345-9f0c-028c12003955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040421887s
STEP: Saw pod success
Nov 16 02:56:48.839: INFO: Pod "pod-6f76e232-fd0b-4345-9f0c-028c12003955" satisfied condition "Succeeded or Failed"
Nov 16 02:56:48.848: INFO: Trying to get logs from node 10.189.71.151 pod pod-6f76e232-fd0b-4345-9f0c-028c12003955 container test-container: <nil>
STEP: delete the pod
Nov 16 02:56:48.905: INFO: Waiting for pod pod-6f76e232-fd0b-4345-9f0c-028c12003955 to disappear
Nov 16 02:56:48.916: INFO: Pod pod-6f76e232-fd0b-4345-9f0c-028c12003955 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:56:48.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4595" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4372,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:56:48.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:56:49.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 02:57:01.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-8463 --namespace=crd-publish-openapi-8463 create -f -'
Nov 16 02:57:03.646: INFO: stderr: ""
Nov 16 02:57:03.646: INFO: stdout: "e2e-test-crd-publish-openapi-9143-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 16 02:57:03.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-8463 --namespace=crd-publish-openapi-8463 delete e2e-test-crd-publish-openapi-9143-crds test-cr'
Nov 16 02:57:03.776: INFO: stderr: ""
Nov 16 02:57:03.776: INFO: stdout: "e2e-test-crd-publish-openapi-9143-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 16 02:57:03.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-8463 --namespace=crd-publish-openapi-8463 apply -f -'
Nov 16 02:57:04.345: INFO: stderr: ""
Nov 16 02:57:04.345: INFO: stdout: "e2e-test-crd-publish-openapi-9143-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 16 02:57:04.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-8463 --namespace=crd-publish-openapi-8463 delete e2e-test-crd-publish-openapi-9143-crds test-cr'
Nov 16 02:57:04.477: INFO: stderr: ""
Nov 16 02:57:04.477: INFO: stdout: "e2e-test-crd-publish-openapi-9143-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 16 02:57:04.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-8463 explain e2e-test-crd-publish-openapi-9143-crds'
Nov 16 02:57:06.224: INFO: stderr: ""
Nov 16 02:57:06.224: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9143-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:57:17.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8463" for this suite.

• [SLOW TEST:28.824 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":232,"skipped":4378,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:57:17.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 16 02:57:18.012: INFO: Waiting up to 5m0s for pod "pod-c1b29438-5559-427c-ad32-87fe66886dd0" in namespace "emptydir-7134" to be "Succeeded or Failed"
Nov 16 02:57:18.029: INFO: Pod "pod-c1b29438-5559-427c-ad32-87fe66886dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743533ms
Nov 16 02:57:20.041: INFO: Pod "pod-c1b29438-5559-427c-ad32-87fe66886dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028801911s
Nov 16 02:57:22.054: INFO: Pod "pod-c1b29438-5559-427c-ad32-87fe66886dd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042028942s
STEP: Saw pod success
Nov 16 02:57:22.055: INFO: Pod "pod-c1b29438-5559-427c-ad32-87fe66886dd0" satisfied condition "Succeeded or Failed"
Nov 16 02:57:22.068: INFO: Trying to get logs from node 10.189.71.151 pod pod-c1b29438-5559-427c-ad32-87fe66886dd0 container test-container: <nil>
STEP: delete the pod
Nov 16 02:57:22.154: INFO: Waiting for pod pod-c1b29438-5559-427c-ad32-87fe66886dd0 to disappear
Nov 16 02:57:22.165: INFO: Pod pod-c1b29438-5559-427c-ad32-87fe66886dd0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:57:22.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7134" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":4378,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:57:22.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Nov 16 02:57:24.529: INFO: running pods: 0 < 1
Nov 16 02:57:26.545: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:57:28.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9029" for this suite.

• [SLOW TEST:6.562 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":234,"skipped":4391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:57:28.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 02:57:29.199: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:57:29.199: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:57:30.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:57:30.257: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 02:57:31.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 02:57:31.234: INFO: Node 10.189.71.157 is running 0 daemon pod, expected 1
Nov 16 02:57:32.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 02:57:32.235: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Nov 16 02:57:32.255: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Nov 16 02:57:32.289: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Nov 16 02:57:32.296: INFO: Observed &DaemonSet event: ADDED
Nov 16 02:57:32.296: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.296: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.296: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.297: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.297: INFO: Found daemon set daemon-set in namespace daemonsets-385 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 02:57:32.297: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Nov 16 02:57:32.336: INFO: Observed &DaemonSet event: ADDED
Nov 16 02:57:32.336: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.336: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.337: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.337: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.337: INFO: Observed daemon set daemon-set in namespace daemonsets-385 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 02:57:32.337: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 02:57:32.337: INFO: Found daemon set daemon-set in namespace daemonsets-385 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 16 02:57:32.337: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-385, will wait for the garbage collector to delete the pods
Nov 16 02:57:32.428: INFO: Deleting DaemonSet.extensions daemon-set took: 22.743316ms
Nov 16 02:57:32.528: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.440254ms
Nov 16 02:57:35.246: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 02:57:35.246: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 16 02:57:35.256: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"129153"},"items":null}

Nov 16 02:57:35.265: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"129153"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:57:35.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-385" for this suite.

• [SLOW TEST:6.604 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":235,"skipped":4426,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:57:35.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:57:35.530: INFO: Creating pod...
Nov 16 02:57:35.622: INFO: Pod Quantity: 1 Status: Pending
Nov 16 02:57:36.675: INFO: Pod Quantity: 1 Status: Pending
Nov 16 02:57:37.648: INFO: Pod Quantity: 1 Status: Pending
Nov 16 02:57:38.676: INFO: Pod Status: Running
Nov 16 02:57:38.676: INFO: Creating service...
Nov 16 02:57:38.779: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/DELETE
Nov 16 02:57:38.822: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 16 02:57:38.822: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/GET
Nov 16 02:57:38.843: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 16 02:57:38.843: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/HEAD
Nov 16 02:57:38.856: INFO: http.Client request:HEAD | StatusCode:200
Nov 16 02:57:38.856: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 16 02:57:38.876: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 16 02:57:38.876: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/PATCH
Nov 16 02:57:38.908: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 16 02:57:38.908: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/POST
Nov 16 02:57:38.953: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 16 02:57:38.954: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/pods/agnhost/proxy/some/path/with/PUT
Nov 16 02:57:39.014: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 16 02:57:39.014: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/DELETE
Nov 16 02:57:39.073: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 16 02:57:39.073: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/GET
Nov 16 02:57:39.121: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 16 02:57:39.121: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/HEAD
Nov 16 02:57:39.191: INFO: http.Client request:HEAD | StatusCode:200
Nov 16 02:57:39.191: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/OPTIONS
Nov 16 02:57:39.260: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 16 02:57:39.260: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/PATCH
Nov 16 02:57:39.323: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 16 02:57:39.323: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/POST
Nov 16 02:57:39.397: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 16 02:57:39.397: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9352/services/test-service/proxy/some/path/with/PUT
Nov 16 02:57:39.443: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:57:39.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9352" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":236,"skipped":4447,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:57:39.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 02:57:39.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5" in namespace "projected-5833" to be "Succeeded or Failed"
Nov 16 02:57:39.715: INFO: Pod "downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.889129ms
Nov 16 02:57:41.726: INFO: Pod "downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023484361s
Nov 16 02:57:43.742: INFO: Pod "downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039448537s
Nov 16 02:57:45.754: INFO: Pod "downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05230349s
STEP: Saw pod success
Nov 16 02:57:45.755: INFO: Pod "downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5" satisfied condition "Succeeded or Failed"
Nov 16 02:57:45.775: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5 container client-container: <nil>
STEP: delete the pod
Nov 16 02:57:45.827: INFO: Waiting for pod downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5 to disappear
Nov 16 02:57:45.834: INFO: Pod downwardapi-volume-a787af0a-ae98-4487-860b-f2e1a807e2d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:57:45.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5833" for this suite.

• [SLOW TEST:6.385 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:57:45.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 02:57:46.076: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 02:58:46.280: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:58:46.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 16 02:58:50.603: INFO: found a healthy node: 10.189.71.151
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 02:59:07.131: INFO: pods created so far: [1 1 1]
Nov 16 02:59:07.131: INFO: length of pods created so far: 3
Nov 16 02:59:13.224: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:59:20.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8063" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 02:59:20.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5702" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:94.922 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":238,"skipped":4489,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 02:59:20.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7856
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-7856
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7856
Nov 16 02:59:21.088: INFO: Found 0 stateful pods, waiting for 1
Nov 16 02:59:31.100: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 16 02:59:31.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:59:31.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:59:31.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:59:31.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:59:31.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 16 02:59:41.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 02:59:41.478: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:59:41.543: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Nov 16 02:59:41.543: INFO: ss-0  10.189.71.151  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  }]
Nov 16 02:59:41.543: INFO: 
Nov 16 02:59:41.543: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 16 02:59:42.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989635962s
Nov 16 02:59:43.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970853447s
Nov 16 02:59:44.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.948241439s
Nov 16 02:59:45.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.93118358s
Nov 16 02:59:46.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.920678614s
Nov 16 02:59:47.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.88858708s
Nov 16 02:59:48.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.860174743s
Nov 16 02:59:49.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.842708544s
Nov 16 02:59:50.720: INFO: Verifying statefulset ss doesn't scale past 3 for another 826.393613ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7856
Nov 16 02:59:51.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:59:51.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 02:59:51.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:59:51.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:59:51.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:59:52.374: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 16 02:59:52.374: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:59:52.374: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:59:52.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 02:59:52.687: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 16 02:59:52.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 02:59:52.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 02:59:52.718: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 02:59:52.718: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 02:59:52.718: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 16 02:59:52.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:59:53.020: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:59:53.020: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:59:53.020: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:59:53.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:59:53.346: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:59:53.346: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:59:53.346: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:59:53.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-7856 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 02:59:53.611: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 02:59:53.611: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 02:59:53.611: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 02:59:53.611: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 02:59:53.627: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 16 03:00:03.708: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 03:00:03.708: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 03:00:03.708: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 03:00:03.874: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Nov 16 03:00:03.874: INFO: ss-0  10.189.71.151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  }]
Nov 16 03:00:03.874: INFO: ss-1  10.189.71.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  }]
Nov 16 03:00:03.874: INFO: ss-2  10.189.71.151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  }]
Nov 16 03:00:03.874: INFO: 
Nov 16 03:00:03.874: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 03:00:04.963: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Nov 16 03:00:04.963: INFO: ss-0  10.189.71.151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  }]
Nov 16 03:00:04.963: INFO: ss-1  10.189.71.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  }]
Nov 16 03:00:04.963: INFO: ss-2  10.189.71.151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  }]
Nov 16 03:00:04.963: INFO: 
Nov 16 03:00:04.963: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 03:00:06.260: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Nov 16 03:00:06.260: INFO: ss-0  10.189.71.151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:21 +0000 UTC  }]
Nov 16 03:00:06.260: INFO: ss-2  10.189.71.151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 02:59:41 +0000 UTC  }]
Nov 16 03:00:06.260: INFO: 
Nov 16 03:00:06.260: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 03:00:07.276: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.58132939s
Nov 16 03:00:08.312: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.565826385s
Nov 16 03:00:09.324: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.529565543s
Nov 16 03:00:10.337: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.517599318s
Nov 16 03:00:11.354: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.50376662s
Nov 16 03:00:12.366: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.486736965s
Nov 16 03:00:13.381: INFO: Verifying statefulset ss doesn't scale past 0 for another 475.107593ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7856
Nov 16 03:00:14.393: INFO: Scaling statefulset ss to 0
Nov 16 03:00:14.436: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 03:00:14.449: INFO: Deleting all statefulset in ns statefulset-7856
Nov 16 03:00:14.463: INFO: Scaling statefulset ss to 0
Nov 16 03:00:14.524: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 03:00:14.538: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:00:14.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7856" for this suite.

• [SLOW TEST:53.842 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":239,"skipped":4510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:00:14.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Nov 16 03:00:14.737: INFO: namespace kubectl-8939
Nov 16 03:00:14.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8939 create -f -'
Nov 16 03:00:15.337: INFO: stderr: ""
Nov 16 03:00:15.338: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 16 03:00:16.361: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 03:00:16.361: INFO: Found 0 / 1
Nov 16 03:00:17.353: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 03:00:17.353: INFO: Found 1 / 1
Nov 16 03:00:17.353: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 16 03:00:17.364: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 03:00:17.364: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 03:00:17.364: INFO: wait on agnhost-primary startup in kubectl-8939 
Nov 16 03:00:17.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8939 logs agnhost-primary-xz9vn agnhost-primary'
Nov 16 03:00:17.534: INFO: stderr: ""
Nov 16 03:00:17.534: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 16 03:00:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8939 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 16 03:00:17.781: INFO: stderr: ""
Nov 16 03:00:17.781: INFO: stdout: "service/rm2 exposed\n"
Nov 16 03:00:17.797: INFO: Service rm2 in namespace kubectl-8939 found.
STEP: exposing service
Nov 16 03:00:19.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8939 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 16 03:00:19.968: INFO: stderr: ""
Nov 16 03:00:19.968: INFO: stdout: "service/rm3 exposed\n"
Nov 16 03:00:19.983: INFO: Service rm3 in namespace kubectl-8939 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:00:22.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8939" for this suite.

• [SLOW TEST:7.412 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":240,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:00:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-cebcbcf6-de62-4827-9e46-b19bb929b0b8
STEP: Creating a pod to test consume configMaps
Nov 16 03:00:22.282: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352" in namespace "projected-4999" to be "Succeeded or Failed"
Nov 16 03:00:22.291: INFO: Pod "pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352": Phase="Pending", Reason="", readiness=false. Elapsed: 8.875993ms
Nov 16 03:00:24.311: INFO: Pod "pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029781307s
Nov 16 03:00:26.325: INFO: Pod "pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043479932s
STEP: Saw pod success
Nov 16 03:00:26.325: INFO: Pod "pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352" satisfied condition "Succeeded or Failed"
Nov 16 03:00:26.339: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 03:00:26.424: INFO: Waiting for pod pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352 to disappear
Nov 16 03:00:26.435: INFO: Pod pod-projected-configmaps-177f612a-8f80-4ad7-ad4b-8977bc6c7352 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:00:26.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4999" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4567,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:00:26.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-4a6b53da-3c8a-476e-b00f-379a58bbffd1 in namespace container-probe-3447
Nov 16 03:00:28.679: INFO: Started pod liveness-4a6b53da-3c8a-476e-b00f-379a58bbffd1 in namespace container-probe-3447
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 03:00:28.691: INFO: Initial restart count of pod liveness-4a6b53da-3c8a-476e-b00f-379a58bbffd1 is 0
Nov 16 03:00:48.869: INFO: Restart count of pod container-probe-3447/liveness-4a6b53da-3c8a-476e-b00f-379a58bbffd1 is now 1 (20.177637347s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:00:48.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3447" for this suite.

• [SLOW TEST:22.455 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4581,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:00:48.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-b7464cd1-bb22-4945-a085-60c23d21b113
STEP: Creating a pod to test consume secrets
Nov 16 03:00:49.254: INFO: Waiting up to 5m0s for pod "pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3" in namespace "secrets-3954" to be "Succeeded or Failed"
Nov 16 03:00:49.295: INFO: Pod "pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3": Phase="Pending", Reason="", readiness=false. Elapsed: 40.283233ms
Nov 16 03:00:51.322: INFO: Pod "pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06739779s
Nov 16 03:00:53.333: INFO: Pod "pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077668039s
Nov 16 03:00:55.346: INFO: Pod "pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090586008s
STEP: Saw pod success
Nov 16 03:00:55.346: INFO: Pod "pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3" satisfied condition "Succeeded or Failed"
Nov 16 03:00:55.356: INFO: Trying to get logs from node 10.189.71.151 pod pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 03:00:55.431: INFO: Waiting for pod pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3 to disappear
Nov 16 03:00:55.479: INFO: Pod pod-secrets-511e2e3b-a312-4869-a9b1-1f7be53749b3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:00:55.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3954" for this suite.
STEP: Destroying namespace "secret-namespace-2870" for this suite.

• [SLOW TEST:6.679 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:00:55.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Nov 16 03:00:56.002: INFO: Waiting up to 5m0s for pod "security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e" in namespace "security-context-3959" to be "Succeeded or Failed"
Nov 16 03:00:56.027: INFO: Pod "security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 25.227159ms
Nov 16 03:00:58.043: INFO: Pod "security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041017236s
Nov 16 03:01:00.061: INFO: Pod "security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059501876s
STEP: Saw pod success
Nov 16 03:01:00.061: INFO: Pod "security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e" satisfied condition "Succeeded or Failed"
Nov 16 03:01:00.073: INFO: Trying to get logs from node 10.189.71.151 pod security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e container test-container: <nil>
STEP: delete the pod
Nov 16 03:01:00.142: INFO: Waiting for pod security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e to disappear
Nov 16 03:01:00.153: INFO: Pod security-context-be4ff22a-3a47-4675-9c39-87b34c331d1e no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:00.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3959" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":244,"skipped":4627,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:00.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-7be1d635-d9f9-4d43-a9c8-626b73a5b1e7
STEP: Creating a pod to test consume secrets
Nov 16 03:01:00.447: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc" in namespace "projected-9241" to be "Succeeded or Failed"
Nov 16 03:01:00.464: INFO: Pod "pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.686842ms
Nov 16 03:01:02.476: INFO: Pod "pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02951625s
Nov 16 03:01:04.489: INFO: Pod "pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042789295s
STEP: Saw pod success
Nov 16 03:01:04.489: INFO: Pod "pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc" satisfied condition "Succeeded or Failed"
Nov 16 03:01:04.516: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 03:01:04.576: INFO: Waiting for pod pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc to disappear
Nov 16 03:01:04.588: INFO: Pod pod-projected-secrets-27af3800-d683-41aa-b661-25ab7108cdbc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:04.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9241" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4627,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:04.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:08.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5637" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":246,"skipped":4647,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:08.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
Nov 16 03:01:09.174: INFO: created test-pod-1
Nov 16 03:01:09.227: INFO: created test-pod-2
Nov 16 03:01:09.281: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Nov 16 03:01:09.282: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-358' to be running and ready
Nov 16 03:01:09.352: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 16 03:01:09.352: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 16 03:01:09.352: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 16 03:01:09.352: INFO: 0 / 3 pods in namespace 'pods-358' are running and ready (0 seconds elapsed)
Nov 16 03:01:09.352: INFO: expected 0 pod replicas in namespace 'pods-358', 0 are Running and Ready.
Nov 16 03:01:09.353: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Nov 16 03:01:09.353: INFO: test-pod-1  10.189.71.151  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  }]
Nov 16 03:01:09.353: INFO: test-pod-2  10.189.71.151  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  }]
Nov 16 03:01:09.353: INFO: test-pod-3  10.189.71.151  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  }]
Nov 16 03:01:09.353: INFO: 
Nov 16 03:01:11.412: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 16 03:01:11.412: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 16 03:01:11.412: INFO: 1 / 3 pods in namespace 'pods-358' are running and ready (2 seconds elapsed)
Nov 16 03:01:11.413: INFO: expected 0 pod replicas in namespace 'pods-358', 0 are Running and Ready.
Nov 16 03:01:11.413: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Nov 16 03:01:11.413: INFO: test-pod-2  10.189.71.151  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  }]
Nov 16 03:01:11.413: INFO: test-pod-3  10.189.71.151  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:01:09 +0000 UTC  }]
Nov 16 03:01:11.413: INFO: 
Nov 16 03:01:13.398: INFO: 3 / 3 pods in namespace 'pods-358' are running and ready (4 seconds elapsed)
Nov 16 03:01:13.399: INFO: expected 0 pod replicas in namespace 'pods-358', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Nov 16 03:01:13.464: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 03:01:14.476: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 03:01:15.474: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:16.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-358" for this suite.

• [SLOW TEST:7.564 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":247,"skipped":4662,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:16.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-7104
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 03:01:16.665: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 03:01:17.132: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:01:19.155: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:01:21.152: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:23.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:25.147: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:27.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:29.146: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:31.144: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:33.143: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:35.153: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:37.148: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 03:01:39.144: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 03:01:39.169: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 03:01:39.189: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 03:01:41.287: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 03:01:41.287: INFO: Breadth first check of 172.30.36.112 on host 10.189.71.150...
Nov 16 03:01:41.300: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.102.223:9080/dial?request=hostname&protocol=http&host=172.30.36.112&port=8083&tries=1'] Namespace:pod-network-test-7104 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:01:41.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:01:41.301: INFO: ExecWithOptions: Clientset creation
Nov 16 03:01:41.301: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7104/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.102.223%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.36.112%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Nov 16 03:01:41.477: INFO: Waiting for responses: map[]
Nov 16 03:01:41.477: INFO: reached 172.30.36.112 after 0/1 tries
Nov 16 03:01:41.477: INFO: Breadth first check of 172.30.102.202 on host 10.189.71.151...
Nov 16 03:01:41.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.102.223:9080/dial?request=hostname&protocol=http&host=172.30.102.202&port=8083&tries=1'] Namespace:pod-network-test-7104 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:01:41.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:01:41.488: INFO: ExecWithOptions: Clientset creation
Nov 16 03:01:41.489: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7104/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.102.223%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.102.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Nov 16 03:01:41.664: INFO: Waiting for responses: map[]
Nov 16 03:01:41.664: INFO: reached 172.30.102.202 after 0/1 tries
Nov 16 03:01:41.665: INFO: Breadth first check of 172.30.169.113 on host 10.189.71.157...
Nov 16 03:01:41.683: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.102.223:9080/dial?request=hostname&protocol=http&host=172.30.169.113&port=8083&tries=1'] Namespace:pod-network-test-7104 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:01:41.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:01:41.684: INFO: ExecWithOptions: Clientset creation
Nov 16 03:01:41.684: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7104/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.102.223%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.169.113%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Nov 16 03:01:41.831: INFO: Waiting for responses: map[]
Nov 16 03:01:41.832: INFO: reached 172.30.169.113 after 0/1 tries
Nov 16 03:01:41.832: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:41.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7104" for this suite.

• [SLOW TEST:25.337 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4665,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:41.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-b0d9da6a-b9be-425d-9df5-c3ca8f05fe1f
STEP: Creating a pod to test consume configMaps
Nov 16 03:01:42.162: INFO: Waiting up to 5m0s for pod "pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434" in namespace "configmap-6500" to be "Succeeded or Failed"
Nov 16 03:01:42.220: INFO: Pod "pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434": Phase="Pending", Reason="", readiness=false. Elapsed: 58.431778ms
Nov 16 03:01:44.234: INFO: Pod "pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072002759s
Nov 16 03:01:46.263: INFO: Pod "pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.101670295s
STEP: Saw pod success
Nov 16 03:01:46.263: INFO: Pod "pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434" satisfied condition "Succeeded or Failed"
Nov 16 03:01:46.275: INFO: Trying to get logs from node 10.189.71.151 pod pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 03:01:46.346: INFO: Waiting for pod pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434 to disappear
Nov 16 03:01:46.358: INFO: Pod pod-configmaps-5205f7d4-0ef8-4568-97c4-8b8f80755434 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:46.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6500" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4677,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:46.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 16 03:01:46.626: INFO: Waiting up to 5m0s for pod "pod-0ab5eb55-95c6-4ad4-a901-4f915029a320" in namespace "emptydir-180" to be "Succeeded or Failed"
Nov 16 03:01:46.635: INFO: Pod "pod-0ab5eb55-95c6-4ad4-a901-4f915029a320": Phase="Pending", Reason="", readiness=false. Elapsed: 8.903746ms
Nov 16 03:01:48.646: INFO: Pod "pod-0ab5eb55-95c6-4ad4-a901-4f915029a320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020399959s
Nov 16 03:01:50.664: INFO: Pod "pod-0ab5eb55-95c6-4ad4-a901-4f915029a320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037827119s
STEP: Saw pod success
Nov 16 03:01:50.664: INFO: Pod "pod-0ab5eb55-95c6-4ad4-a901-4f915029a320" satisfied condition "Succeeded or Failed"
Nov 16 03:01:50.680: INFO: Trying to get logs from node 10.189.71.151 pod pod-0ab5eb55-95c6-4ad4-a901-4f915029a320 container test-container: <nil>
STEP: delete the pod
Nov 16 03:01:50.739: INFO: Waiting for pod pod-0ab5eb55-95c6-4ad4-a901-4f915029a320 to disappear
Nov 16 03:01:50.749: INFO: Pod pod-0ab5eb55-95c6-4ad4-a901-4f915029a320 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:50.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-180" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4691,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:50.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 03:01:51.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501" in namespace "projected-2902" to be "Succeeded or Failed"
Nov 16 03:01:51.024: INFO: Pod "downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501": Phase="Pending", Reason="", readiness=false. Elapsed: 9.631566ms
Nov 16 03:01:53.038: INFO: Pod "downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024175674s
Nov 16 03:01:55.056: INFO: Pod "downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042138184s
STEP: Saw pod success
Nov 16 03:01:55.056: INFO: Pod "downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501" satisfied condition "Succeeded or Failed"
Nov 16 03:01:55.071: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501 container client-container: <nil>
STEP: delete the pod
Nov 16 03:01:55.146: INFO: Waiting for pod downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501 to disappear
Nov 16 03:01:55.182: INFO: Pod downwardapi-volume-b658577f-9ff0-4e97-b04c-533ddf519501 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:01:55.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2902" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4706,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:01:55.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:02:08.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4824" for this suite.

• [SLOW TEST:13.496 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":252,"skipped":4711,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:02:08.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-s5sn
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 03:02:09.053: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s5sn" in namespace "subpath-54" to be "Succeeded or Failed"
Nov 16 03:02:09.064: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.544429ms
Nov 16 03:02:11.079: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 2.025774121s
Nov 16 03:02:13.097: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 4.043339887s
Nov 16 03:02:15.108: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 6.055069591s
Nov 16 03:02:17.122: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 8.06899531s
Nov 16 03:02:19.136: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 10.082706821s
Nov 16 03:02:21.145: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 12.091620364s
Nov 16 03:02:23.158: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 14.10432552s
Nov 16 03:02:25.176: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 16.122348497s
Nov 16 03:02:27.188: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 18.134530946s
Nov 16 03:02:29.204: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=true. Elapsed: 20.150608503s
Nov 16 03:02:31.223: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Running", Reason="", readiness=false. Elapsed: 22.16995533s
Nov 16 03:02:33.248: INFO: Pod "pod-subpath-test-downwardapi-s5sn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.195084624s
STEP: Saw pod success
Nov 16 03:02:33.248: INFO: Pod "pod-subpath-test-downwardapi-s5sn" satisfied condition "Succeeded or Failed"
Nov 16 03:02:33.262: INFO: Trying to get logs from node 10.189.71.151 pod pod-subpath-test-downwardapi-s5sn container test-container-subpath-downwardapi-s5sn: <nil>
STEP: delete the pod
Nov 16 03:02:33.320: INFO: Waiting for pod pod-subpath-test-downwardapi-s5sn to disappear
Nov 16 03:02:33.330: INFO: Pod pod-subpath-test-downwardapi-s5sn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-s5sn
Nov 16 03:02:33.330: INFO: Deleting pod "pod-subpath-test-downwardapi-s5sn" in namespace "subpath-54"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:02:33.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-54" for this suite.

• [SLOW TEST:24.650 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":253,"skipped":4715,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:02:33.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:02:37.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-260" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":254,"skipped":4735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:02:37.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-326
STEP: creating service affinity-clusterip-transition in namespace services-326
STEP: creating replication controller affinity-clusterip-transition in namespace services-326
I1116 03:02:37.948407      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-326, replica count: 3
I1116 03:02:40.999210      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 03:02:41.032: INFO: Creating new exec pod
Nov 16 03:02:44.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-326 exec execpod-affinity8jdh2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Nov 16 03:02:44.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 16 03:02:44.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 03:02:44.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-326 exec execpod-affinity8jdh2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.26.209 80'
Nov 16 03:02:44.654: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.26.209 80\nConnection to 172.21.26.209 80 port [tcp/http] succeeded!\n"
Nov 16 03:02:44.654: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 03:02:44.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-326 exec execpod-affinity8jdh2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.26.209:80/ ; done'
Nov 16 03:02:45.059: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n"
Nov 16 03:02:45.059: INFO: stdout: "\naffinity-clusterip-transition-285nh\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-285nh\naffinity-clusterip-transition-285nh\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-285nh\naffinity-clusterip-transition-2t4d5\naffinity-clusterip-transition-2t4d5\naffinity-clusterip-transition-2t4d5\naffinity-clusterip-transition-285nh\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-2t4d5\naffinity-clusterip-transition-2t4d5\naffinity-clusterip-transition-2t4d5\naffinity-clusterip-transition-2t4d5"
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-285nh
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-285nh
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-285nh
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-285nh
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-285nh
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.059: INFO: Received response from host: affinity-clusterip-transition-2t4d5
Nov 16 03:02:45.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-326 exec execpod-affinity8jdh2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.26.209:80/ ; done'
Nov 16 03:02:45.531: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.26.209:80/\n"
Nov 16 03:02:45.531: INFO: stdout: "\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g\naffinity-clusterip-transition-xdq2g"
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Received response from host: affinity-clusterip-transition-xdq2g
Nov 16 03:02:45.532: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-326, will wait for the garbage collector to delete the pods
Nov 16 03:02:45.688: INFO: Deleting ReplicationController affinity-clusterip-transition took: 36.292498ms
Nov 16 03:02:45.889: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.97143ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:02:48.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-326" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:11.276 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":255,"skipped":4762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:02:49.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Nov 16 03:02:49.363: INFO: observed Pod pod-test in namespace pods-3670 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 16 03:02:49.363: INFO: observed Pod pod-test in namespace pods-3670 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  }]
Nov 16 03:02:49.439: INFO: observed Pod pod-test in namespace pods-3670 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  }]
Nov 16 03:02:50.150: INFO: observed Pod pod-test in namespace pods-3670 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  }]
Nov 16 03:02:50.218: INFO: observed Pod pod-test in namespace pods-3670 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  }]
Nov 16 03:02:50.862: INFO: Found Pod pod-test in namespace pods-3670 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-16 03:02:49 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Nov 16 03:02:50.928: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Nov 16 03:02:50.998: INFO: observed event type ADDED
Nov 16 03:02:50.998: INFO: observed event type MODIFIED
Nov 16 03:02:50.999: INFO: observed event type MODIFIED
Nov 16 03:02:51.000: INFO: observed event type MODIFIED
Nov 16 03:02:51.000: INFO: observed event type MODIFIED
Nov 16 03:02:51.001: INFO: observed event type MODIFIED
Nov 16 03:02:51.003: INFO: observed event type MODIFIED
Nov 16 03:02:51.004: INFO: observed event type MODIFIED
Nov 16 03:02:51.004: INFO: observed event type MODIFIED
Nov 16 03:02:52.851: INFO: observed event type MODIFIED
Nov 16 03:02:53.133: INFO: observed event type MODIFIED
Nov 16 03:02:53.862: INFO: observed event type MODIFIED
Nov 16 03:02:53.889: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:02:53.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3670" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":256,"skipped":4785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:02:53.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
Nov 16 03:02:54.154: INFO: Found Service test-service-kcwz9 in namespace services-257 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 16 03:02:54.154: INFO: Service test-service-kcwz9 created
STEP: Getting /status
Nov 16 03:02:54.169: INFO: Service test-service-kcwz9 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Nov 16 03:02:54.197: INFO: observed Service test-service-kcwz9 in namespace services-257 with annotations: map[] & LoadBalancer: {[]}
Nov 16 03:02:54.197: INFO: Found Service test-service-kcwz9 in namespace services-257 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 16 03:02:54.197: INFO: Service test-service-kcwz9 has service status patched
STEP: updating the ServiceStatus
Nov 16 03:02:54.239: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Nov 16 03:02:54.250: INFO: Observed Service test-service-kcwz9 in namespace services-257 with annotations: map[] & Conditions: {[]}
Nov 16 03:02:54.250: INFO: Observed event: &Service{ObjectMeta:{test-service-kcwz9  services-257  0c2546e5-be25-4277-92d5-0f99b48dd3ed 133443 0 2022-11-16 03:02:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-11-16 03:02:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-11-16 03:02:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.19.54,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.19.54],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 16 03:02:54.250: INFO: Found Service test-service-kcwz9 in namespace services-257 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 03:02:54.250: INFO: Service test-service-kcwz9 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Nov 16 03:02:54.283: INFO: observed Service test-service-kcwz9 in namespace services-257 with labels: map[test-service-static:true]
Nov 16 03:02:54.283: INFO: observed Service test-service-kcwz9 in namespace services-257 with labels: map[test-service-static:true]
Nov 16 03:02:54.284: INFO: observed Service test-service-kcwz9 in namespace services-257 with labels: map[test-service-static:true]
Nov 16 03:02:54.284: INFO: Found Service test-service-kcwz9 in namespace services-257 with labels: map[test-service:patched test-service-static:true]
Nov 16 03:02:54.284: INFO: Service test-service-kcwz9 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Nov 16 03:02:54.346: INFO: Observed event: ADDED
Nov 16 03:02:54.347: INFO: Observed event: MODIFIED
Nov 16 03:02:54.347: INFO: Observed event: MODIFIED
Nov 16 03:02:54.348: INFO: Observed event: MODIFIED
Nov 16 03:02:54.348: INFO: Found Service test-service-kcwz9 in namespace services-257 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Nov 16 03:02:54.348: INFO: Service test-service-kcwz9 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:02:54.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-257" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":257,"skipped":4859,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:02:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:02:54.526: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-608fd8dc-faca-4773-980c-d394fdbda44b
STEP: Creating secret with name s-test-opt-upd-1bf7d7f6-fd7d-4b53-872e-669af2f43882
STEP: Creating the pod
Nov 16 03:02:54.661: INFO: The status of Pod pod-projected-secrets-3da0e179-833b-4a43-9798-eb1bfd2e8857 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:02:56.698: INFO: The status of Pod pod-projected-secrets-3da0e179-833b-4a43-9798-eb1bfd2e8857 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:02:58.683: INFO: The status of Pod pod-projected-secrets-3da0e179-833b-4a43-9798-eb1bfd2e8857 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-608fd8dc-faca-4773-980c-d394fdbda44b
STEP: Updating secret s-test-opt-upd-1bf7d7f6-fd7d-4b53-872e-669af2f43882
STEP: Creating secret with name s-test-opt-create-f6107e17-a49b-41f5-9e0a-fc2059acaa9d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:18.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5094" for this suite.

• [SLOW TEST:84.219 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4875,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:18.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 16 03:04:18.810: INFO: Waiting up to 5m0s for pod "pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3" in namespace "emptydir-278" to be "Succeeded or Failed"
Nov 16 03:04:18.826: INFO: Pod "pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.585859ms
Nov 16 03:04:20.844: INFO: Pod "pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.033173359s
Nov 16 03:04:22.866: INFO: Pod "pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3": Phase="Running", Reason="", readiness=false. Elapsed: 4.055304827s
Nov 16 03:04:24.887: INFO: Pod "pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076447647s
STEP: Saw pod success
Nov 16 03:04:24.887: INFO: Pod "pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3" satisfied condition "Succeeded or Failed"
Nov 16 03:04:24.900: INFO: Trying to get logs from node 10.189.71.151 pod pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3 container test-container: <nil>
STEP: delete the pod
Nov 16 03:04:24.965: INFO: Waiting for pod pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3 to disappear
Nov 16 03:04:24.976: INFO: Pod pod-3720f5eb-df4b-4e35-b0e8-d6d146f1f2f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:24.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-278" for this suite.

• [SLOW TEST:6.403 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":4894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:25.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:04:27.239: INFO: Deleting pod "var-expansion-ae53c99b-9aa6-4045-b640-3470ba36f676" in namespace "var-expansion-5740"
Nov 16 03:04:27.256: INFO: Wait up to 5m0s for pod "var-expansion-ae53c99b-9aa6-4045-b640-3470ba36f676" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:31.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5740" for this suite.

• [SLOW TEST:6.298 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":260,"skipped":4931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:31.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:04:31.631: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f4b61ce7-f88e-4bfc-8412-bee41bc8d6bd" in namespace "security-context-test-3866" to be "Succeeded or Failed"
Nov 16 03:04:31.645: INFO: Pod "busybox-readonly-false-f4b61ce7-f88e-4bfc-8412-bee41bc8d6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.957764ms
Nov 16 03:04:33.659: INFO: Pod "busybox-readonly-false-f4b61ce7-f88e-4bfc-8412-bee41bc8d6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027415611s
Nov 16 03:04:35.683: INFO: Pod "busybox-readonly-false-f4b61ce7-f88e-4bfc-8412-bee41bc8d6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051915761s
Nov 16 03:04:35.683: INFO: Pod "busybox-readonly-false-f4b61ce7-f88e-4bfc-8412-bee41bc8d6bd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:35.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3866" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":5062,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:35.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Nov 16 03:04:35.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8851 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Nov 16 03:04:35.994: INFO: stderr: ""
Nov 16 03:04:35.994: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
Nov 16 03:04:36.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-8851 delete pods e2e-test-httpd-pod'
Nov 16 03:04:39.484: INFO: stderr: ""
Nov 16 03:04:39.485: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:39.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8851" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":262,"skipped":5078,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:39.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Nov 16 03:04:39.780: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:04:41.793: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:04:43.793: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Nov 16 03:04:43.860: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:04:45.877: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Nov 16 03:04:45.910: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 03:04:45.921: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 03:04:47.922: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 03:04:47.933: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 03:04:49.921: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 03:04:49.944: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:50.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4281" for this suite.

• [SLOW TEST:10.533 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":5083,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:50.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:04:51.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 03:04:53.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:04:56.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:56.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2915" for this suite.
STEP: Destroying namespace "webhook-2915-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.967 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":264,"skipped":5100,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:57.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:57.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9063" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":265,"skipped":5123,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:57.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:04:57.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-152" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":266,"skipped":5129,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:04:57.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Nov 16 03:04:58.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
Nov 16 03:04:58.816: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 16 03:05:01.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:05:03.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:05:05.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:05:07.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 4, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:05:09.498: INFO: Waited 396.279746ms for the sample-apiserver to be ready to handle requests.
I1116 03:05:10.725915      21 request.go:665] Waited for 1.003361565s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/monitoring.coreos.com/v1
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Nov 16 03:05:11.404: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:05:12.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9725" for this suite.

• [SLOW TEST:14.252 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":267,"skipped":5144,"failed":0}
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:05:12.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:05:16.338: INFO: Deleting pod "var-expansion-33bf52af-5277-4fed-bafd-e3773604de68" in namespace "var-expansion-6353"
Nov 16 03:05:16.428: INFO: Wait up to 5m0s for pod "var-expansion-33bf52af-5277-4fed-bafd-e3773604de68" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:05:18.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6353" for this suite.

• [SLOW TEST:6.381 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":268,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:05:18.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-69a707ea-766c-407c-92d2-6c3d465b067a in namespace container-probe-1520
Nov 16 03:05:20.726: INFO: Started pod busybox-69a707ea-766c-407c-92d2-6c3d465b067a in namespace container-probe-1520
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 03:05:20.737: INFO: Initial restart count of pod busybox-69a707ea-766c-407c-92d2-6c3d465b067a is 0
Nov 16 03:06:11.314: INFO: Restart count of pod container-probe-1520/busybox-69a707ea-766c-407c-92d2-6c3d465b067a is now 1 (50.576062997s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:06:11.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1520" for this suite.

• [SLOW TEST:52.938 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":5161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:06:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Nov 16 03:06:11.726: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Nov 16 03:06:11.812: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:06:11.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8239" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":270,"skipped":5241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:06:11.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-ac8aaa4d-9edf-4d29-a501-2c1636aa320a
STEP: Creating a pod to test consume secrets
Nov 16 03:06:12.164: INFO: Waiting up to 5m0s for pod "pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a" in namespace "secrets-656" to be "Succeeded or Failed"
Nov 16 03:06:12.173: INFO: Pod "pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.341976ms
Nov 16 03:06:14.193: INFO: Pod "pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02870067s
Nov 16 03:06:16.205: INFO: Pod "pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040863153s
Nov 16 03:06:18.220: INFO: Pod "pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055947208s
STEP: Saw pod success
Nov 16 03:06:18.220: INFO: Pod "pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a" satisfied condition "Succeeded or Failed"
Nov 16 03:06:18.233: INFO: Trying to get logs from node 10.189.71.151 pod pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 03:06:18.333: INFO: Waiting for pod pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a to disappear
Nov 16 03:06:18.344: INFO: Pod pod-secrets-784f6d02-45d4-4744-b77c-30cd0e2cbe9a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:06:18.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-656" for this suite.

• [SLOW TEST:6.428 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":5274,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:06:18.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-1117a0ef-55c4-41f0-8895-caccb58de0d1
STEP: Creating secret with name secret-projected-all-test-volume-734c0cf8-9754-415c-be50-cb419fc76d78
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 16 03:06:18.627: INFO: Waiting up to 5m0s for pod "projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876" in namespace "projected-1755" to be "Succeeded or Failed"
Nov 16 03:06:18.640: INFO: Pod "projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876": Phase="Pending", Reason="", readiness=false. Elapsed: 12.193752ms
Nov 16 03:06:20.654: INFO: Pod "projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026833928s
Nov 16 03:06:22.694: INFO: Pod "projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066650719s
STEP: Saw pod success
Nov 16 03:06:22.694: INFO: Pod "projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876" satisfied condition "Succeeded or Failed"
Nov 16 03:06:22.722: INFO: Trying to get logs from node 10.189.71.151 pod projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 16 03:06:22.793: INFO: Waiting for pod projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876 to disappear
Nov 16 03:06:22.805: INFO: Pod projected-volume-6f2b0acc-6f78-489e-b849-73fc5391c876 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:06:22.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1755" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":272,"skipped":5283,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:06:22.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Nov 16 03:06:22.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 create -f -'
Nov 16 03:06:25.684: INFO: stderr: ""
Nov 16 03:06:25.684: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 03:06:25.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:25.799: INFO: stderr: ""
Nov 16 03:06:25.799: INFO: stdout: "update-demo-nautilus-bhk9b "
STEP: Replicas for name=update-demo: expected=2 actual=1
Nov 16 03:06:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:31.047: INFO: stderr: ""
Nov 16 03:06:31.047: INFO: stdout: "update-demo-nautilus-bhk9b update-demo-nautilus-rv876 "
Nov 16 03:06:31.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:31.227: INFO: stderr: ""
Nov 16 03:06:31.227: INFO: stdout: "true"
Nov 16 03:06:31.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:31.366: INFO: stderr: ""
Nov 16 03:06:31.366: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:31.366: INFO: validating pod update-demo-nautilus-bhk9b
Nov 16 03:06:31.486: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:31.486: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:31.486: INFO: update-demo-nautilus-bhk9b is verified up and running
Nov 16 03:06:31.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-rv876 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:31.635: INFO: stderr: ""
Nov 16 03:06:31.635: INFO: stdout: ""
Nov 16 03:06:31.635: INFO: update-demo-nautilus-rv876 is created but not running
Nov 16 03:06:36.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:36.756: INFO: stderr: ""
Nov 16 03:06:36.756: INFO: stdout: "update-demo-nautilus-bhk9b update-demo-nautilus-rv876 "
Nov 16 03:06:36.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:36.867: INFO: stderr: ""
Nov 16 03:06:36.867: INFO: stdout: "true"
Nov 16 03:06:36.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:36.967: INFO: stderr: ""
Nov 16 03:06:36.967: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:36.967: INFO: validating pod update-demo-nautilus-bhk9b
Nov 16 03:06:36.989: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:36.989: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:36.989: INFO: update-demo-nautilus-bhk9b is verified up and running
Nov 16 03:06:36.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-rv876 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:37.107: INFO: stderr: ""
Nov 16 03:06:37.107: INFO: stdout: "true"
Nov 16 03:06:37.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-rv876 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:37.229: INFO: stderr: ""
Nov 16 03:06:37.229: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:37.229: INFO: validating pod update-demo-nautilus-rv876
Nov 16 03:06:37.277: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:37.277: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:37.277: INFO: update-demo-nautilus-rv876 is verified up and running
STEP: scaling down the replication controller
Nov 16 03:06:37.282: INFO: scanned /root for discovery docs: <nil>
Nov 16 03:06:37.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 16 03:06:38.458: INFO: stderr: ""
Nov 16 03:06:38.458: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 03:06:38.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:38.594: INFO: stderr: ""
Nov 16 03:06:38.594: INFO: stdout: "update-demo-nautilus-bhk9b update-demo-nautilus-rv876 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 16 03:06:43.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:43.721: INFO: stderr: ""
Nov 16 03:06:43.721: INFO: stdout: "update-demo-nautilus-bhk9b "
Nov 16 03:06:43.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:43.839: INFO: stderr: ""
Nov 16 03:06:43.839: INFO: stdout: "true"
Nov 16 03:06:43.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:43.955: INFO: stderr: ""
Nov 16 03:06:43.955: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:43.955: INFO: validating pod update-demo-nautilus-bhk9b
Nov 16 03:06:43.974: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:43.974: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:43.974: INFO: update-demo-nautilus-bhk9b is verified up and running
STEP: scaling up the replication controller
Nov 16 03:06:43.979: INFO: scanned /root for discovery docs: <nil>
Nov 16 03:06:43.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 16 03:06:45.164: INFO: stderr: ""
Nov 16 03:06:45.164: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 03:06:45.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:45.315: INFO: stderr: ""
Nov 16 03:06:45.315: INFO: stdout: "update-demo-nautilus-bhk9b update-demo-nautilus-z88nb "
Nov 16 03:06:45.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:45.498: INFO: stderr: ""
Nov 16 03:06:45.498: INFO: stdout: "true"
Nov 16 03:06:45.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:45.732: INFO: stderr: ""
Nov 16 03:06:45.732: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:45.732: INFO: validating pod update-demo-nautilus-bhk9b
Nov 16 03:06:45.751: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:45.751: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:45.751: INFO: update-demo-nautilus-bhk9b is verified up and running
Nov 16 03:06:45.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-z88nb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:45.862: INFO: stderr: ""
Nov 16 03:06:45.862: INFO: stdout: ""
Nov 16 03:06:45.862: INFO: update-demo-nautilus-z88nb is created but not running
Nov 16 03:06:50.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 03:06:50.987: INFO: stderr: ""
Nov 16 03:06:50.987: INFO: stdout: "update-demo-nautilus-bhk9b update-demo-nautilus-z88nb "
Nov 16 03:06:50.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:51.180: INFO: stderr: ""
Nov 16 03:06:51.180: INFO: stdout: "true"
Nov 16 03:06:51.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-bhk9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:51.312: INFO: stderr: ""
Nov 16 03:06:51.312: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:51.312: INFO: validating pod update-demo-nautilus-bhk9b
Nov 16 03:06:51.326: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:51.326: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:51.326: INFO: update-demo-nautilus-bhk9b is verified up and running
Nov 16 03:06:51.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-z88nb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 03:06:51.431: INFO: stderr: ""
Nov 16 03:06:51.431: INFO: stdout: "true"
Nov 16 03:06:51.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods update-demo-nautilus-z88nb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 03:06:51.526: INFO: stderr: ""
Nov 16 03:06:51.526: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Nov 16 03:06:51.526: INFO: validating pod update-demo-nautilus-z88nb
Nov 16 03:06:51.552: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 03:06:51.552: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 03:06:51.552: INFO: update-demo-nautilus-z88nb is verified up and running
STEP: using delete to clean up resources
Nov 16 03:06:51.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 delete --grace-period=0 --force -f -'
Nov 16 03:06:51.678: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 03:06:51.678: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 16 03:06:51.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get rc,svc -l name=update-demo --no-headers'
Nov 16 03:06:51.831: INFO: stderr: "No resources found in kubectl-2390 namespace.\n"
Nov 16 03:06:51.831: INFO: stdout: ""
Nov 16 03:06:51.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2390 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 03:06:52.352: INFO: stderr: ""
Nov 16 03:06:52.352: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:06:52.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2390" for this suite.

• [SLOW TEST:29.537 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":273,"skipped":5301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:06:52.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 03:06:57.706: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:06:57.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5392" for this suite.

• [SLOW TEST:5.392 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":5352,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:06:57.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 03:07:02.281: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:07:02.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9874" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:07:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 03:07:02.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386" in namespace "downward-api-5408" to be "Succeeded or Failed"
Nov 16 03:07:02.838: INFO: Pod "downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386": Phase="Pending", Reason="", readiness=false. Elapsed: 64.081649ms
Nov 16 03:07:04.853: INFO: Pod "downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078958593s
Nov 16 03:07:06.866: INFO: Pod "downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091736932s
STEP: Saw pod success
Nov 16 03:07:06.866: INFO: Pod "downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386" satisfied condition "Succeeded or Failed"
Nov 16 03:07:06.885: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386 container client-container: <nil>
STEP: delete the pod
Nov 16 03:07:06.938: INFO: Waiting for pod downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386 to disappear
Nov 16 03:07:06.952: INFO: Pod downwardapi-volume-6a3eed37-8582-4153-a6c2-da8e7c04b386 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:07:06.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5408" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5381,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:07:07.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 16 03:07:09.215: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5436 PodName:var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:07:09.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:07:09.215: INFO: ExecWithOptions: Clientset creation
Nov 16 03:07:09.216: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-5436/pods/var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Nov 16 03:07:09.415: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5436 PodName:var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:07:09.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:07:09.416: INFO: ExecWithOptions: Clientset creation
Nov 16 03:07:09.416: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-5436/pods/var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Nov 16 03:07:10.188: INFO: Successfully updated pod "var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 16 03:07:10.201: INFO: Deleting pod "var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19" in namespace "var-expansion-5436"
Nov 16 03:07:10.225: INFO: Wait up to 5m0s for pod "var-expansion-2f46e0a7-3c25-4015-ba18-592ffb2e3f19" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:07:44.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5436" for this suite.

• [SLOW TEST:37.307 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":277,"skipped":5382,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:07:44.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:07:44.525: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 03:07:49.536: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Nov 16 03:07:49.581: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Nov 16 03:07:49.615: INFO: observed ReplicaSet test-rs in namespace replicaset-4228 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 03:07:49.695: INFO: observed ReplicaSet test-rs in namespace replicaset-4228 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 03:07:49.773: INFO: observed ReplicaSet test-rs in namespace replicaset-4228 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 03:07:49.795: INFO: observed ReplicaSet test-rs in namespace replicaset-4228 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 03:07:52.047: INFO: observed ReplicaSet test-rs in namespace replicaset-4228 with ReadyReplicas 2, AvailableReplicas 2
Nov 16 03:07:52.173: INFO: observed Replicaset test-rs in namespace replicaset-4228 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:07:52.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4228" for this suite.

• [SLOW TEST:7.922 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":278,"skipped":5403,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:07:52.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 03:07:52.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 03:07:52.554: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 03:07:53.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 03:07:53.624: INFO: Node 10.189.71.150 is running 0 daemon pod, expected 1
Nov 16 03:07:54.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 16 03:07:54.591: INFO: Node 10.189.71.151 is running 0 daemon pod, expected 1
Nov 16 03:07:55.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 03:07:55.590: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 16 03:07:55.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 03:07:55.659: INFO: Node 10.189.71.151 is running 0 daemon pod, expected 1
Nov 16 03:07:56.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 03:07:56.699: INFO: Node 10.189.71.151 is running 0 daemon pod, expected 1
Nov 16 03:07:57.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 03:07:57.698: INFO: Node 10.189.71.151 is running 0 daemon pod, expected 1
Nov 16 03:07:58.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 03:07:58.695: INFO: Node 10.189.71.151 is running 0 daemon pod, expected 1
Nov 16 03:07:59.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 16 03:07:59.698: INFO: Node 10.189.71.151 is running 0 daemon pod, expected 1
Nov 16 03:08:00.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 16 03:08:00.699: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6870, will wait for the garbage collector to delete the pods
Nov 16 03:08:00.801: INFO: Deleting DaemonSet.extensions daemon-set took: 23.098553ms
Nov 16 03:08:01.002: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.81299ms
Nov 16 03:08:03.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 16 03:08:03.019: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 16 03:08:03.030: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137553"},"items":null}

Nov 16 03:08:03.041: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137553"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:03.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6870" for this suite.

• [SLOW TEST:10.936 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":279,"skipped":5419,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:03.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:03.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6397
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:03.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5297" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:03.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6397" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":280,"skipped":5425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:03.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
Nov 16 03:08:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 16 03:08:04.080: INFO: stderr: ""
Nov 16 03:08:04.080: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
Nov 16 03:08:04.080: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 16 03:08:04.080: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2397" to be "running and ready, or succeeded"
Nov 16 03:08:04.090: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005574ms
Nov 16 03:08:06.100: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.02035782s
Nov 16 03:08:06.100: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 16 03:08:06.100: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 16 03:08:06.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 logs logs-generator logs-generator'
Nov 16 03:08:06.273: INFO: stderr: ""
Nov 16 03:08:06.273: INFO: stdout: "I1116 03:08:05.178187       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/zhx 339\nI1116 03:08:05.378372       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/bbmf 320\nI1116 03:08:05.578305       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/rcf7 516\nI1116 03:08:05.778149       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/vxr 206\nI1116 03:08:05.978739       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2hc7 345\nI1116 03:08:06.179131       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/dt58 211\n"
STEP: limiting log lines
Nov 16 03:08:06.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 logs logs-generator logs-generator --tail=1'
Nov 16 03:08:06.420: INFO: stderr: ""
Nov 16 03:08:06.420: INFO: stdout: "I1116 03:08:06.378381       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/pn9 222\n"
Nov 16 03:08:06.420: INFO: got output "I1116 03:08:06.378381       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/pn9 222\n"
STEP: limiting log bytes
Nov 16 03:08:06.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 logs logs-generator logs-generator --limit-bytes=1'
Nov 16 03:08:06.554: INFO: stderr: ""
Nov 16 03:08:06.554: INFO: stdout: "I"
Nov 16 03:08:06.554: INFO: got output "I"
STEP: exposing timestamps
Nov 16 03:08:06.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 16 03:08:06.697: INFO: stderr: ""
Nov 16 03:08:06.697: INFO: stdout: "2022-11-15T21:08:06.578294370-06:00 I1116 03:08:06.578246       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/qm6 399\n"
Nov 16 03:08:06.697: INFO: got output "2022-11-15T21:08:06.578294370-06:00 I1116 03:08:06.578246       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/qm6 399\n"
STEP: restricting to a time range
Nov 16 03:08:09.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 logs logs-generator logs-generator --since=1s'
Nov 16 03:08:09.342: INFO: stderr: ""
Nov 16 03:08:09.342: INFO: stdout: "I1116 03:08:08.378274       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/vf5r 376\nI1116 03:08:08.578609       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/jf9 444\nI1116 03:08:08.778941       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/7747 335\nI1116 03:08:08.978331       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/jk7 468\nI1116 03:08:09.178684       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/h9g 347\n"
Nov 16 03:08:09.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 logs logs-generator logs-generator --since=24h'
Nov 16 03:08:09.505: INFO: stderr: ""
Nov 16 03:08:09.505: INFO: stdout: "I1116 03:08:05.178187       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/zhx 339\nI1116 03:08:05.378372       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/bbmf 320\nI1116 03:08:05.578305       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/rcf7 516\nI1116 03:08:05.778149       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/vxr 206\nI1116 03:08:05.978739       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2hc7 345\nI1116 03:08:06.179131       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/dt58 211\nI1116 03:08:06.378381       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/pn9 222\nI1116 03:08:06.578246       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/qm6 399\nI1116 03:08:06.778165       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/f56 313\nI1116 03:08:06.979472       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/6qcd 577\nI1116 03:08:07.178237       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/h66 283\nI1116 03:08:07.378617       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/jlbs 219\nI1116 03:08:07.578187       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/kzdx 443\nI1116 03:08:07.779152       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/8fdp 453\nI1116 03:08:07.978493       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/l2ks 455\nI1116 03:08:08.178862       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/x4d8 205\nI1116 03:08:08.378274       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/vf5r 376\nI1116 03:08:08.578609       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/jf9 444\nI1116 03:08:08.778941       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/7747 335\nI1116 03:08:08.978331       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/jk7 468\nI1116 03:08:09.178684       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/h9g 347\nI1116 03:08:09.379086       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/flw 236\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
Nov 16 03:08:09.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2397 delete pod logs-generator'
Nov 16 03:08:10.983: INFO: stderr: ""
Nov 16 03:08:10.983: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:10.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2397" for this suite.

• [SLOW TEST:7.303 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":281,"skipped":5535,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:11.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
Nov 16 03:08:11.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-7297 create -f -'
Nov 16 03:08:11.630: INFO: stderr: ""
Nov 16 03:08:11.630: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 16 03:08:11.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-7297 diff -f -'
Nov 16 03:08:12.113: INFO: rc: 1
Nov 16 03:08:12.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-7297 delete -f -'
Nov 16 03:08:12.216: INFO: stderr: ""
Nov 16 03:08:12.216: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:12.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7297" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":282,"skipped":5539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:12.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:12.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3463" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":283,"skipped":5561,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:12.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8018" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":284,"skipped":5563,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:12.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:13.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-38" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":285,"skipped":5570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:13.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:08:13.597: INFO: Creating ReplicaSet my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf
Nov 16 03:08:13.675: INFO: Pod name my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf: Found 0 pods out of 1
Nov 16 03:08:18.707: INFO: Pod name my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf: Found 1 pods out of 1
Nov 16 03:08:18.707: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf" is running
Nov 16 03:08:18.722: INFO: Pod "my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf-tvsb5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 03:08:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 03:08:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 03:08:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-16 03:08:13 +0000 UTC Reason: Message:}])
Nov 16 03:08:18.722: INFO: Trying to dial the pod
Nov 16 03:08:23.774: INFO: Controller my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf: Got expected result from replica 1 [my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf-tvsb5]: "my-hostname-basic-e5679e2f-4ee5-4f85-9d12-e282b1a609bf-tvsb5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2951" for this suite.

• [SLOW TEST:10.365 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":286,"skipped":5595,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4780
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4780
I1116 03:08:24.103477      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4780, replica count: 2
I1116 03:08:27.154707      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 03:08:27.155: INFO: Creating new exec pod
Nov 16 03:08:30.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4780 exec execpodm682x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 16 03:08:30.661: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 03:08:30.661: INFO: stdout: "externalname-service-2fwlf"
Nov 16 03:08:30.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4780 exec execpodm682x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.186.190 80'
Nov 16 03:08:30.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.186.190 80\nConnection to 172.21.186.190 80 port [tcp/http] succeeded!\n"
Nov 16 03:08:30.962: INFO: stdout: "externalname-service-zsmdc"
Nov 16 03:08:30.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4780 exec execpodm682x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.150 32311'
Nov 16 03:08:31.258: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.150 32311\nConnection to 10.189.71.150 32311 port [tcp/*] succeeded!\n"
Nov 16 03:08:31.258: INFO: stdout: "externalname-service-zsmdc"
Nov 16 03:08:31.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4780 exec execpodm682x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.157 32311'
Nov 16 03:08:31.546: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.157 32311\nConnection to 10.189.71.157 32311 port [tcp/*] succeeded!\n"
Nov 16 03:08:31.546: INFO: stdout: ""
Nov 16 03:08:32.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4780 exec execpodm682x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.189.71.157 32311'
Nov 16 03:08:32.848: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.189.71.157 32311\nConnection to 10.189.71.157 32311 port [tcp/*] succeeded!\n"
Nov 16 03:08:32.848: INFO: stdout: "externalname-service-zsmdc"
Nov 16 03:08:32.848: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:32.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4780" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.161 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":287,"skipped":5600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:32.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-9961bcac-1022-44d8-9913-aaff3c791945
STEP: Creating a pod to test consume secrets
Nov 16 03:08:33.218: INFO: Waiting up to 5m0s for pod "pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d" in namespace "secrets-3634" to be "Succeeded or Failed"
Nov 16 03:08:33.235: INFO: Pod "pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.578738ms
Nov 16 03:08:35.256: INFO: Pod "pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038027429s
Nov 16 03:08:37.268: INFO: Pod "pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049597958s
STEP: Saw pod success
Nov 16 03:08:37.268: INFO: Pod "pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d" satisfied condition "Succeeded or Failed"
Nov 16 03:08:37.280: INFO: Trying to get logs from node 10.189.71.151 pod pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 03:08:37.347: INFO: Waiting for pod pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d to disappear
Nov 16 03:08:37.370: INFO: Pod pod-secrets-e53abb0c-c7e8-48b9-bb9a-041c42b95b8d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:37.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3634" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":5679,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:37.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:08:37.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 03:08:37.765: INFO: The status of Pod pod-logs-websocket-26d3c22b-b00b-4029-9e4c-4c11e992b44b is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:08:39.778: INFO: The status of Pod pod-logs-websocket-26d3c22b-b00b-4029-9e4c-4c11e992b44b is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:39.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1848" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:39.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:08:41.076: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 03:08:43.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 8, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 8, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:08:46.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:08:46.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:49.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4927" for this suite.
STEP: Destroying namespace "webhook-4927-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":290,"skipped":5716,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:49.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:08:50.212: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d80d5e81-c618-4640-9fa3-bbbdb6881102", Controller:(*bool)(0xc005a1a082), BlockOwnerDeletion:(*bool)(0xc005a1a083)}}
Nov 16 03:08:50.235: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"550f836b-a6f0-4f8c-a996-37855fa7baa4", Controller:(*bool)(0xc008245cb2), BlockOwnerDeletion:(*bool)(0xc008245cb3)}}
Nov 16 03:08:50.278: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"eb2aa88a-23f6-436e-8a40-378440ac11b1", Controller:(*bool)(0xc008a84042), BlockOwnerDeletion:(*bool)(0xc008a84043)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:08:55.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8601" for this suite.

• [SLOW TEST:5.478 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":291,"skipped":5720,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:08:55.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:08:56.747: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:08:59.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:00.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-562" for this suite.
STEP: Destroying namespace "webhook-562-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.283 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":292,"skipped":5728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:00.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:00.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4548" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":293,"skipped":5767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:00.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-86d275ac-ed17-4102-8fe5-1c96fd70ed9e
STEP: Creating a pod to test consume configMaps
Nov 16 03:09:01.200: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163" in namespace "projected-9826" to be "Succeeded or Failed"
Nov 16 03:09:01.227: INFO: Pod "pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163": Phase="Pending", Reason="", readiness=false. Elapsed: 27.275692ms
Nov 16 03:09:03.238: INFO: Pod "pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038267043s
Nov 16 03:09:05.257: INFO: Pod "pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057301786s
STEP: Saw pod success
Nov 16 03:09:05.257: INFO: Pod "pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163" satisfied condition "Succeeded or Failed"
Nov 16 03:09:05.269: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 03:09:05.332: INFO: Waiting for pod pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163 to disappear
Nov 16 03:09:05.343: INFO: Pod pod-projected-configmaps-851bb5ad-fa87-4518-99e1-f12456c1d163 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:05.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9826" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:05.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:05.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5461" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":295,"skipped":5828,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:05.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 03:09:05.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232" in namespace "projected-9002" to be "Succeeded or Failed"
Nov 16 03:09:05.916: INFO: Pod "downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232": Phase="Pending", Reason="", readiness=false. Elapsed: 12.605532ms
Nov 16 03:09:07.927: INFO: Pod "downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023881536s
Nov 16 03:09:09.942: INFO: Pod "downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038745559s
Nov 16 03:09:11.958: INFO: Pod "downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054851701s
STEP: Saw pod success
Nov 16 03:09:11.958: INFO: Pod "downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232" satisfied condition "Succeeded or Failed"
Nov 16 03:09:11.971: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232 container client-container: <nil>
STEP: delete the pod
Nov 16 03:09:12.056: INFO: Waiting for pod downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232 to disappear
Nov 16 03:09:12.070: INFO: Pod downwardapi-volume-1a70ef24-b8f2-45f0-9ea9-469a76332232 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:12.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9002" for this suite.

• [SLOW TEST:6.412 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5833,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:12.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Nov 16 03:09:52.405: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1116 03:09:52.405762      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 03:09:52.405: INFO: Deleting pod "simpletest.rc-24rjc" in namespace "gc-6947"
Nov 16 03:09:52.469: INFO: Deleting pod "simpletest.rc-2dngf" in namespace "gc-6947"
Nov 16 03:09:52.501: INFO: Deleting pod "simpletest.rc-2drpc" in namespace "gc-6947"
Nov 16 03:09:52.531: INFO: Deleting pod "simpletest.rc-2jp95" in namespace "gc-6947"
Nov 16 03:09:52.584: INFO: Deleting pod "simpletest.rc-2mztt" in namespace "gc-6947"
Nov 16 03:09:52.610: INFO: Deleting pod "simpletest.rc-2q76c" in namespace "gc-6947"
Nov 16 03:09:52.655: INFO: Deleting pod "simpletest.rc-2s6gv" in namespace "gc-6947"
Nov 16 03:09:52.700: INFO: Deleting pod "simpletest.rc-2tbpg" in namespace "gc-6947"
Nov 16 03:09:52.731: INFO: Deleting pod "simpletest.rc-2tkz8" in namespace "gc-6947"
Nov 16 03:09:52.763: INFO: Deleting pod "simpletest.rc-2vt74" in namespace "gc-6947"
Nov 16 03:09:52.787: INFO: Deleting pod "simpletest.rc-4hgf7" in namespace "gc-6947"
Nov 16 03:09:52.819: INFO: Deleting pod "simpletest.rc-4rknb" in namespace "gc-6947"
Nov 16 03:09:52.859: INFO: Deleting pod "simpletest.rc-4wmj9" in namespace "gc-6947"
Nov 16 03:09:52.899: INFO: Deleting pod "simpletest.rc-5544x" in namespace "gc-6947"
Nov 16 03:09:52.925: INFO: Deleting pod "simpletest.rc-5cm8h" in namespace "gc-6947"
Nov 16 03:09:52.968: INFO: Deleting pod "simpletest.rc-6gt2t" in namespace "gc-6947"
Nov 16 03:09:53.003: INFO: Deleting pod "simpletest.rc-6kdp7" in namespace "gc-6947"
Nov 16 03:09:53.041: INFO: Deleting pod "simpletest.rc-6kzqz" in namespace "gc-6947"
Nov 16 03:09:53.075: INFO: Deleting pod "simpletest.rc-6nckw" in namespace "gc-6947"
Nov 16 03:09:53.112: INFO: Deleting pod "simpletest.rc-7g4t5" in namespace "gc-6947"
Nov 16 03:09:53.205: INFO: Deleting pod "simpletest.rc-7jgz6" in namespace "gc-6947"
Nov 16 03:09:53.230: INFO: Deleting pod "simpletest.rc-7xcvh" in namespace "gc-6947"
Nov 16 03:09:53.262: INFO: Deleting pod "simpletest.rc-8tt54" in namespace "gc-6947"
Nov 16 03:09:53.317: INFO: Deleting pod "simpletest.rc-9cbgm" in namespace "gc-6947"
Nov 16 03:09:53.362: INFO: Deleting pod "simpletest.rc-9dx4l" in namespace "gc-6947"
Nov 16 03:09:53.444: INFO: Deleting pod "simpletest.rc-9jcmp" in namespace "gc-6947"
Nov 16 03:09:53.505: INFO: Deleting pod "simpletest.rc-9kvpw" in namespace "gc-6947"
Nov 16 03:09:53.547: INFO: Deleting pod "simpletest.rc-9lx7s" in namespace "gc-6947"
Nov 16 03:09:53.625: INFO: Deleting pod "simpletest.rc-9vxfc" in namespace "gc-6947"
Nov 16 03:09:53.704: INFO: Deleting pod "simpletest.rc-b4tpx" in namespace "gc-6947"
Nov 16 03:09:53.743: INFO: Deleting pod "simpletest.rc-b7l24" in namespace "gc-6947"
Nov 16 03:09:53.775: INFO: Deleting pod "simpletest.rc-b8l6t" in namespace "gc-6947"
Nov 16 03:09:53.840: INFO: Deleting pod "simpletest.rc-c6nlb" in namespace "gc-6947"
Nov 16 03:09:53.902: INFO: Deleting pod "simpletest.rc-c6t8d" in namespace "gc-6947"
Nov 16 03:09:53.943: INFO: Deleting pod "simpletest.rc-c95s6" in namespace "gc-6947"
Nov 16 03:09:53.987: INFO: Deleting pod "simpletest.rc-cnwsn" in namespace "gc-6947"
Nov 16 03:09:54.054: INFO: Deleting pod "simpletest.rc-cs2nz" in namespace "gc-6947"
Nov 16 03:09:54.099: INFO: Deleting pod "simpletest.rc-d6mft" in namespace "gc-6947"
Nov 16 03:09:54.135: INFO: Deleting pod "simpletest.rc-dh9pk" in namespace "gc-6947"
Nov 16 03:09:54.175: INFO: Deleting pod "simpletest.rc-dm4vx" in namespace "gc-6947"
Nov 16 03:09:54.244: INFO: Deleting pod "simpletest.rc-dtf7g" in namespace "gc-6947"
Nov 16 03:09:54.281: INFO: Deleting pod "simpletest.rc-dvrm7" in namespace "gc-6947"
Nov 16 03:09:54.386: INFO: Deleting pod "simpletest.rc-f8c6l" in namespace "gc-6947"
Nov 16 03:09:54.426: INFO: Deleting pod "simpletest.rc-flngj" in namespace "gc-6947"
Nov 16 03:09:54.469: INFO: Deleting pod "simpletest.rc-fpm2b" in namespace "gc-6947"
Nov 16 03:09:54.563: INFO: Deleting pod "simpletest.rc-fqghr" in namespace "gc-6947"
Nov 16 03:09:54.643: INFO: Deleting pod "simpletest.rc-g5ppk" in namespace "gc-6947"
Nov 16 03:09:54.717: INFO: Deleting pod "simpletest.rc-g6v9b" in namespace "gc-6947"
Nov 16 03:09:54.780: INFO: Deleting pod "simpletest.rc-h5ggx" in namespace "gc-6947"
Nov 16 03:09:54.826: INFO: Deleting pod "simpletest.rc-hclf7" in namespace "gc-6947"
Nov 16 03:09:54.878: INFO: Deleting pod "simpletest.rc-hssfx" in namespace "gc-6947"
Nov 16 03:09:54.935: INFO: Deleting pod "simpletest.rc-hv67l" in namespace "gc-6947"
Nov 16 03:09:55.014: INFO: Deleting pod "simpletest.rc-jjx75" in namespace "gc-6947"
Nov 16 03:09:55.074: INFO: Deleting pod "simpletest.rc-jldjk" in namespace "gc-6947"
Nov 16 03:09:55.140: INFO: Deleting pod "simpletest.rc-jz45b" in namespace "gc-6947"
Nov 16 03:09:55.170: INFO: Deleting pod "simpletest.rc-jzp5t" in namespace "gc-6947"
Nov 16 03:09:55.212: INFO: Deleting pod "simpletest.rc-k452c" in namespace "gc-6947"
Nov 16 03:09:55.307: INFO: Deleting pod "simpletest.rc-k4qqb" in namespace "gc-6947"
Nov 16 03:09:55.360: INFO: Deleting pod "simpletest.rc-k5vvn" in namespace "gc-6947"
Nov 16 03:09:55.403: INFO: Deleting pod "simpletest.rc-kcmpq" in namespace "gc-6947"
Nov 16 03:09:55.449: INFO: Deleting pod "simpletest.rc-kctnw" in namespace "gc-6947"
Nov 16 03:09:55.503: INFO: Deleting pod "simpletest.rc-kjfgz" in namespace "gc-6947"
Nov 16 03:09:55.597: INFO: Deleting pod "simpletest.rc-kqkfr" in namespace "gc-6947"
Nov 16 03:09:55.679: INFO: Deleting pod "simpletest.rc-krc8f" in namespace "gc-6947"
Nov 16 03:09:55.727: INFO: Deleting pod "simpletest.rc-kw6l7" in namespace "gc-6947"
Nov 16 03:09:55.782: INFO: Deleting pod "simpletest.rc-kztmc" in namespace "gc-6947"
Nov 16 03:09:55.814: INFO: Deleting pod "simpletest.rc-l2kt2" in namespace "gc-6947"
Nov 16 03:09:55.839: INFO: Deleting pod "simpletest.rc-llzkt" in namespace "gc-6947"
Nov 16 03:09:55.919: INFO: Deleting pod "simpletest.rc-lm8ts" in namespace "gc-6947"
Nov 16 03:09:55.963: INFO: Deleting pod "simpletest.rc-lq6sh" in namespace "gc-6947"
Nov 16 03:09:56.000: INFO: Deleting pod "simpletest.rc-m4fn6" in namespace "gc-6947"
Nov 16 03:09:56.064: INFO: Deleting pod "simpletest.rc-n65vc" in namespace "gc-6947"
Nov 16 03:09:56.097: INFO: Deleting pod "simpletest.rc-nbfwt" in namespace "gc-6947"
Nov 16 03:09:56.148: INFO: Deleting pod "simpletest.rc-nhfw7" in namespace "gc-6947"
Nov 16 03:09:56.199: INFO: Deleting pod "simpletest.rc-p22c4" in namespace "gc-6947"
Nov 16 03:09:56.257: INFO: Deleting pod "simpletest.rc-p4b7j" in namespace "gc-6947"
Nov 16 03:09:56.313: INFO: Deleting pod "simpletest.rc-q4b8q" in namespace "gc-6947"
Nov 16 03:09:56.367: INFO: Deleting pod "simpletest.rc-q7fvc" in namespace "gc-6947"
Nov 16 03:09:56.411: INFO: Deleting pod "simpletest.rc-r6dct" in namespace "gc-6947"
Nov 16 03:09:56.473: INFO: Deleting pod "simpletest.rc-slk5m" in namespace "gc-6947"
Nov 16 03:09:56.519: INFO: Deleting pod "simpletest.rc-smh79" in namespace "gc-6947"
Nov 16 03:09:56.622: INFO: Deleting pod "simpletest.rc-sn7gs" in namespace "gc-6947"
Nov 16 03:09:56.657: INFO: Deleting pod "simpletest.rc-svxct" in namespace "gc-6947"
Nov 16 03:09:56.711: INFO: Deleting pod "simpletest.rc-t6q8d" in namespace "gc-6947"
Nov 16 03:09:56.757: INFO: Deleting pod "simpletest.rc-tfq4q" in namespace "gc-6947"
Nov 16 03:09:56.811: INFO: Deleting pod "simpletest.rc-v52b2" in namespace "gc-6947"
Nov 16 03:09:56.869: INFO: Deleting pod "simpletest.rc-vnm4n" in namespace "gc-6947"
Nov 16 03:09:56.901: INFO: Deleting pod "simpletest.rc-w9szh" in namespace "gc-6947"
Nov 16 03:09:56.962: INFO: Deleting pod "simpletest.rc-wbb69" in namespace "gc-6947"
Nov 16 03:09:57.084: INFO: Deleting pod "simpletest.rc-wg9dg" in namespace "gc-6947"
Nov 16 03:09:57.119: INFO: Deleting pod "simpletest.rc-wnsm6" in namespace "gc-6947"
Nov 16 03:09:57.162: INFO: Deleting pod "simpletest.rc-xct56" in namespace "gc-6947"
Nov 16 03:09:57.203: INFO: Deleting pod "simpletest.rc-xp7cq" in namespace "gc-6947"
Nov 16 03:09:57.237: INFO: Deleting pod "simpletest.rc-xrdhh" in namespace "gc-6947"
Nov 16 03:09:57.310: INFO: Deleting pod "simpletest.rc-xznbl" in namespace "gc-6947"
Nov 16 03:09:57.372: INFO: Deleting pod "simpletest.rc-z2k6v" in namespace "gc-6947"
Nov 16 03:09:57.421: INFO: Deleting pod "simpletest.rc-z8l6r" in namespace "gc-6947"
Nov 16 03:09:57.461: INFO: Deleting pod "simpletest.rc-zfhtf" in namespace "gc-6947"
Nov 16 03:09:57.527: INFO: Deleting pod "simpletest.rc-zr785" in namespace "gc-6947"
Nov 16 03:09:57.575: INFO: Deleting pod "simpletest.rc-zrrrj" in namespace "gc-6947"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:57.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6947" for this suite.

• [SLOW TEST:45.611 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":297,"skipped":5834,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:57.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:09:58.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9795" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:09:58.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:09:58.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 03:10:13.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-103 --namespace=crd-publish-openapi-103 create -f -'
Nov 16 03:10:15.822: INFO: stderr: ""
Nov 16 03:10:15.822: INFO: stdout: "e2e-test-crd-publish-openapi-5658-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 16 03:10:15.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-103 --namespace=crd-publish-openapi-103 delete e2e-test-crd-publish-openapi-5658-crds test-cr'
Nov 16 03:10:15.940: INFO: stderr: ""
Nov 16 03:10:15.940: INFO: stdout: "e2e-test-crd-publish-openapi-5658-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 16 03:10:15.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-103 --namespace=crd-publish-openapi-103 apply -f -'
Nov 16 03:10:17.919: INFO: stderr: ""
Nov 16 03:10:17.919: INFO: stdout: "e2e-test-crd-publish-openapi-5658-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 16 03:10:17.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-103 --namespace=crd-publish-openapi-103 delete e2e-test-crd-publish-openapi-5658-crds test-cr'
Nov 16 03:10:18.091: INFO: stderr: ""
Nov 16 03:10:18.091: INFO: stdout: "e2e-test-crd-publish-openapi-5658-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 16 03:10:18.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=crd-publish-openapi-103 explain e2e-test-crd-publish-openapi-5658-crds'
Nov 16 03:10:18.591: INFO: stderr: ""
Nov 16 03:10:18.591: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5658-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:10:30.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-103" for this suite.

• [SLOW TEST:32.220 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":299,"skipped":5859,"failed":0}
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:10:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Nov 16 03:10:30.432: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:10:35.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8600" for this suite.

• [SLOW TEST:5.361 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":300,"skipped":5864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:10:35.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:10:35.823: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 16 03:10:40.847: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 03:10:40.847: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 16 03:10:42.857: INFO: Creating deployment "test-rollover-deployment"
Nov 16 03:10:42.879: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 16 03:10:44.909: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 16 03:10:44.928: INFO: Ensure that both replica sets have 1 created replica
Nov 16 03:10:44.945: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 16 03:10:44.970: INFO: Updating deployment test-rollover-deployment
Nov 16 03:10:44.970: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 16 03:10:46.990: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 16 03:10:47.008: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 16 03:10:47.035: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 03:10:47.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:10:49.058: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 03:10:49.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:10:51.075: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 03:10:51.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:10:53.063: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 03:10:53.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:10:55.070: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 03:10:55.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 10, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 10, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 03:10:57.056: INFO: 
Nov 16 03:10:57.056: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 03:10:57.083: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2434  1b1c9e88-5237-45bb-9d4f-086c1965657d 143105 2 2022-11-16 03:10:42 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-11-16 03:10:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:10:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b942e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-16 03:10:43 +0000 UTC,LastTransitionTime:2022-11-16 03:10:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77db6f9f48" has successfully progressed.,LastUpdateTime:2022-11-16 03:10:56 +0000 UTC,LastTransitionTime:2022-11-16 03:10:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 03:10:57.094: INFO: New ReplicaSet "test-rollover-deployment-77db6f9f48" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77db6f9f48  deployment-2434  8085a653-3143-4e90-8c0c-4d98f9ee6b95 143094 2 2022-11-16 03:10:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1b1c9e88-5237-45bb-9d4f-086c1965657d 0xc007b948b7 0xc007b948b8}] []  [{kube-controller-manager Update apps/v1 2022-11-16 03:10:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b1c9e88-5237-45bb-9d4f-086c1965657d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:10:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77db6f9f48,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b949b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 03:10:57.094: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 16 03:10:57.094: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2434  8ae84469-b9b7-4263-a04e-2b16e569e68a 143103 2 2022-11-16 03:10:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1b1c9e88-5237-45bb-9d4f-086c1965657d 0xc007b94767 0xc007b94768}] []  [{e2e.test Update apps/v1 2022-11-16 03:10:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:10:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b1c9e88-5237-45bb-9d4f-086c1965657d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:10:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007b94838 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 03:10:57.094: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-2434  5e01bf13-8c11-43ff-af3c-d192c6d21990 143022 2 2022-11-16 03:10:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1b1c9e88-5237-45bb-9d4f-086c1965657d 0xc007b94a37 0xc007b94a38}] []  [{kube-controller-manager Update apps/v1 2022-11-16 03:10:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b1c9e88-5237-45bb-9d4f-086c1965657d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:10:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b94b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 03:10:57.105: INFO: Pod "test-rollover-deployment-77db6f9f48-qqjss" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77db6f9f48-qqjss test-rollover-deployment-77db6f9f48- deployment-2434  eae3a7e6-2253-4a5b-aea7-e07f3bfe0892 143046 0 2022-11-16 03:10:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[cni.projectcalico.org/containerID:29c4a74b8e056fea5240a056308853060daa8d43d9ee5c68fcee9a0f50b5df89 cni.projectcalico.org/podIP:172.30.102.241/32 cni.projectcalico.org/podIPs:172.30.102.241/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.241"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.241"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-77db6f9f48 8085a653-3143-4e90-8c0c-4d98f9ee6b95 0xc007b95207 0xc007b95208}] []  [{kube-controller-manager Update v1 2022-11-16 03:10:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8085a653-3143-4e90-8c0c-4d98f9ee6b95\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:10:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-11-16 03:10:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-16 03:10:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljl55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljl55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b2d78,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:10:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:10:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:10:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:10:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.241,StartTime:2022-11-16 03:10:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:10:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:cri-o://12d81201c269b2a834b7d344e3976d17a407cd57906e70cff0ab0f516704a354,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:10:57.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2434" for this suite.

• [SLOW TEST:21.445 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":301,"skipped":5888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:10:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 03:10:57.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d" in namespace "downward-api-4788" to be "Succeeded or Failed"
Nov 16 03:10:57.368: INFO: Pod "downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d": Phase="Pending", Reason="", readiness=false. Elapsed: 52.728399ms
Nov 16 03:10:59.388: INFO: Pod "downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072240747s
Nov 16 03:11:01.452: INFO: Pod "downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136840352s
Nov 16 03:11:03.471: INFO: Pod "downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.155578343s
STEP: Saw pod success
Nov 16 03:11:03.471: INFO: Pod "downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d" satisfied condition "Succeeded or Failed"
Nov 16 03:11:03.482: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d container client-container: <nil>
STEP: delete the pod
Nov 16 03:11:03.633: INFO: Waiting for pod downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d to disappear
Nov 16 03:11:03.643: INFO: Pod downwardapi-volume-adfc6214-f1af-4951-808c-bb4611aad90d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:11:03.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4788" for this suite.

• [SLOW TEST:6.534 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:11:03.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:11:03.803: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c4a564ec-e5d5-46ed-b0c9-a7f18325573f" in namespace "security-context-test-8306" to be "Succeeded or Failed"
Nov 16 03:11:03.839: INFO: Pod "busybox-privileged-false-c4a564ec-e5d5-46ed-b0c9-a7f18325573f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.084587ms
Nov 16 03:11:05.853: INFO: Pod "busybox-privileged-false-c4a564ec-e5d5-46ed-b0c9-a7f18325573f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050020652s
Nov 16 03:11:07.875: INFO: Pod "busybox-privileged-false-c4a564ec-e5d5-46ed-b0c9-a7f18325573f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071767307s
Nov 16 03:11:07.875: INFO: Pod "busybox-privileged-false-c4a564ec-e5d5-46ed-b0c9-a7f18325573f" satisfied condition "Succeeded or Failed"
Nov 16 03:11:07.931: INFO: Got logs for pod "busybox-privileged-false-c4a564ec-e5d5-46ed-b0c9-a7f18325573f": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:11:07.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8306" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5969,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:11:07.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:11:08.583: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 16 03:11:10.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 11, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 11, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 11, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 11, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:11:13.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:11:24.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2562" for this suite.
STEP: Destroying namespace "webhook-2562-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.402 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":304,"skipped":5983,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:11:24.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:11:24.490: INFO: Got root ca configmap in namespace "svcaccounts-389"
Nov 16 03:11:24.509: INFO: Deleted root ca configmap in namespace "svcaccounts-389"
STEP: waiting for a new root ca configmap created
Nov 16 03:11:25.023: INFO: Recreated root ca configmap in namespace "svcaccounts-389"
Nov 16 03:11:25.039: INFO: Updated root ca configmap in namespace "svcaccounts-389"
STEP: waiting for the root ca configmap reconciled
Nov 16 03:11:25.551: INFO: Reconciled root ca configmap in namespace "svcaccounts-389"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:11:25.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-389" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":305,"skipped":5996,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:11:25.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-15d43624-0913-42f8-b64e-d46c04b13604
STEP: Creating a pod to test consume configMaps
Nov 16 03:11:25.751: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567" in namespace "projected-5975" to be "Succeeded or Failed"
Nov 16 03:11:25.762: INFO: Pod "pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567": Phase="Pending", Reason="", readiness=false. Elapsed: 11.181495ms
Nov 16 03:11:27.778: INFO: Pod "pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026806838s
Nov 16 03:11:29.867: INFO: Pod "pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567": Phase="Pending", Reason="", readiness=false. Elapsed: 4.116335719s
Nov 16 03:11:31.881: INFO: Pod "pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.129770316s
STEP: Saw pod success
Nov 16 03:11:31.881: INFO: Pod "pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567" satisfied condition "Succeeded or Failed"
Nov 16 03:11:31.893: INFO: Trying to get logs from node 10.189.71.151 pod pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 03:11:32.048: INFO: Waiting for pod pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567 to disappear
Nov 16 03:11:32.088: INFO: Pod pod-projected-configmaps-d0c7a563-ded7-4ec6-b29e-4dbbd688c567 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:11:32.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5975" for this suite.

• [SLOW TEST:6.539 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":6000,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:11:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-50b16b90-5de4-47b5-9127-de7b607a78f6 in namespace container-probe-7730
Nov 16 03:11:34.336: INFO: Started pod busybox-50b16b90-5de4-47b5-9127-de7b607a78f6 in namespace container-probe-7730
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 03:11:34.346: INFO: Initial restart count of pod busybox-50b16b90-5de4-47b5-9127-de7b607a78f6 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:15:34.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7730" for this suite.

• [SLOW TEST:242.648 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":6012,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:15:34.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:05.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3500" for this suite.

• [SLOW TEST:31.164 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":6017,"failed":0}
SSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:05.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:06.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7797" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":309,"skipped":6021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:06.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:06.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8682" for this suite.
STEP: Destroying namespace "nspatchtest-44ce0d20-1860-4f48-b580-2d50a08c31c0-9299" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":310,"skipped":6047,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:06.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Nov 16 03:16:06.711: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:12.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8302" for this suite.

• [SLOW TEST:6.102 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":311,"skipped":6056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:12.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
Nov 16 03:16:12.910: INFO: Creating simple deployment test-deployment-mw5c7
Nov 16 03:16:12.966: INFO: deployment "test-deployment-mw5c7" doesn't have the required revision set
STEP: Getting /status
Nov 16 03:16:15.025: INFO: Deployment test-deployment-mw5c7 has Conditions: [{Available True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mw5c7-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Nov 16 03:16:15.056: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 16, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 16, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 16, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 16, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mw5c7-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Nov 16 03:16:15.060: INFO: Observed &Deployment event: ADDED
Nov 16 03:16:15.060: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:12 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mw5c7-764bc7c4b7"}
Nov 16 03:16:15.061: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:12 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mw5c7-764bc7c4b7"}
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-16 03:16:13 +0000 UTC 2022-11-16 03:16:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 03:16:15.061: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-16 03:16:13 +0000 UTC 2022-11-16 03:16:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:13 +0000 UTC 2022-11-16 03:16:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mw5c7-764bc7c4b7" is progressing.}
Nov 16 03:16:15.061: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mw5c7-764bc7c4b7" has successfully progressed.}
Nov 16 03:16:15.061: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 03:16:15.061: INFO: Observed Deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mw5c7-764bc7c4b7" has successfully progressed.}
Nov 16 03:16:15.061: INFO: Found Deployment test-deployment-mw5c7 in namespace deployment-4353 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 03:16:15.061: INFO: Deployment test-deployment-mw5c7 has an updated status
STEP: patching the Statefulset Status
Nov 16 03:16:15.062: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 16 03:16:15.088: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Nov 16 03:16:15.091: INFO: Observed &Deployment event: ADDED
Nov 16 03:16:15.091: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:12 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mw5c7-764bc7c4b7"}
Nov 16 03:16:15.092: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.092: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:12 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mw5c7-764bc7c4b7"}
Nov 16 03:16:15.092: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-16 03:16:13 +0000 UTC 2022-11-16 03:16:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 03:16:15.092: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.092: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-16 03:16:13 +0000 UTC 2022-11-16 03:16:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 03:16:15.092: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:13 +0000 UTC 2022-11-16 03:16:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mw5c7-764bc7c4b7" is progressing.}
Nov 16 03:16:15.092: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.092: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 03:16:15.092: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mw5c7-764bc7c4b7" has successfully progressed.}
Nov 16 03:16:15.092: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.093: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 03:16:15.093: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-16 03:16:14 +0000 UTC 2022-11-16 03:16:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mw5c7-764bc7c4b7" has successfully progressed.}
Nov 16 03:16:15.093: INFO: Observed deployment test-deployment-mw5c7 in namespace deployment-4353 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 03:16:15.093: INFO: Observed &Deployment event: MODIFIED
Nov 16 03:16:15.093: INFO: Found deployment test-deployment-mw5c7 in namespace deployment-4353 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 16 03:16:15.093: INFO: Deployment test-deployment-mw5c7 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 03:16:15.124: INFO: Deployment "test-deployment-mw5c7":
&Deployment{ObjectMeta:{test-deployment-mw5c7  deployment-4353  0b48690f-a462-4f49-8947-80743e91f70f 145884 1 2022-11-16 03:16:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-11-16 03:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-11-16 03:16:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-11-16 03:16:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eb0e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-mw5c7-764bc7c4b7",LastUpdateTime:2022-11-16 03:16:15 +0000 UTC,LastTransitionTime:2022-11-16 03:16:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 03:16:15.134: INFO: New ReplicaSet "test-deployment-mw5c7-764bc7c4b7" of Deployment "test-deployment-mw5c7":
&ReplicaSet{ObjectMeta:{test-deployment-mw5c7-764bc7c4b7  deployment-4353  ef44cd30-1f56-4019-a468-25889e4c9be1 145877 1 2022-11-16 03:16:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mw5c7 0b48690f-a462-4f49-8947-80743e91f70f 0xc005512ef0 0xc005512ef1}] []  [{kube-controller-manager Update apps/v1 2022-11-16 03:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b48690f-a462-4f49-8947-80743e91f70f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:16:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005512f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 03:16:15.154: INFO: Pod "test-deployment-mw5c7-764bc7c4b7-hmb5j" is available:
&Pod{ObjectMeta:{test-deployment-mw5c7-764bc7c4b7-hmb5j test-deployment-mw5c7-764bc7c4b7- deployment-4353  fdc0d788-e4fa-4699-b8c5-151e776592f9 145876 0 2022-11-16 03:16:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:28ae879019349860baafd84378a2de0429ad6dcbee7e55acdbbd9272d7f1c78f cni.projectcalico.org/podIP:172.30.102.197/32 cni.projectcalico.org/podIPs:172.30.102.197/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.197"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.197"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-mw5c7-764bc7c4b7 ef44cd30-1f56-4019-a468-25889e4c9be1 0xc001e8e6a7 0xc001e8e6a8}] []  [{kube-controller-manager Update v1 2022-11-16 03:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef44cd30-1f56-4019-a468-25889e4c9be1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:16:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:16:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:16:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55xgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55xgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c65,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-4k2pz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:16:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:16:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:16:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.197,StartTime:2022-11-16 03:16:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:16:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7f3145d392466ea56535086a69b4e326016e7ef815206ac60543cb3e3790a335,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:15.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4353" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":312,"skipped":6103,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:15.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:15.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3862" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":313,"skipped":6104,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:15.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 16 03:16:20.120: INFO: Successfully updated pod "adopt-release-bmc9f"
STEP: Checking that the Job readopts the Pod
Nov 16 03:16:20.120: INFO: Waiting up to 15m0s for pod "adopt-release-bmc9f" in namespace "job-1912" to be "adopted"
Nov 16 03:16:20.131: INFO: Pod "adopt-release-bmc9f": Phase="Running", Reason="", readiness=true. Elapsed: 10.868363ms
Nov 16 03:16:22.146: INFO: Pod "adopt-release-bmc9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.02573735s
Nov 16 03:16:22.146: INFO: Pod "adopt-release-bmc9f" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 16 03:16:22.734: INFO: Successfully updated pod "adopt-release-bmc9f"
STEP: Checking that the Job releases the Pod
Nov 16 03:16:22.734: INFO: Waiting up to 15m0s for pod "adopt-release-bmc9f" in namespace "job-1912" to be "released"
Nov 16 03:16:22.756: INFO: Pod "adopt-release-bmc9f": Phase="Running", Reason="", readiness=true. Elapsed: 21.674987ms
Nov 16 03:16:24.770: INFO: Pod "adopt-release-bmc9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.036442045s
Nov 16 03:16:24.770: INFO: Pod "adopt-release-bmc9f" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:16:24.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1912" for this suite.

• [SLOW TEST:9.377 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":314,"skipped":6123,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:16:24.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4220
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Nov 16 03:16:24.999: INFO: Found 0 stateful pods, waiting for 3
Nov 16 03:16:35.041: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:16:35.041: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:16:35.041: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Nov 16 03:16:35.138: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 16 03:16:45.252: INFO: Updating stateful set ss2
Nov 16 03:16:45.273: INFO: Waiting for Pod statefulset-4220/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Nov 16 03:16:55.482: INFO: Found 2 stateful pods, waiting for 3
Nov 16 03:17:05.512: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:17:05.512: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:17:05.512: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 16 03:17:05.582: INFO: Updating stateful set ss2
Nov 16 03:17:05.603: INFO: Waiting for Pod statefulset-4220/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Nov 16 03:17:15.689: INFO: Updating stateful set ss2
Nov 16 03:17:15.710: INFO: Waiting for StatefulSet statefulset-4220/ss2 to complete update
Nov 16 03:17:15.710: INFO: Waiting for Pod statefulset-4220/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 03:17:25.734: INFO: Deleting all statefulset in ns statefulset-4220
Nov 16 03:17:25.743: INFO: Scaling statefulset ss2 to 0
Nov 16 03:17:35.827: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 03:17:35.837: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:17:35.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4220" for this suite.

• [SLOW TEST:71.194 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":315,"skipped":6142,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:17:35.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-936.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-936.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-936.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-936.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 03:17:40.371: INFO: DNS probes using dns-936/dns-test-ea482c9f-457c-49aa-803d-5262a5a75fb0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:17:40.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-936" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":316,"skipped":6151,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:17:40.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Nov 16 03:17:40.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 create -f -'
Nov 16 03:17:41.155: INFO: stderr: ""
Nov 16 03:17:41.155: INFO: stdout: "pod/pause created\n"
Nov 16 03:17:41.155: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 16 03:17:41.155: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2085" to be "running and ready"
Nov 16 03:17:41.166: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.102683ms
Nov 16 03:17:43.182: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.027570457s
Nov 16 03:17:43.183: INFO: Pod "pause" satisfied condition "running and ready"
Nov 16 03:17:43.183: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 16 03:17:43.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 label pods pause testing-label=testing-label-value'
Nov 16 03:17:43.332: INFO: stderr: ""
Nov 16 03:17:43.332: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 16 03:17:43.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 get pod pause -L testing-label'
Nov 16 03:17:43.456: INFO: stderr: ""
Nov 16 03:17:43.456: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 16 03:17:43.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 label pods pause testing-label-'
Nov 16 03:17:43.616: INFO: stderr: ""
Nov 16 03:17:43.616: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 16 03:17:43.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 get pod pause -L testing-label'
Nov 16 03:17:43.724: INFO: stderr: ""
Nov 16 03:17:43.724: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
Nov 16 03:17:43.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 delete --grace-period=0 --force -f -'
Nov 16 03:17:43.851: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 03:17:43.851: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 16 03:17:43.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 get rc,svc -l name=pause --no-headers'
Nov 16 03:17:44.001: INFO: stderr: "No resources found in kubectl-2085 namespace.\n"
Nov 16 03:17:44.001: INFO: stdout: ""
Nov 16 03:17:44.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2085 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 03:17:44.116: INFO: stderr: ""
Nov 16 03:17:44.116: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:17:44.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2085" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":317,"skipped":6160,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:17:44.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4432
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4432
STEP: creating replication controller externalsvc in namespace services-4432
I1116 03:17:44.383328      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4432, replica count: 2
I1116 03:17:47.435398      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 16 03:17:47.524: INFO: Creating new exec pod
Nov 16 03:17:49.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=services-4432 exec execpodf78dh -- /bin/sh -x -c nslookup nodeport-service.services-4432.svc.cluster.local'
Nov 16 03:17:50.019: INFO: stderr: "+ nslookup nodeport-service.services-4432.svc.cluster.local\n"
Nov 16 03:17:50.019: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-4432.svc.cluster.local\tcanonical name = externalsvc.services-4432.svc.cluster.local.\nName:\texternalsvc.services-4432.svc.cluster.local\nAddress: 172.21.45.194\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4432, will wait for the garbage collector to delete the pods
Nov 16 03:17:50.125: INFO: Deleting ReplicationController externalsvc took: 43.465585ms
Nov 16 03:17:50.225: INFO: Terminating ReplicationController externalsvc pods took: 100.52595ms
Nov 16 03:17:53.095: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:17:53.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4432" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.020 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":318,"skipped":6163,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:17:53.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Nov 16 03:17:53.258: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:17:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8938" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":319,"skipped":6174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:17:57.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Nov 16 03:17:57.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2239 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 16 03:17:57.446: INFO: stderr: ""
Nov 16 03:17:57.446: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 16 03:17:57.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2239 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Nov 16 03:17:57.921: INFO: stderr: ""
Nov 16 03:17:57.921: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Nov 16 03:17:57.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-2239 delete pods e2e-test-httpd-pod'
Nov 16 03:18:00.515: INFO: stderr: ""
Nov 16 03:18:00.515: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:00.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2239" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":320,"skipped":6229,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:00.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:18:00.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=kubectl-3833 version'
Nov 16 03:18:00.751: INFO: stderr: ""
Nov 16 03:18:00.751: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.12\", GitCommit:\"c6939792865ef0f70f92006081690d77411c8ed5\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T12:20:29Z\", GoVersion:\"go1.17.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.12+7566c4d\", GitCommit:\"22147459669562208c3082141eece03824c0a417\", GitTreeState:\"clean\", BuildDate:\"2022-10-24T10:40:54Z\", GoVersion:\"go1.17.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:00.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3833" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":321,"skipped":6229,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:00.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 03:18:06.055: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:06.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2347" for this suite.

• [SLOW TEST:5.347 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":6247,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 16 03:18:06.314: INFO: Waiting up to 5m0s for pod "pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b" in namespace "emptydir-9522" to be "Succeeded or Failed"
Nov 16 03:18:06.332: INFO: Pod "pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021196ms
Nov 16 03:18:08.371: INFO: Pod "pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057739813s
Nov 16 03:18:10.388: INFO: Pod "pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074884185s
STEP: Saw pod success
Nov 16 03:18:10.389: INFO: Pod "pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b" satisfied condition "Succeeded or Failed"
Nov 16 03:18:10.399: INFO: Trying to get logs from node 10.189.71.151 pod pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b container test-container: <nil>
STEP: delete the pod
Nov 16 03:18:10.497: INFO: Waiting for pod pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b to disappear
Nov 16 03:18:10.513: INFO: Pod pod-779785d4-b1bf-4a4a-af95-8d34bbe7e52b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:10.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9522" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":6259,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:10.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:18:11.449: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:18:14.683: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:14.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5272" for this suite.
STEP: Destroying namespace "webhook-5272-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":324,"skipped":6267,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:14.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:18:15.061: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Nov 16 03:18:15.061: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:15.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6128" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":325,"skipped":6283,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:15.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 03:18:15.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522" in namespace "downward-api-3447" to be "Succeeded or Failed"
Nov 16 03:18:15.266: INFO: Pod "downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522": Phase="Pending", Reason="", readiness=false. Elapsed: 10.605966ms
Nov 16 03:18:17.278: INFO: Pod "downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022498604s
Nov 16 03:18:19.292: INFO: Pod "downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036904791s
Nov 16 03:18:21.305: INFO: Pod "downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049825441s
STEP: Saw pod success
Nov 16 03:18:21.305: INFO: Pod "downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522" satisfied condition "Succeeded or Failed"
Nov 16 03:18:21.324: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522 container client-container: <nil>
STEP: delete the pod
Nov 16 03:18:21.450: INFO: Waiting for pod downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522 to disappear
Nov 16 03:18:21.463: INFO: Pod downwardapi-volume-a679e005-7ae0-4038-b692-be21b6e81522 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:21.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3447" for this suite.

• [SLOW TEST:6.379 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6283,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:21.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 16 03:18:21.699: INFO: Waiting up to 5m0s for pod "pod-51c0527c-8989-4b51-aedb-3e70dbf34b39" in namespace "emptydir-8995" to be "Succeeded or Failed"
Nov 16 03:18:21.710: INFO: Pod "pod-51c0527c-8989-4b51-aedb-3e70dbf34b39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.603111ms
Nov 16 03:18:23.724: INFO: Pod "pod-51c0527c-8989-4b51-aedb-3e70dbf34b39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025252292s
Nov 16 03:18:25.737: INFO: Pod "pod-51c0527c-8989-4b51-aedb-3e70dbf34b39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037988624s
STEP: Saw pod success
Nov 16 03:18:25.737: INFO: Pod "pod-51c0527c-8989-4b51-aedb-3e70dbf34b39" satisfied condition "Succeeded or Failed"
Nov 16 03:18:25.755: INFO: Trying to get logs from node 10.189.71.151 pod pod-51c0527c-8989-4b51-aedb-3e70dbf34b39 container test-container: <nil>
STEP: delete the pod
Nov 16 03:18:25.815: INFO: Waiting for pod pod-51c0527c-8989-4b51-aedb-3e70dbf34b39 to disappear
Nov 16 03:18:25.826: INFO: Pod pod-51c0527c-8989-4b51-aedb-3e70dbf34b39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:25.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8995" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6289,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:25.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:18:26.744: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 16 03:18:28.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 16, 3, 18, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 18, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 16, 3, 18, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 16, 3, 18, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:18:31.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:32.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9240" for this suite.
STEP: Destroying namespace "webhook-9240-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.885 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":328,"skipped":6297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:32.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 16 03:18:32.958: INFO: Waiting up to 5m0s for pod "pod-b3680596-ba17-4ada-b002-b664e251bbce" in namespace "emptydir-8492" to be "Succeeded or Failed"
Nov 16 03:18:33.008: INFO: Pod "pod-b3680596-ba17-4ada-b002-b664e251bbce": Phase="Pending", Reason="", readiness=false. Elapsed: 50.497029ms
Nov 16 03:18:35.021: INFO: Pod "pod-b3680596-ba17-4ada-b002-b664e251bbce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063781056s
Nov 16 03:18:37.037: INFO: Pod "pod-b3680596-ba17-4ada-b002-b664e251bbce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07938109s
STEP: Saw pod success
Nov 16 03:18:37.037: INFO: Pod "pod-b3680596-ba17-4ada-b002-b664e251bbce" satisfied condition "Succeeded or Failed"
Nov 16 03:18:37.048: INFO: Trying to get logs from node 10.189.71.151 pod pod-b3680596-ba17-4ada-b002-b664e251bbce container test-container: <nil>
STEP: delete the pod
Nov 16 03:18:37.114: INFO: Waiting for pod pod-b3680596-ba17-4ada-b002-b664e251bbce to disappear
Nov 16 03:18:37.125: INFO: Pod pod-b3680596-ba17-4ada-b002-b664e251bbce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:37.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8492" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6385,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:37.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:18:37.296: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-b33f825b-69c3-4090-98ff-3979fcb9e2fa
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:39.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8182" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":6385,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 03:18:43.838: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:43.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-304" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":6389,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:18:44.767: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:18:47.831: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:18:47.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6838-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:18:51.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-452" for this suite.
STEP: Destroying namespace "webhook-452-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.304 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":332,"skipped":6392,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:18:51.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Nov 16 03:18:51.545: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:18:53.567: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:18:55.565: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.189.71.150 on the node which pod1 resides and expect scheduled
Nov 16 03:18:55.637: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:18:57.648: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.189.71.150 but use UDP protocol on the node which pod2 resides
Nov 16 03:18:57.758: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:18:59.783: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:19:01.775: INFO: The status of Pod pod3 is Running (Ready = true)
Nov 16 03:19:01.824: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:19:03.843: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Nov 16 03:19:03.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.189.71.150 http://127.0.0.1:54323/hostname] Namespace:hostport-6283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:19:03.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:19:03.855: INFO: ExecWithOptions: Clientset creation
Nov 16 03:19:03.855: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.189.71.150+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.189.71.150, port: 54323
Nov 16 03:19:04.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.189.71.150:54323/hostname] Namespace:hostport-6283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:19:04.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:19:04.169: INFO: ExecWithOptions: Clientset creation
Nov 16 03:19:04.169: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.189.71.150%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.189.71.150, port: 54323 UDP
Nov 16 03:19:04.417: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.189.71.150 54323] Namespace:hostport-6283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:19:04.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:19:04.418: INFO: ExecWithOptions: Clientset creation
Nov 16 03:19:04.418: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.189.71.150+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:19:09.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6283" for this suite.

• [SLOW TEST:18.384 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":333,"skipped":6410,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:19:09.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-f39c5964-37d8-42d3-aa25-beb85295cbbc
STEP: Creating a pod to test consume secrets
Nov 16 03:19:09.847: INFO: Waiting up to 5m0s for pod "pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca" in namespace "secrets-1319" to be "Succeeded or Failed"
Nov 16 03:19:09.863: INFO: Pod "pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca": Phase="Pending", Reason="", readiness=false. Elapsed: 15.194305ms
Nov 16 03:19:11.879: INFO: Pod "pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030770021s
Nov 16 03:19:13.898: INFO: Pod "pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050097561s
STEP: Saw pod success
Nov 16 03:19:13.898: INFO: Pod "pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca" satisfied condition "Succeeded or Failed"
Nov 16 03:19:13.909: INFO: Trying to get logs from node 10.189.71.151 pod pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 03:19:13.983: INFO: Waiting for pod pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca to disappear
Nov 16 03:19:13.998: INFO: Pod pod-secrets-90427dc3-4e7a-4d20-9f0b-16e68db437ca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:19:13.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1319" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":6427,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:19:14.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Nov 16 03:19:14.317: INFO: The status of Pod annotationupdate53b9d7ab-f9f3-4e3f-8337-589ddc0cf8de is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:19:16.331: INFO: The status of Pod annotationupdate53b9d7ab-f9f3-4e3f-8337-589ddc0cf8de is Running (Ready = true)
Nov 16 03:19:16.967: INFO: Successfully updated pod "annotationupdate53b9d7ab-f9f3-4e3f-8337-589ddc0cf8de"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:19:21.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7685" for this suite.

• [SLOW TEST:7.065 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":6436,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:19:21.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:19:33.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-900" for this suite.

• [SLOW TEST:12.154 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":336,"skipped":6450,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:19:33.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 03:19:34.721: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 03:19:34.769: INFO: waiting for watch events with expected annotations
Nov 16 03:19:34.769: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:19:34.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6778" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":337,"skipped":6451,"failed":0}
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:19:34.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 16 03:19:35.134: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 03:20:35.274: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:20:35.296: INFO: Starting informer...
STEP: Starting pod...
Nov 16 03:20:35.570: INFO: Pod is running on 10.189.71.151. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 16 03:20:35.612: INFO: Pod wasn't evicted. Proceeding
Nov 16 03:20:35.612: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 16 03:21:50.678: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:21:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6300" for this suite.

• [SLOW TEST:135.742 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":338,"skipped":6458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:21:50.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 16 03:21:50.931: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1723  5d03ca24-2985-4358-9905-21d2a3760770 150901 0 2022-11-16 03:21:50 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-11-16 03:21:50 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7259l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7259l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:21:50.983: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 16 03:21:53.024: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 16 03:21:53.024: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1723 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:21:53.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:21:53.025: INFO: ExecWithOptions: Clientset creation
Nov 16 03:21:53.025: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1723/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Nov 16 03:21:53.268: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1723 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 03:21:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:21:53.269: INFO: ExecWithOptions: Clientset creation
Nov 16 03:21:53.269: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1723/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Nov 16 03:21:53.497: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:21:53.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1723" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":339,"skipped":6501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:21:53.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Nov 16 03:21:53.876: INFO: Waiting up to 5m0s for pod "security-context-e6deaac1-57a4-4deb-82fd-42573372c058" in namespace "security-context-4550" to be "Succeeded or Failed"
Nov 16 03:21:53.898: INFO: Pod "security-context-e6deaac1-57a4-4deb-82fd-42573372c058": Phase="Pending", Reason="", readiness=false. Elapsed: 21.852861ms
Nov 16 03:21:55.910: INFO: Pod "security-context-e6deaac1-57a4-4deb-82fd-42573372c058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033510084s
Nov 16 03:21:57.927: INFO: Pod "security-context-e6deaac1-57a4-4deb-82fd-42573372c058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050827644s
STEP: Saw pod success
Nov 16 03:21:57.927: INFO: Pod "security-context-e6deaac1-57a4-4deb-82fd-42573372c058" satisfied condition "Succeeded or Failed"
Nov 16 03:21:57.940: INFO: Trying to get logs from node 10.189.71.151 pod security-context-e6deaac1-57a4-4deb-82fd-42573372c058 container test-container: <nil>
STEP: delete the pod
Nov 16 03:21:58.034: INFO: Waiting for pod security-context-e6deaac1-57a4-4deb-82fd-42573372c058 to disappear
Nov 16 03:21:58.045: INFO: Pod security-context-e6deaac1-57a4-4deb-82fd-42573372c058 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:21:58.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4550" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":340,"skipped":6584,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:21:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Nov 16 03:21:58.202: INFO: Creating deployment "webserver-deployment"
Nov 16 03:21:58.220: INFO: Waiting for observed generation 1
Nov 16 03:22:00.271: INFO: Waiting for all required pods to come up
Nov 16 03:22:00.294: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 16 03:22:02.334: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 16 03:22:02.351: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 16 03:22:02.447: INFO: Updating deployment webserver-deployment
Nov 16 03:22:02.447: INFO: Waiting for observed generation 2
Nov 16 03:22:04.465: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 16 03:22:04.476: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 16 03:22:04.501: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 16 03:22:04.526: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 16 03:22:04.526: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 16 03:22:04.534: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 16 03:22:04.557: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 16 03:22:04.557: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 16 03:22:04.591: INFO: Updating deployment webserver-deployment
Nov 16 03:22:04.591: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 16 03:22:04.609: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 16 03:22:04.618: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 03:22:06.658: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4806  8e6d4029-1a6e-4bb1-a9a5-36ef5952cfd2 151630 3 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ba04d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-11-16 03:22:04 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-11-16 03:22:05 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 16 03:22:06.671: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-4806  aaa77203-4320-43e3-9436-302633b86ee1 151619 3 2022-11-16 03:22:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8e6d4029-1a6e-4bb1-a9a5-36ef5952cfd2 0xc0049d78d7 0xc0049d78d8}] []  [{kube-controller-manager Update apps/v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e6d4029-1a6e-4bb1-a9a5-36ef5952cfd2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d7978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 03:22:06.671: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 16 03:22:06.686: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-4806  88d61c69-71a6-43cb-835f-2a859c3ac7e7 151622 3 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8e6d4029-1a6e-4bb1-a9a5-36ef5952cfd2 0xc0049d79d7 0xc0049d79d8}] []  [{kube-controller-manager Update apps/v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e6d4029-1a6e-4bb1-a9a5-36ef5952cfd2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d7a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 16 03:22:06.743: INFO: Pod "webserver-deployment-566f96c878-22zvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-22zvf webserver-deployment-566f96c878- deployment-4806  c4293e86-1ea9-4432-ad04-0a592ce234d6 151711 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:221992a50a3020678d330233a3c0f2095d5c690168d262c5928e0048df95295e cni.projectcalico.org/podIP:172.30.169.155/32 cni.projectcalico.org/podIPs:172.30.169.155/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.155"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.155"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba051a7 0xc00ba051a8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9k9gd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9k9gd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.744: INFO: Pod "webserver-deployment-566f96c878-2snv2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-2snv2 webserver-deployment-566f96c878- deployment-4806  a912dcc6-e95d-41ae-aeab-1548fd9b658a 151604 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba05427 0xc00ba05428}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlh6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlh6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.744: INFO: Pod "webserver-deployment-566f96c878-4hpcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-4hpcv webserver-deployment-566f96c878- deployment-4806  9520da5a-a31c-46ef-891c-75ce6949d89f 151633 0 2022-11-16 03:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba05667 0xc00ba05668}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgqnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgqnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.744: INFO: Pod "webserver-deployment-566f96c878-5vz9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-5vz9p webserver-deployment-566f96c878- deployment-4806  67776aad-1b46-4480-aa62-17ed305869d1 151621 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba05897 0xc00ba05898}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgnxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgnxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.745: INFO: Pod "webserver-deployment-566f96c878-9zlcn" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9zlcn webserver-deployment-566f96c878- deployment-4806  02fb5dc8-9e11-4271-9171-b0c1cf83c43c 151600 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba05ac7 0xc00ba05ac8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7q7sx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7q7sx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.745: INFO: Pod "webserver-deployment-566f96c878-ct6gc" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-ct6gc webserver-deployment-566f96c878- deployment-4806  5497000b-527f-4d4b-8ced-ce71815dfb15 151716 0 2022-11-16 03:22:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:1965c741a8379ec972b03c070c6b2abac24516d8c7011eddcbe74255f4bff15f cni.projectcalico.org/podIP:172.30.102.231/32 cni.projectcalico.org/podIPs:172.30.102.231/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.231"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.231"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba05d07 0xc00ba05d08}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wzvxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wzvxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.231,StartTime:2022-11-16 03:22:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.745: INFO: Pod "webserver-deployment-566f96c878-fvs8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-fvs8q webserver-deployment-566f96c878- deployment-4806  cd5f557d-f334-4acc-b554-d4130d09dda8 151693 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:180214c9bfe9b6932ddf92c5dc6730f8806189d5efb6efe1f0c02660645c1a16 cni.projectcalico.org/podIP:172.30.169.154/32 cni.projectcalico.org/podIPs:172.30.169.154/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.154"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.154"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc00ba05fb7 0xc00ba05fb8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbvjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbvjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.745: INFO: Pod "webserver-deployment-566f96c878-ghrsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-ghrsr webserver-deployment-566f96c878- deployment-4806  837387de-1ac5-4bb3-be3c-7a671fd4e0f2 151672 0 2022-11-16 03:22:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:02bf69d825014f7efe736d0a1d1a9abc268ae51308ebfc77d24fa9170adb41db cni.projectcalico.org/podIP:172.30.169.153/32 cni.projectcalico.org/podIPs:172.30.169.153/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.153"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.153"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc002aa2237 0xc002aa2238}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.169.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hp664,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hp664,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:172.30.169.153,StartTime:2022-11-16 03:22:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.169.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.746: INFO: Pod "webserver-deployment-566f96c878-hp7mc" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-hp7mc webserver-deployment-566f96c878- deployment-4806  ecf3610c-0190-4274-8a62-9a67c0d25adf 151641 0 2022-11-16 03:22:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:e7d1ed8ccdfbe3244328aafd52d2dd09a40a25ab4166bf493d101970ffcf399a cni.projectcalico.org/podIP:172.30.36.127/32 cni.projectcalico.org/podIPs:172.30.36.127/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.127"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.127"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc002aa2527 0xc002aa2528}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hgtvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hgtvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.127,StartTime:2022-11-16 03:22:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.746: INFO: Pod "webserver-deployment-566f96c878-k6rmv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-k6rmv webserver-deployment-566f96c878- deployment-4806  5ea8f999-8f89-4272-a4cc-a04a27c31d4f 151683 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:49a8a85bdc736eab89f7511ce5d3457461956e84eca61e8629a9b3ef4ad70b48 cni.projectcalico.org/podIP:172.30.36.81/32 cni.projectcalico.org/podIPs:172.30.36.81/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.81"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.81"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc002aa29a7 0xc002aa29a8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrs4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrs4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.746: INFO: Pod "webserver-deployment-566f96c878-p78rc" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-p78rc webserver-deployment-566f96c878- deployment-4806  857dd8c9-1662-4c4e-892a-66a767748438 151651 0 2022-11-16 03:22:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:fcdfa938cb8f8678a3567e7f00827bb8b89d504624526002e15116b45ae5b646 cni.projectcalico.org/podIP:172.30.102.206/32 cni.projectcalico.org/podIPs:172.30.102.206/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.206"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.206"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc002aa2c27 0xc002aa2c28}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k77kv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k77kv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.206,StartTime:2022-11-16 03:22:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.746: INFO: Pod "webserver-deployment-566f96c878-znpk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-znpk5 webserver-deployment-566f96c878- deployment-4806  fb110a30-8d51-475c-b6b8-9813459b9a5d 151703 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:22e5bd37fc822fe4fdae481a7ff695e2e7e458b8ab278284e4db1d458c017660 cni.projectcalico.org/podIP:172.30.36.105/32 cni.projectcalico.org/podIPs:172.30.36.105/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.105"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.105"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc002aa2ef7 0xc002aa2ef8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r9dcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r9dcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.747: INFO: Pod "webserver-deployment-566f96c878-zskd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-zskd8 webserver-deployment-566f96c878- deployment-4806  9e9f5fc3-bd92-4072-9e3f-1bc908a41093 151647 0 2022-11-16 03:22:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:c47499a9b9ec623ad8ab23836decf5f31a942a14de03549c03ad71d11c711550 cni.projectcalico.org/podIP:172.30.102.234/32 cni.projectcalico.org/podIPs:172.30.102.234/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.234"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.234"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 aaa77203-4320-43e3-9436-302633b86ee1 0xc002aa3177 0xc002aa3178}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaa77203-4320-43e3-9436-302633b86ee1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xd5kn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xd5kn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.234,StartTime:2022-11-16 03:22:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.747: INFO: Pod "webserver-deployment-5d9fdcc779-2mzjk" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2mzjk webserver-deployment-5d9fdcc779- deployment-4806  912d12bf-0672-4cd7-8a3a-64d6e86d2114 151348 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:dfe09aad6862a77eb69a35406512ca46d1a7152a2945b1dca8b8d1c27d9f3f9f cni.projectcalico.org/podIP:172.30.36.125/32 cni.projectcalico.org/podIPs:172.30.36.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.125"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc002aa3447 0xc002aa3448}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hb95l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hb95l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.125,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b8472f621fc88b267e19feba9f6c821bac82f93d8e604c5205ff87ff96ecd57f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.747: INFO: Pod "webserver-deployment-5d9fdcc779-4hdw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4hdw5 webserver-deployment-5d9fdcc779- deployment-4806  39240d84-b2b4-479b-96ed-f09b92c03f42 151620 0 2022-11-16 03:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc002aa36c7 0xc002aa36c8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q94pf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q94pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.747: INFO: Pod "webserver-deployment-5d9fdcc779-6rsbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-6rsbv webserver-deployment-5d9fdcc779- deployment-4806  7d17700f-5147-4a4c-8e93-e6a21871b841 151745 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:6fc05d0552af029b8251f8507fcd15d72d8f31d35e3107708314c0cdc24724c1 cni.projectcalico.org/podIP:172.30.169.157/32 cni.projectcalico.org/podIPs:172.30.169.157/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.157"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc002aa38d7 0xc002aa38d8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72rbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72rbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.748: INFO: Pod "webserver-deployment-5d9fdcc779-7645w" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7645w webserver-deployment-5d9fdcc779- deployment-4806  178b6cdd-c1ca-4bc6-8389-c0e340b7bd51 151728 0 2022-11-16 03:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:3ba84f5911877b0e444feef315365d2d389e38b2415a52ddc9adc626b73d7c41 cni.projectcalico.org/podIP:172.30.169.156/32 cni.projectcalico.org/podIPs:172.30.169.156/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.156"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.156"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc002aa3b47 0xc002aa3b48}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fp6ls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fp6ls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.748: INFO: Pod "webserver-deployment-5d9fdcc779-8d4v6" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-8d4v6 webserver-deployment-5d9fdcc779- deployment-4806  20f20663-3110-4f6f-a365-d6ec8b4f9f0a 151351 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:9a9a4ed75dc53c30cbaed89435bb28814b3210a49de75411c0fd61a7151ef9d5 cni.projectcalico.org/podIP:172.30.36.116/32 cni.projectcalico.org/podIPs:172.30.36.116/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.116"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.116"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc002aa3dd7 0xc002aa3dd8}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqcjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqcjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.116,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5ad044fa2a661d761d7174ff63cb07ca6206fda393cc67fa634dbfdb29312a1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.749: INFO: Pod "webserver-deployment-5d9fdcc779-9gwnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9gwnc webserver-deployment-5d9fdcc779- deployment-4806  8a040965-51a5-4716-88ce-67e10705af2b 151725 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:1d2407768d218d2e9cf8028f0eac6a829d9cb285854b92a5b4561680ebd2f3cc cni.projectcalico.org/podIP:172.30.36.119/32 cni.projectcalico.org/podIPs:172.30.36.119/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.119"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.119"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055ba077 0xc0055ba078}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqpbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqpbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.750: INFO: Pod "webserver-deployment-5d9fdcc779-b42sp" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-b42sp webserver-deployment-5d9fdcc779- deployment-4806  0af41265-0127-4e10-bf76-d628dd57b003 151694 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:8a9c13bacc1f8290073cfab4e5618c2314025dfe06864cb8b4302ab37c00e6ff cni.projectcalico.org/podIP:172.30.102.241/32 cni.projectcalico.org/podIPs:172.30.102.241/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.241"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.241"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055ba2d7 0xc0055ba2d8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4d57v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4d57v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.750: INFO: Pod "webserver-deployment-5d9fdcc779-cm7x4" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-cm7x4 webserver-deployment-5d9fdcc779- deployment-4806  ed2361c1-418d-4c7c-a53c-e14d1fcde1af 151368 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:0066c0ed3428dbaebbd5e36cd0b4909f9d3154f17b035884fec632cd4b839f2f cni.projectcalico.org/podIP:172.30.169.151/32 cni.projectcalico.org/podIPs:172.30.169.151/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.151"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.151"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055ba547 0xc0055ba548}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.169.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgqgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgqgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:172.30.169.151,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0f4580c36e0aa91e045540f050e82b4422420e5d2f11088e3026ff357412bb47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.169.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.751: INFO: Pod "webserver-deployment-5d9fdcc779-cnrr6" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-cnrr6 webserver-deployment-5d9fdcc779- deployment-4806  b57c6207-369f-49cc-9a7d-f15dedd4e302 151634 0 2022-11-16 03:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055ba7c7 0xc0055ba7c8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8kcm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8kcm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.751: INFO: Pod "webserver-deployment-5d9fdcc779-gxjrl" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gxjrl webserver-deployment-5d9fdcc779- deployment-4806  8a2e88ea-7df3-45cc-a543-330a50c8b1f9 151712 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:a8531416cee389693a382873f4e079f24bf8d35428c802de4ebef4f464d86cfb cni.projectcalico.org/podIP:172.30.102.209/32 cni.projectcalico.org/podIPs:172.30.102.209/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.209"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.209"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055ba9d7 0xc0055ba9d8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvw7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvw7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.752: INFO: Pod "webserver-deployment-5d9fdcc779-h462d" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-h462d webserver-deployment-5d9fdcc779- deployment-4806  fc939fb6-e490-4e01-b518-e9e6fc2c153b 151356 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:022a9a22dd2b26a75e03f7edd492d00efc041b6ae53e024e0ba01a61245580d7 cni.projectcalico.org/podIP:172.30.36.123/32 cni.projectcalico.org/podIPs:172.30.36.123/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.123"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.36.123"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bac67 0xc0055bac68}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.36.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8fnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8fnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:172.30.36.123,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e16b13a1fc4e961fd5f47fd6606968c80092a7fc1f7933dfdb98d3c427fa3d3a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.36.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.752: INFO: Pod "webserver-deployment-5d9fdcc779-j8k7m" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-j8k7m webserver-deployment-5d9fdcc779- deployment-4806  ada2af84-6c06-4c60-ba8a-57bd721900c8 151735 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:dced1391ed5196f0e1986a786075fe96b26c1f5076f72599e195cb4842308a6b cni.projectcalico.org/podIP:172.30.36.85/32 cni.projectcalico.org/podIPs:172.30.36.85/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055baf07 0xc0055baf08}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ln979,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ln979,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.150,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.753: INFO: Pod "webserver-deployment-5d9fdcc779-nk7pw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-nk7pw webserver-deployment-5d9fdcc779- deployment-4806  a35e8a1f-ac83-49a9-a2b0-d8c3728acbed 151352 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:77a43acd8fd91820a7d8fa1a96c80a9ac01d673b5f98ad7745b699489509e89c cni.projectcalico.org/podIP:172.30.102.246/32 cni.projectcalico.org/podIPs:172.30.102.246/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.246"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.246"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bb167 0xc0055bb168}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j772t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j772t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.246,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://74c2f7da4428812cd5bcb51eb4e3ab184edda022d76ee1a65a2d15398fd3ab03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.753: INFO: Pod "webserver-deployment-5d9fdcc779-nrb8m" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-nrb8m webserver-deployment-5d9fdcc779- deployment-4806  bb8a38f7-edee-4b6a-b991-810178abd33e 151311 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:886173a664732d729ad5a447e6b09b53ce6ecd28b4f681a2046a1cd6d150af1d cni.projectcalico.org/podIP:172.30.169.150/32 cni.projectcalico.org/podIPs:172.30.169.150/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.150"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.150"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bb3f7 0xc0055bb3f8}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.169.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjrg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjrg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:172.30.169.150,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a71c629505d80e495075e7b0743b5d67c65f7f034bd32653545298811e1428c3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.169.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.755: INFO: Pod "webserver-deployment-5d9fdcc779-nsn2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-nsn2t webserver-deployment-5d9fdcc779- deployment-4806  7d8cbbfd-c090-4f8f-bc90-2aecfa6979e1 151681 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:ca85e04bdebff32d0e76b57915bd84125448f51423da36c0ae3d8c2be8016a8f cni.projectcalico.org/podIP:172.30.102.225/32 cni.projectcalico.org/podIPs:172.30.102.225/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.225"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.225"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bb677 0xc0055bb678}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sncw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sncw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.756: INFO: Pod "webserver-deployment-5d9fdcc779-plh99" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-plh99 webserver-deployment-5d9fdcc779- deployment-4806  d2f5518c-f3da-4580-9f16-82138272b143 151643 0 2022-11-16 03:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bb8d7 0xc0055bb8d8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkfv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkfv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.756: INFO: Pod "webserver-deployment-5d9fdcc779-ps56q" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ps56q webserver-deployment-5d9fdcc779- deployment-4806  9e20f7a0-532d-4919-b7c2-261998d7bf38 151370 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:2be47fa670935f9ffd3be79154c3ab716913f861acaf0f06e4d1dd5792b7097a cni.projectcalico.org/podIP:172.30.169.152/32 cni.projectcalico.org/podIPs:172.30.169.152/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.152"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.169.152"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bbaf7 0xc0055bbaf8}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.169.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4vxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4vxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.157,PodIP:172.30.169.152,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:22:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ab07bd548a8e5b1ee60bf6a2e4bb10c6e9ab2778102a2c7c932993b86a704f5f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.169.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.757: INFO: Pod "webserver-deployment-5d9fdcc779-tw98m" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-tw98m webserver-deployment-5d9fdcc779- deployment-4806  e3a45aa5-249b-4d63-a862-ca6fbc10a19e 151287 0 2022-11-16 03:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:f12e274feabb1f988301da5ee0af5430a6619c5eb5da89a2bcbae65602e3f52e cni.projectcalico.org/podIP:172.30.102.216/32 cni.projectcalico.org/podIPs:172.30.102.216/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.216"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.216"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bbd77 0xc0055bbd78}] []  [{kube-controller-manager Update v1 2022-11-16 03:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-16 03:22:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8dsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8dsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:172.30.102.216,StartTime:2022-11-16 03:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-16 03:21:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c9cc3ec65cda3d5d6d388906b617316c9690444b291d37a6dfc69f0bc9898bf9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.763: INFO: Pod "webserver-deployment-5d9fdcc779-vk9n6" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vk9n6 webserver-deployment-5d9fdcc779- deployment-4806  6e52898f-7e8f-480d-a772-425fbc847afe 151737 0 2022-11-16 03:22:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:43a8ba2f8304a0893ad41787ba87b06fd25f595a2293d06e61ff97f0c8e27726 cni.projectcalico.org/podIP:172.30.102.227/32 cni.projectcalico.org/podIPs:172.30.102.227/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.227"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.102.227"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc0055bbff7 0xc0055bbff8}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-11-16 03:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99zhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99zhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 03:22:06.764: INFO: Pod "webserver-deployment-5d9fdcc779-xrhgk" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xrhgk webserver-deployment-5d9fdcc779- deployment-4806  0b062352-8e3b-439b-ae4a-52c4a5e4553e 151628 0 2022-11-16 03:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 88d61c69-71a6-43cb-835f-2a859c3ac7e7 0xc001e8e267 0xc001e8e268}] []  [{kube-controller-manager Update v1 2022-11-16 03:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d61c69-71a6-43cb-835f-2a859c3ac7e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-16 03:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5c9sg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5c9sg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.71.151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xt2sk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-16 03:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.71.151,PodIP:,StartTime:2022-11-16 03:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:22:06.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4806" for this suite.

• [SLOW TEST:8.754 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":341,"skipped":6587,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:22:06.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Nov 16 03:22:07.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5" in namespace "projected-5982" to be "Succeeded or Failed"
Nov 16 03:22:07.146: INFO: Pod "downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.306025ms
Nov 16 03:22:09.206: INFO: Pod "downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08533013s
Nov 16 03:22:11.219: INFO: Pod "downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098141614s
Nov 16 03:22:13.240: INFO: Pod "downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.118721665s
STEP: Saw pod success
Nov 16 03:22:13.240: INFO: Pod "downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5" satisfied condition "Succeeded or Failed"
Nov 16 03:22:13.252: INFO: Trying to get logs from node 10.189.71.151 pod downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5 container client-container: <nil>
STEP: delete the pod
Nov 16 03:22:13.315: INFO: Waiting for pod downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5 to disappear
Nov 16 03:22:13.325: INFO: Pod downwardapi-volume-0b82aaeb-8df6-4054-ac11-ec59b3b3f0d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:22:13.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5982" for this suite.

• [SLOW TEST:6.502 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":6603,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:22:13.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-j4hm
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 03:22:13.603: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-j4hm" in namespace "subpath-8929" to be "Succeeded or Failed"
Nov 16 03:22:13.612: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.182527ms
Nov 16 03:22:15.626: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 2.02246679s
Nov 16 03:22:17.640: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 4.037346737s
Nov 16 03:22:19.651: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 6.048213717s
Nov 16 03:22:21.664: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 8.060920506s
Nov 16 03:22:23.677: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 10.073425469s
Nov 16 03:22:25.691: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 12.087643166s
Nov 16 03:22:27.709: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 14.106010564s
Nov 16 03:22:29.728: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 16.124827046s
Nov 16 03:22:31.742: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 18.138650658s
Nov 16 03:22:33.757: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=true. Elapsed: 20.15406096s
Nov 16 03:22:35.769: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Running", Reason="", readiness=false. Elapsed: 22.16633771s
Nov 16 03:22:37.792: INFO: Pod "pod-subpath-test-projected-j4hm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.189130006s
STEP: Saw pod success
Nov 16 03:22:37.792: INFO: Pod "pod-subpath-test-projected-j4hm" satisfied condition "Succeeded or Failed"
Nov 16 03:22:37.804: INFO: Trying to get logs from node 10.189.71.151 pod pod-subpath-test-projected-j4hm container test-container-subpath-projected-j4hm: <nil>
STEP: delete the pod
Nov 16 03:22:37.876: INFO: Waiting for pod pod-subpath-test-projected-j4hm to disappear
Nov 16 03:22:37.886: INFO: Pod pod-subpath-test-projected-j4hm no longer exists
STEP: Deleting pod pod-subpath-test-projected-j4hm
Nov 16 03:22:37.886: INFO: Deleting pod "pod-subpath-test-projected-j4hm" in namespace "subpath-8929"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:22:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8929" for this suite.

• [SLOW TEST:24.577 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":343,"skipped":6619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:22:37.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3007
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Nov 16 03:22:38.121: INFO: Found 0 stateful pods, waiting for 3
Nov 16 03:22:48.136: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:22:48.136: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:22:48.136: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 03:22:48.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-3007 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 03:22:48.559: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 03:22:48.559: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 03:22:48.559: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Nov 16 03:22:58.675: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 16 03:23:08.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-3007 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 03:23:09.129: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 03:23:09.129: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 03:23:09.129: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 03:23:19.223: INFO: Waiting for StatefulSet statefulset-3007/ss2 to complete update
STEP: Rolling back to a previous revision
Nov 16 03:23:29.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-3007 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 03:23:29.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 03:23:29.538: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 03:23:29.538: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 03:23:39.653: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 16 03:23:49.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1928056134 --namespace=statefulset-3007 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 03:23:50.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 03:23:50.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 03:23:50.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 03:24:00.118: INFO: Waiting for StatefulSet statefulset-3007/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Nov 16 03:24:10.147: INFO: Deleting all statefulset in ns statefulset-3007
Nov 16 03:24:10.156: INFO: Scaling statefulset ss2 to 0
Nov 16 03:24:20.215: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 03:24:20.223: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:24:20.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3007" for this suite.

• [SLOW TEST:102.357 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":344,"skipped":6674,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:24:20.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 16 03:24:20.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
Nov 16 03:24:31.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:25:13.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6273" for this suite.

• [SLOW TEST:53.428 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":345,"skipped":6679,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 03:25:13.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1928056134
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 03:25:14.706: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 03:25:17.798: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 03:25:18.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7644" for this suite.
STEP: Destroying namespace "webhook-7644-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":346,"skipped":6687,"failed":0}
SSSSSSSSSSSSSSSSSSSNov 16 03:25:18.309: INFO: Running AfterSuite actions on all nodes
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Nov 16 03:25:18.309: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Nov 16 03:25:18.309: INFO: Running AfterSuite actions on node 1
Nov 16 03:25:18.309: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6706,"failed":0}

Ran 346 of 7052 Specs in 6529.546 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6706 Skipped
PASS

Ginkgo ran 1 suite in 1h48m52.954219511s
Test Suite Passed
