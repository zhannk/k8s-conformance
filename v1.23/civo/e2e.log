I0913 11:11:40.701141      23 e2e.go:132] Starting e2e run "40abff15-449d-475b-824e-8301d7601346" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1663067500 - Will randomize all specs
Will run 346 of 7044 specs

Sep 13 11:11:42.982: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:11:42.985: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 13 11:12:12.987: INFO: Unexpected error listing nodes: Get "https://10.43.0.1:443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0": dial tcp 10.43.0.1:443: i/o timeout
Sep 13 11:12:43.033: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 13 11:12:43.069: INFO: 5 / 5 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 13 11:12:43.069: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 13 11:12:43.069: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 13 11:12:43.075: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'civo-csi-node' (0 seconds elapsed)
Sep 13 11:12:43.075: INFO: e2e test version: v1.23.6
Sep 13 11:12:43.077: INFO: kube-apiserver version: v1.23.6+k3s1
Sep 13 11:12:43.077: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:12:43.081: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:12:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
W0913 11:12:43.121143      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Sep 13 11:12:43.121: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-95d6a756-ebec-4437-9261-327c6fcbf5b6
STEP: Creating a pod to test consume secrets
Sep 13 11:12:43.174: INFO: Waiting up to 5m0s for pod "pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b" in namespace "secrets-6011" to be "Succeeded or Failed"
Sep 13 11:12:43.182: INFO: Pod "pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.239081ms
Sep 13 11:12:45.201: INFO: Pod "pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027362897s
Sep 13 11:12:47.215: INFO: Pod "pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041237733s
STEP: Saw pod success
Sep 13 11:12:47.216: INFO: Pod "pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b" satisfied condition "Succeeded or Failed"
Sep 13 11:12:47.222: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:12:47.282: INFO: Waiting for pod pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b to disappear
Sep 13 11:12:47.288: INFO: Pod pod-secrets-1b10f7ba-848f-4771-9755-c1e1b8db351b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:12:47.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6011" for this suite.
STEP: Destroying namespace "secret-namespace-7943" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":19,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:12:47.324: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 13 11:12:51.437: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:12:51.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9826" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":48,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:12:51.481: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 13 11:12:51.519: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 11:12:51.534: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 11:12:51.538: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom before test
Sep 13 11:12:51.544: INFO: civo-csi-controller-0 from kube-system started at 2022-09-09 17:03:22 +0000 UTC (4 container statuses recorded)
Sep 13 11:12:51.544: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:12:51.544: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 13 11:12:51.544: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 13 11:12:51.544: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 13 11:12:51.544: INFO: civo-csi-node-62s5w from kube-system started at 2022-09-09 16:30:36 +0000 UTC (2 container statuses recorded)
Sep 13 11:12:51.544: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:12:51.544: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 11:12:51.544: INFO: civo-ccm-869574f9b7-sjssw from kube-system started at 2022-09-09 16:30:24 +0000 UTC (1 container statuses recorded)
Sep 13 11:12:51.544: INFO: 	Container civo-ccm ready: true, restart count 1
Sep 13 11:12:51.544: INFO: coredns-d76bd69b-h4czc from kube-system started at 2022-09-09 16:31:40 +0000 UTC (1 container statuses recorded)
Sep 13 11:12:51.544: INFO: 	Container coredns ready: true, restart count 0
Sep 13 11:12:51.544: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl before test
Sep 13 11:12:51.552: INFO: civo-csi-node-5h4x4 from kube-system started at 2022-09-13 10:37:12 +0000 UTC (2 container statuses recorded)
Sep 13 11:12:51.552: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:12:51.552: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 11:12:51.552: INFO: sonobuoy-e2e-job-7e148db81c5d4b8a from sonobuoy started at 2022-09-13 11:11:39 +0000 UTC (2 container statuses recorded)
Sep 13 11:12:51.552: INFO: 	Container e2e ready: true, restart count 0
Sep 13 11:12:51.552: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 11:12:51.552: INFO: sonobuoy from sonobuoy started at 2022-09-13 11:11:38 +0000 UTC (1 container statuses recorded)
Sep 13 11:12:51.552: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
STEP: verifying the node has the label node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
Sep 13 11:12:51.609: INFO: Pod civo-csi-controller-0 requesting resource cpu=0m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
Sep 13 11:12:51.609: INFO: Pod civo-csi-node-62s5w requesting resource cpu=0m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
Sep 13 11:12:51.609: INFO: Pod civo-ccm-869574f9b7-sjssw requesting resource cpu=0m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
Sep 13 11:12:51.609: INFO: Pod coredns-d76bd69b-h4czc requesting resource cpu=100m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
Sep 13 11:12:51.609: INFO: Pod civo-csi-node-5h4x4 requesting resource cpu=0m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
Sep 13 11:12:51.609: INFO: Pod sonobuoy-e2e-job-7e148db81c5d4b8a requesting resource cpu=0m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
Sep 13 11:12:51.609: INFO: Pod sonobuoy requesting resource cpu=0m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
STEP: Starting Pods to consume most of the cluster CPU.
Sep 13 11:12:51.609: INFO: Creating a pod which consumes cpu=2730m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
Sep 13 11:12:51.627: INFO: Creating a pod which consumes cpu=2800m on Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0.171467254ff2ff09], Reason = [Scheduled], Message = [Successfully assigned sched-pred-108/filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0 to k3s-conformance-test-12310-81a66c-node-pool-5423-44vom]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501.17146725504b66b2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-108/filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501 to k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0.17146724668512c7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501.171467254b67c427], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0.17146724683ca838], Reason = [Created], Message = [Created container filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501.171467254d1ecfd3], Reason = [Created], Message = [Created container filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0.171467246e766ea5], Reason = [Started], Message = [Started container filler-pod-b670ec58-ad78-4b9e-9080-be969f0488f0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501.171467255455b00a], Reason = [Started], Message = [Started container filler-pod-7ebca1d8-d963-4db9-900e-e5068cd6f501]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17146725c9d1a854], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:12:54.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-108" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":3,"skipped":134,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:12:54.788: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Sep 13 11:12:54.819: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:13:10.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4631" for this suite.

• [SLOW TEST:15.471 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":4,"skipped":137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:13:10.260: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
Sep 13 11:13:10.319: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:13:12.340: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:13:13.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3274" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":5,"skipped":178,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:13:13.398: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 13 11:13:13.451: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 13 11:14:13.492: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:14:13.498: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:14:13.544: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Sep 13 11:14:13.547: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:14:13.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6191" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:14:13.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4127" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.266 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":6,"skipped":179,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:14:13.665: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:14:20.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-476" for this suite.

• [SLOW TEST:7.099 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":7,"skipped":180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:14:20.764: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-9370
STEP: creating service affinity-nodeport in namespace services-9370
STEP: creating replication controller affinity-nodeport in namespace services-9370
I0913 11:14:20.830305      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9370, replica count: 3
I0913 11:14:23.881838      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:14:23.920: INFO: Creating new exec pod
Sep 13 11:14:26.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Sep 13 11:14:27.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 13 11:14:27.501: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:14:27.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.67.7 80'
Sep 13 11:14:27.737: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.67.7 80\nConnection to 10.43.67.7 80 port [tcp/http] succeeded!\n"
Sep 13 11:14:27.737: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:14:27.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 32539'
Sep 13 11:16:41.344: INFO: rc: 1
Sep 13 11:16:41.344: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 32539:
Command stdout:

stderr:
error: error sending request: Post "https://10.43.0.1:443/api/v1/namespaces/services-9370/pods/execpod-affinity7jkb8/exec?command=%2Fbin%2Fsh&command=-x&command=-c&command=echo+hostName+%7C+nc+-v+-t+-w+2+192.168.1.5+32539&container=agnhost-container&stderr=true&stdout=true": dial tcp 10.43.0.1:443: connect: connection timed out

error:
exit status 1
Retrying...
Sep 13 11:16:42.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 32539'
Sep 13 11:16:42.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.5 32539\nConnection to 192.168.1.5 32539 port [tcp/*] succeeded!\n"
Sep 13 11:16:42.598: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:16:42.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.6 32539'
Sep 13 11:16:42.822: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.6 32539\nConnection to 192.168.1.6 32539 port [tcp/*] succeeded!\n"
Sep 13 11:16:42.822: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:16:42.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9370 exec execpod-affinity7jkb8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.5:32539/ ; done'
Sep 13 11:16:43.225: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:32539/\n"
Sep 13 11:16:43.225: INFO: stdout: "\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj\naffinity-nodeport-vmnnj"
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Received response from host: affinity-nodeport-vmnnj
Sep 13 11:16:43.225: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9370, will wait for the garbage collector to delete the pods
Sep 13 11:16:43.324: INFO: Deleting ReplicationController affinity-nodeport took: 11.604856ms
Sep 13 11:16:43.424: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.486941ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:16:45.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9370" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:144.931 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":8,"skipped":219,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:16:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:16:45.728: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 13 11:16:46.766: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:16:46.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4735" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":9,"skipped":221,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:16:46.795: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 11:16:46.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:16:46.891: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:16:47.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:16:47.912: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:16:48.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 11:16:48.917: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 13 11:16:48.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:16:48.959: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:16:49.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:16:49.986: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:16:50.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:16:50.977: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:16:51.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:16:51.974: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:16:52.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 11:16:52.983: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4499, will wait for the garbage collector to delete the pods
Sep 13 11:16:53.065: INFO: Deleting DaemonSet.extensions daemon-set took: 13.839952ms
Sep 13 11:16:53.166: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.777777ms
Sep 13 11:16:55.583: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:16:55.583: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 13 11:16:55.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"400330"},"items":null}

Sep 13 11:16:55.599: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"400330"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:16:55.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4499" for this suite.

• [SLOW TEST:8.835 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":10,"skipped":238,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:16:55.630: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5851
Sep 13 11:16:55.670: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:16:57.682: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep 13 11:16:57.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 13 11:16:57.934: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 13 11:16:57.934: INFO: stdout: "iptables"
Sep 13 11:16:57.934: INFO: proxyMode: iptables
Sep 13 11:16:57.958: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 13 11:16:57.964: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5851
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5851
I0913 11:16:57.983544      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5851, replica count: 3
I0913 11:17:01.035163      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:17:01.048: INFO: Creating new exec pod
Sep 13 11:17:04.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Sep 13 11:17:04.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep 13 11:17:04.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:17:04.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.175.179 80'
Sep 13 11:17:04.599: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.175.179 80\nConnection to 10.43.175.179 80 port [tcp/http] succeeded!\n"
Sep 13 11:17:04.599: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:17:04.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 30029'
Sep 13 11:17:04.841: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.5 30029\nConnection to 192.168.1.5 30029 port [tcp/*] succeeded!\n"
Sep 13 11:17:04.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:17:04.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.6 30029'
Sep 13 11:17:05.091: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.6 30029\nConnection to 192.168.1.6 30029 port [tcp/*] succeeded!\n"
Sep 13 11:17:05.091: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:17:05.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.5:30029/ ; done'
Sep 13 11:17:05.567: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n"
Sep 13 11:17:05.567: INFO: stdout: "\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd\naffinity-nodeport-timeout-x77nd"
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.567: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Received response from host: affinity-nodeport-timeout-x77nd
Sep 13 11:17:05.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.5:30029/'
Sep 13 11:17:05.818: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n"
Sep 13 11:17:05.818: INFO: stdout: "affinity-nodeport-timeout-x77nd"
Sep 13 11:17:25.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-5851 exec execpod-affinityt5gr8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.5:30029/'
Sep 13 11:17:26.067: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.5:30029/\n"
Sep 13 11:17:26.067: INFO: stdout: "affinity-nodeport-timeout-cv6mr"
Sep 13 11:17:26.067: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5851, will wait for the garbage collector to delete the pods
Sep 13 11:17:26.163: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.415129ms
Sep 13 11:17:26.265: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.075237ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5851" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:32.378 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":11,"skipped":243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:28.009: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:17:28.037: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 13 11:17:30.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 create -f -'
Sep 13 11:17:31.195: INFO: stderr: ""
Sep 13 11:17:31.195: INFO: stdout: "e2e-test-crd-publish-openapi-5674-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 13 11:17:31.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 delete e2e-test-crd-publish-openapi-5674-crds test-foo'
Sep 13 11:17:31.338: INFO: stderr: ""
Sep 13 11:17:31.338: INFO: stdout: "e2e-test-crd-publish-openapi-5674-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 13 11:17:31.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 apply -f -'
Sep 13 11:17:31.671: INFO: stderr: ""
Sep 13 11:17:31.671: INFO: stdout: "e2e-test-crd-publish-openapi-5674-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 13 11:17:31.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 delete e2e-test-crd-publish-openapi-5674-crds test-foo'
Sep 13 11:17:31.769: INFO: stderr: ""
Sep 13 11:17:31.769: INFO: stdout: "e2e-test-crd-publish-openapi-5674-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Sep 13 11:17:31.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 create -f -'
Sep 13 11:17:32.036: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 13 11:17:32.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 create -f -'
Sep 13 11:17:32.300: INFO: rc: 1
Sep 13 11:17:32.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 apply -f -'
Sep 13 11:17:32.556: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 13 11:17:32.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 create -f -'
Sep 13 11:17:32.799: INFO: rc: 1
Sep 13 11:17:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 --namespace=crd-publish-openapi-7483 apply -f -'
Sep 13 11:17:33.081: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 13 11:17:33.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 explain e2e-test-crd-publish-openapi-5674-crds'
Sep 13 11:17:33.320: INFO: stderr: ""
Sep 13 11:17:33.320: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5674-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 13 11:17:33.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 explain e2e-test-crd-publish-openapi-5674-crds.metadata'
Sep 13 11:17:33.569: INFO: stderr: ""
Sep 13 11:17:33.569: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5674-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 13 11:17:33.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 explain e2e-test-crd-publish-openapi-5674-crds.spec'
Sep 13 11:17:33.809: INFO: stderr: ""
Sep 13 11:17:33.809: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5674-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 13 11:17:33.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 explain e2e-test-crd-publish-openapi-5674-crds.spec.bars'
Sep 13 11:17:34.056: INFO: stderr: ""
Sep 13 11:17:34.057: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5674-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 13 11:17:34.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-7483 explain e2e-test-crd-publish-openapi-5674-crds.spec.bars2'
Sep 13 11:17:34.310: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:36.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7483" for this suite.

• [SLOW TEST:8.396 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":12,"skipped":286,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:36.405: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:17:36.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25" in namespace "projected-2841" to be "Succeeded or Failed"
Sep 13 11:17:36.461: INFO: Pod "downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879474ms
Sep 13 11:17:38.485: INFO: Pod "downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027626592s
Sep 13 11:17:40.506: INFO: Pod "downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048483871s
STEP: Saw pod success
Sep 13 11:17:40.506: INFO: Pod "downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25" satisfied condition "Succeeded or Failed"
Sep 13 11:17:40.512: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25 container client-container: <nil>
STEP: delete the pod
Sep 13 11:17:40.557: INFO: Waiting for pod downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25 to disappear
Sep 13 11:17:40.567: INFO: Pod downwardapi-volume-81e96810-1362-448b-aa84-5e5efe6a9e25 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:40.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2841" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":300,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:40.585: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-af8671fb-7075-49de-954a-00b8916b582f
STEP: Creating secret with name s-test-opt-upd-f169b1f4-6cb0-41bb-9de8-0eea431d8163
STEP: Creating the pod
Sep 13 11:17:40.651: INFO: The status of Pod pod-projected-secrets-14cd95ab-2205-47a2-988b-edeb440bee1c is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:17:42.662: INFO: The status of Pod pod-projected-secrets-14cd95ab-2205-47a2-988b-edeb440bee1c is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-af8671fb-7075-49de-954a-00b8916b582f
STEP: Updating secret s-test-opt-upd-f169b1f4-6cb0-41bb-9de8-0eea431d8163
STEP: Creating secret with name s-test-opt-create-bc8a26df-820e-4a8f-8de2-6410faa1aec5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:44.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8034" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":303,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:44.791: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:17:44.821: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:45.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7769" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":15,"skipped":311,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:45.412: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
Sep 13 11:17:45.493: INFO: Waiting up to 5m0s for pod "client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488" in namespace "containers-7879" to be "Succeeded or Failed"
Sep 13 11:17:45.505: INFO: Pod "client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488": Phase="Pending", Reason="", readiness=false. Elapsed: 12.45753ms
Sep 13 11:17:47.524: INFO: Pod "client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031645723s
Sep 13 11:17:49.546: INFO: Pod "client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053042873s
STEP: Saw pod success
Sep 13 11:17:49.546: INFO: Pod "client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488" satisfied condition "Succeeded or Failed"
Sep 13 11:17:49.552: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom pod client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:17:49.600: INFO: Waiting for pod client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488 to disappear
Sep 13 11:17:49.605: INFO: Pod client-containers-d5f9e645-dad6-4973-b5c5-6e275c769488 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:49.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7879" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":321,"failed":0}
SS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:49.615: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Sep 13 11:17:49.652: INFO: Waiting up to 5m0s for pod "security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632" in namespace "security-context-9990" to be "Succeeded or Failed"
Sep 13 11:17:49.658: INFO: Pod "security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632": Phase="Pending", Reason="", readiness=false. Elapsed: 6.314991ms
Sep 13 11:17:51.672: INFO: Pod "security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020126174s
Sep 13 11:17:53.695: INFO: Pod "security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043542692s
STEP: Saw pod success
Sep 13 11:17:53.695: INFO: Pod "security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632" satisfied condition "Succeeded or Failed"
Sep 13 11:17:53.702: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632 container test-container: <nil>
STEP: delete the pod
Sep 13 11:17:53.746: INFO: Waiting for pod security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632 to disappear
Sep 13 11:17:53.752: INFO: Pod security-context-ef742448-7a0b-40f5-aa60-6f8b95cb1632 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:53.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9990" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":17,"skipped":323,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:53.772: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:17:53.812: INFO: Creating deployment "test-recreate-deployment"
Sep 13 11:17:53.821: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 13 11:17:53.836: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Sep 13 11:17:55.853: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 13 11:17:55.859: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 13 11:17:55.870: INFO: Updating deployment test-recreate-deployment
Sep 13 11:17:55.870: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:17:55.956: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2418  996a7f86-9be8-497c-a50e-19725d6b0b30 400759 2 2022-09-13 11:17:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cbdae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-13 11:17:56 +0000 UTC,LastTransitionTime:2022-09-13 11:17:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-09-13 11:17:56 +0000 UTC,LastTransitionTime:2022-09-13 11:17:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 13 11:17:55.963: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-2418  937a404d-1804-461e-8c5e-9c44e88cdae3 400755 1 2022-09-13 11:17:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 996a7f86-9be8-497c-a50e-19725d6b0b30 0xc004d691d7 0xc004d691d8}] []  [{k3s Update apps/v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"996a7f86-9be8-497c-a50e-19725d6b0b30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d69288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:17:55.963: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 13 11:17:55.964: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-2418  905e6a90-3319-4226-9c84-51a34fa3abed 400748 2 2022-09-13 11:17:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 996a7f86-9be8-497c-a50e-19725d6b0b30 0xc004d690a7 0xc004d690a8}] []  [{k3s Update apps/v1 2022-09-13 11:17:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"996a7f86-9be8-497c-a50e-19725d6b0b30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d69168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:17:55.967: INFO: Pod "test-recreate-deployment-5b99bd5487-xjnfm" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-xjnfm test-recreate-deployment-5b99bd5487- deployment-2418  f9ad55d7-406f-436d-b182-1bfaeec793d8 400760 0 2022-09-13 11:17:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 937a404d-1804-461e-8c5e-9c44e88cdae3 0xc004d696e7 0xc004d696e8}] []  [{k3s Update v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937a404d-1804-461e-8c5e-9c44e88cdae3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:17:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvh9x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvh9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:17:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:17:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:17:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:17:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2022-09-13 11:17:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:17:55.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2418" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":18,"skipped":333,"failed":0}
SSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:17:55.978: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Sep 13 11:17:56.022: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:17:58.038: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.1.5 on the node which pod1 resides and expect scheduled
Sep 13 11:17:58.049: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:00.069: INFO: The status of Pod pod2 is Running (Ready = false)
Sep 13 11:18:02.066: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.1.5 but use UDP protocol on the node which pod2 resides
Sep 13 11:18:02.084: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:04.112: INFO: The status of Pod pod3 is Running (Ready = true)
Sep 13 11:18:04.121: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:06.135: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Sep 13 11:18:06.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.1.5 http://127.0.0.1:54323/hostname] Namespace:hostport-5975 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:18:06.140: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:18:06.141: INFO: ExecWithOptions: Clientset creation
Sep 13 11:18:06.141: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-5975/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.1.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.5, port: 54323
Sep 13 11:18:06.265: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.1.5:54323/hostname] Namespace:hostport-5975 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:18:06.265: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:18:06.265: INFO: ExecWithOptions: Clientset creation
Sep 13 11:18:06.265: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-5975/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.1.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.5, port: 54323 UDP
Sep 13 11:18:06.349: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.1.5 54323] Namespace:hostport-5975 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:18:06.349: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:18:06.349: INFO: ExecWithOptions: Clientset creation
Sep 13 11:18:06.350: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-5975/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+192.168.1.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:11.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5975" for this suite.

• [SLOW TEST:15.502 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":19,"skipped":338,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:11.481: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-6689
STEP: creating service affinity-clusterip-transition in namespace services-6689
STEP: creating replication controller affinity-clusterip-transition in namespace services-6689
I0913 11:18:11.543401      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6689, replica count: 3
I0913 11:18:14.593922      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:18:14.620: INFO: Creating new exec pod
Sep 13 11:18:17.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6689 exec execpod-affinityqtv76 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Sep 13 11:18:17.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 13 11:18:17.948: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:18:17.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6689 exec execpod-affinityqtv76 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.237.183 80'
Sep 13 11:18:18.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.237.183 80\nConnection to 10.43.237.183 80 port [tcp/http] succeeded!\n"
Sep 13 11:18:18.148: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:18:18.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6689 exec execpod-affinityqtv76 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.237.183:80/ ; done'
Sep 13 11:18:18.594: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n"
Sep 13 11:18:18.594: INFO: stdout: "\naffinity-clusterip-transition-lnvwh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-lnvwh\naffinity-clusterip-transition-lnvwh\naffinity-clusterip-transition-lnvwh\naffinity-clusterip-transition-fxgt2\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-fxgt2\naffinity-clusterip-transition-lnvwh\naffinity-clusterip-transition-lnvwh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-fxgt2\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-lnvwh"
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-fxgt2
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-fxgt2
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-fxgt2
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:18.594: INFO: Received response from host: affinity-clusterip-transition-lnvwh
Sep 13 11:18:18.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6689 exec execpod-affinityqtv76 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.237.183:80/ ; done'
Sep 13 11:18:19.021: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.237.183:80/\n"
Sep 13 11:18:19.022: INFO: stdout: "\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh\naffinity-clusterip-transition-v7rbh"
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Received response from host: affinity-clusterip-transition-v7rbh
Sep 13 11:18:19.022: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6689, will wait for the garbage collector to delete the pods
Sep 13 11:18:19.103: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.834524ms
Sep 13 11:18:19.203: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.693022ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:21.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6689" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.558 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":20,"skipped":343,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:21.040: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Sep 13 11:18:21.066: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:24.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-246" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":21,"skipped":344,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:24.950: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:38.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9720" for this suite.
STEP: Destroying namespace "nsdeletetest-6525" for this suite.
Sep 13 11:18:38.092: INFO: Namespace nsdeletetest-6525 was already deleted
STEP: Destroying namespace "nsdeletetest-6166" for this suite.

• [SLOW TEST:13.149 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":22,"skipped":347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:38.102: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
Sep 13 11:18:38.138: INFO: Creating simple deployment test-deployment-hm29b
Sep 13 11:18:38.168: INFO: deployment "test-deployment-hm29b" doesn't have the required revision set
STEP: Getting /status
Sep 13 11:18:40.204: INFO: Deployment test-deployment-hm29b has Conditions: [{Available True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hm29b-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Sep 13 11:18:40.217: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 18, 40, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 18, 38, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-hm29b-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Sep 13 11:18:40.221: INFO: Observed &Deployment event: ADDED
Sep 13 11:18:40.221: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hm29b-764bc7c4b7"}
Sep 13 11:18:40.221: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.221: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hm29b-764bc7c4b7"}
Sep 13 11:18:40.221: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 13 11:18:40.222: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.222: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 13 11:18:40.222: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hm29b-764bc7c4b7" is progressing.}
Sep 13 11:18:40.222: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.222: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 13 11:18:40.222: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hm29b-764bc7c4b7" has successfully progressed.}
Sep 13 11:18:40.222: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.222: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 13 11:18:40.222: INFO: Observed Deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hm29b-764bc7c4b7" has successfully progressed.}
Sep 13 11:18:40.222: INFO: Found Deployment test-deployment-hm29b in namespace deployment-5318 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 13 11:18:40.222: INFO: Deployment test-deployment-hm29b has an updated status
STEP: patching the Statefulset Status
Sep 13 11:18:40.222: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 13 11:18:40.237: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Sep 13 11:18:40.240: INFO: Observed &Deployment event: ADDED
Sep 13 11:18:40.240: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hm29b-764bc7c4b7"}
Sep 13 11:18:40.240: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.240: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hm29b-764bc7c4b7"}
Sep 13 11:18:40.240: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 13 11:18:40.241: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.241: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 13 11:18:40.241: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:38 +0000 UTC 2022-09-13 11:18:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hm29b-764bc7c4b7" is progressing.}
Sep 13 11:18:40.242: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.242: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 13 11:18:40.242: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hm29b-764bc7c4b7" has successfully progressed.}
Sep 13 11:18:40.242: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.242: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 13 11:18:40.242: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-09-13 11:18:40 +0000 UTC 2022-09-13 11:18:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hm29b-764bc7c4b7" has successfully progressed.}
Sep 13 11:18:40.242: INFO: Observed deployment test-deployment-hm29b in namespace deployment-5318 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 13 11:18:40.242: INFO: Observed &Deployment event: MODIFIED
Sep 13 11:18:40.243: INFO: Found deployment test-deployment-hm29b in namespace deployment-5318 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep 13 11:18:40.243: INFO: Deployment test-deployment-hm29b has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:18:40.248: INFO: Deployment "test-deployment-hm29b":
&Deployment{ObjectMeta:{test-deployment-hm29b  deployment-5318  9294aab6-9ccd-46c7-a8a8-c387d34b5bfd 401130 1 2022-09-13 11:18:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-09-13 11:18:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-09-13 11:18:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {k3s Update apps/v1 2022-09-13 11:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005034f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-hm29b-764bc7c4b7",LastUpdateTime:2022-09-13 11:18:40 +0000 UTC,LastTransitionTime:2022-09-13 11:18:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 11:18:40.256: INFO: New ReplicaSet "test-deployment-hm29b-764bc7c4b7" of Deployment "test-deployment-hm29b":
&ReplicaSet{ObjectMeta:{test-deployment-hm29b-764bc7c4b7  deployment-5318  20f223d9-6297-4974-8947-239680ae4ca4 401126 1 2022-09-13 11:18:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-hm29b 9294aab6-9ccd-46c7-a8a8-c387d34b5bfd 0xc0050353e0 0xc0050353e1}] []  [{k3s Update apps/v1 2022-09-13 11:18:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9294aab6-9ccd-46c7-a8a8-c387d34b5bfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:18:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050354c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:18:40.264: INFO: Pod "test-deployment-hm29b-764bc7c4b7-sbxnk" is available:
&Pod{ObjectMeta:{test-deployment-hm29b-764bc7c4b7-sbxnk test-deployment-hm29b-764bc7c4b7- deployment-5318  fc39f73c-199f-4858-8384-91741a6640b5 401125 0 2022-09-13 11:18:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [{apps/v1 ReplicaSet test-deployment-hm29b-764bc7c4b7 20f223d9-6297-4974-8947-239680ae4ca4 0xc0050358e0 0xc0050358e1}] []  [{k3s Update v1 2022-09-13 11:18:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20f223d9-6297-4974-8947-239680ae4ca4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:18:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwlpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwlpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:18:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:18:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:18:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.247,StartTime:2022-09-13 11:18:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:18:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5c1ef642a045322e58f6146d968520f987b4385a2397290117ae394531ad59bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5318" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":23,"skipped":375,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:40.282: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Sep 13 11:18:40.335: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:42.353: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Sep 13 11:18:42.382: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:44.405: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 13 11:18:44.438: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 11:18:44.445: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 11:18:46.445: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 11:18:46.455: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 11:18:48.445: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 11:18:48.463: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:48.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6367" for this suite.

• [SLOW TEST:8.201 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:48.484: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-aeec067f-ddf1-4632-b581-1cf01d3e52cd
STEP: Creating a pod to test consume configMaps
Sep 13 11:18:48.581: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86" in namespace "projected-7157" to be "Succeeded or Failed"
Sep 13 11:18:48.587: INFO: Pod "pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3048ms
Sep 13 11:18:50.608: INFO: Pod "pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027536138s
Sep 13 11:18:52.619: INFO: Pod "pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037613315s
STEP: Saw pod success
Sep 13 11:18:52.619: INFO: Pod "pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86" satisfied condition "Succeeded or Failed"
Sep 13 11:18:52.623: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:18:52.656: INFO: Waiting for pod pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86 to disappear
Sep 13 11:18:52.661: INFO: Pod pod-projected-configmaps-8d83f2cf-49a4-4d27-a855-488f911cae86 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7157" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":408,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:52.673: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:18:52.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-5931 version'
Sep 13 11:18:52.808: INFO: stderr: ""
Sep 13 11:18:52.808: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6+k3s1\", GitCommit:\"418c3fa858b69b12b9cefbcff0526f666a6236b9\", GitTreeState:\"clean\", BuildDate:\"2022-04-28T22:16:18Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:52.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5931" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":26,"skipped":412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:52.830: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:18:52.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7289" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":27,"skipped":437,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:18:52.944: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Sep 13 11:18:52.993: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:55.015: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Sep 13 11:18:55.042: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:18:57.051: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Sep 13 11:18:57.061: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 13 11:18:57.064: INFO: Pod pod-with-prestop-http-hook still exists
Sep 13 11:18:59.065: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 13 11:18:59.078: INFO: Pod pod-with-prestop-http-hook still exists
Sep 13 11:19:01.065: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 13 11:19:01.077: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:19:01.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3231" for this suite.

• [SLOW TEST:8.172 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":442,"failed":0}
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:19:01.116: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-08274c23-de01-47a9-a829-9d47a84fa0a6
STEP: Creating secret with name s-test-opt-upd-1cbe3ebd-ae94-4dc4-9a40-8ff0504b260d
STEP: Creating the pod
Sep 13 11:19:01.197: INFO: The status of Pod pod-secrets-9a7ab5af-2aa7-406b-b1ec-cdcdc22be728 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:19:03.219: INFO: The status of Pod pod-secrets-9a7ab5af-2aa7-406b-b1ec-cdcdc22be728 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-08274c23-de01-47a9-a829-9d47a84fa0a6
STEP: Updating secret s-test-opt-upd-1cbe3ebd-ae94-4dc4-9a40-8ff0504b260d
STEP: Creating secret with name s-test-opt-create-b7089ca5-01a1-4d4b-9329-cfcf849ef04e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:19:07.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7843" for this suite.

• [SLOW TEST:6.296 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":442,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:19:07.412: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:19:07.447: INFO: Creating simple deployment test-new-deployment
Sep 13 11:19:07.472: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:19:09.552: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4830  8d536dc0-1036-4c02-a423-6c1de109cd07 401385 3 2022-09-13 11:19:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-09-13 11:19:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00524a388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-13 11:19:09 +0000 UTC,LastTransitionTime:2022-09-13 11:19:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-09-13 11:19:09 +0000 UTC,LastTransitionTime:2022-09-13 11:19:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 11:19:09.564: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-4830  f774b9e3-d0e1-45b2-bc7c-beed35936636 401388 3 2022-09-13 11:19:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8d536dc0-1036-4c02-a423-6c1de109cd07 0xc00524a827 0xc00524a828}] []  [{k3s Update apps/v1 2022-09-13 11:19:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d536dc0-1036-4c02-a423-6c1de109cd07\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:19:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00524a8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:19:09.572: INFO: Pod "test-new-deployment-5d9fdcc779-4fkwj" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-4fkwj test-new-deployment-5d9fdcc779- deployment-4830  474cd16b-ef0c-4ac9-81f5-a870bf2d30f8 401379 0 2022-09-13 11:19:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 f774b9e3-d0e1-45b2-bc7c-beed35936636 0xc00524acb7 0xc00524acb8}] []  [{k3s Update v1 2022-09-13 11:19:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f774b9e3-d0e1-45b2-bc7c-beed35936636\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vr82x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vr82x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:19:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.167,StartTime:2022-09-13 11:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:19:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e97cbc4b19262758390cbfd7b0325c4f65b842556b6e9c585241cf9bf0143250,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:19:09.573: INFO: Pod "test-new-deployment-5d9fdcc779-m4q46" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-m4q46 test-new-deployment-5d9fdcc779- deployment-4830  fa22fad8-dc75-4632-8a0c-afe87cbdb721 401390 0 2022-09-13 11:19:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 f774b9e3-d0e1-45b2-bc7c-beed35936636 0xc00524ae97 0xc00524ae98}] []  [{k3s Update v1 2022-09-13 11:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f774b9e3-d0e1-45b2-bc7c-beed35936636\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2gps4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2gps4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:19:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:19:09.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4830" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":30,"skipped":458,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:19:09.592: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-c248fd36-9fbe-4594-a5bd-299ed086d4cf in namespace container-probe-3898
Sep 13 11:19:11.675: INFO: Started pod busybox-c248fd36-9fbe-4594-a5bd-299ed086d4cf in namespace container-probe-3898
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 11:19:11.682: INFO: Initial restart count of pod busybox-c248fd36-9fbe-4594-a5bd-299ed086d4cf is 0
Sep 13 11:20:02.063: INFO: Restart count of pod container-probe-3898/busybox-c248fd36-9fbe-4594-a5bd-299ed086d4cf is now 1 (50.381164093s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:02.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3898" for this suite.

• [SLOW TEST:52.508 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":460,"failed":0}
SS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:02.101: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
Sep 13 11:20:02.140: INFO: created test-event-1
Sep 13 11:20:02.149: INFO: created test-event-2
Sep 13 11:20:02.154: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep 13 11:20:02.159: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep 13 11:20:02.190: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:02.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7308" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":32,"skipped":462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:02.214: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:20:02.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:20:05.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:15.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4476" for this suite.
STEP: Destroying namespace "webhook-4476-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.790 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":33,"skipped":501,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:16.005: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5996.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5996.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5996.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5996.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:20:18.141: INFO: DNS probes using dns-5996/dns-test-0317b6c4-5a37-4286-8755-8f46c0d746dd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:18.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5996" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":34,"skipped":505,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:18.191: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Sep 13 11:20:38.377: INFO: EndpointSlice for Service endpointslice-4617/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:48.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4617" for this suite.

• [SLOW TEST:30.259 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":35,"skipped":511,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:48.450: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3
Sep 13 11:20:48.518: INFO: Pod name my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3: Found 0 pods out of 1
Sep 13 11:20:53.536: INFO: Pod name my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3: Found 1 pods out of 1
Sep 13 11:20:53.536: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3" are running
Sep 13 11:20:53.541: INFO: Pod "my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3-5ngr7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:20:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:20:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:20:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:20:49 +0000 UTC Reason: Message:}])
Sep 13 11:20:53.541: INFO: Trying to dial the pod
Sep 13 11:20:58.569: INFO: Controller my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3: Got expected result from replica 1 [my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3-5ngr7]: "my-hostname-basic-474ecad7-1aee-4fe9-b813-e457d105edc3-5ngr7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:58.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3193" for this suite.

• [SLOW TEST:10.138 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":36,"skipped":512,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:58.588: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:20:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7612" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":37,"skipped":532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:20:58.651: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:20:58.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1" in namespace "downward-api-4116" to be "Succeeded or Failed"
Sep 13 11:20:58.696: INFO: Pod "downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.850991ms
Sep 13 11:21:00.719: INFO: Pod "downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1": Phase="Running", Reason="", readiness=false. Elapsed: 2.027540385s
Sep 13 11:21:02.740: INFO: Pod "downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049212956s
STEP: Saw pod success
Sep 13 11:21:02.740: INFO: Pod "downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1" satisfied condition "Succeeded or Failed"
Sep 13 11:21:02.748: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1 container client-container: <nil>
STEP: delete the pod
Sep 13 11:21:02.789: INFO: Waiting for pod downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1 to disappear
Sep 13 11:21:02.797: INFO: Pod downwardapi-volume-a0f3af43-965b-4db8-85b6-90f935701de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:02.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4116" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-1295
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1295
STEP: Deleting pre-stop pod
Sep 13 11:21:11.973: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": null,
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:12.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1295" for this suite.

• [SLOW TEST:9.212 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":39,"skipped":609,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:12.026: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
Sep 13 11:21:12.056: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3363 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:12.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3363" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":40,"skipped":623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:12.138: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:21:12.180: INFO: Creating ReplicaSet my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99
Sep 13 11:21:12.193: INFO: Pod name my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99: Found 0 pods out of 1
Sep 13 11:21:17.200: INFO: Pod name my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99: Found 1 pods out of 1
Sep 13 11:21:17.200: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99" is running
Sep 13 11:21:17.204: INFO: Pod "my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99-l72c8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:21:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:21:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:21:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-13 11:21:12 +0000 UTC Reason: Message:}])
Sep 13 11:21:17.204: INFO: Trying to dial the pod
Sep 13 11:21:22.240: INFO: Controller my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99: Got expected result from replica 1 [my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99-l72c8]: "my-hostname-basic-27c183a0-f949-47c6-bd21-877673871a99-l72c8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:22.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5891" for this suite.

• [SLOW TEST:10.126 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":41,"skipped":667,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:22.264: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:21:22.316: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-90f46033-ec0e-422b-9b65-324170948414" in namespace "security-context-test-2825" to be "Succeeded or Failed"
Sep 13 11:21:22.322: INFO: Pod "busybox-readonly-false-90f46033-ec0e-422b-9b65-324170948414": Phase="Pending", Reason="", readiness=false. Elapsed: 5.505731ms
Sep 13 11:21:24.337: INFO: Pod "busybox-readonly-false-90f46033-ec0e-422b-9b65-324170948414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020609828s
Sep 13 11:21:26.347: INFO: Pod "busybox-readonly-false-90f46033-ec0e-422b-9b65-324170948414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03073006s
Sep 13 11:21:26.347: INFO: Pod "busybox-readonly-false-90f46033-ec0e-422b-9b65-324170948414" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:26.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2825" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":686,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:26.363: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-36284baa-89c0-46c4-8156-3ea8b7526504
STEP: Creating configMap with name cm-test-opt-upd-13788937-af8e-41cd-b6b3-a94c005d2fae
STEP: Creating the pod
Sep 13 11:21:26.438: INFO: The status of Pod pod-configmaps-58c6a02f-a24e-4d19-886b-766b3ee7498e is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:21:28.455: INFO: The status of Pod pod-configmaps-58c6a02f-a24e-4d19-886b-766b3ee7498e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-36284baa-89c0-46c4-8156-3ea8b7526504
STEP: Updating configmap cm-test-opt-upd-13788937-af8e-41cd-b6b3-a94c005d2fae
STEP: Creating configMap with name cm-test-opt-create-cbf508f6-bb11-4728-adbc-250d52be8b0c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:30.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8603" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":691,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:30.590: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:21:30.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513" in namespace "downward-api-3296" to be "Succeeded or Failed"
Sep 13 11:21:30.639: INFO: Pod "downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513": Phase="Pending", Reason="", readiness=false. Elapsed: 7.701919ms
Sep 13 11:21:32.658: INFO: Pod "downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513": Phase="Running", Reason="", readiness=false. Elapsed: 2.02714619s
Sep 13 11:21:34.670: INFO: Pod "downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039114076s
STEP: Saw pod success
Sep 13 11:21:34.670: INFO: Pod "downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513" satisfied condition "Succeeded or Failed"
Sep 13 11:21:34.675: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom pod downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513 container client-container: <nil>
STEP: delete the pod
Sep 13 11:21:34.713: INFO: Waiting for pod downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513 to disappear
Sep 13 11:21:34.719: INFO: Pod downwardapi-volume-a4e1a5f7-2069-433b-af29-ed678d761513 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:34.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3296" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":696,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:34.737: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
Sep 13 11:21:34.795: INFO: Waiting up to 5m0s for pod "client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b" in namespace "containers-2353" to be "Succeeded or Failed"
Sep 13 11:21:34.801: INFO: Pod "client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710939ms
Sep 13 11:21:36.816: INFO: Pod "client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021031001s
Sep 13 11:21:38.837: INFO: Pod "client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041816076s
STEP: Saw pod success
Sep 13 11:21:38.837: INFO: Pod "client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b" satisfied condition "Succeeded or Failed"
Sep 13 11:21:38.846: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom pod client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:21:38.874: INFO: Waiting for pod client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b to disappear
Sep 13 11:21:38.880: INFO: Pod client-containers-12830683-5838-4721-9ed8-73ef06fa6e9b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:38.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2353" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":708,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:38.892: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:38.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5190" for this suite.
STEP: Destroying namespace "nspatchtest-800277e2-f516-400f-ad2b-c89c88b24197-7390" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":46,"skipped":713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:38.974: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:21:39.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a" in namespace "projected-2068" to be "Succeeded or Failed"
Sep 13 11:21:39.019: INFO: Pod "downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.435637ms
Sep 13 11:21:41.025: INFO: Pod "downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011396119s
Sep 13 11:21:43.049: INFO: Pod "downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035633441s
STEP: Saw pod success
Sep 13 11:21:43.049: INFO: Pod "downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a" satisfied condition "Succeeded or Failed"
Sep 13 11:21:43.056: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a container client-container: <nil>
STEP: delete the pod
Sep 13 11:21:43.078: INFO: Waiting for pod downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a to disappear
Sep 13 11:21:43.081: INFO: Pod downwardapi-volume-2241d422-37f2-48ab-a253-59c6c2f8099a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:21:43.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2068" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":752,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:21:43.091: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-5202dcd1-6037-4c6b-b468-696f4fe407f7 in namespace container-probe-3700
Sep 13 11:21:45.153: INFO: Started pod busybox-5202dcd1-6037-4c6b-b468-696f4fe407f7 in namespace container-probe-3700
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 11:21:45.162: INFO: Initial restart count of pod busybox-5202dcd1-6037-4c6b-b468-696f4fe407f7 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:25:47.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3700" for this suite.

• [SLOW TEST:244.065 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:25:47.156: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:03.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-943" for this suite.

• [SLOW TEST:16.277 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":49,"skipped":788,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:03.434: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:03.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9247" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":50,"skipped":803,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:03.494: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Sep 13 11:26:03.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 create -f -'
Sep 13 11:26:04.528: INFO: stderr: ""
Sep 13 11:26:04.528: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 11:26:04.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:26:04.618: INFO: stderr: ""
Sep 13 11:26:04.618: INFO: stdout: "update-demo-nautilus-shhg5 update-demo-nautilus-hnp7l "
Sep 13 11:26:04.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-shhg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:04.702: INFO: stderr: ""
Sep 13 11:26:04.702: INFO: stdout: ""
Sep 13 11:26:04.702: INFO: update-demo-nautilus-shhg5 is created but not running
Sep 13 11:26:09.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:26:09.832: INFO: stderr: ""
Sep 13 11:26:09.832: INFO: stdout: "update-demo-nautilus-hnp7l update-demo-nautilus-shhg5 "
Sep 13 11:26:09.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:09.944: INFO: stderr: ""
Sep 13 11:26:09.944: INFO: stdout: "true"
Sep 13 11:26:09.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:26:10.069: INFO: stderr: ""
Sep 13 11:26:10.069: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:26:10.069: INFO: validating pod update-demo-nautilus-hnp7l
Sep 13 11:26:10.080: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:26:10.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:26:10.080: INFO: update-demo-nautilus-hnp7l is verified up and running
Sep 13 11:26:10.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-shhg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:10.150: INFO: stderr: ""
Sep 13 11:26:10.150: INFO: stdout: "true"
Sep 13 11:26:10.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-shhg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:26:10.254: INFO: stderr: ""
Sep 13 11:26:10.254: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:26:10.254: INFO: validating pod update-demo-nautilus-shhg5
Sep 13 11:26:10.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:26:10.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:26:10.268: INFO: update-demo-nautilus-shhg5 is verified up and running
STEP: scaling down the replication controller
Sep 13 11:26:10.272: INFO: scanned /root for discovery docs: <nil>
Sep 13 11:26:10.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep 13 11:26:11.433: INFO: stderr: ""
Sep 13 11:26:11.433: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 11:26:11.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:26:11.537: INFO: stderr: ""
Sep 13 11:26:11.537: INFO: stdout: "update-demo-nautilus-hnp7l "
Sep 13 11:26:11.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:11.630: INFO: stderr: ""
Sep 13 11:26:11.630: INFO: stdout: "true"
Sep 13 11:26:11.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:26:11.721: INFO: stderr: ""
Sep 13 11:26:11.721: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:26:11.721: INFO: validating pod update-demo-nautilus-hnp7l
Sep 13 11:26:11.730: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:26:11.730: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:26:11.730: INFO: update-demo-nautilus-hnp7l is verified up and running
STEP: scaling up the replication controller
Sep 13 11:26:11.733: INFO: scanned /root for discovery docs: <nil>
Sep 13 11:26:11.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep 13 11:26:12.911: INFO: stderr: ""
Sep 13 11:26:12.911: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 11:26:12.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:26:13.026: INFO: stderr: ""
Sep 13 11:26:13.027: INFO: stdout: "update-demo-nautilus-hnp7l update-demo-nautilus-2pvn9 "
Sep 13 11:26:13.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:13.131: INFO: stderr: ""
Sep 13 11:26:13.131: INFO: stdout: "true"
Sep 13 11:26:13.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:26:13.245: INFO: stderr: ""
Sep 13 11:26:13.245: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:26:13.246: INFO: validating pod update-demo-nautilus-hnp7l
Sep 13 11:26:13.253: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:26:13.253: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:26:13.253: INFO: update-demo-nautilus-hnp7l is verified up and running
Sep 13 11:26:13.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-2pvn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:13.372: INFO: stderr: ""
Sep 13 11:26:13.372: INFO: stdout: ""
Sep 13 11:26:13.372: INFO: update-demo-nautilus-2pvn9 is created but not running
Sep 13 11:26:18.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:26:18.491: INFO: stderr: ""
Sep 13 11:26:18.491: INFO: stdout: "update-demo-nautilus-hnp7l update-demo-nautilus-2pvn9 "
Sep 13 11:26:18.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:18.579: INFO: stderr: ""
Sep 13 11:26:18.579: INFO: stdout: "true"
Sep 13 11:26:18.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-hnp7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:26:18.669: INFO: stderr: ""
Sep 13 11:26:18.669: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:26:18.669: INFO: validating pod update-demo-nautilus-hnp7l
Sep 13 11:26:18.679: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:26:18.679: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:26:18.679: INFO: update-demo-nautilus-hnp7l is verified up and running
Sep 13 11:26:18.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-2pvn9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:26:18.800: INFO: stderr: ""
Sep 13 11:26:18.800: INFO: stdout: "true"
Sep 13 11:26:18.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods update-demo-nautilus-2pvn9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:26:18.907: INFO: stderr: ""
Sep 13 11:26:18.907: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:26:18.907: INFO: validating pod update-demo-nautilus-2pvn9
Sep 13 11:26:18.915: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:26:18.915: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:26:18.915: INFO: update-demo-nautilus-2pvn9 is verified up and running
STEP: using delete to clean up resources
Sep 13 11:26:18.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 delete --grace-period=0 --force -f -'
Sep 13 11:26:19.051: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:26:19.051: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 13 11:26:19.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get rc,svc -l name=update-demo --no-headers'
Sep 13 11:26:19.173: INFO: stderr: "No resources found in kubectl-9188 namespace.\n"
Sep 13 11:26:19.173: INFO: stdout: ""
Sep 13 11:26:19.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-9188 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 11:26:19.295: INFO: stderr: ""
Sep 13 11:26:19.295: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:19.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9188" for this suite.

• [SLOW TEST:15.823 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":51,"skipped":814,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:19.317: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:26:19.345: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 13 11:26:19.359: INFO: The status of Pod pod-exec-websocket-80026ef3-1c40-4be4-a925-f3f76729f554 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:26:21.364: INFO: The status of Pod pod-exec-websocket-80026ef3-1c40-4be4-a925-f3f76729f554 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:21.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":832,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:21.482: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:26:21.515: INFO: Got root ca configmap in namespace "svcaccounts-7339"
Sep 13 11:26:21.521: INFO: Deleted root ca configmap in namespace "svcaccounts-7339"
STEP: waiting for a new root ca configmap created
Sep 13 11:26:22.035: INFO: Recreated root ca configmap in namespace "svcaccounts-7339"
Sep 13 11:26:22.045: INFO: Updated root ca configmap in namespace "svcaccounts-7339"
STEP: waiting for the root ca configmap reconciled
Sep 13 11:26:22.556: INFO: Reconciled root ca configmap in namespace "svcaccounts-7339"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:22.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7339" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":53,"skipped":848,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:26:22.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874" in namespace "downward-api-4712" to be "Succeeded or Failed"
Sep 13 11:26:22.636: INFO: Pod "downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874": Phase="Pending", Reason="", readiness=false. Elapsed: 7.229813ms
Sep 13 11:26:24.653: INFO: Pod "downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874": Phase="Running", Reason="", readiness=false. Elapsed: 2.023948843s
Sep 13 11:26:26.660: INFO: Pod "downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030981607s
STEP: Saw pod success
Sep 13 11:26:26.660: INFO: Pod "downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874" satisfied condition "Succeeded or Failed"
Sep 13 11:26:26.665: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874 container client-container: <nil>
STEP: delete the pod
Sep 13 11:26:26.697: INFO: Waiting for pod downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874 to disappear
Sep 13 11:26:26.700: INFO: Pod downwardapi-volume-2552443a-a05a-4cde-8264-cb1611c37874 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:26.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4712" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":866,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:26.724: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 13 11:26:26.769: INFO: Waiting up to 5m0s for pod "pod-7a3399b6-a48c-4990-b1cc-42b34044eb13" in namespace "emptydir-3823" to be "Succeeded or Failed"
Sep 13 11:26:26.773: INFO: Pod "pod-7a3399b6-a48c-4990-b1cc-42b34044eb13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504281ms
Sep 13 11:26:28.796: INFO: Pod "pod-7a3399b6-a48c-4990-b1cc-42b34044eb13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026942187s
Sep 13 11:26:30.805: INFO: Pod "pod-7a3399b6-a48c-4990-b1cc-42b34044eb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036564559s
STEP: Saw pod success
Sep 13 11:26:30.806: INFO: Pod "pod-7a3399b6-a48c-4990-b1cc-42b34044eb13" satisfied condition "Succeeded or Failed"
Sep 13 11:26:30.813: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-7a3399b6-a48c-4990-b1cc-42b34044eb13 container test-container: <nil>
STEP: delete the pod
Sep 13 11:26:30.844: INFO: Waiting for pod pod-7a3399b6-a48c-4990-b1cc-42b34044eb13 to disappear
Sep 13 11:26:30.851: INFO: Pod pod-7a3399b6-a48c-4990-b1cc-42b34044eb13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:30.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3823" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":877,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:30.868: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-4ee71993-fffe-4c79-82ef-af77ed4a8169
STEP: Creating a pod to test consume secrets
Sep 13 11:26:30.937: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f" in namespace "projected-3791" to be "Succeeded or Failed"
Sep 13 11:26:30.942: INFO: Pod "pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.816787ms
Sep 13 11:26:32.962: INFO: Pod "pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024853068s
Sep 13 11:26:34.985: INFO: Pod "pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047250857s
STEP: Saw pod success
Sep 13 11:26:34.985: INFO: Pod "pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f" satisfied condition "Succeeded or Failed"
Sep 13 11:26:34.990: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:26:35.019: INFO: Waiting for pod pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f to disappear
Sep 13 11:26:35.025: INFO: Pod pod-projected-secrets-b6fec823-9e76-4b33-8a38-d1ad54454c3f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:35.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3791" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":892,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:35.040: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 13 11:26:35.873: INFO: starting watch
STEP: patching
STEP: updating
Sep 13 11:26:35.893: INFO: waiting for watch events with expected annotations
Sep 13 11:26:35.893: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:35.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5836" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":57,"skipped":906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:35.972: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
Sep 13 11:26:36.081: INFO: The status of Pod pod-hostip-4ccab9c5-f1f6-4620-97bc-ab5d8b30228c is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:26:38.100: INFO: The status of Pod pod-hostip-4ccab9c5-f1f6-4620-97bc-ab5d8b30228c is Running (Ready = true)
Sep 13 11:26:38.112: INFO: Pod pod-hostip-4ccab9c5-f1f6-4620-97bc-ab5d8b30228c has hostIP: 192.168.1.5
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:38.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5932" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:38.134: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:26:38.542: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:26:41.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:26:41.596: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7217-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:44.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2282" for this suite.
STEP: Destroying namespace "webhook-2282-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.722 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":59,"skipped":974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:44.861: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:26:44.905: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 13 11:26:49.922: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 13 11:26:49.922: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:26:52.041: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4700  1a8852bc-bd12-4080-a10f-dc3b16395a58 402931 1 2022-09-13 11:26:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-09-13 11:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:26:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051b9758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-13 11:26:50 +0000 UTC,LastTransitionTime:2022-09-13 11:26:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-56cd759769" has successfully progressed.,LastUpdateTime:2022-09-13 11:26:52 +0000 UTC,LastTransitionTime:2022-09-13 11:26:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 11:26:52.048: INFO: New ReplicaSet "test-cleanup-deployment-56cd759769" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-56cd759769  deployment-4700  0468e579-b529-4266-b915-a05907f66012 402921 1 2022-09-13 11:26:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 1a8852bc-bd12-4080-a10f-dc3b16395a58 0xc0051b9b07 0xc0051b9b08}] []  [{k3s Update apps/v1 2022-09-13 11:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8852bc-bd12-4080-a10f-dc3b16395a58\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:26:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 56cd759769,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051b9bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:26:52.053: INFO: Pod "test-cleanup-deployment-56cd759769-fhspm" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-56cd759769-fhspm test-cleanup-deployment-56cd759769- deployment-4700  92b8cfa6-e597-4a7c-8f04-cbf866585430 402920 0 2022-09-13 11:26:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-56cd759769 0468e579-b529-4266-b915-a05907f66012 0xc0051b9f57 0xc0051b9f58}] []  [{k3s Update v1 2022-09-13 11:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0468e579-b529-4266-b915-a05907f66012\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:26:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jn9f5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jn9f5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:26:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:26:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:26:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.22,StartTime:2022-09-13 11:26:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:26:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://9a3bef30d639d9ca497a07b03cb41b5b5f23a37106e1e85931d6d7ace43c2a29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:52.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4700" for this suite.

• [SLOW TEST:7.208 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":60,"skipped":1029,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:52.069: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 13 11:26:52.119: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:56.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8944" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1031,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:56.668: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
Sep 13 11:26:56.717: INFO: Found Service test-service-zb98b in namespace services-9776 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep 13 11:26:56.717: INFO: Service test-service-zb98b created
STEP: Getting /status
Sep 13 11:26:56.720: INFO: Service test-service-zb98b has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Sep 13 11:26:56.729: INFO: observed Service test-service-zb98b in namespace services-9776 with annotations: map[] & LoadBalancer: {[]}
Sep 13 11:26:56.729: INFO: Found Service test-service-zb98b in namespace services-9776 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep 13 11:26:56.730: INFO: Service test-service-zb98b has service status patched
STEP: updating the ServiceStatus
Sep 13 11:26:56.738: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Sep 13 11:26:56.741: INFO: Observed Service test-service-zb98b in namespace services-9776 with annotations: map[] & Conditions: {[]}
Sep 13 11:26:56.742: INFO: Observed event: &Service{ObjectMeta:{test-service-zb98b  services-9776  e8c4ca4f-9bf6-4bcc-be2c-6ab691f3ee5a 402964 0 2022-09-13 11:26:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-09-13 11:26:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-09-13 11:26:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.230.134,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.230.134],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep 13 11:26:56.742: INFO: Found Service test-service-zb98b in namespace services-9776 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 13 11:26:56.742: INFO: Service test-service-zb98b has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Sep 13 11:26:56.749: INFO: observed Service test-service-zb98b in namespace services-9776 with labels: map[test-service-static:true]
Sep 13 11:26:56.749: INFO: observed Service test-service-zb98b in namespace services-9776 with labels: map[test-service-static:true]
Sep 13 11:26:56.749: INFO: observed Service test-service-zb98b in namespace services-9776 with labels: map[test-service-static:true]
Sep 13 11:26:56.749: INFO: Found Service test-service-zb98b in namespace services-9776 with labels: map[test-service:patched test-service-static:true]
Sep 13 11:26:56.749: INFO: Service test-service-zb98b patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Sep 13 11:26:56.762: INFO: Observed event: ADDED
Sep 13 11:26:56.762: INFO: Observed event: MODIFIED
Sep 13 11:26:56.762: INFO: Observed event: MODIFIED
Sep 13 11:26:56.762: INFO: Observed event: MODIFIED
Sep 13 11:26:56.762: INFO: Found Service test-service-zb98b in namespace services-9776 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep 13 11:26:56.762: INFO: Service test-service-zb98b deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:26:56.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9776" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":62,"skipped":1042,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:26:56.774: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-ct5s
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 11:26:56.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ct5s" in namespace "subpath-4751" to be "Succeeded or Failed"
Sep 13 11:26:56.835: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Pending", Reason="", readiness=false. Elapsed: 13.431501ms
Sep 13 11:26:58.855: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.033348948s
Sep 13 11:27:00.867: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.045176735s
Sep 13 11:27:02.879: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.057152682s
Sep 13 11:27:04.899: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.076813684s
Sep 13 11:27:06.915: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.092808231s
Sep 13 11:27:08.939: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.116740571s
Sep 13 11:27:10.949: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.127482344s
Sep 13 11:27:12.965: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.143583976s
Sep 13 11:27:14.978: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.156568331s
Sep 13 11:27:16.994: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.172459625s
Sep 13 11:27:19.009: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.186721533s
Sep 13 11:27:21.022: INFO: Pod "pod-subpath-test-secret-ct5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.200485511s
STEP: Saw pod success
Sep 13 11:27:21.022: INFO: Pod "pod-subpath-test-secret-ct5s" satisfied condition "Succeeded or Failed"
Sep 13 11:27:21.030: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-subpath-test-secret-ct5s container test-container-subpath-secret-ct5s: <nil>
STEP: delete the pod
Sep 13 11:27:21.076: INFO: Waiting for pod pod-subpath-test-secret-ct5s to disappear
Sep 13 11:27:21.087: INFO: Pod pod-subpath-test-secret-ct5s no longer exists
STEP: Deleting pod pod-subpath-test-secret-ct5s
Sep 13 11:27:21.087: INFO: Deleting pod "pod-subpath-test-secret-ct5s" in namespace "subpath-4751"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:21.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4751" for this suite.

• [SLOW TEST:24.339 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":63,"skipped":1042,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:21.114: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:27:21.487: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:27:24.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:24.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5900" for this suite.
STEP: Destroying namespace "webhook-5900-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":64,"skipped":1052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:24.765: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Sep 13 11:27:24.808: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:27:26.824: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 13 11:27:27.870: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:28.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-469" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":65,"skipped":1099,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:28.938: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 13 11:27:39.051: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0913 11:27:39.050989      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:39.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9039" for this suite.

• [SLOW TEST:10.134 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":66,"skipped":1123,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:39.072: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:27:39.113: INFO: Creating deployment "webserver-deployment"
Sep 13 11:27:39.121: INFO: Waiting for observed generation 1
Sep 13 11:27:41.137: INFO: Waiting for all required pods to come up
Sep 13 11:27:41.147: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 13 11:27:43.195: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 13 11:27:43.202: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 13 11:27:43.214: INFO: Updating deployment webserver-deployment
Sep 13 11:27:43.214: INFO: Waiting for observed generation 2
Sep 13 11:27:45.240: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 13 11:27:45.246: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 13 11:27:45.251: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 13 11:27:45.274: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 13 11:27:45.274: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 13 11:27:45.280: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 13 11:27:45.294: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 13 11:27:45.294: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 13 11:27:45.317: INFO: Updating deployment webserver-deployment
Sep 13 11:27:45.317: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 13 11:27:45.332: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 13 11:27:45.342: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:27:45.367: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8690  930ccd9c-5823-4416-92d8-92f51b84ca8a 403468 3 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f47b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-09-13 11:27:44 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-13 11:27:46 +0000 UTC,LastTransitionTime:2022-09-13 11:27:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 13 11:27:45.394: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-8690  a365df63-8fd7-4d43-b69d-020d5160478a 403460 3 2022-09-13 11:27:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 930ccd9c-5823-4416-92d8-92f51b84ca8a 0xc0050fa027 0xc0050fa028}] []  [{k3s Update apps/v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"930ccd9c-5823-4416-92d8-92f51b84ca8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050fa0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:27:45.394: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 13 11:27:45.394: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-8690  553370f0-e214-4cc9-ad6c-e7dc6b643ba5 403458 3 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 930ccd9c-5823-4416-92d8-92f51b84ca8a 0xc004f47f27 0xc004f47f28}] []  [{k3s Update apps/v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"930ccd9c-5823-4416-92d8-92f51b84ca8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:27:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f47fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:27:45.412: INFO: Pod "webserver-deployment-5d9fdcc779-xztnc" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xztnc webserver-deployment-5d9fdcc779- deployment-8690  a6dda733-ca8c-4654-8ed9-e6dbd694b75f 403341 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538e8d7 0xc00538e8d8}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhqdq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhqdq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.174,StartTime:2022-09-13 11:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9b97db814b9092c9dbe2c143177e171edb2752b4d29a8a89e9937e811a3f4062,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.413: INFO: Pod "webserver-deployment-5d9fdcc779-cdxtj" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-cdxtj webserver-deployment-5d9fdcc779- deployment-8690  a79ebb5e-43d3-4213-817c-608bf87d8f8a 403344 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538eab7 0xc00538eab8}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5d8nl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5d8nl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.30,StartTime:2022-09-13 11:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0d0491962e704547f8d3d262d2eb8afb0b72e01db6a9d60d5dfd6a7f6b2da806,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.413: INFO: Pod "webserver-deployment-5d9fdcc779-fr56t" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-fr56t webserver-deployment-5d9fdcc779- deployment-8690  63131c10-b2a7-4dac-86c2-b4ab0b79c0a3 403352 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538eca0 0xc00538eca1}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9swlj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9swlj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.175,StartTime:2022-09-13 11:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d1c145b4a8de54e6933100328e508f91894bfdfef012756c607e0569ae39bfd2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.413: INFO: Pod "webserver-deployment-5d9fdcc779-x9fzj" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-x9fzj webserver-deployment-5d9fdcc779- deployment-8690  31df2992-4cd9-4631-8115-0859c78a5acb 403357 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538ee77 0xc00538ee78}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmf7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmf7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.31,StartTime:2022-09-13 11:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b882280271dafb40b844351329b5cd86da4017ebad6829520217fc3a8d0326ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.414: INFO: Pod "webserver-deployment-5d9fdcc779-2lsnb" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2lsnb webserver-deployment-5d9fdcc779- deployment-8690  12dbdb44-3fa3-460a-913d-8e7f8b4de7aa 403358 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538f050 0xc00538f051}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drhst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drhst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.173,StartTime:2022-09-13 11:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a15df28bffd216e1204ae005a3d72f1d7f9760a474bbd891f1cd44023058e22c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.414: INFO: Pod "webserver-deployment-5d9fdcc779-wztn7" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-wztn7 webserver-deployment-5d9fdcc779- deployment-8690  84f90efb-c44b-450b-b0db-e0dc177fc091 403361 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538f227 0xc00538f228}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzh8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzh8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.33,StartTime:2022-09-13 11:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea866eca779276f48574a5aafa2cab53bb2fa7e69cf2ad740135fd47492d5c66,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.414: INFO: Pod "webserver-deployment-5d9fdcc779-6lwvt" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-6lwvt webserver-deployment-5d9fdcc779- deployment-8690  fc76451d-a21f-4d6b-aa79-6fa89da31aa7 403362 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538f400 0xc00538f401}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9tr6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9tr6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.176,StartTime:2022-09-13 11:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0946cc117141ebcb6d3bd05cd461c462fe9eca9b861510c803c62e7c06711b13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.415: INFO: Pod "webserver-deployment-5d9fdcc779-2t8t7" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2t8t7 webserver-deployment-5d9fdcc779- deployment-8690  a200d61c-5b0a-462c-bcdc-32b1c6f7df09 403370 0 2022-09-13 11:27:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00538f5e7 0xc00538f5e8}] []  [{k3s Update v1 2022-09-13 11:27:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-srcgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-srcgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.177,StartTime:2022-09-13 11:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:27:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://781ad32b63ee05d09b7a136e68b9864d73e670658edcde22c6990d12edba5cc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.415: INFO: Pod "webserver-deployment-566f96c878-wtsk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-wtsk2 webserver-deployment-566f96c878- deployment-8690  20fcf37d-6b97-4874-b600-5b2ad0cbb742 403393 0 2022-09-13 11:27:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00538f7c7 0xc00538f7c8}] []  [{k3s Update v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btvbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btvbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2022-09-13 11:27:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.415: INFO: Pod "webserver-deployment-566f96c878-gc2qp" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-gc2qp webserver-deployment-566f96c878- deployment-8690  0abb4555-aa51-4478-b5d2-0989edd6b5ea 403401 0 2022-09-13 11:27:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00538f9a0 0xc00538f9a1}] []  [{k3s Update v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lwrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lwrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:,StartTime:2022-09-13 11:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.416: INFO: Pod "webserver-deployment-566f96c878-nv8km" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-nv8km webserver-deployment-566f96c878- deployment-8690  3fcb559b-2686-49d5-a464-7468af4dbfa6 403403 0 2022-09-13 11:27:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00538fb70 0xc00538fb71}] []  [{k3s Update v1 2022-09-13 11:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qhcx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qhcx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2022-09-13 11:27:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.416: INFO: Pod "webserver-deployment-566f96c878-h9lx8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-h9lx8 webserver-deployment-566f96c878- deployment-8690  cd66586f-93c9-473e-be9b-512b9e5e85af 403421 0 2022-09-13 11:27:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00538fd40 0xc00538fd41}] []  [{k3s Update v1 2022-09-13 11:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfs48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfs48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:,StartTime:2022-09-13 11:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.417: INFO: Pod "webserver-deployment-566f96c878-mdzng" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-mdzng webserver-deployment-566f96c878- deployment-8690  62f825d0-90e7-4ffc-a7a7-3bd4f78692c4 403432 0 2022-09-13 11:27:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00538ff10 0xc00538ff11}] []  [{k3s Update v1 2022-09-13 11:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bjdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bjdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2022-09-13 11:27:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.417: INFO: Pod "webserver-deployment-566f96c878-clk7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-clk7b webserver-deployment-566f96c878- deployment-8690  fa686bcf-1a09-415e-8c10-65c9371d8271 403472 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00275a0f0 0xc00275a0f1}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7sgmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7sgmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.417: INFO: Pod "webserver-deployment-5d9fdcc779-sq8jr" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-sq8jr webserver-deployment-5d9fdcc779- deployment-8690  01e71c2f-0391-47b0-9000-dc602b81c35e 403473 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275a250 0xc00275a251}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8n9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8n9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.417: INFO: Pod "webserver-deployment-566f96c878-xzsmq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-xzsmq webserver-deployment-566f96c878- deployment-8690  24c66f78-cf90-42a7-a0b9-4f14e9c86391 403474 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00275a3a0 0xc00275a3a1}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tbh9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tbh9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.418: INFO: Pod "webserver-deployment-5d9fdcc779-v6mdn" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-v6mdn webserver-deployment-5d9fdcc779- deployment-8690  e5859be3-ee3f-4329-91bf-b02442aa5af5 403478 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275a4f0 0xc00275a4f1}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpnjm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpnjm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.418: INFO: Pod "webserver-deployment-5d9fdcc779-sn99j" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-sn99j webserver-deployment-5d9fdcc779- deployment-8690  b84b8b4d-3b2a-407c-a5e8-239ca9986fa8 403479 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275a640 0xc00275a641}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnb8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnb8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2022-09-13 11:27:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.418: INFO: Pod "webserver-deployment-5d9fdcc779-2kl9g" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2kl9g webserver-deployment-5d9fdcc779- deployment-8690  e2d4ada0-3be2-4e6c-b3b5-e4ce0e58598a 403480 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275a7f7 0xc00275a7f8}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7nng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7nng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.419: INFO: Pod "webserver-deployment-5d9fdcc779-2ctnn" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2ctnn webserver-deployment-5d9fdcc779- deployment-8690  93a83bec-43b3-463b-9e30-f8788fae6940 403481 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275a950 0xc00275a951}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jd4pt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jd4pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.419: INFO: Pod "webserver-deployment-5d9fdcc779-m6zlf" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-m6zlf webserver-deployment-5d9fdcc779- deployment-8690  5710ec0c-329c-468b-9f13-c60709f1c3e6 403482 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275aab0 0xc00275aab1}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xr9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xr9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.419: INFO: Pod "webserver-deployment-566f96c878-n8d8f" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-n8d8f webserver-deployment-566f96c878- deployment-8690  c6ce584b-3b39-4796-bb34-188db221fc02 403483 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00275abf0 0xc00275abf1}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x8g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x8g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:27:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.420: INFO: Pod "webserver-deployment-5d9fdcc779-vbgxn" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vbgxn webserver-deployment-5d9fdcc779- deployment-8690  1a4d4f40-b6a0-49c6-9b17-a0cd14b4ecb9 403484 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 553370f0-e214-4cc9-ad6c-e7dc6b643ba5 0xc00275ad60 0xc00275ad61}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"553370f0-e214-4cc9-ad6c-e7dc6b643ba5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zcfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zcfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.420: INFO: Pod "webserver-deployment-566f96c878-rg2b9" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-rg2b9 webserver-deployment-566f96c878- deployment-8690  6d6ee134-63c3-4f98-9fa0-967e6e0c9171 403486 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00275aea0 0xc00275aea1}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjsn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjsn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:27:45.420: INFO: Pod "webserver-deployment-566f96c878-tdkmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-tdkmx webserver-deployment-566f96c878- deployment-8690  bf1670b1-534d-4b51-9210-6929f03480e5 403488 0 2022-09-13 11:27:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a365df63-8fd7-4d43-b69d-020d5160478a 0xc00275b000 0xc00275b001}] []  [{k3s Update v1 2022-09-13 11:27:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a365df63-8fd7-4d43-b69d-020d5160478a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgsjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgsjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:45.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8690" for this suite.

• [SLOW TEST:6.403 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":67,"skipped":1127,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:45.475: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-6330/configmap-test-b8c1487c-d198-4f5a-8b78-de9b4a61dbc7
STEP: Creating a pod to test consume configMaps
Sep 13 11:27:45.579: INFO: Waiting up to 5m0s for pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6" in namespace "configmap-6330" to be "Succeeded or Failed"
Sep 13 11:27:45.585: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.752743ms
Sep 13 11:27:47.594: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015063084s
Sep 13 11:27:49.617: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038023777s
Sep 13 11:27:51.622: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043274424s
Sep 13 11:27:53.641: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061806377s
Sep 13 11:27:55.666: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.086920284s
STEP: Saw pod success
Sep 13 11:27:55.666: INFO: Pod "pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6" satisfied condition "Succeeded or Failed"
Sep 13 11:27:55.673: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6 container env-test: <nil>
STEP: delete the pod
Sep 13 11:27:55.715: INFO: Waiting for pod pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6 to disappear
Sep 13 11:27:55.721: INFO: Pod pod-configmaps-899c3f5e-c137-4137-9109-e598ad56cda6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:55.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6330" for this suite.

• [SLOW TEST:10.264 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1128,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:55.741: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-fba68947-249c-450c-8704-a87b9fa70546
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:27:59.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4610" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1144,"failed":0}

------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:27:59.874: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:28:09.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6128" for this suite.

• [SLOW TEST:10.075 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":70,"skipped":1144,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:28:09.948: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5285
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 11:28:09.981: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 13 11:28:10.029: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:28:12.042: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:14.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:16.043: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:18.042: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:20.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:22.042: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:24.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:26.036: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:28.047: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:30.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:28:32.042: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 13 11:28:32.056: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 13 11:28:34.128: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 13 11:28:34.129: INFO: Going to poll 10.42.0.181 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep 13 11:28:34.135: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5285 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:28:34.135: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:28:34.137: INFO: ExecWithOptions: Clientset creation
Sep 13 11:28:34.137: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-5285/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.0.181+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:28:35.289: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 13 11:28:35.289: INFO: Going to poll 10.42.1.42 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep 13 11:28:35.304: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.42 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5285 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:28:35.304: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:28:35.305: INFO: ExecWithOptions: Clientset creation
Sep 13 11:28:35.306: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-5285/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.1.42+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:28:36.435: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:28:36.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5285" for this suite.

• [SLOW TEST:26.522 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1151,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:28:36.471: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-79f8588b-2254-4f3f-b14f-ff8db57a53dd
STEP: Creating a pod to test consume secrets
Sep 13 11:28:36.544: INFO: Waiting up to 5m0s for pod "pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a" in namespace "secrets-9472" to be "Succeeded or Failed"
Sep 13 11:28:36.552: INFO: Pod "pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13265ms
Sep 13 11:28:38.564: INFO: Pod "pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019993617s
Sep 13 11:28:40.577: INFO: Pod "pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032934413s
STEP: Saw pod success
Sep 13 11:28:40.577: INFO: Pod "pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a" satisfied condition "Succeeded or Failed"
Sep 13 11:28:40.583: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a container secret-env-test: <nil>
STEP: delete the pod
Sep 13 11:28:40.608: INFO: Waiting for pod pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a to disappear
Sep 13 11:28:40.615: INFO: Pod pod-secrets-ca5a401e-5428-4b34-aaf4-66590312003a no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:28:40.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9472" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1165,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:28:40.629: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:28:40.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291" in namespace "downward-api-7315" to be "Succeeded or Failed"
Sep 13 11:28:40.668: INFO: Pod "downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291": Phase="Pending", Reason="", readiness=false. Elapsed: 3.083152ms
Sep 13 11:28:42.675: INFO: Pod "downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010588011s
Sep 13 11:28:44.693: INFO: Pod "downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0281731s
STEP: Saw pod success
Sep 13 11:28:44.693: INFO: Pod "downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291" satisfied condition "Succeeded or Failed"
Sep 13 11:28:44.700: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291 container client-container: <nil>
STEP: delete the pod
Sep 13 11:28:44.736: INFO: Waiting for pod downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291 to disappear
Sep 13 11:28:44.742: INFO: Pod downwardapi-volume-c33e658a-0e78-4ec1-afae-669af4da8291 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:28:44.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7315" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1180,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:28:44.758: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-0e4c7b03-1ab2-4a84-841b-edb09fbb2c9b in namespace container-probe-9872
Sep 13 11:28:46.835: INFO: Started pod liveness-0e4c7b03-1ab2-4a84-841b-edb09fbb2c9b in namespace container-probe-9872
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 11:28:46.843: INFO: Initial restart count of pod liveness-0e4c7b03-1ab2-4a84-841b-edb09fbb2c9b is 0
Sep 13 11:29:07.021: INFO: Restart count of pod container-probe-9872/liveness-0e4c7b03-1ab2-4a84-841b-edb09fbb2c9b is now 1 (20.178621798s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:07.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9872" for this suite.

• [SLOW TEST:22.300 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1195,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:07.059: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 13 11:29:07.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1444  054f09cc-508f-400e-b88d-ce1669f2b3f0 404150 0 2022-09-13 11:29:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-13 11:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:29:07.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1444  054f09cc-508f-400e-b88d-ce1669f2b3f0 404151 0 2022-09-13 11:29:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-13 11:29:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:29:07.125: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1444  054f09cc-508f-400e-b88d-ce1669f2b3f0 404152 0 2022-09-13 11:29:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-13 11:29:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 13 11:29:17.188: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1444  054f09cc-508f-400e-b88d-ce1669f2b3f0 404175 0 2022-09-13 11:29:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-13 11:29:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:29:17.188: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1444  054f09cc-508f-400e-b88d-ce1669f2b3f0 404176 0 2022-09-13 11:29:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-13 11:29:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:29:17.189: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1444  054f09cc-508f-400e-b88d-ce1669f2b3f0 404177 0 2022-09-13 11:29:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-13 11:29:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:17.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1444" for this suite.

• [SLOW TEST:10.151 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":75,"skipped":1211,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:17.212: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:29:17.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07" in namespace "downward-api-6502" to be "Succeeded or Failed"
Sep 13 11:29:17.272: INFO: Pod "downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.894082ms
Sep 13 11:29:19.293: INFO: Pod "downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024898538s
Sep 13 11:29:21.304: INFO: Pod "downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035052872s
STEP: Saw pod success
Sep 13 11:29:21.304: INFO: Pod "downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07" satisfied condition "Succeeded or Failed"
Sep 13 11:29:21.309: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07 container client-container: <nil>
STEP: delete the pod
Sep 13 11:29:21.345: INFO: Waiting for pod downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07 to disappear
Sep 13 11:29:21.350: INFO: Pod downwardapi-volume-289f7297-afba-47e1-83e2-e92a78f64d07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:21.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6502" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:21.367: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:29:21.394: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 13 11:29:25.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 create -f -'
Sep 13 11:29:26.115: INFO: stderr: ""
Sep 13 11:29:26.115: INFO: stdout: "e2e-test-crd-publish-openapi-8224-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 13 11:29:26.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 delete e2e-test-crd-publish-openapi-8224-crds test-cr'
Sep 13 11:29:26.245: INFO: stderr: ""
Sep 13 11:29:26.245: INFO: stdout: "e2e-test-crd-publish-openapi-8224-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 13 11:29:26.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 apply -f -'
Sep 13 11:29:26.522: INFO: stderr: ""
Sep 13 11:29:26.522: INFO: stdout: "e2e-test-crd-publish-openapi-8224-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 13 11:29:26.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 delete e2e-test-crd-publish-openapi-8224-crds test-cr'
Sep 13 11:29:26.652: INFO: stderr: ""
Sep 13 11:29:26.652: INFO: stdout: "e2e-test-crd-publish-openapi-8224-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 13 11:29:26.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-3090 explain e2e-test-crd-publish-openapi-8224-crds'
Sep 13 11:29:26.883: INFO: stderr: ""
Sep 13 11:29:26.883: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8224-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:30.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3090" for this suite.

• [SLOW TEST:8.661 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":77,"skipped":1237,"failed":0}
SSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:30.029: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Sep 13 11:29:30.144: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Sep 13 11:29:30.172: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:30.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1785" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":78,"skipped":1243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:30.233: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 13 11:29:32.830: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9788 pod-service-account-e826661d-302e-4bff-984b-88dadd071bbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 13 11:29:33.042: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9788 pod-service-account-e826661d-302e-4bff-984b-88dadd071bbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 13 11:29:33.280: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9788 pod-service-account-e826661d-302e-4bff-984b-88dadd071bbc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9788" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":79,"skipped":1274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:33.541: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:29:35.611: INFO: Deleting pod "var-expansion-e0c44b83-b206-487b-bebd-0c11169e4ed3" in namespace "var-expansion-3129"
Sep 13 11:29:35.631: INFO: Wait up to 5m0s for pod "var-expansion-e0c44b83-b206-487b-bebd-0c11169e4ed3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3129" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":80,"skipped":1299,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:37.697: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-1610
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1610
STEP: Waiting until pod test-pod will start running in namespace statefulset-1610
STEP: Creating statefulset with conflicting port in namespace statefulset-1610
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1610
Sep 13 11:29:39.825: INFO: Observed stateful pod in namespace: statefulset-1610, name: ss-0, uid: 106a23e7-d3bb-41d9-a30d-4998a8b7e4fd, status phase: Pending. Waiting for statefulset controller to delete.
Sep 13 11:29:39.847: INFO: Observed stateful pod in namespace: statefulset-1610, name: ss-0, uid: 106a23e7-d3bb-41d9-a30d-4998a8b7e4fd, status phase: Failed. Waiting for statefulset controller to delete.
Sep 13 11:29:39.854: INFO: Observed stateful pod in namespace: statefulset-1610, name: ss-0, uid: 106a23e7-d3bb-41d9-a30d-4998a8b7e4fd, status phase: Failed. Waiting for statefulset controller to delete.
Sep 13 11:29:39.858: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1610
STEP: Removing pod with conflicting port in namespace statefulset-1610
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1610 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 11:29:41.907: INFO: Deleting all statefulset in ns statefulset-1610
Sep 13 11:29:41.915: INFO: Scaling statefulset ss to 0
Sep 13 11:29:51.961: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:29:51.967: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:52.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1610" for this suite.

• [SLOW TEST:14.325 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":81,"skipped":1306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:52.022: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:52.050: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1891
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:58.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5896" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:29:58.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1891" for this suite.

• [SLOW TEST:6.202 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":82,"skipped":1331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:29:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-9667
STEP: creating service affinity-clusterip in namespace services-9667
STEP: creating replication controller affinity-clusterip in namespace services-9667
I0913 11:29:58.270436      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9667, replica count: 3
I0913 11:30:01.322243      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:30:01.344: INFO: Creating new exec pod
Sep 13 11:30:04.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9667 exec execpod-affinitytj8dz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Sep 13 11:30:04.677: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 13 11:30:04.677: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:30:04.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9667 exec execpod-affinitytj8dz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.238.123 80'
Sep 13 11:30:04.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.238.123 80\nConnection to 10.43.238.123 80 port [tcp/http] succeeded!\n"
Sep 13 11:30:04.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:30:04.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9667 exec execpod-affinitytj8dz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.238.123:80/ ; done'
Sep 13 11:30:05.301: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.238.123:80/\n"
Sep 13 11:30:05.301: INFO: stdout: "\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts\naffinity-clusterip-bf8ts"
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.301: INFO: Received response from host: affinity-clusterip-bf8ts
Sep 13 11:30:05.302: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9667, will wait for the garbage collector to delete the pods
Sep 13 11:30:05.404: INFO: Deleting ReplicationController affinity-clusterip took: 15.431066ms
Sep 13 11:30:05.505: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.161863ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:30:07.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9667" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.425 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":83,"skipped":1363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:30:07.654: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
Sep 13 11:30:07.705: INFO: Waiting up to 5m0s for pod "var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710" in namespace "var-expansion-1321" to be "Succeeded or Failed"
Sep 13 11:30:07.714: INFO: Pod "var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710": Phase="Pending", Reason="", readiness=false. Elapsed: 9.414746ms
Sep 13 11:30:09.741: INFO: Pod "var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710": Phase="Running", Reason="", readiness=false. Elapsed: 2.036380965s
Sep 13 11:30:11.750: INFO: Pod "var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044812969s
STEP: Saw pod success
Sep 13 11:30:11.750: INFO: Pod "var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710" satisfied condition "Succeeded or Failed"
Sep 13 11:30:11.756: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710 container dapi-container: <nil>
STEP: delete the pod
Sep 13 11:30:11.795: INFO: Waiting for pod var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710 to disappear
Sep 13 11:30:11.801: INFO: Pod var-expansion-42aa595a-d741-4a2c-b120-959a34d2d710 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:30:11.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1321" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1418,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:30:11.823: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-82d32f29-bdbb-49d0-97e0-8d0e0898ad07
STEP: Creating a pod to test consume secrets
Sep 13 11:30:11.904: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158" in namespace "projected-6476" to be "Succeeded or Failed"
Sep 13 11:30:11.916: INFO: Pod "pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158": Phase="Pending", Reason="", readiness=false. Elapsed: 11.627976ms
Sep 13 11:30:13.926: INFO: Pod "pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021470934s
Sep 13 11:30:15.937: INFO: Pod "pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033136215s
STEP: Saw pod success
Sep 13 11:30:15.937: INFO: Pod "pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158" satisfied condition "Succeeded or Failed"
Sep 13 11:30:15.945: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:30:15.984: INFO: Waiting for pod pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158 to disappear
Sep 13 11:30:15.994: INFO: Pod pod-projected-secrets-5765ea48-53eb-4800-b8f5-4e819b60f158 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:30:15.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6476" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1426,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:30:16.012: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Sep 13 11:30:16.050: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:30:31.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1781" for this suite.

• [SLOW TEST:15.119 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":86,"skipped":1442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:30:31.133: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
Sep 13 11:30:31.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 13 11:30:31.278: INFO: stderr: ""
Sep 13 11:30:31.278: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
Sep 13 11:30:31.278: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 13 11:30:31.278: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3238" to be "running and ready, or succeeded"
Sep 13 11:30:31.289: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.233318ms
Sep 13 11:30:33.310: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.032046615s
Sep 13 11:30:33.310: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 13 11:30:33.310: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 13 11:30:33.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 logs logs-generator logs-generator'
Sep 13 11:30:33.454: INFO: stderr: ""
Sep 13 11:30:33.454: INFO: stdout: "I0913 11:30:32.130758       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/gwvc 543\nI0913 11:30:32.330974       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/v8rq 470\nI0913 11:30:32.530847       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jh2r 385\nI0913 11:30:32.731489       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/z5v 311\nI0913 11:30:32.930885       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2wk 396\nI0913 11:30:33.131457       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/7cf 217\nI0913 11:30:33.330854       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/nxsd 385\n"
STEP: limiting log lines
Sep 13 11:30:33.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 logs logs-generator logs-generator --tail=1'
Sep 13 11:30:33.580: INFO: stderr: ""
Sep 13 11:30:33.580: INFO: stdout: "I0913 11:30:33.534496       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/9nv 440\n"
Sep 13 11:30:33.580: INFO: got output "I0913 11:30:33.534496       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/9nv 440\n"
STEP: limiting log bytes
Sep 13 11:30:33.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 logs logs-generator logs-generator --limit-bytes=1'
Sep 13 11:30:33.730: INFO: stderr: ""
Sep 13 11:30:33.730: INFO: stdout: "I"
Sep 13 11:30:33.730: INFO: got output "I"
STEP: exposing timestamps
Sep 13 11:30:33.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 logs logs-generator logs-generator --tail=1 --timestamps'
Sep 13 11:30:33.848: INFO: stderr: ""
Sep 13 11:30:33.848: INFO: stdout: "2022-09-13T11:30:33.730977435Z I0913 11:30:33.730856       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/x6l 570\n"
Sep 13 11:30:33.848: INFO: got output "2022-09-13T11:30:33.730977435Z I0913 11:30:33.730856       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/x6l 570\n"
STEP: restricting to a time range
Sep 13 11:30:36.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 logs logs-generator logs-generator --since=1s'
Sep 13 11:30:36.483: INFO: stderr: ""
Sep 13 11:30:36.483: INFO: stdout: "I0913 11:30:35.531363       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/hkh 306\nI0913 11:30:35.730847       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rsx5 290\nI0913 11:30:35.931343       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/lrbb 567\nI0913 11:30:36.130815       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/xtc 228\nI0913 11:30:36.331388       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/tkpj 407\n"
Sep 13 11:30:36.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 logs logs-generator logs-generator --since=24h'
Sep 13 11:30:36.622: INFO: stderr: ""
Sep 13 11:30:36.622: INFO: stdout: "I0913 11:30:32.130758       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/gwvc 543\nI0913 11:30:32.330974       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/v8rq 470\nI0913 11:30:32.530847       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jh2r 385\nI0913 11:30:32.731489       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/z5v 311\nI0913 11:30:32.930885       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2wk 396\nI0913 11:30:33.131457       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/7cf 217\nI0913 11:30:33.330854       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/nxsd 385\nI0913 11:30:33.534496       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/9nv 440\nI0913 11:30:33.730856       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/x6l 570\nI0913 11:30:33.931304       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/km2 544\nI0913 11:30:34.131874       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/zbk 579\nI0913 11:30:34.331249       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/f27 523\nI0913 11:30:34.531610       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/p5q 565\nI0913 11:30:34.731410       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/k82 217\nI0913 11:30:34.930959       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/hvgq 559\nI0913 11:30:35.131606       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/smt9 505\nI0913 11:30:35.331327       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/mpp7 584\nI0913 11:30:35.531363       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/hkh 306\nI0913 11:30:35.730847       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rsx5 290\nI0913 11:30:35.931343       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/lrbb 567\nI0913 11:30:36.130815       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/xtc 228\nI0913 11:30:36.331388       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/tkpj 407\nI0913 11:30:36.531750       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/8nvn 417\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
Sep 13 11:30:36.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3238 delete pod logs-generator'
Sep 13 11:30:37.645: INFO: stderr: ""
Sep 13 11:30:37.645: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:30:37.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3238" for this suite.

• [SLOW TEST:6.530 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":87,"skipped":1473,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:30:37.664: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:30:37.693: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:30:38.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4918" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":88,"skipped":1489,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:30:38.754: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-8199
Sep 13 11:30:38.810: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:30:40.818: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep 13 11:30:40.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 13 11:30:41.070: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 13 11:30:41.070: INFO: stdout: "iptables"
Sep 13 11:30:41.070: INFO: proxyMode: iptables
Sep 13 11:30:41.094: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 13 11:30:41.100: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8199
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8199
I0913 11:30:41.118717      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8199, replica count: 3
I0913 11:30:44.169977      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:30:44.185: INFO: Creating new exec pod
Sep 13 11:30:47.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec execpod-affinity9qg5v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Sep 13 11:30:47.474: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep 13 11:30:47.474: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:30:47.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec execpod-affinity9qg5v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.199.58 80'
Sep 13 11:30:47.689: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.199.58 80\nConnection to 10.43.199.58 80 port [tcp/http] succeeded!\n"
Sep 13 11:30:47.689: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 11:30:47.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec execpod-affinity9qg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.199.58:80/ ; done'
Sep 13 11:30:48.054: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n"
Sep 13 11:30:48.054: INFO: stdout: "\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t\naffinity-clusterip-timeout-zmt4t"
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Received response from host: affinity-clusterip-timeout-zmt4t
Sep 13 11:30:48.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec execpod-affinity9qg5v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.199.58:80/'
Sep 13 11:30:48.335: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n"
Sep 13 11:30:48.335: INFO: stdout: "affinity-clusterip-timeout-zmt4t"
Sep 13 11:31:08.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec execpod-affinity9qg5v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.199.58:80/'
Sep 13 11:31:08.585: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n"
Sep 13 11:31:08.585: INFO: stdout: "affinity-clusterip-timeout-zmt4t"
Sep 13 11:31:28.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-8199 exec execpod-affinity9qg5v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.199.58:80/'
Sep 13 11:31:28.832: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.199.58:80/\n"
Sep 13 11:31:28.832: INFO: stdout: "affinity-clusterip-timeout-7v77g"
Sep 13 11:31:28.832: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8199, will wait for the garbage collector to delete the pods
Sep 13 11:31:28.924: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.82629ms
Sep 13 11:31:29.024: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.757504ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:31:31.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8199" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:52.719 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":89,"skipped":1492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:31:31.474: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3396.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3396.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3396.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3396.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:31:33.587: INFO: DNS probes using dns-3396/dns-test-4b7882f2-1dcc-463e-b784-00c871d6829a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:31:33.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3396" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":90,"skipped":1519,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:31:33.630: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 13 11:31:37.750: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:31:37.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-810" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1535,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:31:37.811: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Sep 13 11:31:37.839: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:31:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1062" for this suite.

• [SLOW TEST:5.078 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":92,"skipped":1545,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:31:42.890: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:31:42.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9813" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":93,"skipped":1563,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:31:42.961: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:31:43.008: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 13 11:31:48.021: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 13 11:31:48.021: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 13 11:31:50.045: INFO: Creating deployment "test-rollover-deployment"
Sep 13 11:31:50.056: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 13 11:31:52.072: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 13 11:31:52.084: INFO: Ensure that both replica sets have 1 created replica
Sep 13 11:31:52.099: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 13 11:31:52.121: INFO: Updating deployment test-rollover-deployment
Sep 13 11:31:52.121: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 13 11:31:54.141: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 13 11:31:54.159: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 13 11:31:54.170: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 11:31:54.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 11:31:56.189: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 11:31:56.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 11:31:58.189: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 11:31:58.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 11:32:00.201: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 11:32:00.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 11:32:02.191: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 11:32:02.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 31, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 11:32:04.195: INFO: 
Sep 13 11:32:04.195: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:32:04.215: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2880  bee567f6-f114-479a-9186-86adf254d460 405168 2 2022-09-13 11:31:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-09-13 11:31:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005491628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-13 11:31:50 +0000 UTC,LastTransitionTime:2022-09-13 11:31:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-09-13 11:32:04 +0000 UTC,LastTransitionTime:2022-09-13 11:31:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 11:32:04.224: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-2880  22ff692d-ef45-478a-9236-fbf3233004b3 405158 2 2022-09-13 11:31:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bee567f6-f114-479a-9186-86adf254d460 0xc005491b27 0xc005491b28}] []  [{k3s Update apps/v1 2022-09-13 11:31:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bee567f6-f114-479a-9186-86adf254d460\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:32:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005491be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:32:04.224: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 13 11:32:04.224: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2880  03fde7d5-71bb-43a1-9e18-181eeaedc69b 405167 2 2022-09-13 11:31:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bee567f6-f114-479a-9186-86adf254d460 0xc005491c47 0xc005491c48}] []  [{e2e.test Update apps/v1 2022-09-13 11:31:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bee567f6-f114-479a-9186-86adf254d460\"}":{}}},"f:spec":{"f:replicas":{}}} } {k3s Update apps/v1 2022-09-13 11:32:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005491d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:32:04.224: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-2880  f36f75bb-b7af-4bb3-8ab1-4605470670b1 405138 2 2022-09-13 11:31:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bee567f6-f114-479a-9186-86adf254d460 0xc0054919f7 0xc0054919f8}] []  [{k3s Update apps/v1 2022-09-13 11:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bee567f6-f114-479a-9186-86adf254d460\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:31:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005491ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:32:04.233: INFO: Pod "test-rollover-deployment-668b7f667d-k2wjl" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-k2wjl test-rollover-deployment-668b7f667d- deployment-2880  f3fe788f-2fa2-42bb-b85f-4bf1b9ebe003 405148 0 2022-09-13 11:31:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d 22ff692d-ef45-478a-9236-fbf3233004b3 0xc00542d117 0xc00542d118}] []  [{k3s Update v1 2022-09-13 11:31:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22ff692d-ef45-478a-9236-fbf3233004b3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:31:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2jfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2jfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:31:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:31:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:31:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.64,StartTime:2022-09-13 11:31:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:31:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://2aab366b52e7a1b125e9c912fe42637e0a1a3d193f42f8b235ed920a18ab3d75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:32:04.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2880" for this suite.

• [SLOW TEST:21.293 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":94,"skipped":1568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:32:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Sep 13 11:32:04.322: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:32:06.330: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Sep 13 11:32:06.345: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:32:08.371: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 13 11:32:08.413: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 11:32:08.417: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 11:32:10.418: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 11:32:10.439: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 11:32:12.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 11:32:12.433: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:32:12.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5033" for this suite.

• [SLOW TEST:8.193 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:32:12.449: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 13 11:32:12.501: INFO: Waiting up to 5m0s for pod "pod-d11da7f9-f609-4860-990d-cbb5a5da5787" in namespace "emptydir-6326" to be "Succeeded or Failed"
Sep 13 11:32:12.507: INFO: Pod "pod-d11da7f9-f609-4860-990d-cbb5a5da5787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.816904ms
Sep 13 11:32:14.534: INFO: Pod "pod-d11da7f9-f609-4860-990d-cbb5a5da5787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032170698s
Sep 13 11:32:16.552: INFO: Pod "pod-d11da7f9-f609-4860-990d-cbb5a5da5787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049950929s
STEP: Saw pod success
Sep 13 11:32:16.552: INFO: Pod "pod-d11da7f9-f609-4860-990d-cbb5a5da5787" satisfied condition "Succeeded or Failed"
Sep 13 11:32:16.560: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-d11da7f9-f609-4860-990d-cbb5a5da5787 container test-container: <nil>
STEP: delete the pod
Sep 13 11:32:16.595: INFO: Waiting for pod pod-d11da7f9-f609-4860-990d-cbb5a5da5787 to disappear
Sep 13 11:32:16.602: INFO: Pod pod-d11da7f9-f609-4860-990d-cbb5a5da5787 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:32:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6326" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1642,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:32:16.626: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:32:16.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb" in namespace "projected-813" to be "Succeeded or Failed"
Sep 13 11:32:16.683: INFO: Pod "downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669224ms
Sep 13 11:32:18.705: INFO: Pod "downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026040208s
Sep 13 11:32:20.765: INFO: Pod "downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085713641s
STEP: Saw pod success
Sep 13 11:32:20.765: INFO: Pod "downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb" satisfied condition "Succeeded or Failed"
Sep 13 11:32:20.794: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb container client-container: <nil>
STEP: delete the pod
Sep 13 11:32:20.879: INFO: Waiting for pod downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb to disappear
Sep 13 11:32:20.894: INFO: Pod downwardapi-volume-07ba3b98-12d7-4ffa-9fd4-8ce9cb96cfbb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:32:20.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-813" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:32:20.952: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:32:21.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f" in namespace "projected-483" to be "Succeeded or Failed"
Sep 13 11:32:21.054: INFO: Pod "downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.872507ms
Sep 13 11:32:23.070: INFO: Pod "downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034667439s
Sep 13 11:32:25.091: INFO: Pod "downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056152714s
STEP: Saw pod success
Sep 13 11:32:25.091: INFO: Pod "downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f" satisfied condition "Succeeded or Failed"
Sep 13 11:32:25.097: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f container client-container: <nil>
STEP: delete the pod
Sep 13 11:32:25.129: INFO: Waiting for pod downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f to disappear
Sep 13 11:32:25.137: INFO: Pod downwardapi-volume-7fe4bcbf-c573-43c8-a335-c3fd84dbaf2f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:32:25.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-483" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:32:25.157: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:32:25.792: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:32:28.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:32:29.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8011" for this suite.
STEP: Destroying namespace "webhook-8011-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":99,"skipped":1750,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:32:29.262: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 13 11:32:29.748: INFO: Pod name wrapped-volume-race-79e684ef-b94b-4c40-9756-b10720d6c9a5: Found 0 pods out of 5
Sep 13 11:32:34.784: INFO: Pod name wrapped-volume-race-79e684ef-b94b-4c40-9756-b10720d6c9a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-79e684ef-b94b-4c40-9756-b10720d6c9a5 in namespace emptydir-wrapper-8917, will wait for the garbage collector to delete the pods
Sep 13 11:32:44.918: INFO: Deleting ReplicationController wrapped-volume-race-79e684ef-b94b-4c40-9756-b10720d6c9a5 took: 13.990424ms
Sep 13 11:32:45.018: INFO: Terminating ReplicationController wrapped-volume-race-79e684ef-b94b-4c40-9756-b10720d6c9a5 pods took: 100.233005ms
STEP: Creating RC which spawns configmap-volume pods
Sep 13 11:32:47.975: INFO: Pod name wrapped-volume-race-2b0c6b30-7f10-49ff-9517-2daa59b202e8: Found 0 pods out of 5
Sep 13 11:32:53.000: INFO: Pod name wrapped-volume-race-2b0c6b30-7f10-49ff-9517-2daa59b202e8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2b0c6b30-7f10-49ff-9517-2daa59b202e8 in namespace emptydir-wrapper-8917, will wait for the garbage collector to delete the pods
Sep 13 11:33:05.163: INFO: Deleting ReplicationController wrapped-volume-race-2b0c6b30-7f10-49ff-9517-2daa59b202e8 took: 20.115692ms
Sep 13 11:33:05.264: INFO: Terminating ReplicationController wrapped-volume-race-2b0c6b30-7f10-49ff-9517-2daa59b202e8 pods took: 100.682238ms
STEP: Creating RC which spawns configmap-volume pods
Sep 13 11:33:08.403: INFO: Pod name wrapped-volume-race-e9d316b3-e016-4b45-bcd1-8878e77daebd: Found 0 pods out of 5
Sep 13 11:33:13.427: INFO: Pod name wrapped-volume-race-e9d316b3-e016-4b45-bcd1-8878e77daebd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e9d316b3-e016-4b45-bcd1-8878e77daebd in namespace emptydir-wrapper-8917, will wait for the garbage collector to delete the pods
Sep 13 11:33:23.559: INFO: Deleting ReplicationController wrapped-volume-race-e9d316b3-e016-4b45-bcd1-8878e77daebd took: 16.040709ms
Sep 13 11:33:23.660: INFO: Terminating ReplicationController wrapped-volume-race-e9d316b3-e016-4b45-bcd1-8878e77daebd pods took: 100.808356ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:27.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8917" for this suite.

• [SLOW TEST:58.013 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":100,"skipped":1753,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:27.275: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:33:27.526: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:33:30.566: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:33:30.584: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:33.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4576" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.572 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":101,"skipped":1754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:33.848: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:33:35.978: INFO: DNS probes using dns-5526/dns-test-0fb01887-2e11-491c-9ca7-0abdbdb9ddc4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:36.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5526" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":102,"skipped":1780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:36.018: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-f164ca03-fe7d-42e5-8a0a-ca6fdf632843
STEP: Creating a pod to test consume secrets
Sep 13 11:33:36.062: INFO: Waiting up to 5m0s for pod "pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81" in namespace "secrets-8482" to be "Succeeded or Failed"
Sep 13 11:33:36.066: INFO: Pod "pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350668ms
Sep 13 11:33:38.076: INFO: Pod "pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013977685s
Sep 13 11:33:40.095: INFO: Pod "pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032719332s
STEP: Saw pod success
Sep 13 11:33:40.095: INFO: Pod "pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81" satisfied condition "Succeeded or Failed"
Sep 13 11:33:40.101: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:33:40.140: INFO: Waiting for pod pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81 to disappear
Sep 13 11:33:40.146: INFO: Pod pod-secrets-08647d39-fa83-45cc-bc7f-fcc0167e3c81 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:40.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8482" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":1814,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:40.162: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
Sep 13 11:33:40.750: INFO: created pod pod-service-account-defaultsa
Sep 13 11:33:40.750: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 13 11:33:40.760: INFO: created pod pod-service-account-mountsa
Sep 13 11:33:40.760: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 13 11:33:40.779: INFO: created pod pod-service-account-nomountsa
Sep 13 11:33:40.779: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 13 11:33:40.786: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 13 11:33:40.786: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 13 11:33:40.795: INFO: created pod pod-service-account-mountsa-mountspec
Sep 13 11:33:40.795: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 13 11:33:40.806: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 13 11:33:40.806: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 13 11:33:40.822: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 13 11:33:40.822: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 13 11:33:40.832: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 13 11:33:40.832: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 13 11:33:40.840: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 13 11:33:40.840: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:40.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9761" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":104,"skipped":1833,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:40.917: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-6012/secret-test-1550fe75-1737-4a47-bb4b-202344cb8cde
STEP: Creating a pod to test consume secrets
Sep 13 11:33:40.983: INFO: Waiting up to 5m0s for pod "pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce" in namespace "secrets-6012" to be "Succeeded or Failed"
Sep 13 11:33:40.990: INFO: Pod "pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.806995ms
Sep 13 11:33:43.006: INFO: Pod "pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023466904s
Sep 13 11:33:45.028: INFO: Pod "pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04513376s
STEP: Saw pod success
Sep 13 11:33:45.028: INFO: Pod "pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce" satisfied condition "Succeeded or Failed"
Sep 13 11:33:45.036: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom pod pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce container env-test: <nil>
STEP: delete the pod
Sep 13 11:33:45.083: INFO: Waiting for pod pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce to disappear
Sep 13 11:33:45.088: INFO: Pod pod-configmaps-6581be5a-36e0-4dab-a3e7-4582d46d46ce no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:45.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6012" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":1842,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:45.101: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 13 11:33:45.129: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:33:47.227: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:33:58.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1624" for this suite.

• [SLOW TEST:13.440 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":106,"skipped":1863,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:33:58.542: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:33:59.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:34:02.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:02.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2037" for this suite.
STEP: Destroying namespace "webhook-2037-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":107,"skipped":1866,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:34:02.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:34:05.849: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
Sep 13 11:34:05.885: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 13 11:34:06.010: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:06.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1756" for this suite.
STEP: Destroying namespace "webhook-1756-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":108,"skipped":1874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:06.120: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 13 11:34:06.181: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 13 11:34:06.187: INFO: starting watch
STEP: patching
STEP: updating
Sep 13 11:34:06.206: INFO: waiting for watch events with expected annotations
Sep 13 11:34:06.207: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:06.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9701" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":109,"skipped":1927,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:06.257: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:34:06.292: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8457
I0913 11:34:06.300912      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8457, replica count: 1
I0913 11:34:07.352032      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0913 11:34:08.352424      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:34:08.478: INFO: Created: latency-svc-bntfs
Sep 13 11:34:08.489: INFO: Got endpoints: latency-svc-bntfs [36.016601ms]
Sep 13 11:34:08.502: INFO: Created: latency-svc-kx4dk
Sep 13 11:34:08.508: INFO: Got endpoints: latency-svc-kx4dk [19.082561ms]
Sep 13 11:34:08.510: INFO: Created: latency-svc-tlhwp
Sep 13 11:34:08.518: INFO: Created: latency-svc-vzrgd
Sep 13 11:34:08.521: INFO: Got endpoints: latency-svc-tlhwp [31.371633ms]
Sep 13 11:34:08.525: INFO: Got endpoints: latency-svc-vzrgd [35.375372ms]
Sep 13 11:34:08.527: INFO: Created: latency-svc-xfkq7
Sep 13 11:34:08.538: INFO: Got endpoints: latency-svc-xfkq7 [49.20768ms]
Sep 13 11:34:08.540: INFO: Created: latency-svc-cmtm2
Sep 13 11:34:08.548: INFO: Created: latency-svc-scd4q
Sep 13 11:34:08.553: INFO: Created: latency-svc-qxvz7
Sep 13 11:34:08.558: INFO: Created: latency-svc-q2kfw
Sep 13 11:34:08.563: INFO: Got endpoints: latency-svc-cmtm2 [73.261306ms]
Sep 13 11:34:08.572: INFO: Created: latency-svc-dzlg5
Sep 13 11:34:08.583: INFO: Got endpoints: latency-svc-scd4q [93.375503ms]
Sep 13 11:34:08.590: INFO: Got endpoints: latency-svc-dzlg5 [100.023431ms]
Sep 13 11:34:08.591: INFO: Created: latency-svc-vtkxs
Sep 13 11:34:08.604: INFO: Got endpoints: latency-svc-qxvz7 [114.754501ms]
Sep 13 11:34:08.604: INFO: Got endpoints: latency-svc-q2kfw [114.889438ms]
Sep 13 11:34:08.610: INFO: Created: latency-svc-z4jfj
Sep 13 11:34:08.619: INFO: Got endpoints: latency-svc-vtkxs [129.158185ms]
Sep 13 11:34:08.627: INFO: Got endpoints: latency-svc-z4jfj [137.518259ms]
Sep 13 11:34:08.631: INFO: Created: latency-svc-wmm5m
Sep 13 11:34:08.638: INFO: Created: latency-svc-6466t
Sep 13 11:34:08.642: INFO: Got endpoints: latency-svc-wmm5m [151.882ms]
Sep 13 11:34:08.650: INFO: Got endpoints: latency-svc-6466t [159.918594ms]
Sep 13 11:34:08.662: INFO: Created: latency-svc-z849j
Sep 13 11:34:08.662: INFO: Got endpoints: latency-svc-z849j [172.684376ms]
Sep 13 11:34:08.670: INFO: Created: latency-svc-chmbs
Sep 13 11:34:08.683: INFO: Created: latency-svc-fdbp8
Sep 13 11:34:08.685: INFO: Got endpoints: latency-svc-chmbs [194.823444ms]
Sep 13 11:34:08.691: INFO: Created: latency-svc-72bzz
Sep 13 11:34:08.698: INFO: Got endpoints: latency-svc-fdbp8 [189.578522ms]
Sep 13 11:34:08.699: INFO: Got endpoints: latency-svc-72bzz [178.177213ms]
Sep 13 11:34:08.722: INFO: Created: latency-svc-dwfw7
Sep 13 11:34:08.725: INFO: Created: latency-svc-xlpr2
Sep 13 11:34:08.735: INFO: Created: latency-svc-jhrwj
Sep 13 11:34:08.739: INFO: Got endpoints: latency-svc-dwfw7 [41.380756ms]
Sep 13 11:34:08.742: INFO: Got endpoints: latency-svc-xlpr2 [216.527004ms]
Sep 13 11:34:08.744: INFO: Got endpoints: latency-svc-jhrwj [181.420851ms]
Sep 13 11:34:08.749: INFO: Created: latency-svc-p4gvs
Sep 13 11:34:08.753: INFO: Got endpoints: latency-svc-p4gvs [170.220531ms]
Sep 13 11:34:08.795: INFO: Created: latency-svc-8df66
Sep 13 11:34:08.803: INFO: Got endpoints: latency-svc-8df66 [161.654273ms]
Sep 13 11:34:08.805: INFO: Created: latency-svc-xtzhq
Sep 13 11:34:08.806: INFO: Created: latency-svc-658mp
Sep 13 11:34:08.806: INFO: Created: latency-svc-x94s6
Sep 13 11:34:08.813: INFO: Created: latency-svc-9kw54
Sep 13 11:34:08.816: INFO: Created: latency-svc-4th96
Sep 13 11:34:08.816: INFO: Created: latency-svc-77mwh
Sep 13 11:34:08.817: INFO: Created: latency-svc-jkkdp
Sep 13 11:34:08.818: INFO: Created: latency-svc-tr4cc
Sep 13 11:34:08.819: INFO: Created: latency-svc-p76xm
Sep 13 11:34:08.819: INFO: Created: latency-svc-z2kf4
Sep 13 11:34:08.819: INFO: Created: latency-svc-57t9h
Sep 13 11:34:08.820: INFO: Created: latency-svc-zkqjw
Sep 13 11:34:08.820: INFO: Created: latency-svc-v96s2
Sep 13 11:34:08.820: INFO: Got endpoints: latency-svc-658mp [201.563055ms]
Sep 13 11:34:08.825: INFO: Created: latency-svc-7b2n2
Sep 13 11:34:08.825: INFO: Created: latency-svc-nm6t6
Sep 13 11:34:08.844: INFO: Got endpoints: latency-svc-x94s6 [216.416466ms]
Sep 13 11:34:08.844: INFO: Got endpoints: latency-svc-9kw54 [90.570607ms]
Sep 13 11:34:08.844: INFO: Got endpoints: latency-svc-4th96 [159.611206ms]
Sep 13 11:34:08.844: INFO: Got endpoints: latency-svc-77mwh [254.69996ms]
Sep 13 11:34:08.845: INFO: Got endpoints: latency-svc-xtzhq [105.277021ms]
Sep 13 11:34:08.851: INFO: Created: latency-svc-fnc94
Sep 13 11:34:08.856: INFO: Got endpoints: latency-svc-tr4cc [251.685316ms]
Sep 13 11:34:08.856: INFO: Got endpoints: latency-svc-jkkdp [317.667945ms]
Sep 13 11:34:08.856: INFO: Got endpoints: latency-svc-p76xm [156.985332ms]
Sep 13 11:34:08.856: INFO: Got endpoints: latency-svc-z2kf4 [52.914391ms]
Sep 13 11:34:08.864: INFO: Got endpoints: latency-svc-57t9h [259.426231ms]
Sep 13 11:34:08.864: INFO: Got endpoints: latency-svc-v96s2 [201.875656ms]
Sep 13 11:34:08.874: INFO: Created: latency-svc-jp6q4
Sep 13 11:34:08.875: INFO: Got endpoints: latency-svc-zkqjw [225.50619ms]
Sep 13 11:34:08.876: INFO: Got endpoints: latency-svc-nm6t6 [134.876666ms]
Sep 13 11:34:08.887: INFO: Got endpoints: latency-svc-7b2n2 [143.255835ms]
Sep 13 11:34:08.939: INFO: Got endpoints: latency-svc-fnc94 [118.357841ms]
Sep 13 11:34:08.960: INFO: Created: latency-svc-29ncz
Sep 13 11:34:08.964: INFO: Created: latency-svc-4twqt
Sep 13 11:34:08.969: INFO: Created: latency-svc-4zdgz
Sep 13 11:34:08.969: INFO: Created: latency-svc-j7v6m
Sep 13 11:34:08.970: INFO: Created: latency-svc-bgd4j
Sep 13 11:34:08.970: INFO: Created: latency-svc-kq86t
Sep 13 11:34:08.972: INFO: Created: latency-svc-zm999
Sep 13 11:34:08.973: INFO: Created: latency-svc-pxr2m
Sep 13 11:34:08.974: INFO: Created: latency-svc-x7sdk
Sep 13 11:34:08.976: INFO: Created: latency-svc-jxvqq
Sep 13 11:34:08.976: INFO: Created: latency-svc-2m97x
Sep 13 11:34:08.976: INFO: Created: latency-svc-zzjw5
Sep 13 11:34:08.978: INFO: Created: latency-svc-px5w9
Sep 13 11:34:08.978: INFO: Created: latency-svc-hm6pv
Sep 13 11:34:08.983: INFO: Got endpoints: latency-svc-jp6q4 [139.329783ms]
Sep 13 11:34:08.992: INFO: Created: latency-svc-ctzpb
Sep 13 11:34:09.041: INFO: Got endpoints: latency-svc-29ncz [196.652515ms]
Sep 13 11:34:09.058: INFO: Created: latency-svc-2vm52
Sep 13 11:34:09.098: INFO: Got endpoints: latency-svc-4twqt [234.114873ms]
Sep 13 11:34:09.115: INFO: Created: latency-svc-l2mlr
Sep 13 11:34:09.139: INFO: Got endpoints: latency-svc-j7v6m [200.510389ms]
Sep 13 11:34:09.159: INFO: Created: latency-svc-gv8q6
Sep 13 11:34:09.187: INFO: Got endpoints: latency-svc-kq86t [310.720067ms]
Sep 13 11:34:09.202: INFO: Created: latency-svc-db6rd
Sep 13 11:34:09.241: INFO: Got endpoints: latency-svc-bgd4j [376.65304ms]
Sep 13 11:34:09.257: INFO: Created: latency-svc-zss82
Sep 13 11:34:09.286: INFO: Got endpoints: latency-svc-4zdgz [410.525676ms]
Sep 13 11:34:09.298: INFO: Created: latency-svc-k5jh8
Sep 13 11:34:09.340: INFO: Got endpoints: latency-svc-zm999 [495.694503ms]
Sep 13 11:34:09.355: INFO: Created: latency-svc-nkxzm
Sep 13 11:34:09.393: INFO: Got endpoints: latency-svc-pxr2m [537.281451ms]
Sep 13 11:34:09.412: INFO: Created: latency-svc-s6rmq
Sep 13 11:34:09.442: INFO: Got endpoints: latency-svc-x7sdk [585.74795ms]
Sep 13 11:34:09.460: INFO: Created: latency-svc-755f7
Sep 13 11:34:09.495: INFO: Got endpoints: latency-svc-jxvqq [650.713564ms]
Sep 13 11:34:09.512: INFO: Created: latency-svc-drxv7
Sep 13 11:34:09.540: INFO: Got endpoints: latency-svc-zzjw5 [683.408851ms]
Sep 13 11:34:09.561: INFO: Created: latency-svc-gqxd2
Sep 13 11:34:09.588: INFO: Got endpoints: latency-svc-2m97x [700.559446ms]
Sep 13 11:34:09.603: INFO: Created: latency-svc-nhkf5
Sep 13 11:34:09.640: INFO: Got endpoints: latency-svc-px5w9 [783.935511ms]
Sep 13 11:34:09.659: INFO: Created: latency-svc-wn2tf
Sep 13 11:34:09.686: INFO: Got endpoints: latency-svc-hm6pv [841.611297ms]
Sep 13 11:34:09.699: INFO: Created: latency-svc-pnjdq
Sep 13 11:34:09.735: INFO: Got endpoints: latency-svc-ctzpb [752.241735ms]
Sep 13 11:34:09.758: INFO: Created: latency-svc-k889b
Sep 13 11:34:09.789: INFO: Got endpoints: latency-svc-2vm52 [747.336405ms]
Sep 13 11:34:09.802: INFO: Created: latency-svc-n9nmm
Sep 13 11:34:09.839: INFO: Got endpoints: latency-svc-l2mlr [740.973621ms]
Sep 13 11:34:09.851: INFO: Created: latency-svc-bk2r6
Sep 13 11:34:09.893: INFO: Got endpoints: latency-svc-gv8q6 [753.316101ms]
Sep 13 11:34:09.906: INFO: Created: latency-svc-n7ldl
Sep 13 11:34:09.943: INFO: Got endpoints: latency-svc-db6rd [755.258267ms]
Sep 13 11:34:09.963: INFO: Created: latency-svc-qpts8
Sep 13 11:34:09.992: INFO: Got endpoints: latency-svc-zss82 [750.534133ms]
Sep 13 11:34:10.006: INFO: Created: latency-svc-6lfqt
Sep 13 11:34:10.043: INFO: Got endpoints: latency-svc-k5jh8 [757.382881ms]
Sep 13 11:34:10.060: INFO: Created: latency-svc-26wzp
Sep 13 11:34:10.091: INFO: Got endpoints: latency-svc-nkxzm [750.571452ms]
Sep 13 11:34:10.107: INFO: Created: latency-svc-bwm95
Sep 13 11:34:10.142: INFO: Got endpoints: latency-svc-s6rmq [748.635466ms]
Sep 13 11:34:10.157: INFO: Created: latency-svc-v8zcs
Sep 13 11:34:10.186: INFO: Got endpoints: latency-svc-755f7 [744.301913ms]
Sep 13 11:34:10.206: INFO: Created: latency-svc-2mhhz
Sep 13 11:34:10.242: INFO: Got endpoints: latency-svc-drxv7 [746.622363ms]
Sep 13 11:34:10.258: INFO: Created: latency-svc-7grwq
Sep 13 11:34:10.293: INFO: Got endpoints: latency-svc-gqxd2 [752.990435ms]
Sep 13 11:34:10.308: INFO: Created: latency-svc-m9jls
Sep 13 11:34:10.344: INFO: Got endpoints: latency-svc-nhkf5 [755.18079ms]
Sep 13 11:34:10.364: INFO: Created: latency-svc-msfbj
Sep 13 11:34:10.390: INFO: Got endpoints: latency-svc-wn2tf [749.274713ms]
Sep 13 11:34:10.408: INFO: Created: latency-svc-f5lmm
Sep 13 11:34:10.442: INFO: Got endpoints: latency-svc-pnjdq [755.54171ms]
Sep 13 11:34:10.471: INFO: Created: latency-svc-p77qz
Sep 13 11:34:10.511: INFO: Got endpoints: latency-svc-k889b [775.21895ms]
Sep 13 11:34:10.528: INFO: Created: latency-svc-tttjn
Sep 13 11:34:10.538: INFO: Got endpoints: latency-svc-n9nmm [749.383132ms]
Sep 13 11:34:10.547: INFO: Created: latency-svc-98gtv
Sep 13 11:34:10.589: INFO: Got endpoints: latency-svc-bk2r6 [749.631405ms]
Sep 13 11:34:10.598: INFO: Created: latency-svc-2hxkd
Sep 13 11:34:10.646: INFO: Got endpoints: latency-svc-n7ldl [753.232422ms]
Sep 13 11:34:10.656: INFO: Created: latency-svc-k89gp
Sep 13 11:34:10.686: INFO: Got endpoints: latency-svc-qpts8 [743.267797ms]
Sep 13 11:34:10.698: INFO: Created: latency-svc-jxgtw
Sep 13 11:34:10.740: INFO: Got endpoints: latency-svc-6lfqt [748.507622ms]
Sep 13 11:34:10.758: INFO: Created: latency-svc-xx222
Sep 13 11:34:10.789: INFO: Got endpoints: latency-svc-26wzp [745.922088ms]
Sep 13 11:34:10.803: INFO: Created: latency-svc-ggfzn
Sep 13 11:34:10.837: INFO: Got endpoints: latency-svc-bwm95 [746.168405ms]
Sep 13 11:34:10.852: INFO: Created: latency-svc-8ng5p
Sep 13 11:34:10.896: INFO: Got endpoints: latency-svc-v8zcs [753.423554ms]
Sep 13 11:34:10.912: INFO: Created: latency-svc-4mwqn
Sep 13 11:34:10.941: INFO: Got endpoints: latency-svc-2mhhz [754.868714ms]
Sep 13 11:34:10.958: INFO: Created: latency-svc-44s5w
Sep 13 11:34:10.989: INFO: Got endpoints: latency-svc-7grwq [746.389253ms]
Sep 13 11:34:11.008: INFO: Created: latency-svc-zk7k4
Sep 13 11:34:11.042: INFO: Got endpoints: latency-svc-m9jls [749.04712ms]
Sep 13 11:34:11.063: INFO: Created: latency-svc-stnxf
Sep 13 11:34:11.088: INFO: Got endpoints: latency-svc-msfbj [744.477795ms]
Sep 13 11:34:11.114: INFO: Created: latency-svc-nsfrt
Sep 13 11:34:11.140: INFO: Got endpoints: latency-svc-f5lmm [750.782929ms]
Sep 13 11:34:11.173: INFO: Created: latency-svc-htsn6
Sep 13 11:34:11.183: INFO: Got endpoints: latency-svc-p77qz [740.778754ms]
Sep 13 11:34:11.201: INFO: Created: latency-svc-ctbg5
Sep 13 11:34:11.236: INFO: Got endpoints: latency-svc-tttjn [724.830717ms]
Sep 13 11:34:11.248: INFO: Created: latency-svc-9m6qb
Sep 13 11:34:11.287: INFO: Got endpoints: latency-svc-98gtv [748.623502ms]
Sep 13 11:34:11.301: INFO: Created: latency-svc-6nw24
Sep 13 11:34:11.337: INFO: Got endpoints: latency-svc-2hxkd [748.060183ms]
Sep 13 11:34:11.351: INFO: Created: latency-svc-55qxh
Sep 13 11:34:11.387: INFO: Got endpoints: latency-svc-k89gp [740.868656ms]
Sep 13 11:34:11.396: INFO: Created: latency-svc-6dccd
Sep 13 11:34:11.437: INFO: Got endpoints: latency-svc-jxgtw [750.65323ms]
Sep 13 11:34:11.454: INFO: Created: latency-svc-6hbzb
Sep 13 11:34:11.488: INFO: Got endpoints: latency-svc-xx222 [747.287479ms]
Sep 13 11:34:11.504: INFO: Created: latency-svc-fbg67
Sep 13 11:34:11.535: INFO: Got endpoints: latency-svc-ggfzn [745.075705ms]
Sep 13 11:34:11.542: INFO: Created: latency-svc-2nr8h
Sep 13 11:34:11.584: INFO: Got endpoints: latency-svc-8ng5p [747.182899ms]
Sep 13 11:34:11.593: INFO: Created: latency-svc-h2sl4
Sep 13 11:34:11.638: INFO: Got endpoints: latency-svc-4mwqn [741.316166ms]
Sep 13 11:34:11.651: INFO: Created: latency-svc-n9qgn
Sep 13 11:34:11.692: INFO: Got endpoints: latency-svc-44s5w [750.206162ms]
Sep 13 11:34:11.705: INFO: Created: latency-svc-cs2kf
Sep 13 11:34:11.737: INFO: Got endpoints: latency-svc-zk7k4 [748.151294ms]
Sep 13 11:34:11.751: INFO: Created: latency-svc-l8hgj
Sep 13 11:34:11.792: INFO: Got endpoints: latency-svc-stnxf [750.243196ms]
Sep 13 11:34:11.809: INFO: Created: latency-svc-fh8t8
Sep 13 11:34:11.841: INFO: Got endpoints: latency-svc-nsfrt [752.950399ms]
Sep 13 11:34:11.858: INFO: Created: latency-svc-c98vm
Sep 13 11:34:11.887: INFO: Got endpoints: latency-svc-htsn6 [745.970165ms]
Sep 13 11:34:11.897: INFO: Created: latency-svc-928mw
Sep 13 11:34:11.942: INFO: Got endpoints: latency-svc-ctbg5 [759.249835ms]
Sep 13 11:34:11.960: INFO: Created: latency-svc-jzrn9
Sep 13 11:34:11.985: INFO: Got endpoints: latency-svc-9m6qb [748.811476ms]
Sep 13 11:34:12.004: INFO: Created: latency-svc-vql8t
Sep 13 11:34:12.051: INFO: Got endpoints: latency-svc-6nw24 [764.177557ms]
Sep 13 11:34:12.093: INFO: Got endpoints: latency-svc-55qxh [755.85472ms]
Sep 13 11:34:12.100: INFO: Created: latency-svc-j5c2k
Sep 13 11:34:12.110: INFO: Created: latency-svc-hhhd5
Sep 13 11:34:12.140: INFO: Got endpoints: latency-svc-6dccd [752.388674ms]
Sep 13 11:34:12.159: INFO: Created: latency-svc-wl2fq
Sep 13 11:34:12.191: INFO: Got endpoints: latency-svc-6hbzb [754.503788ms]
Sep 13 11:34:12.211: INFO: Created: latency-svc-8fsgh
Sep 13 11:34:12.241: INFO: Got endpoints: latency-svc-fbg67 [753.22292ms]
Sep 13 11:34:12.258: INFO: Created: latency-svc-tzmvh
Sep 13 11:34:12.293: INFO: Got endpoints: latency-svc-2nr8h [758.410744ms]
Sep 13 11:34:12.315: INFO: Created: latency-svc-mkbgg
Sep 13 11:34:12.341: INFO: Got endpoints: latency-svc-h2sl4 [757.010718ms]
Sep 13 11:34:12.356: INFO: Created: latency-svc-tq4vf
Sep 13 11:34:12.390: INFO: Got endpoints: latency-svc-n9qgn [752.135617ms]
Sep 13 11:34:12.402: INFO: Created: latency-svc-924sn
Sep 13 11:34:12.443: INFO: Got endpoints: latency-svc-cs2kf [750.894568ms]
Sep 13 11:34:12.457: INFO: Created: latency-svc-sqrqv
Sep 13 11:34:12.487: INFO: Got endpoints: latency-svc-l8hgj [750.201547ms]
Sep 13 11:34:12.503: INFO: Created: latency-svc-tq7n5
Sep 13 11:34:12.542: INFO: Got endpoints: latency-svc-fh8t8 [749.50669ms]
Sep 13 11:34:12.556: INFO: Created: latency-svc-d97bl
Sep 13 11:34:12.590: INFO: Got endpoints: latency-svc-c98vm [748.487898ms]
Sep 13 11:34:12.604: INFO: Created: latency-svc-dp2bt
Sep 13 11:34:12.635: INFO: Got endpoints: latency-svc-928mw [748.029438ms]
Sep 13 11:34:12.650: INFO: Created: latency-svc-bgmff
Sep 13 11:34:12.688: INFO: Got endpoints: latency-svc-jzrn9 [746.260392ms]
Sep 13 11:34:12.699: INFO: Created: latency-svc-tjjfb
Sep 13 11:34:12.738: INFO: Got endpoints: latency-svc-vql8t [752.985614ms]
Sep 13 11:34:12.748: INFO: Created: latency-svc-xxj2v
Sep 13 11:34:12.783: INFO: Got endpoints: latency-svc-j5c2k [731.988655ms]
Sep 13 11:34:12.794: INFO: Created: latency-svc-w7nr2
Sep 13 11:34:12.836: INFO: Got endpoints: latency-svc-hhhd5 [742.640543ms]
Sep 13 11:34:12.844: INFO: Created: latency-svc-x8zq7
Sep 13 11:34:12.912: INFO: Got endpoints: latency-svc-wl2fq [771.95745ms]
Sep 13 11:34:12.927: INFO: Created: latency-svc-p2vk7
Sep 13 11:34:12.934: INFO: Got endpoints: latency-svc-8fsgh [742.416763ms]
Sep 13 11:34:12.946: INFO: Created: latency-svc-tn7tz
Sep 13 11:34:12.994: INFO: Got endpoints: latency-svc-tzmvh [753.201583ms]
Sep 13 11:34:13.004: INFO: Created: latency-svc-thnnz
Sep 13 11:34:13.045: INFO: Got endpoints: latency-svc-mkbgg [751.361706ms]
Sep 13 11:34:13.060: INFO: Created: latency-svc-s2gk6
Sep 13 11:34:13.090: INFO: Got endpoints: latency-svc-tq4vf [748.680103ms]
Sep 13 11:34:13.107: INFO: Created: latency-svc-gqchj
Sep 13 11:34:13.142: INFO: Got endpoints: latency-svc-924sn [751.889028ms]
Sep 13 11:34:13.151: INFO: Created: latency-svc-gkdrv
Sep 13 11:34:13.185: INFO: Got endpoints: latency-svc-sqrqv [741.754453ms]
Sep 13 11:34:13.195: INFO: Created: latency-svc-2pt4d
Sep 13 11:34:13.237: INFO: Got endpoints: latency-svc-tq7n5 [749.610405ms]
Sep 13 11:34:13.248: INFO: Created: latency-svc-c88z5
Sep 13 11:34:13.294: INFO: Got endpoints: latency-svc-d97bl [752.507828ms]
Sep 13 11:34:13.320: INFO: Created: latency-svc-w7kbl
Sep 13 11:34:13.335: INFO: Got endpoints: latency-svc-dp2bt [745.459068ms]
Sep 13 11:34:13.347: INFO: Created: latency-svc-kscz5
Sep 13 11:34:13.386: INFO: Got endpoints: latency-svc-bgmff [750.418536ms]
Sep 13 11:34:13.400: INFO: Created: latency-svc-2wrdf
Sep 13 11:34:13.434: INFO: Got endpoints: latency-svc-tjjfb [745.822341ms]
Sep 13 11:34:13.444: INFO: Created: latency-svc-c5b7p
Sep 13 11:34:13.491: INFO: Got endpoints: latency-svc-xxj2v [752.96598ms]
Sep 13 11:34:13.504: INFO: Created: latency-svc-x6t8f
Sep 13 11:34:13.538: INFO: Got endpoints: latency-svc-w7nr2 [754.754482ms]
Sep 13 11:34:13.550: INFO: Created: latency-svc-j42h7
Sep 13 11:34:13.584: INFO: Got endpoints: latency-svc-x8zq7 [747.981025ms]
Sep 13 11:34:13.594: INFO: Created: latency-svc-vfrzv
Sep 13 11:34:13.637: INFO: Got endpoints: latency-svc-p2vk7 [725.754058ms]
Sep 13 11:34:13.647: INFO: Created: latency-svc-5rrcg
Sep 13 11:34:13.690: INFO: Got endpoints: latency-svc-tn7tz [755.730232ms]
Sep 13 11:34:13.715: INFO: Created: latency-svc-nwpcf
Sep 13 11:34:13.735: INFO: Got endpoints: latency-svc-thnnz [740.555325ms]
Sep 13 11:34:13.745: INFO: Created: latency-svc-72sxz
Sep 13 11:34:13.786: INFO: Got endpoints: latency-svc-s2gk6 [741.24575ms]
Sep 13 11:34:13.804: INFO: Created: latency-svc-zxns4
Sep 13 11:34:13.835: INFO: Got endpoints: latency-svc-gqchj [744.306778ms]
Sep 13 11:34:13.843: INFO: Created: latency-svc-mg66c
Sep 13 11:34:13.884: INFO: Got endpoints: latency-svc-gkdrv [742.115498ms]
Sep 13 11:34:13.897: INFO: Created: latency-svc-x4gb6
Sep 13 11:34:13.939: INFO: Got endpoints: latency-svc-2pt4d [753.827172ms]
Sep 13 11:34:13.953: INFO: Created: latency-svc-xql64
Sep 13 11:34:13.996: INFO: Got endpoints: latency-svc-c88z5 [758.506194ms]
Sep 13 11:34:14.011: INFO: Created: latency-svc-6zfmm
Sep 13 11:34:14.036: INFO: Got endpoints: latency-svc-w7kbl [741.215756ms]
Sep 13 11:34:14.047: INFO: Created: latency-svc-8tczv
Sep 13 11:34:14.093: INFO: Got endpoints: latency-svc-kscz5 [757.649026ms]
Sep 13 11:34:14.107: INFO: Created: latency-svc-5f2gr
Sep 13 11:34:14.135: INFO: Got endpoints: latency-svc-2wrdf [749.070482ms]
Sep 13 11:34:14.146: INFO: Created: latency-svc-skgqb
Sep 13 11:34:14.185: INFO: Got endpoints: latency-svc-c5b7p [750.402411ms]
Sep 13 11:34:14.206: INFO: Created: latency-svc-2xngf
Sep 13 11:34:14.241: INFO: Got endpoints: latency-svc-x6t8f [749.447589ms]
Sep 13 11:34:14.255: INFO: Created: latency-svc-ckzzj
Sep 13 11:34:14.285: INFO: Got endpoints: latency-svc-j42h7 [746.508737ms]
Sep 13 11:34:14.297: INFO: Created: latency-svc-4lh2q
Sep 13 11:34:14.342: INFO: Got endpoints: latency-svc-vfrzv [758.067155ms]
Sep 13 11:34:14.366: INFO: Created: latency-svc-pmmxt
Sep 13 11:34:14.388: INFO: Got endpoints: latency-svc-5rrcg [750.647812ms]
Sep 13 11:34:14.402: INFO: Created: latency-svc-75nwx
Sep 13 11:34:14.436: INFO: Got endpoints: latency-svc-nwpcf [745.959204ms]
Sep 13 11:34:14.464: INFO: Created: latency-svc-jltvx
Sep 13 11:34:14.486: INFO: Got endpoints: latency-svc-72sxz [750.703383ms]
Sep 13 11:34:14.508: INFO: Created: latency-svc-59t4q
Sep 13 11:34:14.536: INFO: Got endpoints: latency-svc-zxns4 [749.699134ms]
Sep 13 11:34:14.547: INFO: Created: latency-svc-gkg8q
Sep 13 11:34:14.586: INFO: Got endpoints: latency-svc-mg66c [751.17585ms]
Sep 13 11:34:14.610: INFO: Created: latency-svc-q5w4g
Sep 13 11:34:14.635: INFO: Got endpoints: latency-svc-x4gb6 [750.354679ms]
Sep 13 11:34:14.649: INFO: Created: latency-svc-2q4xg
Sep 13 11:34:14.693: INFO: Got endpoints: latency-svc-xql64 [754.011406ms]
Sep 13 11:34:14.711: INFO: Created: latency-svc-lkhph
Sep 13 11:34:14.739: INFO: Got endpoints: latency-svc-6zfmm [743.397604ms]
Sep 13 11:34:14.749: INFO: Created: latency-svc-vz658
Sep 13 11:34:14.785: INFO: Got endpoints: latency-svc-8tczv [748.511989ms]
Sep 13 11:34:14.795: INFO: Created: latency-svc-lg4dg
Sep 13 11:34:14.833: INFO: Got endpoints: latency-svc-5f2gr [739.433846ms]
Sep 13 11:34:14.847: INFO: Created: latency-svc-p8752
Sep 13 11:34:14.886: INFO: Got endpoints: latency-svc-skgqb [750.810029ms]
Sep 13 11:34:14.894: INFO: Created: latency-svc-skbfd
Sep 13 11:34:14.936: INFO: Got endpoints: latency-svc-2xngf [751.617077ms]
Sep 13 11:34:14.950: INFO: Created: latency-svc-jqzh4
Sep 13 11:34:14.992: INFO: Got endpoints: latency-svc-ckzzj [751.35511ms]
Sep 13 11:34:15.011: INFO: Created: latency-svc-l6ckq
Sep 13 11:34:15.038: INFO: Got endpoints: latency-svc-4lh2q [753.1914ms]
Sep 13 11:34:15.055: INFO: Created: latency-svc-2jf4n
Sep 13 11:34:15.093: INFO: Got endpoints: latency-svc-pmmxt [750.71789ms]
Sep 13 11:34:15.111: INFO: Created: latency-svc-7l6zn
Sep 13 11:34:15.138: INFO: Got endpoints: latency-svc-75nwx [749.529572ms]
Sep 13 11:34:15.147: INFO: Created: latency-svc-4kss8
Sep 13 11:34:15.192: INFO: Got endpoints: latency-svc-jltvx [755.923529ms]
Sep 13 11:34:15.200: INFO: Created: latency-svc-7pgqd
Sep 13 11:34:15.242: INFO: Got endpoints: latency-svc-59t4q [756.227694ms]
Sep 13 11:34:15.251: INFO: Created: latency-svc-lwpbp
Sep 13 11:34:15.291: INFO: Got endpoints: latency-svc-gkg8q [755.229999ms]
Sep 13 11:34:15.304: INFO: Created: latency-svc-7vcb7
Sep 13 11:34:15.337: INFO: Got endpoints: latency-svc-q5w4g [750.636605ms]
Sep 13 11:34:15.348: INFO: Created: latency-svc-jzvrx
Sep 13 11:34:15.393: INFO: Got endpoints: latency-svc-2q4xg [758.100039ms]
Sep 13 11:34:15.404: INFO: Created: latency-svc-2zt9h
Sep 13 11:34:15.439: INFO: Got endpoints: latency-svc-lkhph [745.796169ms]
Sep 13 11:34:15.457: INFO: Created: latency-svc-q2zqr
Sep 13 11:34:15.495: INFO: Got endpoints: latency-svc-vz658 [755.73112ms]
Sep 13 11:34:15.511: INFO: Created: latency-svc-zkqrn
Sep 13 11:34:15.548: INFO: Got endpoints: latency-svc-lg4dg [763.074173ms]
Sep 13 11:34:15.561: INFO: Created: latency-svc-kfjpj
Sep 13 11:34:15.587: INFO: Got endpoints: latency-svc-p8752 [754.190015ms]
Sep 13 11:34:15.599: INFO: Created: latency-svc-schbc
Sep 13 11:34:15.642: INFO: Got endpoints: latency-svc-skbfd [755.860048ms]
Sep 13 11:34:15.657: INFO: Created: latency-svc-tzcc4
Sep 13 11:34:15.691: INFO: Got endpoints: latency-svc-jqzh4 [754.095878ms]
Sep 13 11:34:15.745: INFO: Created: latency-svc-kjrnr
Sep 13 11:34:15.746: INFO: Got endpoints: latency-svc-l6ckq [753.333966ms]
Sep 13 11:34:15.764: INFO: Created: latency-svc-zxpgk
Sep 13 11:34:15.787: INFO: Got endpoints: latency-svc-2jf4n [748.678878ms]
Sep 13 11:34:15.796: INFO: Created: latency-svc-7w9qt
Sep 13 11:34:15.836: INFO: Got endpoints: latency-svc-7l6zn [743.323314ms]
Sep 13 11:34:15.845: INFO: Created: latency-svc-bt2bv
Sep 13 11:34:15.894: INFO: Got endpoints: latency-svc-4kss8 [755.951046ms]
Sep 13 11:34:15.914: INFO: Created: latency-svc-fzmlr
Sep 13 11:34:15.938: INFO: Got endpoints: latency-svc-7pgqd [746.244527ms]
Sep 13 11:34:15.953: INFO: Created: latency-svc-7h8j4
Sep 13 11:34:15.998: INFO: Got endpoints: latency-svc-lwpbp [755.790989ms]
Sep 13 11:34:16.009: INFO: Created: latency-svc-qzvk8
Sep 13 11:34:16.035: INFO: Got endpoints: latency-svc-7vcb7 [743.508168ms]
Sep 13 11:34:16.044: INFO: Created: latency-svc-qp7bv
Sep 13 11:34:16.090: INFO: Got endpoints: latency-svc-jzvrx [753.385393ms]
Sep 13 11:34:16.106: INFO: Created: latency-svc-k7vw9
Sep 13 11:34:16.139: INFO: Got endpoints: latency-svc-2zt9h [745.653287ms]
Sep 13 11:34:16.156: INFO: Created: latency-svc-vvzl8
Sep 13 11:34:16.189: INFO: Got endpoints: latency-svc-q2zqr [750.410574ms]
Sep 13 11:34:16.204: INFO: Created: latency-svc-cpl54
Sep 13 11:34:16.240: INFO: Got endpoints: latency-svc-zkqrn [745.075934ms]
Sep 13 11:34:16.253: INFO: Created: latency-svc-xt5fx
Sep 13 11:34:16.294: INFO: Got endpoints: latency-svc-kfjpj [746.319358ms]
Sep 13 11:34:16.309: INFO: Created: latency-svc-cqvbv
Sep 13 11:34:16.340: INFO: Got endpoints: latency-svc-schbc [752.788175ms]
Sep 13 11:34:16.392: INFO: Got endpoints: latency-svc-tzcc4 [750.322887ms]
Sep 13 11:34:16.443: INFO: Got endpoints: latency-svc-kjrnr [752.812599ms]
Sep 13 11:34:16.491: INFO: Got endpoints: latency-svc-zxpgk [745.259283ms]
Sep 13 11:34:16.536: INFO: Got endpoints: latency-svc-7w9qt [749.631572ms]
Sep 13 11:34:16.593: INFO: Got endpoints: latency-svc-bt2bv [757.164086ms]
Sep 13 11:34:16.642: INFO: Got endpoints: latency-svc-fzmlr [747.95873ms]
Sep 13 11:34:16.698: INFO: Got endpoints: latency-svc-7h8j4 [759.386125ms]
Sep 13 11:34:16.736: INFO: Got endpoints: latency-svc-qzvk8 [738.242092ms]
Sep 13 11:34:16.791: INFO: Got endpoints: latency-svc-qp7bv [755.254913ms]
Sep 13 11:34:16.837: INFO: Got endpoints: latency-svc-k7vw9 [746.270621ms]
Sep 13 11:34:16.884: INFO: Got endpoints: latency-svc-vvzl8 [745.547007ms]
Sep 13 11:34:16.937: INFO: Got endpoints: latency-svc-cpl54 [747.438553ms]
Sep 13 11:34:16.988: INFO: Got endpoints: latency-svc-xt5fx [747.98807ms]
Sep 13 11:34:17.036: INFO: Got endpoints: latency-svc-cqvbv [741.56754ms]
Sep 13 11:34:17.036: INFO: Latencies: [19.082561ms 31.371633ms 35.375372ms 41.380756ms 49.20768ms 52.914391ms 73.261306ms 90.570607ms 93.375503ms 100.023431ms 105.277021ms 114.754501ms 114.889438ms 118.357841ms 129.158185ms 134.876666ms 137.518259ms 139.329783ms 143.255835ms 151.882ms 156.985332ms 159.611206ms 159.918594ms 161.654273ms 170.220531ms 172.684376ms 178.177213ms 181.420851ms 189.578522ms 194.823444ms 196.652515ms 200.510389ms 201.563055ms 201.875656ms 216.416466ms 216.527004ms 225.50619ms 234.114873ms 251.685316ms 254.69996ms 259.426231ms 310.720067ms 317.667945ms 376.65304ms 410.525676ms 495.694503ms 537.281451ms 585.74795ms 650.713564ms 683.408851ms 700.559446ms 724.830717ms 725.754058ms 731.988655ms 738.242092ms 739.433846ms 740.555325ms 740.778754ms 740.868656ms 740.973621ms 741.215756ms 741.24575ms 741.316166ms 741.56754ms 741.754453ms 742.115498ms 742.416763ms 742.640543ms 743.267797ms 743.323314ms 743.397604ms 743.508168ms 744.301913ms 744.306778ms 744.477795ms 745.075705ms 745.075934ms 745.259283ms 745.459068ms 745.547007ms 745.653287ms 745.796169ms 745.822341ms 745.922088ms 745.959204ms 745.970165ms 746.168405ms 746.244527ms 746.260392ms 746.270621ms 746.319358ms 746.389253ms 746.508737ms 746.622363ms 747.182899ms 747.287479ms 747.336405ms 747.438553ms 747.95873ms 747.981025ms 747.98807ms 748.029438ms 748.060183ms 748.151294ms 748.487898ms 748.507622ms 748.511989ms 748.623502ms 748.635466ms 748.678878ms 748.680103ms 748.811476ms 749.04712ms 749.070482ms 749.274713ms 749.383132ms 749.447589ms 749.50669ms 749.529572ms 749.610405ms 749.631405ms 749.631572ms 749.699134ms 750.201547ms 750.206162ms 750.243196ms 750.322887ms 750.354679ms 750.402411ms 750.410574ms 750.418536ms 750.534133ms 750.571452ms 750.636605ms 750.647812ms 750.65323ms 750.703383ms 750.71789ms 750.782929ms 750.810029ms 750.894568ms 751.17585ms 751.35511ms 751.361706ms 751.617077ms 751.889028ms 752.135617ms 752.241735ms 752.388674ms 752.507828ms 752.788175ms 752.812599ms 752.950399ms 752.96598ms 752.985614ms 752.990435ms 753.1914ms 753.201583ms 753.22292ms 753.232422ms 753.316101ms 753.333966ms 753.385393ms 753.423554ms 753.827172ms 754.011406ms 754.095878ms 754.190015ms 754.503788ms 754.754482ms 754.868714ms 755.18079ms 755.229999ms 755.254913ms 755.258267ms 755.54171ms 755.730232ms 755.73112ms 755.790989ms 755.85472ms 755.860048ms 755.923529ms 755.951046ms 756.227694ms 757.010718ms 757.164086ms 757.382881ms 757.649026ms 758.067155ms 758.100039ms 758.410744ms 758.506194ms 759.249835ms 759.386125ms 763.074173ms 764.177557ms 771.95745ms 775.21895ms 783.935511ms 841.611297ms]
Sep 13 11:34:17.036: INFO: 50 %ile: 747.98807ms
Sep 13 11:34:17.036: INFO: 90 %ile: 755.860048ms
Sep 13 11:34:17.036: INFO: 99 %ile: 783.935511ms
Sep 13 11:34:17.036: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:17.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8457" for this suite.

• [SLOW TEST:10.797 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":110,"skipped":1939,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:17.055: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2224
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2224
STEP: creating replication controller externalsvc in namespace services-2224
I0913 11:34:17.118577      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2224, replica count: 2
I0913 11:34:20.169995      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 13 11:34:20.225: INFO: Creating new exec pod
Sep 13 11:34:22.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-2224 exec execpodmn4x4 -- /bin/sh -x -c nslookup nodeport-service.services-2224.svc.cluster.local'
Sep 13 11:34:22.607: INFO: stderr: "+ nslookup nodeport-service.services-2224.svc.cluster.local\n"
Sep 13 11:34:22.607: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-2224.svc.cluster.local\tcanonical name = externalsvc.services-2224.svc.cluster.local.\nName:\texternalsvc.services-2224.svc.cluster.local\nAddress: 10.43.252.213\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2224, will wait for the garbage collector to delete the pods
Sep 13 11:34:22.675: INFO: Deleting ReplicationController externalsvc took: 10.13678ms
Sep 13 11:34:22.775: INFO: Terminating ReplicationController externalsvc pods took: 100.203684ms
Sep 13 11:34:24.998: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:25.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2224" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.994 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":111,"skipped":1956,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:25.050: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
Sep 13 11:34:25.084: INFO: Major version: 1
STEP: Confirm minor version
Sep 13 11:34:25.084: INFO: cleanMinorVersion: 23
Sep 13 11:34:25.084: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:25.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1886" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":112,"skipped":1974,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:25.096: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:34:27.222: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.229: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.234: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.239: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.244: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.250: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.254: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.262: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965: the server could not find the requested resource (get pods dns-test-6cfc24cc-d730-430c-b843-4c0587c97965)
Sep 13 11:34:27.262: INFO: Lookups using dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 13 11:34:32.339: INFO: DNS probes using dns-3163/dns-test-6cfc24cc-d730-430c-b843-4c0587c97965 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:32.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3163" for this suite.

• [SLOW TEST:7.310 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":113,"skipped":1981,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:32.406: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:34:32.431: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 13 11:34:32.445: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 13 11:34:37.450: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 13 11:34:37.450: INFO: Creating deployment "test-rolling-update-deployment"
Sep 13 11:34:37.458: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 13 11:34:37.470: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 13 11:34:39.501: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 13 11:34:39.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 34, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 34, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 11, 34, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 11, 34, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 11:34:41.524: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 11:34:41.547: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5934  69d593ec-5447-4f66-9b96-96ef65bf73ef 408751 1 2022-09-13 11:34:38 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-09-13 11:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:34:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dff5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-13 11:34:38 +0000 UTC,LastTransitionTime:2022-09-13 11:34:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-09-13 11:34:40 +0000 UTC,LastTransitionTime:2022-09-13 11:34:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 11:34:41.555: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-5934  2ce4ecfb-cf07-4876-8c5e-2b492ceee014 408741 1 2022-09-13 11:34:38 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 69d593ec-5447-4f66-9b96-96ef65bf73ef 0xc003f24257 0xc003f24258}] []  [{k3s Update apps/v1 2022-09-13 11:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d593ec-5447-4f66-9b96-96ef65bf73ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:34:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f24388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:34:41.555: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 13 11:34:41.556: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5934  ba8a55cb-2300-4895-91e7-dc81ed335cf8 408750 2 2022-09-13 11:34:33 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 69d593ec-5447-4f66-9b96-96ef65bf73ef 0xc003f243e7 0xc003f243e8}] []  [{e2e.test Update apps/v1 2022-09-13 11:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 11:34:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d593ec-5447-4f66-9b96-96ef65bf73ef\"}":{}}},"f:spec":{"f:replicas":{}}} } {k3s Update apps/v1 2022-09-13 11:34:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f244c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 13 11:34:41.573: INFO: Pod "test-rolling-update-deployment-796dbc4547-g988w" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-g988w test-rolling-update-deployment-796dbc4547- deployment-5934  1afd5f16-106f-424c-be35-595182145183 408739 0 2022-09-13 11:34:38 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 2ce4ecfb-cf07-4876-8c5e-2b492ceee014 0xc003f249c7 0xc003f249c8}] []  [{k3s Update v1 2022-09-13 11:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ce4ecfb-cf07-4876-8c5e-2b492ceee014\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 11:34:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkr2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkr2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:34:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:34:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 11:34:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.200,StartTime:2022-09-13 11:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 11:34:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://b7364342428aebd057ede1aa96ad11b098be5fd55252bceb439cf703c9e9314e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:41.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5934" for this suite.

• [SLOW TEST:9.182 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":114,"skipped":1982,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:41.589: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:34:41.622: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 13 11:34:41.636: INFO: The status of Pod pod-logs-websocket-24c5bc9e-e8ad-4c16-af5a-2e9650d7d755 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:34:43.655: INFO: The status of Pod pod-logs-websocket-24c5bc9e-e8ad-4c16-af5a-2e9650d7d755 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:43.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8975" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":1998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:43.702: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
Sep 13 11:34:43.761: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 13 11:34:43.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 create -f -'
Sep 13 11:34:44.426: INFO: stderr: ""
Sep 13 11:34:44.426: INFO: stdout: "service/agnhost-replica created\n"
Sep 13 11:34:44.426: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 13 11:34:44.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 create -f -'
Sep 13 11:34:44.685: INFO: stderr: ""
Sep 13 11:34:44.685: INFO: stdout: "service/agnhost-primary created\n"
Sep 13 11:34:44.685: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 13 11:34:44.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 create -f -'
Sep 13 11:34:45.020: INFO: stderr: ""
Sep 13 11:34:45.020: INFO: stdout: "service/frontend created\n"
Sep 13 11:34:45.020: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 13 11:34:45.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 create -f -'
Sep 13 11:34:45.310: INFO: stderr: ""
Sep 13 11:34:45.310: INFO: stdout: "deployment.apps/frontend created\n"
Sep 13 11:34:45.310: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 13 11:34:45.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 create -f -'
Sep 13 11:34:45.575: INFO: stderr: ""
Sep 13 11:34:45.575: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 13 11:34:45.575: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 13 11:34:45.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 create -f -'
Sep 13 11:34:45.837: INFO: stderr: ""
Sep 13 11:34:45.837: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep 13 11:34:45.837: INFO: Waiting for all frontend pods to be Running.
Sep 13 11:34:50.888: INFO: Waiting for frontend to serve content.
Sep 13 11:34:50.914: INFO: Trying to add a new entry to the guestbook.
Sep 13 11:34:50.944: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 13 11:34:50.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 delete --grace-period=0 --force -f -'
Sep 13 11:34:51.109: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:34:51.109: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 11:34:51.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 delete --grace-period=0 --force -f -'
Sep 13 11:34:51.233: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:34:51.233: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 11:34:51.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 delete --grace-period=0 --force -f -'
Sep 13 11:34:51.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:34:51.379: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 11:34:51.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 delete --grace-period=0 --force -f -'
Sep 13 11:34:51.503: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:34:51.503: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 11:34:51.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 delete --grace-period=0 --force -f -'
Sep 13 11:34:51.621: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:34:51.621: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 11:34:51.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7421 delete --grace-period=0 --force -f -'
Sep 13 11:34:51.687: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:34:51.687: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:51.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7421" for this suite.

• [SLOW TEST:8.009 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":116,"skipped":2057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:51.711: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-7031/configmap-test-ebf7a608-4ad9-440a-9268-cca890c21bc3
STEP: Creating a pod to test consume configMaps
Sep 13 11:34:51.786: INFO: Waiting up to 5m0s for pod "pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069" in namespace "configmap-7031" to be "Succeeded or Failed"
Sep 13 11:34:51.790: INFO: Pod "pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912524ms
Sep 13 11:34:53.815: INFO: Pod "pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028680107s
Sep 13 11:34:55.825: INFO: Pod "pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038864114s
STEP: Saw pod success
Sep 13 11:34:55.825: INFO: Pod "pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069" satisfied condition "Succeeded or Failed"
Sep 13 11:34:55.834: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069 container env-test: <nil>
STEP: delete the pod
Sep 13 11:34:55.875: INFO: Waiting for pod pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069 to disappear
Sep 13 11:34:55.882: INFO: Pod pod-configmaps-04d21c85-0128-467d-854d-3c1edf054069 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:34:55.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7031" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2100,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:34:55.899: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7621.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7621.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7621.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7621.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:34:58.014: INFO: DNS probes using dns-test-72a953e6-f4ec-420b-922b-7f79aa2a0dc2 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7621.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7621.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7621.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7621.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:35:00.145: INFO: File wheezy_udp@dns-test-service-3.dns-7621.svc.cluster.local from pod  dns-7621/dns-test-2615af49-6059-4e7c-b7ff-eb11a974e836 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 13 11:35:00.150: INFO: File jessie_udp@dns-test-service-3.dns-7621.svc.cluster.local from pod  dns-7621/dns-test-2615af49-6059-4e7c-b7ff-eb11a974e836 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 13 11:35:00.150: INFO: Lookups using dns-7621/dns-test-2615af49-6059-4e7c-b7ff-eb11a974e836 failed for: [wheezy_udp@dns-test-service-3.dns-7621.svc.cluster.local jessie_udp@dns-test-service-3.dns-7621.svc.cluster.local]

Sep 13 11:35:05.173: INFO: DNS probes using dns-test-2615af49-6059-4e7c-b7ff-eb11a974e836 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7621.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7621.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7621.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7621.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:35:07.312: INFO: DNS probes using dns-test-a925a7c1-e068-4f67-8345-7b522eb4d26d succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:35:07.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7621" for this suite.

• [SLOW TEST:11.474 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":118,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:35:07.376: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7058, will wait for the garbage collector to delete the pods
Sep 13 11:35:09.533: INFO: Deleting Job.batch foo took: 34.904641ms
Sep 13 11:35:09.633: INFO: Terminating Job.batch foo pods took: 100.285578ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:35:42.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7058" for this suite.

• [SLOW TEST:34.990 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":119,"skipped":2141,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:35:42.366: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Sep 13 11:35:42.440: INFO: The status of Pod labelsupdateb97813e9-f9f5-473a-b858-fb646545bcba is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:35:44.461: INFO: The status of Pod labelsupdateb97813e9-f9f5-473a-b858-fb646545bcba is Running (Ready = true)
Sep 13 11:35:45.016: INFO: Successfully updated pod "labelsupdateb97813e9-f9f5-473a-b858-fb646545bcba"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:35:49.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9798" for this suite.

• [SLOW TEST:6.741 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":120,"skipped":2145,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:35:49.108: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4744
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-4744
Sep 13 11:35:49.162: INFO: Found 0 stateful pods, waiting for 1
Sep 13 11:35:59.190: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 11:35:59.268: INFO: Deleting all statefulset in ns statefulset-4744
Sep 13 11:35:59.275: INFO: Scaling statefulset ss to 0
Sep 13 11:36:09.360: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:36:09.366: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:36:09.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4744" for this suite.

• [SLOW TEST:20.346 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":121,"skipped":2151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:36:09.454: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:36:20.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2555" for this suite.

• [SLOW TEST:11.313 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":122,"skipped":2180,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:36:20.768: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-5b4907ae-ec75-4e0a-95d6-800570726528
STEP: Creating a pod to test consume configMaps
Sep 13 11:36:20.830: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3" in namespace "projected-4578" to be "Succeeded or Failed"
Sep 13 11:36:20.835: INFO: Pod "pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.885196ms
Sep 13 11:36:22.843: INFO: Pod "pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013600093s
Sep 13 11:36:24.852: INFO: Pod "pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021818543s
STEP: Saw pod success
Sep 13 11:36:24.852: INFO: Pod "pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3" satisfied condition "Succeeded or Failed"
Sep 13 11:36:24.858: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:36:24.888: INFO: Waiting for pod pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3 to disappear
Sep 13 11:36:24.893: INFO: Pod pod-projected-configmaps-b77b691d-c38f-4cc1-8e17-d3006c3080e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:36:24.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4578" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2181,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:36:24.906: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:36:24.948: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:36:28.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4091" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":124,"skipped":2182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:36:28.159: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Sep 13 11:36:28.225: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:36:30.238: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Sep 13 11:36:30.263: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:36:32.274: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 13 11:36:32.283: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:32.283: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:32.285: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:32.285: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:32.453: INFO: Exec stderr: ""
Sep 13 11:36:32.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:32.453: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:32.454: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:32.454: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:32.527: INFO: Exec stderr: ""
Sep 13 11:36:32.527: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:32.527: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:32.528: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:32.528: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:32.660: INFO: Exec stderr: ""
Sep 13 11:36:32.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:32.661: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:32.661: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:32.752: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 13 11:36:32.752: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:32.752: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:32.754: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:32.754: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:32.926: INFO: Exec stderr: ""
Sep 13 11:36:32.926: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:32.926: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:32.927: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:32.927: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:33.004: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 13 11:36:33.004: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:33.004: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:33.005: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:33.005: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:33.148: INFO: Exec stderr: ""
Sep 13 11:36:33.148: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:33.149: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:33.149: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:33.289: INFO: Exec stderr: ""
Sep 13 11:36:33.289: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:33.289: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:33.290: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:33.291: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:33.439: INFO: Exec stderr: ""
Sep 13 11:36:33.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7729 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:33.440: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:33.441: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:33.441: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7729/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:36:33.560: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:36:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7729" for this suite.

• [SLOW TEST:5.415 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2221,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:36:33.575: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8544
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-8544
Sep 13 11:36:33.628: INFO: Found 0 stateful pods, waiting for 1
Sep 13 11:36:43.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Sep 13 11:36:43.689: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Sep 13 11:36:43.707: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Sep 13 11:36:43.711: INFO: Observed &StatefulSet event: ADDED
Sep 13 11:36:43.711: INFO: Found Statefulset ss in namespace statefulset-8544 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 13 11:36:43.711: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Sep 13 11:36:43.711: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 13 11:36:43.721: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Sep 13 11:36:43.724: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 11:36:43.724: INFO: Deleting all statefulset in ns statefulset-8544
Sep 13 11:36:43.730: INFO: Scaling statefulset ss to 0
Sep 13 11:36:53.772: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:36:53.777: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:36:53.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8544" for this suite.

• [SLOW TEST:20.273 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":126,"skipped":2236,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:36:53.848: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep 13 11:36:55.916: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4393 PodName:var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:55.916: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:55.917: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:55.918: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-4393/pods/var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Sep 13 11:36:56.075: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4393 PodName:var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:36:56.075: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:36:56.075: INFO: ExecWithOptions: Clientset creation
Sep 13 11:36:56.075: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-4393/pods/var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Sep 13 11:36:56.693: INFO: Successfully updated pod "var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep 13 11:36:56.702: INFO: Deleting pod "var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695" in namespace "var-expansion-4393"
Sep 13 11:36:56.716: INFO: Wait up to 5m0s for pod "var-expansion-fcdba6b7-4f0d-4d3c-a0a5-eba4647dd695" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:37:30.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4393" for this suite.

• [SLOW TEST:36.920 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":127,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:37:30.771: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-7d73d553-0eae-4f92-86aa-59f5d8646411
STEP: Creating a pod to test consume secrets
Sep 13 11:37:30.828: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56" in namespace "projected-6622" to be "Succeeded or Failed"
Sep 13 11:37:30.834: INFO: Pod "pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.184871ms
Sep 13 11:37:32.845: INFO: Pod "pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016645336s
Sep 13 11:37:34.864: INFO: Pod "pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036483225s
STEP: Saw pod success
Sep 13 11:37:34.865: INFO: Pod "pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56" satisfied condition "Succeeded or Failed"
Sep 13 11:37:34.871: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:37:34.940: INFO: Waiting for pod pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56 to disappear
Sep 13 11:37:34.953: INFO: Pod pod-projected-secrets-85119226-5560-4e85-9d1b-eb6e2cb7ee56 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:37:34.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6622" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2276,"failed":0}
S
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:37:34.974: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Sep 13 11:37:35.032: INFO: observed Pod pod-test in namespace pods-9964 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep 13 11:37:35.038: INFO: observed Pod pod-test in namespace pods-9964 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC  }]
Sep 13 11:37:35.051: INFO: observed Pod pod-test in namespace pods-9964 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC  }]
Sep 13 11:37:36.231: INFO: Found Pod pod-test in namespace pods-9964 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:37:35 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Sep 13 11:37:36.248: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Sep 13 11:37:36.284: INFO: observed event type ADDED
Sep 13 11:37:36.284: INFO: observed event type MODIFIED
Sep 13 11:37:36.285: INFO: observed event type MODIFIED
Sep 13 11:37:36.285: INFO: observed event type MODIFIED
Sep 13 11:37:36.285: INFO: observed event type MODIFIED
Sep 13 11:37:36.285: INFO: observed event type MODIFIED
Sep 13 11:37:36.285: INFO: observed event type MODIFIED
Sep 13 11:37:38.260: INFO: observed event type MODIFIED
Sep 13 11:37:39.257: INFO: observed event type MODIFIED
Sep 13 11:37:39.271: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:37:39.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9964" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":129,"skipped":2277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:37:39.307: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-ac175a12-b7b9-4c2a-91e7-ccff99957980
STEP: Creating a pod to test consume configMaps
Sep 13 11:37:39.364: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1" in namespace "configmap-6708" to be "Succeeded or Failed"
Sep 13 11:37:39.370: INFO: Pod "pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.286692ms
Sep 13 11:37:41.384: INFO: Pod "pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1": Phase="Running", Reason="", readiness=false. Elapsed: 2.020185091s
Sep 13 11:37:43.398: INFO: Pod "pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033791493s
STEP: Saw pod success
Sep 13 11:37:43.398: INFO: Pod "pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1" satisfied condition "Succeeded or Failed"
Sep 13 11:37:43.407: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:37:43.449: INFO: Waiting for pod pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1 to disappear
Sep 13 11:37:43.456: INFO: Pod pod-configmaps-e4745d6d-652b-4ca1-8d2f-0ae797a832a1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:37:43.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6708" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2329,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:37:43.473: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:37:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1660" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":131,"skipped":2332,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:37:43.563: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-gmpf
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 11:37:43.614: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gmpf" in namespace "subpath-1440" to be "Succeeded or Failed"
Sep 13 11:37:43.618: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079007ms
Sep 13 11:37:45.639: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 2.024492373s
Sep 13 11:37:47.660: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.04544311s
Sep 13 11:37:49.675: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 6.060197603s
Sep 13 11:37:51.688: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 8.073938661s
Sep 13 11:37:53.704: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 10.089409297s
Sep 13 11:37:55.733: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 12.118881634s
Sep 13 11:37:57.748: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 14.133722571s
Sep 13 11:37:59.768: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 16.153866846s
Sep 13 11:38:01.787: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 18.172989471s
Sep 13 11:38:03.804: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=true. Elapsed: 20.189900446s
Sep 13 11:38:05.818: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Running", Reason="", readiness=false. Elapsed: 22.203721876s
Sep 13 11:38:07.837: INFO: Pod "pod-subpath-test-downwardapi-gmpf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.222152983s
STEP: Saw pod success
Sep 13 11:38:07.837: INFO: Pod "pod-subpath-test-downwardapi-gmpf" satisfied condition "Succeeded or Failed"
Sep 13 11:38:07.846: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-subpath-test-downwardapi-gmpf container test-container-subpath-downwardapi-gmpf: <nil>
STEP: delete the pod
Sep 13 11:38:07.892: INFO: Waiting for pod pod-subpath-test-downwardapi-gmpf to disappear
Sep 13 11:38:07.899: INFO: Pod pod-subpath-test-downwardapi-gmpf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gmpf
Sep 13 11:38:07.899: INFO: Deleting pod "pod-subpath-test-downwardapi-gmpf" in namespace "subpath-1440"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:07.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1440" for this suite.

• [SLOW TEST:24.361 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":132,"skipped":2344,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:07.924: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:38:07.974: INFO: Create a RollingUpdate DaemonSet
Sep 13 11:38:07.980: INFO: Check that daemon pods launch on every node of the cluster
Sep 13 11:38:07.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:38:07.992: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:38:09.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:38:09.017: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 11:38:10.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 11:38:10.024: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep 13 11:38:10.024: INFO: Update the DaemonSet to trigger a rollout
Sep 13 11:38:10.043: INFO: Updating DaemonSet daemon-set
Sep 13 11:38:13.073: INFO: Roll back the DaemonSet before rollout is complete
Sep 13 11:38:13.090: INFO: Updating DaemonSet daemon-set
Sep 13 11:38:13.090: INFO: Make sure DaemonSet rollback is complete
Sep 13 11:38:13.105: INFO: Wrong image for pod: daemon-set-xllsw. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Sep 13 11:38:13.105: INFO: Pod daemon-set-xllsw is not available
Sep 13 11:38:16.128: INFO: Pod daemon-set-lgjtz is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3734, will wait for the garbage collector to delete the pods
Sep 13 11:38:16.217: INFO: Deleting DaemonSet.extensions daemon-set took: 13.445144ms
Sep 13 11:38:16.319: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.170415ms
Sep 13 11:38:17.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:38:17.432: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 13 11:38:17.440: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"410021"},"items":null}

Sep 13 11:38:17.444: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"410021"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:17.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3734" for this suite.

• [SLOW TEST:9.542 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":133,"skipped":2350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:17.468: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:38:17.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283" in namespace "projected-6617" to be "Succeeded or Failed"
Sep 13 11:38:17.527: INFO: Pod "downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162882ms
Sep 13 11:38:19.553: INFO: Pod "downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283": Phase="Running", Reason="", readiness=false. Elapsed: 2.032330792s
Sep 13 11:38:21.572: INFO: Pod "downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05092996s
STEP: Saw pod success
Sep 13 11:38:21.572: INFO: Pod "downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283" satisfied condition "Succeeded or Failed"
Sep 13 11:38:21.580: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283 container client-container: <nil>
STEP: delete the pod
Sep 13 11:38:21.613: INFO: Waiting for pod downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283 to disappear
Sep 13 11:38:21.618: INFO: Pod downwardapi-volume-95f6d5f7-3213-46a7-b8c3-75c397aa7283 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:21.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6617" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2385,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:21.632: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:38:22.081: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:38:25.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:38:25.128: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2637" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.880 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":135,"skipped":2392,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:28.513: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-r5rfv in namespace proxy-1092
I0913 11:38:28.598403      23 runners.go:193] Created replication controller with name: proxy-service-r5rfv, namespace: proxy-1092, replica count: 1
I0913 11:38:29.650015      23 runners.go:193] proxy-service-r5rfv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 11:38:30.650833      23 runners.go:193] proxy-service-r5rfv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:38:30.674: INFO: setup took 2.10634368s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 13 11:38:30.710: INFO: (0) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 35.893655ms)
Sep 13 11:38:30.710: INFO: (0) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 35.23212ms)
Sep 13 11:38:30.710: INFO: (0) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 35.7629ms)
Sep 13 11:38:30.710: INFO: (0) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 35.278677ms)
Sep 13 11:38:30.710: INFO: (0) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 35.890749ms)
Sep 13 11:38:30.710: INFO: (0) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 35.647537ms)
Sep 13 11:38:30.718: INFO: (0) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 43.398154ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 43.643993ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 44.218178ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 44.387187ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 44.329916ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 45.169117ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 45.192265ms)
Sep 13 11:38:30.719: INFO: (0) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 44.164779ms)
Sep 13 11:38:30.720: INFO: (0) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 45.866139ms)
Sep 13 11:38:30.720: INFO: (0) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 45.262782ms)
Sep 13 11:38:30.728: INFO: (1) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 7.817654ms)
Sep 13 11:38:30.741: INFO: (1) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 20.362041ms)
Sep 13 11:38:30.749: INFO: (1) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 28.433478ms)
Sep 13 11:38:30.750: INFO: (1) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 29.296442ms)
Sep 13 11:38:30.750: INFO: (1) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 29.657621ms)
Sep 13 11:38:30.750: INFO: (1) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 29.484519ms)
Sep 13 11:38:30.752: INFO: (1) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 30.683321ms)
Sep 13 11:38:30.752: INFO: (1) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 31.060004ms)
Sep 13 11:38:30.752: INFO: (1) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 31.671901ms)
Sep 13 11:38:30.752: INFO: (1) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 30.940509ms)
Sep 13 11:38:30.752: INFO: (1) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 31.565676ms)
Sep 13 11:38:30.752: INFO: (1) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 31.070268ms)
Sep 13 11:38:30.753: INFO: (1) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 32.201793ms)
Sep 13 11:38:30.753: INFO: (1) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 32.140271ms)
Sep 13 11:38:30.753: INFO: (1) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 31.960752ms)
Sep 13 11:38:30.753: INFO: (1) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 32.463511ms)
Sep 13 11:38:30.779: INFO: (2) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 25.313289ms)
Sep 13 11:38:30.779: INFO: (2) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 25.764046ms)
Sep 13 11:38:30.779: INFO: (2) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 25.311414ms)
Sep 13 11:38:30.780: INFO: (2) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 26.189018ms)
Sep 13 11:38:30.780: INFO: (2) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 26.199859ms)
Sep 13 11:38:30.780: INFO: (2) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 26.189692ms)
Sep 13 11:38:30.781: INFO: (2) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 26.830637ms)
Sep 13 11:38:30.781: INFO: (2) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 27.320679ms)
Sep 13 11:38:30.781: INFO: (2) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 26.936357ms)
Sep 13 11:38:30.781: INFO: (2) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 26.888966ms)
Sep 13 11:38:30.781: INFO: (2) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 27.081177ms)
Sep 13 11:38:30.783: INFO: (2) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 29.618647ms)
Sep 13 11:38:30.784: INFO: (2) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 30.022499ms)
Sep 13 11:38:30.784: INFO: (2) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 30.372932ms)
Sep 13 11:38:30.785: INFO: (2) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 30.781934ms)
Sep 13 11:38:30.785: INFO: (2) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 31.227967ms)
Sep 13 11:38:30.796: INFO: (3) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 10.405059ms)
Sep 13 11:38:30.819: INFO: (3) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 33.211286ms)
Sep 13 11:38:30.820: INFO: (3) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 32.176798ms)
Sep 13 11:38:30.822: INFO: (3) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 34.691875ms)
Sep 13 11:38:30.827: INFO: (3) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 40.2758ms)
Sep 13 11:38:30.827: INFO: (3) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 40.503726ms)
Sep 13 11:38:30.829: INFO: (3) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 42.679774ms)
Sep 13 11:38:30.829: INFO: (3) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 43.114978ms)
Sep 13 11:38:30.829: INFO: (3) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 42.297336ms)
Sep 13 11:38:30.829: INFO: (3) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 42.77659ms)
Sep 13 11:38:30.831: INFO: (3) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 44.037622ms)
Sep 13 11:38:30.834: INFO: (3) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 48.073423ms)
Sep 13 11:38:30.835: INFO: (3) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 47.441608ms)
Sep 13 11:38:30.836: INFO: (3) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 48.997583ms)
Sep 13 11:38:30.838: INFO: (3) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 51.575306ms)
Sep 13 11:38:30.838: INFO: (3) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 50.685795ms)
Sep 13 11:38:30.853: INFO: (4) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 15.173799ms)
Sep 13 11:38:30.862: INFO: (4) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 23.928644ms)
Sep 13 11:38:30.867: INFO: (4) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 28.777494ms)
Sep 13 11:38:30.869: INFO: (4) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 31.413149ms)
Sep 13 11:38:30.872: INFO: (4) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 33.982158ms)
Sep 13 11:38:30.874: INFO: (4) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 35.14112ms)
Sep 13 11:38:30.874: INFO: (4) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 36.169307ms)
Sep 13 11:38:30.874: INFO: (4) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 36.279986ms)
Sep 13 11:38:30.877: INFO: (4) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 38.628895ms)
Sep 13 11:38:30.877: INFO: (4) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 38.194185ms)
Sep 13 11:38:30.877: INFO: (4) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 38.914259ms)
Sep 13 11:38:30.879: INFO: (4) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 41.251766ms)
Sep 13 11:38:30.880: INFO: (4) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 41.728469ms)
Sep 13 11:38:30.884: INFO: (4) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 46.362797ms)
Sep 13 11:38:30.886: INFO: (4) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 47.704781ms)
Sep 13 11:38:30.887: INFO: (4) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 48.760989ms)
Sep 13 11:38:30.904: INFO: (5) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 16.538887ms)
Sep 13 11:38:30.909: INFO: (5) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 21.587869ms)
Sep 13 11:38:30.910: INFO: (5) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 22.991665ms)
Sep 13 11:38:30.920: INFO: (5) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 32.711138ms)
Sep 13 11:38:30.922: INFO: (5) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 34.68464ms)
Sep 13 11:38:30.922: INFO: (5) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 34.674886ms)
Sep 13 11:38:30.927: INFO: (5) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 39.296672ms)
Sep 13 11:38:30.927: INFO: (5) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 39.15089ms)
Sep 13 11:38:30.927: INFO: (5) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 39.517919ms)
Sep 13 11:38:30.932: INFO: (5) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 44.043768ms)
Sep 13 11:38:30.933: INFO: (5) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 46.052836ms)
Sep 13 11:38:30.933: INFO: (5) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 45.674083ms)
Sep 13 11:38:30.935: INFO: (5) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 47.785646ms)
Sep 13 11:38:30.935: INFO: (5) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 48.081105ms)
Sep 13 11:38:30.935: INFO: (5) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 47.795611ms)
Sep 13 11:38:30.937: INFO: (5) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 49.2256ms)
Sep 13 11:38:30.959: INFO: (6) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 22.084962ms)
Sep 13 11:38:30.969: INFO: (6) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 30.7285ms)
Sep 13 11:38:30.972: INFO: (6) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 34.737137ms)
Sep 13 11:38:30.976: INFO: (6) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 38.074263ms)
Sep 13 11:38:30.980: INFO: (6) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 41.801752ms)
Sep 13 11:38:30.980: INFO: (6) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 42.360296ms)
Sep 13 11:38:30.981: INFO: (6) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 43.327953ms)
Sep 13 11:38:30.984: INFO: (6) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 46.12359ms)
Sep 13 11:38:30.985: INFO: (6) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 47.362191ms)
Sep 13 11:38:30.987: INFO: (6) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 49.13941ms)
Sep 13 11:38:30.987: INFO: (6) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 49.324244ms)
Sep 13 11:38:30.987: INFO: (6) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 49.587049ms)
Sep 13 11:38:30.989: INFO: (6) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 50.87411ms)
Sep 13 11:38:30.990: INFO: (6) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 52.547115ms)
Sep 13 11:38:30.992: INFO: (6) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 54.43009ms)
Sep 13 11:38:30.992: INFO: (6) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 55.000505ms)
Sep 13 11:38:31.010: INFO: (7) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 16.432635ms)
Sep 13 11:38:31.010: INFO: (7) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 16.301813ms)
Sep 13 11:38:31.011: INFO: (7) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 17.51282ms)
Sep 13 11:38:31.017: INFO: (7) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 23.387958ms)
Sep 13 11:38:31.017: INFO: (7) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 23.59567ms)
Sep 13 11:38:31.018: INFO: (7) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 24.868708ms)
Sep 13 11:38:31.024: INFO: (7) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 31.441185ms)
Sep 13 11:38:31.026: INFO: (7) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 33.211227ms)
Sep 13 11:38:31.027: INFO: (7) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 34.280736ms)
Sep 13 11:38:31.029: INFO: (7) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 35.902561ms)
Sep 13 11:38:31.031: INFO: (7) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 37.45024ms)
Sep 13 11:38:31.034: INFO: (7) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 41.253112ms)
Sep 13 11:38:31.035: INFO: (7) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 42.066531ms)
Sep 13 11:38:31.037: INFO: (7) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 43.886477ms)
Sep 13 11:38:31.038: INFO: (7) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 44.390741ms)
Sep 13 11:38:31.038: INFO: (7) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 45.093652ms)
Sep 13 11:38:31.047: INFO: (8) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 8.904344ms)
Sep 13 11:38:31.060: INFO: (8) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 21.295383ms)
Sep 13 11:38:31.060: INFO: (8) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 20.869506ms)
Sep 13 11:38:31.068: INFO: (8) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 27.647668ms)
Sep 13 11:38:31.069: INFO: (8) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 29.733667ms)
Sep 13 11:38:31.069: INFO: (8) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 30.229406ms)
Sep 13 11:38:31.073: INFO: (8) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 32.767586ms)
Sep 13 11:38:31.075: INFO: (8) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 34.630249ms)
Sep 13 11:38:31.075: INFO: (8) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 34.950107ms)
Sep 13 11:38:31.078: INFO: (8) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 38.434287ms)
Sep 13 11:38:31.078: INFO: (8) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 39.966895ms)
Sep 13 11:38:31.080: INFO: (8) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 41.312248ms)
Sep 13 11:38:31.080: INFO: (8) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 40.143701ms)
Sep 13 11:38:31.081: INFO: (8) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 42.281375ms)
Sep 13 11:38:31.086: INFO: (8) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 47.198459ms)
Sep 13 11:38:31.086: INFO: (8) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 46.786005ms)
Sep 13 11:38:31.105: INFO: (9) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 18.173663ms)
Sep 13 11:38:31.107: INFO: (9) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 20.686729ms)
Sep 13 11:38:31.109: INFO: (9) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 22.140317ms)
Sep 13 11:38:31.114: INFO: (9) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 27.761616ms)
Sep 13 11:38:31.114: INFO: (9) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 27.213804ms)
Sep 13 11:38:31.118: INFO: (9) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 30.778188ms)
Sep 13 11:38:31.119: INFO: (9) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 32.381896ms)
Sep 13 11:38:31.120: INFO: (9) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 33.108328ms)
Sep 13 11:38:31.120: INFO: (9) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 33.058372ms)
Sep 13 11:38:31.120: INFO: (9) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 33.058548ms)
Sep 13 11:38:31.126: INFO: (9) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 39.010495ms)
Sep 13 11:38:31.128: INFO: (9) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 40.843287ms)
Sep 13 11:38:31.130: INFO: (9) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 42.965051ms)
Sep 13 11:38:31.130: INFO: (9) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 43.193328ms)
Sep 13 11:38:31.131: INFO: (9) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 43.869056ms)
Sep 13 11:38:31.131: INFO: (9) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 44.197251ms)
Sep 13 11:38:31.138: INFO: (10) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 6.988483ms)
Sep 13 11:38:31.141: INFO: (10) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 9.479852ms)
Sep 13 11:38:31.149: INFO: (10) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 17.11186ms)
Sep 13 11:38:31.154: INFO: (10) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 22.352133ms)
Sep 13 11:38:31.169: INFO: (10) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 37.37525ms)
Sep 13 11:38:31.169: INFO: (10) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 37.329156ms)
Sep 13 11:38:31.173: INFO: (10) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 41.865786ms)
Sep 13 11:38:31.174: INFO: (10) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 41.99336ms)
Sep 13 11:38:31.175: INFO: (10) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 43.324956ms)
Sep 13 11:38:31.180: INFO: (10) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 48.189165ms)
Sep 13 11:38:31.184: INFO: (10) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 52.743638ms)
Sep 13 11:38:31.185: INFO: (10) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 53.562353ms)
Sep 13 11:38:31.185: INFO: (10) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 53.67768ms)
Sep 13 11:38:31.185: INFO: (10) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 53.792977ms)
Sep 13 11:38:31.185: INFO: (10) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 53.413934ms)
Sep 13 11:38:31.186: INFO: (10) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 54.086045ms)
Sep 13 11:38:31.200: INFO: (11) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 13.596436ms)
Sep 13 11:38:31.213: INFO: (11) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 27.092453ms)
Sep 13 11:38:31.213: INFO: (11) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 26.812124ms)
Sep 13 11:38:31.214: INFO: (11) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 27.771249ms)
Sep 13 11:38:31.214: INFO: (11) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 27.523479ms)
Sep 13 11:38:31.214: INFO: (11) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 27.376349ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 28.718233ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 28.317538ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 28.464214ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 28.69971ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 28.83633ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 28.667704ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 28.998154ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 28.972771ms)
Sep 13 11:38:31.215: INFO: (11) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 28.891585ms)
Sep 13 11:38:31.216: INFO: (11) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 29.086048ms)
Sep 13 11:38:31.230: INFO: (12) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 13.602804ms)
Sep 13 11:38:31.239: INFO: (12) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 21.520128ms)
Sep 13 11:38:31.239: INFO: (12) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 22.915039ms)
Sep 13 11:38:31.239: INFO: (12) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 22.657927ms)
Sep 13 11:38:31.239: INFO: (12) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 21.058862ms)
Sep 13 11:38:31.239: INFO: (12) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 22.85021ms)
Sep 13 11:38:31.239: INFO: (12) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 21.23503ms)
Sep 13 11:38:31.240: INFO: (12) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 22.953344ms)
Sep 13 11:38:31.240: INFO: (12) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 24.39829ms)
Sep 13 11:38:31.240: INFO: (12) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 23.12494ms)
Sep 13 11:38:31.240: INFO: (12) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 22.380126ms)
Sep 13 11:38:31.240: INFO: (12) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 23.773459ms)
Sep 13 11:38:31.241: INFO: (12) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 22.893684ms)
Sep 13 11:38:31.241: INFO: (12) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 23.097856ms)
Sep 13 11:38:31.241: INFO: (12) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 24.288533ms)
Sep 13 11:38:31.242: INFO: (12) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 24.525994ms)
Sep 13 11:38:31.259: INFO: (13) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 17.122913ms)
Sep 13 11:38:31.261: INFO: (13) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 17.888006ms)
Sep 13 11:38:31.261: INFO: (13) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 18.262945ms)
Sep 13 11:38:31.261: INFO: (13) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 18.745213ms)
Sep 13 11:38:31.261: INFO: (13) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 18.479898ms)
Sep 13 11:38:31.261: INFO: (13) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 18.623307ms)
Sep 13 11:38:31.262: INFO: (13) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 18.828658ms)
Sep 13 11:38:31.262: INFO: (13) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 19.208891ms)
Sep 13 11:38:31.262: INFO: (13) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 19.123009ms)
Sep 13 11:38:31.263: INFO: (13) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 21.048217ms)
Sep 13 11:38:31.269: INFO: (13) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 26.29775ms)
Sep 13 11:38:31.269: INFO: (13) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 26.018593ms)
Sep 13 11:38:31.269: INFO: (13) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 26.502921ms)
Sep 13 11:38:31.269: INFO: (13) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 26.385785ms)
Sep 13 11:38:31.269: INFO: (13) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 26.530146ms)
Sep 13 11:38:31.270: INFO: (13) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 28.280434ms)
Sep 13 11:38:31.281: INFO: (14) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 10.374841ms)
Sep 13 11:38:31.283: INFO: (14) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 11.827468ms)
Sep 13 11:38:31.283: INFO: (14) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 11.503617ms)
Sep 13 11:38:31.283: INFO: (14) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 11.824839ms)
Sep 13 11:38:31.294: INFO: (14) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 22.847608ms)
Sep 13 11:38:31.300: INFO: (14) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 28.729438ms)
Sep 13 11:38:31.303: INFO: (14) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 32.611124ms)
Sep 13 11:38:31.305: INFO: (14) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 34.441171ms)
Sep 13 11:38:31.307: INFO: (14) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 35.911563ms)
Sep 13 11:38:31.313: INFO: (14) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 41.880398ms)
Sep 13 11:38:31.313: INFO: (14) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 42.282315ms)
Sep 13 11:38:31.313: INFO: (14) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 42.104015ms)
Sep 13 11:38:31.315: INFO: (14) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 43.992892ms)
Sep 13 11:38:31.315: INFO: (14) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 43.903841ms)
Sep 13 11:38:31.315: INFO: (14) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 44.288066ms)
Sep 13 11:38:31.315: INFO: (14) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 44.654598ms)
Sep 13 11:38:31.328: INFO: (15) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 11.706533ms)
Sep 13 11:38:31.328: INFO: (15) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 12.205093ms)
Sep 13 11:38:31.339: INFO: (15) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 22.737476ms)
Sep 13 11:38:31.340: INFO: (15) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 24.000907ms)
Sep 13 11:38:31.341: INFO: (15) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 24.609497ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 25.812783ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 25.385925ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 25.794232ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 26.169629ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 25.457295ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 25.450456ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 25.581027ms)
Sep 13 11:38:31.342: INFO: (15) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 26.501546ms)
Sep 13 11:38:31.343: INFO: (15) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 27.272904ms)
Sep 13 11:38:31.343: INFO: (15) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 26.655133ms)
Sep 13 11:38:31.343: INFO: (15) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 26.992022ms)
Sep 13 11:38:31.353: INFO: (16) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 9.789834ms)
Sep 13 11:38:31.353: INFO: (16) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 9.619227ms)
Sep 13 11:38:31.364: INFO: (16) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 20.557057ms)
Sep 13 11:38:31.364: INFO: (16) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 20.234853ms)
Sep 13 11:38:31.364: INFO: (16) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 19.957504ms)
Sep 13 11:38:31.365: INFO: (16) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 21.464409ms)
Sep 13 11:38:31.365: INFO: (16) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 21.434367ms)
Sep 13 11:38:31.365: INFO: (16) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 21.251449ms)
Sep 13 11:38:31.365: INFO: (16) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 21.567014ms)
Sep 13 11:38:31.365: INFO: (16) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 21.975313ms)
Sep 13 11:38:31.366: INFO: (16) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 21.921454ms)
Sep 13 11:38:31.366: INFO: (16) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 22.203873ms)
Sep 13 11:38:31.366: INFO: (16) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 21.967619ms)
Sep 13 11:38:31.366: INFO: (16) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 22.231925ms)
Sep 13 11:38:31.366: INFO: (16) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 22.534325ms)
Sep 13 11:38:31.366: INFO: (16) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 22.01397ms)
Sep 13 11:38:31.375: INFO: (17) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 8.767908ms)
Sep 13 11:38:31.377: INFO: (17) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 10.984324ms)
Sep 13 11:38:31.378: INFO: (17) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 11.228033ms)
Sep 13 11:38:31.379: INFO: (17) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 12.506713ms)
Sep 13 11:38:31.390: INFO: (17) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 23.076451ms)
Sep 13 11:38:31.390: INFO: (17) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 23.038087ms)
Sep 13 11:38:31.390: INFO: (17) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 23.102659ms)
Sep 13 11:38:31.390: INFO: (17) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 23.502673ms)
Sep 13 11:38:31.390: INFO: (17) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 23.360148ms)
Sep 13 11:38:31.390: INFO: (17) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 23.511813ms)
Sep 13 11:38:31.391: INFO: (17) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 23.438739ms)
Sep 13 11:38:31.391: INFO: (17) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 23.678827ms)
Sep 13 11:38:31.391: INFO: (17) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 23.623613ms)
Sep 13 11:38:31.391: INFO: (17) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 23.785773ms)
Sep 13 11:38:31.391: INFO: (17) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 23.849001ms)
Sep 13 11:38:31.391: INFO: (17) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 24.144488ms)
Sep 13 11:38:31.409: INFO: (18) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 18.411562ms)
Sep 13 11:38:31.415: INFO: (18) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 23.237026ms)
Sep 13 11:38:31.415: INFO: (18) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 23.817796ms)
Sep 13 11:38:31.415: INFO: (18) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 23.736737ms)
Sep 13 11:38:31.415: INFO: (18) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 23.55147ms)
Sep 13 11:38:31.417: INFO: (18) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 25.373036ms)
Sep 13 11:38:31.417: INFO: (18) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 25.60327ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 26.650274ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 27.013116ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 26.849244ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 26.846646ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 27.038802ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 27.005148ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 27.039362ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 26.635437ms)
Sep 13 11:38:31.418: INFO: (18) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 26.985779ms)
Sep 13 11:38:31.434: INFO: (19) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">test<... (200; 15.369503ms)
Sep 13 11:38:31.438: INFO: (19) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 18.609342ms)
Sep 13 11:38:31.438: INFO: (19) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:443/proxy/tlsrewritem... (200; 18.481932ms)
Sep 13 11:38:31.438: INFO: (19) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 19.156828ms)
Sep 13 11:38:31.438: INFO: (19) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:460/proxy/: tls baz (200; 19.255899ms)
Sep 13 11:38:31.439: INFO: (19) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p:160/proxy/: foo (200; 19.431537ms)
Sep 13 11:38:31.438: INFO: (19) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:1080/proxy/rewriteme">... (200; 19.195855ms)
Sep 13 11:38:31.439: INFO: (19) /api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/: <a href="/api/v1/namespaces/proxy-1092/pods/proxy-service-r5rfv-6ct2p/proxy/rewriteme">test</a> (200; 19.762754ms)
Sep 13 11:38:31.442: INFO: (19) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname2/proxy/: tls qux (200; 22.886288ms)
Sep 13 11:38:31.443: INFO: (19) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname2/proxy/: bar (200; 24.275436ms)
Sep 13 11:38:31.442: INFO: (19) /api/v1/namespaces/proxy-1092/services/https:proxy-service-r5rfv:tlsportname1/proxy/: tls baz (200; 23.082873ms)
Sep 13 11:38:31.443: INFO: (19) /api/v1/namespaces/proxy-1092/pods/http:proxy-service-r5rfv-6ct2p:162/proxy/: bar (200; 22.967564ms)
Sep 13 11:38:31.443: INFO: (19) /api/v1/namespaces/proxy-1092/pods/https:proxy-service-r5rfv-6ct2p:462/proxy/: tls qux (200; 23.380725ms)
Sep 13 11:38:31.444: INFO: (19) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname1/proxy/: foo (200; 23.918929ms)
Sep 13 11:38:31.444: INFO: (19) /api/v1/namespaces/proxy-1092/services/http:proxy-service-r5rfv:portname2/proxy/: bar (200; 24.84144ms)
Sep 13 11:38:31.444: INFO: (19) /api/v1/namespaces/proxy-1092/services/proxy-service-r5rfv:portname1/proxy/: foo (200; 24.419056ms)
STEP: deleting ReplicationController proxy-service-r5rfv in namespace proxy-1092, will wait for the garbage collector to delete the pods
Sep 13 11:38:31.517: INFO: Deleting ReplicationController proxy-service-r5rfv took: 17.741038ms
Sep 13 11:38:31.619: INFO: Terminating ReplicationController proxy-service-r5rfv pods took: 101.500047ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:33.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1092" for this suite.

• [SLOW TEST:5.034 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":136,"skipped":2397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:33.550: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-a826870e-fb41-4e7e-97cc-dd41c45bd7db
STEP: Creating a pod to test consume configMaps
Sep 13 11:38:33.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318" in namespace "configmap-9069" to be "Succeeded or Failed"
Sep 13 11:38:33.623: INFO: Pod "pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318": Phase="Pending", Reason="", readiness=false. Elapsed: 9.517088ms
Sep 13 11:38:35.638: INFO: Pod "pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024175605s
Sep 13 11:38:37.654: INFO: Pod "pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040387602s
STEP: Saw pod success
Sep 13 11:38:37.654: INFO: Pod "pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318" satisfied condition "Succeeded or Failed"
Sep 13 11:38:37.659: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:38:37.685: INFO: Waiting for pod pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318 to disappear
Sep 13 11:38:37.692: INFO: Pod pod-configmaps-77a3e970-c756-4914-bf53-e6798dd0b318 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9069" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2428,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:37.711: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Sep 13 11:38:37.756: INFO: The status of Pod annotationupdate6e91e24a-6d9d-4384-bbd6-6cb00e7a6154 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:38:39.777: INFO: The status of Pod annotationupdate6e91e24a-6d9d-4384-bbd6-6cb00e7a6154 is Running (Ready = true)
Sep 13 11:38:40.337: INFO: Successfully updated pod "annotationupdate6e91e24a-6d9d-4384-bbd6-6cb00e7a6154"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:44.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2526" for this suite.

• [SLOW TEST:6.721 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2435,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:44.433: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 13 11:38:44.493: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 13 11:38:44.504: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 13 11:38:44.504: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 13 11:38:44.512: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 13 11:38:44.512: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 13 11:38:44.529: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 13 11:38:44.529: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 13 11:38:51.603: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:51.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-74" for this suite.

• [SLOW TEST:7.212 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":139,"skipped":2456,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:51.647: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Sep 13 11:38:51.685: INFO: Waiting up to 5m0s for pod "downward-api-550e5321-9046-4536-b119-48d7f017ca47" in namespace "downward-api-1159" to be "Succeeded or Failed"
Sep 13 11:38:51.691: INFO: Pod "downward-api-550e5321-9046-4536-b119-48d7f017ca47": Phase="Pending", Reason="", readiness=false. Elapsed: 5.486343ms
Sep 13 11:38:53.705: INFO: Pod "downward-api-550e5321-9046-4536-b119-48d7f017ca47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019464215s
Sep 13 11:38:55.720: INFO: Pod "downward-api-550e5321-9046-4536-b119-48d7f017ca47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034400842s
STEP: Saw pod success
Sep 13 11:38:55.720: INFO: Pod "downward-api-550e5321-9046-4536-b119-48d7f017ca47" satisfied condition "Succeeded or Failed"
Sep 13 11:38:55.727: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downward-api-550e5321-9046-4536-b119-48d7f017ca47 container dapi-container: <nil>
STEP: delete the pod
Sep 13 11:38:55.774: INFO: Waiting for pod downward-api-550e5321-9046-4536-b119-48d7f017ca47 to disappear
Sep 13 11:38:55.780: INFO: Pod downward-api-550e5321-9046-4536-b119-48d7f017ca47 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:55.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1159" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2459,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-6413233d-857f-478a-98f4-9fb7859256cd
STEP: Creating a pod to test consume configMaps
Sep 13 11:38:55.855: INFO: Waiting up to 5m0s for pod "pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23" in namespace "configmap-2672" to be "Succeeded or Failed"
Sep 13 11:38:55.859: INFO: Pod "pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844682ms
Sep 13 11:38:57.885: INFO: Pod "pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029552197s
Sep 13 11:38:59.911: INFO: Pod "pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055739773s
STEP: Saw pod success
Sep 13 11:38:59.911: INFO: Pod "pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23" satisfied condition "Succeeded or Failed"
Sep 13 11:38:59.916: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:38:59.942: INFO: Waiting for pod pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23 to disappear
Sep 13 11:38:59.948: INFO: Pod pod-configmaps-03699764-c51f-4279-87a7-3e7fc7fc1c23 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:38:59.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2672" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2470,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:38:59.962: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 13 11:39:00.013: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9743  415431c5-1f42-4a80-8d86-d42ea2a041f8 410393 0 2022-09-13 11:39:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-13 11:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:39:00.013: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9743  415431c5-1f42-4a80-8d86-d42ea2a041f8 410396 0 2022-09-13 11:39:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-13 11:39:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 13 11:39:00.030: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9743  415431c5-1f42-4a80-8d86-d42ea2a041f8 410397 0 2022-09-13 11:39:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-13 11:39:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:39:00.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9743  415431c5-1f42-4a80-8d86-d42ea2a041f8 410398 0 2022-09-13 11:39:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-13 11:39:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:00.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9743" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":142,"skipped":2482,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:00.042: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2361" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":143,"skipped":2501,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:00.140: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:39:00.179: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 13 11:39:02.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 create -f -'
Sep 13 11:39:03.407: INFO: stderr: ""
Sep 13 11:39:03.407: INFO: stdout: "e2e-test-crd-publish-openapi-2655-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 13 11:39:03.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 delete e2e-test-crd-publish-openapi-2655-crds test-cr'
Sep 13 11:39:03.558: INFO: stderr: ""
Sep 13 11:39:03.558: INFO: stdout: "e2e-test-crd-publish-openapi-2655-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 13 11:39:03.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 apply -f -'
Sep 13 11:39:03.873: INFO: stderr: ""
Sep 13 11:39:03.873: INFO: stdout: "e2e-test-crd-publish-openapi-2655-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 13 11:39:03.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-8176 --namespace=crd-publish-openapi-8176 delete e2e-test-crd-publish-openapi-2655-crds test-cr'
Sep 13 11:39:03.970: INFO: stderr: ""
Sep 13 11:39:03.970: INFO: stdout: "e2e-test-crd-publish-openapi-2655-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 13 11:39:03.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-8176 explain e2e-test-crd-publish-openapi-2655-crds'
Sep 13 11:39:04.258: INFO: stderr: ""
Sep 13 11:39:04.258: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2655-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:06.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8176" for this suite.

• [SLOW TEST:6.438 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":144,"skipped":2505,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:06.581: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8059" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2523,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:10.701: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-5b57de6c-ca66-4a3b-a6eb-ba0631dea2be
STEP: Creating a pod to test consume configMaps
Sep 13 11:39:10.757: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db" in namespace "projected-4377" to be "Succeeded or Failed"
Sep 13 11:39:10.762: INFO: Pod "pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737953ms
Sep 13 11:39:12.768: INFO: Pod "pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01165616s
Sep 13 11:39:14.790: INFO: Pod "pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033508675s
STEP: Saw pod success
Sep 13 11:39:14.790: INFO: Pod "pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db" satisfied condition "Succeeded or Failed"
Sep 13 11:39:14.800: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:39:14.846: INFO: Waiting for pod pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db to disappear
Sep 13 11:39:14.853: INFO: Pod pod-projected-configmaps-19941fc4-ac61-495d-8469-1382378ca0db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:14.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4377" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2539,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:14.882: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Sep 13 11:39:14.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 create -f -'
Sep 13 11:39:15.569: INFO: stderr: ""
Sep 13 11:39:15.569: INFO: stdout: "pod/pause created\n"
Sep 13 11:39:15.569: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 13 11:39:15.569: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7558" to be "running and ready"
Sep 13 11:39:15.583: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.673662ms
Sep 13 11:39:17.600: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.030520364s
Sep 13 11:39:17.600: INFO: Pod "pause" satisfied condition "running and ready"
Sep 13 11:39:17.600: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 13 11:39:17.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 label pods pause testing-label=testing-label-value'
Sep 13 11:39:17.742: INFO: stderr: ""
Sep 13 11:39:17.742: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 13 11:39:17.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 get pod pause -L testing-label'
Sep 13 11:39:17.848: INFO: stderr: ""
Sep 13 11:39:17.848: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 13 11:39:17.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 label pods pause testing-label-'
Sep 13 11:39:17.991: INFO: stderr: ""
Sep 13 11:39:17.991: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 13 11:39:17.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 get pod pause -L testing-label'
Sep 13 11:39:18.096: INFO: stderr: ""
Sep 13 11:39:18.096: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
Sep 13 11:39:18.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 delete --grace-period=0 --force -f -'
Sep 13 11:39:18.243: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:39:18.243: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 13 11:39:18.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 get rc,svc -l name=pause --no-headers'
Sep 13 11:39:18.354: INFO: stderr: "No resources found in kubectl-7558 namespace.\n"
Sep 13 11:39:18.354: INFO: stdout: ""
Sep 13 11:39:18.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7558 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 11:39:18.480: INFO: stderr: ""
Sep 13 11:39:18.480: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:18.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7558" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":147,"skipped":2540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:18.501: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Sep 13 11:39:18.555: INFO: Waiting up to 5m0s for pod "downward-api-af208cc1-73d7-43e1-9482-670358f0cc46" in namespace "downward-api-5551" to be "Succeeded or Failed"
Sep 13 11:39:18.560: INFO: Pod "downward-api-af208cc1-73d7-43e1-9482-670358f0cc46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537775ms
Sep 13 11:39:20.580: INFO: Pod "downward-api-af208cc1-73d7-43e1-9482-670358f0cc46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024775184s
Sep 13 11:39:22.591: INFO: Pod "downward-api-af208cc1-73d7-43e1-9482-670358f0cc46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03528319s
STEP: Saw pod success
Sep 13 11:39:22.591: INFO: Pod "downward-api-af208cc1-73d7-43e1-9482-670358f0cc46" satisfied condition "Succeeded or Failed"
Sep 13 11:39:22.603: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downward-api-af208cc1-73d7-43e1-9482-670358f0cc46 container dapi-container: <nil>
STEP: delete the pod
Sep 13 11:39:22.642: INFO: Waiting for pod downward-api-af208cc1-73d7-43e1-9482-670358f0cc46 to disappear
Sep 13 11:39:22.648: INFO: Pod downward-api-af208cc1-73d7-43e1-9482-670358f0cc46 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:39:22.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5551" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:39:22.662: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:40:22.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-217" for this suite.

• [SLOW TEST:60.071 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":2635,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:40:22.734: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 13 11:40:22.775: INFO: The status of Pod pod-update-54f6e845-af88-4723-abad-b05d6741ada3 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:40:24.789: INFO: The status of Pod pod-update-54f6e845-af88-4723-abad-b05d6741ada3 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 13 11:40:25.334: INFO: Successfully updated pod "pod-update-54f6e845-af88-4723-abad-b05d6741ada3"
STEP: verifying the updated pod is in kubernetes
Sep 13 11:40:25.347: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:40:25.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4679" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:40:25.369: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:40:25.406: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 13 11:40:27.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-4676 --namespace=crd-publish-openapi-4676 create -f -'
Sep 13 11:40:28.754: INFO: stderr: ""
Sep 13 11:40:28.754: INFO: stdout: "e2e-test-crd-publish-openapi-1248-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 13 11:40:28.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-4676 --namespace=crd-publish-openapi-4676 delete e2e-test-crd-publish-openapi-1248-crds test-cr'
Sep 13 11:40:28.891: INFO: stderr: ""
Sep 13 11:40:28.891: INFO: stdout: "e2e-test-crd-publish-openapi-1248-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 13 11:40:28.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-4676 --namespace=crd-publish-openapi-4676 apply -f -'
Sep 13 11:40:29.169: INFO: stderr: ""
Sep 13 11:40:29.170: INFO: stdout: "e2e-test-crd-publish-openapi-1248-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 13 11:40:29.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-4676 --namespace=crd-publish-openapi-4676 delete e2e-test-crd-publish-openapi-1248-crds test-cr'
Sep 13 11:40:29.290: INFO: stderr: ""
Sep 13 11:40:29.290: INFO: stdout: "e2e-test-crd-publish-openapi-1248-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 13 11:40:29.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=crd-publish-openapi-4676 explain e2e-test-crd-publish-openapi-1248-crds'
Sep 13 11:40:29.543: INFO: stderr: ""
Sep 13 11:40:29.543: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1248-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:40:31.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4676" for this suite.

• [SLOW TEST:6.217 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":151,"skipped":2690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:40:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
Sep 13 11:42:32.215: INFO: Successfully updated pod "var-expansion-d463a13b-8a5d-4bb6-85f6-727c3ff89019"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep 13 11:42:34.242: INFO: Deleting pod "var-expansion-d463a13b-8a5d-4bb6-85f6-727c3ff89019" in namespace "var-expansion-4279"
Sep 13 11:42:34.258: INFO: Wait up to 5m0s for pod "var-expansion-d463a13b-8a5d-4bb6-85f6-727c3ff89019" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:43:06.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4279" for this suite.

• [SLOW TEST:154.716 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":152,"skipped":2724,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:43:06.304: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:43:06.376: INFO: The status of Pod pod-secrets-ff328388-d3be-4e42-b364-4f3cf4145b0c is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:43:08.400: INFO: The status of Pod pod-secrets-ff328388-d3be-4e42-b364-4f3cf4145b0c is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:43:08.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7716" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":153,"skipped":2729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:43:08.479: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 13 11:43:08.515: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 13 11:44:08.556: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:44:08.563: INFO: Starting informer...
STEP: Starting pods...
Sep 13 11:44:08.804: INFO: Pod1 is running on k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl. Tainting Node
Sep 13 11:44:11.047: INFO: Pod2 is running on k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 13 11:44:16.617: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 13 11:44:36.680: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:44:36.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9105" for this suite.

• [SLOW TEST:88.250 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":154,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:44:36.730: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-18a48c97-2f0e-41d1-809e-5c4ed89f1623
STEP: Creating the pod
Sep 13 11:44:36.785: INFO: The status of Pod pod-configmaps-9ba62f32-cc3e-40b2-a210-5fb47fa6cf25 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:44:38.807: INFO: The status of Pod pod-configmaps-9ba62f32-cc3e-40b2-a210-5fb47fa6cf25 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-18a48c97-2f0e-41d1-809e-5c4ed89f1623
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:44:40.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8502" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2856,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:44:40.894: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 13 11:44:40.938: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 13 11:44:45.947: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:44:45.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9711" for this suite.

• [SLOW TEST:5.091 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":156,"skipped":2874,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:44:45.985: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-xgn9
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 11:44:46.088: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xgn9" in namespace "subpath-2204" to be "Succeeded or Failed"
Sep 13 11:44:46.093: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99311ms
Sep 13 11:44:48.114: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 2.025775327s
Sep 13 11:44:50.124: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 4.03514794s
Sep 13 11:44:52.143: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 6.054231473s
Sep 13 11:44:54.159: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 8.070188469s
Sep 13 11:44:56.167: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 10.078239805s
Sep 13 11:44:58.186: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 12.097387512s
Sep 13 11:45:00.206: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 14.117430247s
Sep 13 11:45:02.224: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 16.135269963s
Sep 13 11:45:04.241: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 18.15215268s
Sep 13 11:45:06.251: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=true. Elapsed: 20.162666081s
Sep 13 11:45:08.273: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Running", Reason="", readiness=false. Elapsed: 22.184569405s
Sep 13 11:45:10.296: INFO: Pod "pod-subpath-test-configmap-xgn9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.207269916s
STEP: Saw pod success
Sep 13 11:45:10.296: INFO: Pod "pod-subpath-test-configmap-xgn9" satisfied condition "Succeeded or Failed"
Sep 13 11:45:10.302: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-subpath-test-configmap-xgn9 container test-container-subpath-configmap-xgn9: <nil>
STEP: delete the pod
Sep 13 11:45:10.339: INFO: Waiting for pod pod-subpath-test-configmap-xgn9 to disappear
Sep 13 11:45:10.347: INFO: Pod pod-subpath-test-configmap-xgn9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xgn9
Sep 13 11:45:10.347: INFO: Deleting pod "pod-subpath-test-configmap-xgn9" in namespace "subpath-2204"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:10.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2204" for this suite.

• [SLOW TEST:24.383 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":157,"skipped":2877,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:10.369: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Sep 13 11:45:10.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-8366 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep 13 11:45:10.524: INFO: stderr: ""
Sep 13 11:45:10.525: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 13 11:45:15.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-8366 get pod e2e-test-httpd-pod -o json'
Sep 13 11:45:15.702: INFO: stderr: ""
Sep 13 11:45:15.702: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-09-13T11:45:11Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8366\",\n        \"resourceVersion\": \"411248\",\n        \"uid\": \"ed748e73-ddfb-41f2-b07f-56e33ed49264\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6x9lg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6x9lg\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-13T11:45:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-13T11:45:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-13T11:45:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-13T11:45:11Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://bb9e0273804ba1b93a76b61d04c885bd8b827832e45b43fadef480b077ed3f95\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-09-13T11:45:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.1.131\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.1.131\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-09-13T11:45:10Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 13 11:45:15.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-8366 replace -f -'
Sep 13 11:45:16.405: INFO: stderr: ""
Sep 13 11:45:16.405: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
Sep 13 11:45:16.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-8366 delete pods e2e-test-httpd-pod'
Sep 13 11:45:17.885: INFO: stderr: ""
Sep 13 11:45:17.885: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:17.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8366" for this suite.

• [SLOW TEST:7.530 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":158,"skipped":2884,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:17.899: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 13 11:45:17.976: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 13 11:45:17.983: INFO: starting watch
STEP: patching
STEP: updating
Sep 13 11:45:18.013: INFO: waiting for watch events with expected annotations
Sep 13 11:45:18.013: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:18.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4913" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":159,"skipped":2901,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:18.089: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 90.34.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.34.90_udp@PTR;check="$$(dig +tcp +noall +answer +search 90.34.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.34.90_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 90.34.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.34.90_udp@PTR;check="$$(dig +tcp +noall +answer +search 90.34.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.34.90_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 11:45:20.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.184: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.189: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.195: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.228: INFO: Unable to read jessie_udp@dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.243: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.248: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local from pod dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43: the server could not find the requested resource (get pods dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43)
Sep 13 11:45:20.271: INFO: Lookups using dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43 failed for: [wheezy_udp@dns-test-service.dns-7697.svc.cluster.local wheezy_tcp@dns-test-service.dns-7697.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local jessie_udp@dns-test-service.dns-7697.svc.cluster.local jessie_tcp@dns-test-service.dns-7697.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7697.svc.cluster.local]

Sep 13 11:45:25.391: INFO: DNS probes using dns-7697/dns-test-ae8f72cb-8f46-40bf-8d91-494ae0bece43 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:25.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7697" for this suite.

• [SLOW TEST:7.400 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":160,"skipped":2912,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:25.490: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
Sep 13 11:45:25.521: INFO: Waiting up to 5m0s for pod "var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc" in namespace "var-expansion-6325" to be "Succeeded or Failed"
Sep 13 11:45:25.526: INFO: Pod "var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379988ms
Sep 13 11:45:27.532: INFO: Pod "var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011541224s
Sep 13 11:45:29.546: INFO: Pod "var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024919767s
STEP: Saw pod success
Sep 13 11:45:29.546: INFO: Pod "var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc" satisfied condition "Succeeded or Failed"
Sep 13 11:45:29.549: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc container dapi-container: <nil>
STEP: delete the pod
Sep 13 11:45:29.580: INFO: Waiting for pod var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc to disappear
Sep 13 11:45:29.584: INFO: Pod var-expansion-aac0f639-7519-4d57-b7ba-d702b2c7affc no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:29.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6325" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":2913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:29.600: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-77e2a891-322a-493e-99f6-48209c39cc61
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3365" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":162,"skipped":2969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:29.647: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-c977c616-af48-4522-92b0-df61f80a34bb
STEP: Creating a pod to test consume configMaps
Sep 13 11:45:29.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22" in namespace "configmap-2838" to be "Succeeded or Failed"
Sep 13 11:45:29.704: INFO: Pod "pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.152218ms
Sep 13 11:45:31.719: INFO: Pod "pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018901417s
Sep 13 11:45:33.734: INFO: Pod "pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033950468s
STEP: Saw pod success
Sep 13 11:45:33.734: INFO: Pod "pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22" satisfied condition "Succeeded or Failed"
Sep 13 11:45:33.742: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 11:45:33.780: INFO: Waiting for pod pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22 to disappear
Sep 13 11:45:33.787: INFO: Pod pod-configmaps-74cbda70-4109-402e-b7ae-55a565ed1e22 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:33.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2838" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3044,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:33.808: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:45:33.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d" in namespace "downward-api-162" to be "Succeeded or Failed"
Sep 13 11:45:33.867: INFO: Pod "downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.384406ms
Sep 13 11:45:35.878: INFO: Pod "downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016834593s
Sep 13 11:45:37.897: INFO: Pod "downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035948661s
STEP: Saw pod success
Sep 13 11:45:37.897: INFO: Pod "downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d" satisfied condition "Succeeded or Failed"
Sep 13 11:45:37.904: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d container client-container: <nil>
STEP: delete the pod
Sep 13 11:45:37.938: INFO: Waiting for pod downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d to disappear
Sep 13 11:45:37.945: INFO: Pod downwardapi-volume-72a6bda5-b642-4a33-a99d-bd636de0e69d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:37.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-162" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3057,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:37.957: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 13 11:45:37.999: INFO: Waiting up to 5m0s for pod "pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9" in namespace "emptydir-2345" to be "Succeeded or Failed"
Sep 13 11:45:38.003: INFO: Pod "pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033176ms
Sep 13 11:45:40.024: INFO: Pod "pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025189393s
Sep 13 11:45:42.038: INFO: Pod "pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038569238s
STEP: Saw pod success
Sep 13 11:45:42.038: INFO: Pod "pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9" satisfied condition "Succeeded or Failed"
Sep 13 11:45:42.045: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9 container test-container: <nil>
STEP: delete the pod
Sep 13 11:45:42.071: INFO: Waiting for pod pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9 to disappear
Sep 13 11:45:42.075: INFO: Pod pod-2093db3c-eedc-4f7d-a43e-d1c43b5d77b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:42.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2345" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":3058,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:42.084: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 13 11:45:42.115: INFO: Waiting up to 5m0s for pod "pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed" in namespace "emptydir-3660" to be "Succeeded or Failed"
Sep 13 11:45:42.119: INFO: Pod "pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243332ms
Sep 13 11:45:44.132: INFO: Pod "pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed": Phase="Running", Reason="", readiness=false. Elapsed: 2.017333801s
Sep 13 11:45:46.145: INFO: Pod "pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029735434s
STEP: Saw pod success
Sep 13 11:45:46.145: INFO: Pod "pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed" satisfied condition "Succeeded or Failed"
Sep 13 11:45:46.154: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed container test-container: <nil>
STEP: delete the pod
Sep 13 11:45:46.188: INFO: Waiting for pod pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed to disappear
Sep 13 11:45:46.195: INFO: Pod pod-e5fb9167-4629-4bd0-bb2a-62afd13322ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3660" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":3060,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:46.214: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:45:47.005: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:45:50.067: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:50.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-928" for this suite.
STEP: Destroying namespace "webhook-928-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":167,"skipped":3110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:50.418: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
Sep 13 11:45:50.465: INFO: created test-pod-1
Sep 13 11:45:52.493: INFO: running and ready test-pod-1
Sep 13 11:45:52.504: INFO: created test-pod-2
Sep 13 11:45:54.526: INFO: running and ready test-pod-2
Sep 13 11:45:54.539: INFO: created test-pod-3
Sep 13 11:45:56.565: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Sep 13 11:45:56.608: INFO: Pod quantity 3 is different from expected quantity 0
Sep 13 11:45:57.623: INFO: Pod quantity 1 is different from expected quantity 0
Sep 13 11:45:58.622: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:45:59.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1988" for this suite.

• [SLOW TEST:9.219 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":168,"skipped":3139,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:45:59.637: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Sep 13 11:46:10.077: INFO: 73 pods remaining
Sep 13 11:46:10.077: INFO: 73 pods has nil DeletionTimestamp
Sep 13 11:46:10.077: INFO: 
STEP: Gathering metrics
Sep 13 11:46:15.089: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 13 11:46:15.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-6l4dv" in namespace "gc-1290"
W0913 11:46:15.089619      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 13 11:46:15.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-wq7wl" in namespace "gc-1290"
Sep 13 11:46:15.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-dswmh" in namespace "gc-1290"
Sep 13 11:46:15.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch9cc" in namespace "gc-1290"
Sep 13 11:46:15.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4t2q" in namespace "gc-1290"
Sep 13 11:46:15.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-m75tg" in namespace "gc-1290"
Sep 13 11:46:15.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-sqr5d" in namespace "gc-1290"
Sep 13 11:46:15.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk85q" in namespace "gc-1290"
Sep 13 11:46:15.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-r6g5f" in namespace "gc-1290"
Sep 13 11:46:15.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xr5m" in namespace "gc-1290"
Sep 13 11:46:15.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-76jsd" in namespace "gc-1290"
Sep 13 11:46:15.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjckn" in namespace "gc-1290"
Sep 13 11:46:15.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-pd4n2" in namespace "gc-1290"
Sep 13 11:46:15.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pxn2" in namespace "gc-1290"
Sep 13 11:46:15.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfjkr" in namespace "gc-1290"
Sep 13 11:46:15.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-wgq7n" in namespace "gc-1290"
Sep 13 11:46:15.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqdv5" in namespace "gc-1290"
Sep 13 11:46:15.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-mmt8l" in namespace "gc-1290"
Sep 13 11:46:15.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-lp5nw" in namespace "gc-1290"
Sep 13 11:46:15.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-qmp4j" in namespace "gc-1290"
Sep 13 11:46:15.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-lxwj9" in namespace "gc-1290"
Sep 13 11:46:15.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-kp49w" in namespace "gc-1290"
Sep 13 11:46:15.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-62hg4" in namespace "gc-1290"
Sep 13 11:46:15.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-q2v6k" in namespace "gc-1290"
Sep 13 11:46:15.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-kwqlx" in namespace "gc-1290"
Sep 13 11:46:15.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-sgk27" in namespace "gc-1290"
Sep 13 11:46:15.370: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tgnp" in namespace "gc-1290"
Sep 13 11:46:15.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwmsh" in namespace "gc-1290"
Sep 13 11:46:15.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-vfhfw" in namespace "gc-1290"
Sep 13 11:46:15.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-drb6t" in namespace "gc-1290"
Sep 13 11:46:15.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtttm" in namespace "gc-1290"
Sep 13 11:46:15.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2dhv" in namespace "gc-1290"
Sep 13 11:46:15.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-v72l2" in namespace "gc-1290"
Sep 13 11:46:15.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2nf9" in namespace "gc-1290"
Sep 13 11:46:15.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-wc8t7" in namespace "gc-1290"
Sep 13 11:46:15.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-rkq7z" in namespace "gc-1290"
Sep 13 11:46:15.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-m4phz" in namespace "gc-1290"
Sep 13 11:46:15.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-nfbdh" in namespace "gc-1290"
Sep 13 11:46:15.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9xnt" in namespace "gc-1290"
Sep 13 11:46:15.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-pkq69" in namespace "gc-1290"
Sep 13 11:46:15.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-mlpbg" in namespace "gc-1290"
Sep 13 11:46:15.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-d22hg" in namespace "gc-1290"
Sep 13 11:46:15.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-txslw" in namespace "gc-1290"
Sep 13 11:46:15.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-btdzv" in namespace "gc-1290"
Sep 13 11:46:15.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7556" in namespace "gc-1290"
Sep 13 11:46:15.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl62x" in namespace "gc-1290"
Sep 13 11:46:15.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-n7smq" in namespace "gc-1290"
Sep 13 11:46:15.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxstz" in namespace "gc-1290"
Sep 13 11:46:15.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tmfm" in namespace "gc-1290"
Sep 13 11:46:15.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-lvlz2" in namespace "gc-1290"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:15.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1290" for this suite.

• [SLOW TEST:16.096 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":169,"skipped":3142,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:15.733: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 13 11:46:15.759: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 11:46:15.766: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 11:46:15.772: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom before test
Sep 13 11:46:15.778: INFO: civo-csi-controller-0 from kube-system started at 2022-09-09 17:03:22 +0000 UTC (4 container statuses recorded)
Sep 13 11:46:15.778: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:46:15.778: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 13 11:46:15.778: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 13 11:46:15.778: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 13 11:46:15.778: INFO: civo-csi-node-62s5w from kube-system started at 2022-09-09 16:30:36 +0000 UTC (2 container statuses recorded)
Sep 13 11:46:15.778: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:46:15.778: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 11:46:15.778: INFO: civo-ccm-869574f9b7-sjssw from kube-system started at 2022-09-09 16:30:24 +0000 UTC (1 container statuses recorded)
Sep 13 11:46:15.778: INFO: 	Container civo-ccm ready: true, restart count 1
Sep 13 11:46:15.778: INFO: coredns-d76bd69b-h4czc from kube-system started at 2022-09-09 16:31:40 +0000 UTC (1 container statuses recorded)
Sep 13 11:46:15.778: INFO: 	Container coredns ready: true, restart count 0
Sep 13 11:46:15.778: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl before test
Sep 13 11:46:15.785: INFO: sonobuoy-e2e-job-7e148db81c5d4b8a from sonobuoy started at 2022-09-13 11:11:39 +0000 UTC (2 container statuses recorded)
Sep 13 11:46:15.785: INFO: 	Container e2e ready: true, restart count 0
Sep 13 11:46:15.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 11:46:15.785: INFO: civo-csi-node-g5k77 from kube-system started at 2022-09-13 11:44:36 +0000 UTC (2 container statuses recorded)
Sep 13 11:46:15.785: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:46:15.785: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 11:46:15.785: INFO: sonobuoy from sonobuoy started at 2022-09-13 11:11:38 +0000 UTC (1 container statuses recorded)
Sep 13 11:46:15.785: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bb7a27b4-5cff-4c76-9c4e-20684bd047b3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-bb7a27b4-5cff-4c76-9c4e-20684bd047b3 off the node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bb7a27b4-5cff-4c76-9c4e-20684bd047b3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:23.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7506" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.191 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":170,"skipped":3145,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:23.927: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Sep 13 11:46:23.982: INFO: The status of Pod annotationupdate4f1097cf-2841-4e8c-866c-63f7cb05f009 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:46:25.996: INFO: The status of Pod annotationupdate4f1097cf-2841-4e8c-866c-63f7cb05f009 is Running (Ready = true)
Sep 13 11:46:26.553: INFO: Successfully updated pod "annotationupdate4f1097cf-2841-4e8c-866c-63f7cb05f009"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:28.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1288" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3161,"failed":0}

------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:28.611: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:28.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9931" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:28.691: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
Sep 13 11:46:28.728: INFO: Waiting up to 5m0s for pod "var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6" in namespace "var-expansion-8417" to be "Succeeded or Failed"
Sep 13 11:46:28.734: INFO: Pod "var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.320814ms
Sep 13 11:46:30.757: INFO: Pod "var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029203607s
Sep 13 11:46:32.780: INFO: Pod "var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051956035s
STEP: Saw pod success
Sep 13 11:46:32.780: INFO: Pod "var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6" satisfied condition "Succeeded or Failed"
Sep 13 11:46:32.787: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6 container dapi-container: <nil>
STEP: delete the pod
Sep 13 11:46:32.829: INFO: Waiting for pod var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6 to disappear
Sep 13 11:46:32.837: INFO: Pod var-expansion-b2f7c8ff-ef04-431c-a552-4246853926f6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8417" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3225,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:32.849: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:46:32.901: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 11:46:32.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:32.948: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:33.964: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:33.964: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:34.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 11:46:34.977: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 13 11:46:35.035: INFO: Wrong image for pod: daemon-set-pfkpp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Sep 13 11:46:35.035: INFO: Wrong image for pod: daemon-set-9ggnm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Sep 13 11:46:36.054: INFO: Wrong image for pod: daemon-set-9ggnm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Sep 13 11:46:37.056: INFO: Wrong image for pod: daemon-set-9ggnm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Sep 13 11:46:38.050: INFO: Wrong image for pod: daemon-set-9ggnm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Sep 13 11:46:38.050: INFO: Pod daemon-set-gdw8w is not available
Sep 13 11:46:40.051: INFO: Pod daemon-set-cgp4d is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 13 11:46:40.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:46:40.075: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:41.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 11:46:41.094: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2628, will wait for the garbage collector to delete the pods
Sep 13 11:46:41.200: INFO: Deleting DaemonSet.extensions daemon-set took: 14.330254ms
Sep 13 11:46:41.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.914808ms
Sep 13 11:46:43.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:43.815: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 13 11:46:43.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"413129"},"items":null}

Sep 13 11:46:43.831: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"413129"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:43.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2628" for this suite.

• [SLOW TEST:11.011 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":174,"skipped":3233,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:43.860: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:46:43.925: INFO: Waiting up to 5m0s for pod "downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21" in namespace "downward-api-1702" to be "Succeeded or Failed"
Sep 13 11:46:43.937: INFO: Pod "downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21": Phase="Pending", Reason="", readiness=false. Elapsed: 11.604988ms
Sep 13 11:46:45.948: INFO: Pod "downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022734861s
Sep 13 11:46:47.968: INFO: Pod "downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041943371s
STEP: Saw pod success
Sep 13 11:46:47.968: INFO: Pod "downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21" satisfied condition "Succeeded or Failed"
Sep 13 11:46:47.976: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21 container client-container: <nil>
STEP: delete the pod
Sep 13 11:46:48.009: INFO: Waiting for pod downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21 to disappear
Sep 13 11:46:48.015: INFO: Pod downwardapi-volume-553e90ea-7985-4837-b685-f9df260dbb21 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:48.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1702" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:48.029: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:46:48.088: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 13 11:46:48.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:48.104: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Sep 13 11:46:48.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:48.126: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:49.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:49.135: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:50.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:46:50.143: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 13 11:46:50.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:46:50.177: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep 13 11:46:51.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:51.190: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 13 11:46:51.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:51.210: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:52.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:52.229: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:53.223: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:53.223: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 11:46:54.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 11:46:54.232: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4099, will wait for the garbage collector to delete the pods
Sep 13 11:46:54.311: INFO: Deleting DaemonSet.extensions daemon-set took: 12.61281ms
Sep 13 11:46:54.411: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.730737ms
Sep 13 11:46:56.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 11:46:56.924: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 13 11:46:56.941: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"413226"},"items":null}

Sep 13 11:46:56.951: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"413226"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:46:56.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4099" for this suite.

• [SLOW TEST:8.984 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":176,"skipped":3268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:46:57.017: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 13 11:46:57.063: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 13 11:47:06.932: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:47:10.174: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7364" for this suite.

• [SLOW TEST:23.477 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":177,"skipped":3326,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:20.495: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 13 11:47:20.551: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-263  4e0b866b-e036-4fd5-b86c-2824b017e86e 413296 0 2022-09-13 11:47:21 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-09-13 11:47:21 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4nt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4nt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 13 11:47:20.555: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:47:22.575: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 13 11:47:22.575: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-263 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:47:22.575: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:47:22.576: INFO: ExecWithOptions: Clientset creation
Sep 13 11:47:22.576: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-263/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Sep 13 11:47:22.726: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-263 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:47:22.726: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:47:22.727: INFO: ExecWithOptions: Clientset creation
Sep 13 11:47:22.727: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-263/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:47:22.889: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:22.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-263" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":178,"skipped":3336,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:22.936: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0913 11:47:23.690649      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 13 11:47:23.690: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:23.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2671" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":179,"skipped":3346,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-5b2076cf-3e49-43bb-8162-2c041c20c6a5
STEP: Creating a pod to test consume secrets
Sep 13 11:47:23.739: INFO: Waiting up to 5m0s for pod "pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7" in namespace "secrets-820" to be "Succeeded or Failed"
Sep 13 11:47:23.743: INFO: Pod "pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.445775ms
Sep 13 11:47:25.752: INFO: Pod "pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012422894s
Sep 13 11:47:27.768: INFO: Pod "pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028816772s
STEP: Saw pod success
Sep 13 11:47:27.768: INFO: Pod "pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7" satisfied condition "Succeeded or Failed"
Sep 13 11:47:27.776: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:47:27.814: INFO: Waiting for pod pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7 to disappear
Sep 13 11:47:27.821: INFO: Pod pod-secrets-c19e5115-265f-41f9-b39e-7dd5c81408f7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:27.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-820" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3353,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:27.839: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:47:28.216: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:47:31.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:43.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-65" for this suite.
STEP: Destroying namespace "webhook-65-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.782 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":181,"skipped":3369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:43.621: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 13 11:47:47.766: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:47.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4010" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3410,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:47.810: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6881" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":183,"skipped":3411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:47.914: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:47:47.964: INFO: The status of Pod busybox-scheduling-48e536bd-2dd9-4ecf-bf68-05519bac62cc is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:47:49.984: INFO: The status of Pod busybox-scheduling-48e536bd-2dd9-4ecf-bf68-05519bac62cc is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:50.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8979" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3433,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:50.026: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-1f079673-c3ea-47e4-b025-aa1e25141669
STEP: Creating a pod to test consume configMaps
Sep 13 11:47:50.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc" in namespace "configmap-362" to be "Succeeded or Failed"
Sep 13 11:47:50.076: INFO: Pod "pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241791ms
Sep 13 11:47:52.094: INFO: Pod "pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc": Phase="Running", Reason="", readiness=false. Elapsed: 2.022031392s
Sep 13 11:47:54.109: INFO: Pod "pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036780702s
STEP: Saw pod success
Sep 13 11:47:54.109: INFO: Pod "pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc" satisfied condition "Succeeded or Failed"
Sep 13 11:47:54.117: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:47:54.149: INFO: Waiting for pod pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc to disappear
Sep 13 11:47:54.158: INFO: Pod pod-configmaps-f8c826e2-93ed-4d42-a9db-01eefa50eedc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:54.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-362" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3433,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:54.174: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep 13 11:47:55.324: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0913 11:47:55.324728      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 13 11:47:55.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9370" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":186,"skipped":3447,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:55.341: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
Sep 13 11:47:55.391: INFO: Creating e2e-svc-a-9k2cl
Sep 13 11:47:55.405: INFO: Creating e2e-svc-b-qfrcv
Sep 13 11:47:55.415: INFO: Creating e2e-svc-c-2hqzz
STEP: deleting service collection
Sep 13 11:47:55.461: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:47:55.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9035" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":187,"skipped":3457,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:47:55.472: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 13 11:47:55.512: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 13 11:48:55.565: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Sep 13 11:48:55.624: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep 13 11:48:55.636: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep 13 11:48:55.658: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep 13 11:48:55.664: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:49:11.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9747" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.365 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":188,"skipped":3478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:49:11.838: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:49:11.886: INFO: The status of Pod busybox-host-aliases734754f1-7268-42cd-813b-faab93715650 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:49:13.898: INFO: The status of Pod busybox-host-aliases734754f1-7268-42cd-813b-faab93715650 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:49:13.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2463" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:49:13.936: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 13 11:49:13.990: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 13 11:50:14.030: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:50:14.038: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep 13 11:50:16.128: INFO: found a healthy node: k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:50:24.338: INFO: pods created so far: [1 1 1]
Sep 13 11:50:24.338: INFO: length of pods created so far: 3
Sep 13 11:50:26.362: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:50:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9283" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:50:33.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6858" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:79.596 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":190,"skipped":3574,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:50:33.532: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-4826
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 11:50:33.567: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 13 11:50:33.597: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:50:35.620: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:37.611: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:39.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:41.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:43.617: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:45.618: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:47.618: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:49.618: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:51.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:53.626: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 11:50:55.617: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 13 11:50:55.626: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 13 11:50:57.679: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 13 11:50:57.679: INFO: Going to poll 10.42.1.214 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep 13 11:50:57.683: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.214:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4826 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:50:57.683: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:50:57.685: INFO: ExecWithOptions: Clientset creation
Sep 13 11:50:57.685: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.1.214%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:50:57.829: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 13 11:50:57.829: INFO: Going to poll 10.42.0.6 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep 13 11:50:57.833: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.6:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4826 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:50:57.833: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:50:57.834: INFO: ExecWithOptions: Clientset creation
Sep 13 11:50:57.834: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.0.6%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:50:57.954: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:50:57.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4826" for this suite.

• [SLOW TEST:24.437 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3577,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:50:57.970: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 13 11:50:58.022: INFO: The status of Pod pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 11:51:00.042: INFO: The status of Pod pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 13 11:51:00.586: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4"
Sep 13 11:51:00.586: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4" in namespace "pods-4418" to be "terminated due to deadline exceeded"
Sep 13 11:51:00.593: INFO: Pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4": Phase="Running", Reason="", readiness=true. Elapsed: 7.238817ms
Sep 13 11:51:02.617: INFO: Pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031441428s
Sep 13 11:51:04.625: INFO: Pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4": Phase="Running", Reason="", readiness=true. Elapsed: 4.038637477s
Sep 13 11:51:06.645: INFO: Pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.059237056s
Sep 13 11:51:06.645: INFO: Pod "pod-update-activedeadlineseconds-6e46a92a-244a-43e7-99f7-0815894499a4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:51:06.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4418" for this suite.

• [SLOW TEST:8.701 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3594,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:51:06.671: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-002ab5cb-2ee0-47b1-b2c7-a8467fbe9f46
STEP: Creating a pod to test consume configMaps
Sep 13 11:51:06.737: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6" in namespace "projected-1125" to be "Succeeded or Failed"
Sep 13 11:51:06.743: INFO: Pod "pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.475319ms
Sep 13 11:51:08.755: INFO: Pod "pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6": Phase="Running", Reason="", readiness=false. Elapsed: 2.017909371s
Sep 13 11:51:10.772: INFO: Pod "pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035819259s
STEP: Saw pod success
Sep 13 11:51:10.773: INFO: Pod "pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6" satisfied condition "Succeeded or Failed"
Sep 13 11:51:10.780: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 11:51:10.852: INFO: Waiting for pod pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6 to disappear
Sep 13 11:51:10.864: INFO: Pod pod-projected-configmaps-9766d237-a594-4ead-b034-0c683a5991c6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:51:10.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1125" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:51:10.884: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Sep 13 11:51:10.923: INFO: PodSpec: initContainers in spec.initContainers
Sep 13 11:51:50.762: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-57cc4a92-a73b-4112-ad5f-c0ecdfe13e44", GenerateName:"", Namespace:"init-container-239", SelfLink:"", UID:"cb2da5d9-77f7-4478-ac61-deb4c329ec06", ResourceVersion:"414394", Generation:0, CreationTimestamp:time.Date(2022, time.September, 13, 11, 51, 11, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"923490180"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.September, 13, 11, 51, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030b8e70), Subresource:""}, v1.ManagedFieldsEntry{Manager:"k3s", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.September, 13, 11, 51, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030b8ed0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-z8r29", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00258e4c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z8r29", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z8r29", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z8r29", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00500e368), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0029d8150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00500e3e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00500e400)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00500e408), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00500e40c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0056881e0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 13, 11, 51, 10, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 13, 11, 51, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 13, 11, 51, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.September, 13, 11, 51, 11, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.5", PodIP:"10.42.1.218", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.1.218"}}, StartTime:time.Date(2022, time.September, 13, 11, 51, 10, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029d8230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029d82a0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://6bb81c98324792fb60013ece1a65dae30aee26aefa70d29016b26995c9528ba5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00258e580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00258e540), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00500e48f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:51:50.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-239" for this suite.

• [SLOW TEST:39.893 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":194,"skipped":3646,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:51:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Sep 13 11:51:50.813: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:51:54.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3600" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":195,"skipped":3660,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:51:54.830: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:51:57.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-501" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":196,"skipped":3667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:51:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Sep 13 11:51:59.896: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9214 PodName:pod-sharedvolume-a49e7484-19fe-4e79-9001-654f14ed8eb3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 11:51:59.896: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 11:51:59.897: INFO: ExecWithOptions: Clientset creation
Sep 13 11:51:59.897: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-9214/pods/pod-sharedvolume-a49e7484-19fe-4e79-9001-654f14ed8eb3/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Sep 13 11:52:00.031: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:52:00.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9214" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":197,"skipped":3703,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:52:00.050: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4931
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-4931
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4931
Sep 13 11:52:00.111: INFO: Found 0 stateful pods, waiting for 1
Sep 13 11:52:10.134: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 13 11:52:10.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:52:10.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:52:10.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:52:10.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:52:10.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 13 11:52:20.728: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:52:20.728: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:52:20.753: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Sep 13 11:52:20.753: INFO: ss-0  k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:00 +0000 UTC  }]
Sep 13 11:52:20.753: INFO: 
Sep 13 11:52:20.753: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 13 11:52:21.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994719017s
Sep 13 11:52:22.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978096849s
Sep 13 11:52:23.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.959461638s
Sep 13 11:52:24.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.935665567s
Sep 13 11:52:25.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.916377027s
Sep 13 11:52:26.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.904969622s
Sep 13 11:52:27.882: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897296675s
Sep 13 11:52:28.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.86569431s
Sep 13 11:52:29.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 852.502602ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4931
Sep 13 11:52:30.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:52:31.193: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 11:52:31.193: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:52:31.193: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:52:31.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:52:31.430: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 13 11:52:31.430: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:52:31.430: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:52:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:52:31.663: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 13 11:52:31.663: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:52:31.663: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:52:31.674: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 13 11:52:41.700: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 11:52:41.701: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 11:52:41.701: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 13 11:52:41.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:52:41.989: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:52:41.989: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:52:41.989: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:52:41.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:52:42.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:52:42.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:52:42.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:52:42.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-4931 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:52:42.433: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:52:42.433: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:52:42.433: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:52:42.433: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:52:42.442: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 13 11:52:52.473: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:52:52.473: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:52:52.473: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:52:52.492: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Sep 13 11:52:52.492: INFO: ss-2  k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:21 +0000 UTC  }]
Sep 13 11:52:52.492: INFO: ss-1  k3s-conformance-test-12310-81a66c-node-pool-5423-44vom  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:21 +0000 UTC  }]
Sep 13 11:52:52.492: INFO: ss-0  k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:00 +0000 UTC  }]
Sep 13 11:52:52.492: INFO: 
Sep 13 11:52:52.492: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 13 11:52:53.517: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Sep 13 11:52:53.517: INFO: ss-1  k3s-conformance-test-12310-81a66c-node-pool-5423-44vom  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-13 11:52:21 +0000 UTC  }]
Sep 13 11:52:53.517: INFO: 
Sep 13 11:52:53.517: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 13 11:52:54.531: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.970106793s
Sep 13 11:52:55.554: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.95486841s
Sep 13 11:52:56.574: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.932506318s
Sep 13 11:52:57.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.912868992s
Sep 13 11:52:58.612: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.896236692s
Sep 13 11:52:59.627: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.875337045s
Sep 13 11:53:00.649: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.859734414s
Sep 13 11:53:01.669: INFO: Verifying statefulset ss doesn't scale past 0 for another 838.264164ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4931
Sep 13 11:53:02.687: INFO: Scaling statefulset ss to 0
Sep 13 11:53:02.712: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 11:53:02.718: INFO: Deleting all statefulset in ns statefulset-4931
Sep 13 11:53:02.724: INFO: Scaling statefulset ss to 0
Sep 13 11:53:02.753: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:53:02.759: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:02.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4931" for this suite.

• [SLOW TEST:62.755 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":198,"skipped":3722,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:02.806: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:19.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9154" for this suite.

• [SLOW TEST:16.286 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":199,"skipped":3733,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:19.092: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Sep 13 11:53:19.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 create -f -'
Sep 13 11:53:19.742: INFO: stderr: ""
Sep 13 11:53:19.742: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 11:53:19.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:53:19.854: INFO: stderr: ""
Sep 13 11:53:19.854: INFO: stdout: "update-demo-nautilus-gcsl9 update-demo-nautilus-gd92l "
Sep 13 11:53:19.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods update-demo-nautilus-gcsl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:53:19.966: INFO: stderr: ""
Sep 13 11:53:19.966: INFO: stdout: ""
Sep 13 11:53:19.966: INFO: update-demo-nautilus-gcsl9 is created but not running
Sep 13 11:53:24.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 13 11:53:25.091: INFO: stderr: ""
Sep 13 11:53:25.091: INFO: stdout: "update-demo-nautilus-gd92l update-demo-nautilus-gcsl9 "
Sep 13 11:53:25.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods update-demo-nautilus-gd92l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:53:25.205: INFO: stderr: ""
Sep 13 11:53:25.205: INFO: stdout: "true"
Sep 13 11:53:25.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods update-demo-nautilus-gd92l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:53:25.324: INFO: stderr: ""
Sep 13 11:53:25.324: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:53:25.324: INFO: validating pod update-demo-nautilus-gd92l
Sep 13 11:53:25.335: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:53:25.336: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:53:25.336: INFO: update-demo-nautilus-gd92l is verified up and running
Sep 13 11:53:25.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods update-demo-nautilus-gcsl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 13 11:53:25.463: INFO: stderr: ""
Sep 13 11:53:25.463: INFO: stdout: "true"
Sep 13 11:53:25.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods update-demo-nautilus-gcsl9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 13 11:53:25.595: INFO: stderr: ""
Sep 13 11:53:25.595: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Sep 13 11:53:25.595: INFO: validating pod update-demo-nautilus-gcsl9
Sep 13 11:53:25.605: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 11:53:25.605: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 11:53:25.605: INFO: update-demo-nautilus-gcsl9 is verified up and running
STEP: using delete to clean up resources
Sep 13 11:53:25.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Sep 13 11:53:25.736: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 11:53:25.736: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 13 11:53:25.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get rc,svc -l name=update-demo --no-headers'
Sep 13 11:53:25.866: INFO: stderr: "No resources found in kubectl-6553 namespace.\n"
Sep 13 11:53:25.866: INFO: stdout: ""
Sep 13 11:53:25.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6553 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 11:53:25.956: INFO: stderr: ""
Sep 13 11:53:25.956: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:25.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6553" for this suite.

• [SLOW TEST:6.887 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":200,"skipped":3737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:25.981: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 11:53:26.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9" in namespace "projected-771" to be "Succeeded or Failed"
Sep 13 11:53:26.040: INFO: Pod "downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.703854ms
Sep 13 11:53:28.062: INFO: Pod "downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034859344s
Sep 13 11:53:30.080: INFO: Pod "downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052510111s
STEP: Saw pod success
Sep 13 11:53:30.080: INFO: Pod "downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9" satisfied condition "Succeeded or Failed"
Sep 13 11:53:30.086: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9 container client-container: <nil>
STEP: delete the pod
Sep 13 11:53:30.128: INFO: Waiting for pod downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9 to disappear
Sep 13 11:53:30.137: INFO: Pod downwardapi-volume-42d4ecb1-377a-4fa3-be3f-0416dff8cae9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:30.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-771" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:30.152: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
Sep 13 11:53:30.190: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3561 proxy --unix-socket=/tmp/kubectl-proxy-unix2378165940/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:30.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3561" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":202,"skipped":3863,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:30.277: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 13 11:53:30.321: INFO: Waiting up to 5m0s for pod "pod-148e17f6-19c5-4ab3-97da-f7fc5567430b" in namespace "emptydir-6752" to be "Succeeded or Failed"
Sep 13 11:53:30.328: INFO: Pod "pod-148e17f6-19c5-4ab3-97da-f7fc5567430b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50107ms
Sep 13 11:53:32.346: INFO: Pod "pod-148e17f6-19c5-4ab3-97da-f7fc5567430b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024466159s
Sep 13 11:53:34.362: INFO: Pod "pod-148e17f6-19c5-4ab3-97da-f7fc5567430b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040325861s
STEP: Saw pod success
Sep 13 11:53:34.362: INFO: Pod "pod-148e17f6-19c5-4ab3-97da-f7fc5567430b" satisfied condition "Succeeded or Failed"
Sep 13 11:53:34.370: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-148e17f6-19c5-4ab3-97da-f7fc5567430b container test-container: <nil>
STEP: delete the pod
Sep 13 11:53:34.398: INFO: Waiting for pod pod-148e17f6-19c5-4ab3-97da-f7fc5567430b to disappear
Sep 13 11:53:34.403: INFO: Pod pod-148e17f6-19c5-4ab3-97da-f7fc5567430b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:34.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6752" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":3872,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:34.416: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:53:34.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:53:38.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:38.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4628" for this suite.
STEP: Destroying namespace "webhook-4628-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":204,"skipped":3886,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:38.114: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 13 11:53:38.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5333  4274b2bb-4f2f-410f-976a-6c4df34fd663 415032 0 2022-09-13 11:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-09-13 11:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 11:53:38.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5333  4274b2bb-4f2f-410f-976a-6c4df34fd663 415033 0 2022-09-13 11:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-09-13 11:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:38.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5333" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":205,"skipped":3892,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:38.212: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2516
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2516
I0913 11:53:38.344529      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2516, replica count: 2
Sep 13 11:53:41.395: INFO: Creating new exec pod
I0913 11:53:41.395745      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 11:53:44.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-2516 exec execpod228mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 13 11:53:44.688: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 13 11:53:44.688: INFO: stdout: ""
Sep 13 11:53:45.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-2516 exec execpod228mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 13 11:53:45.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 13 11:53:45.930: INFO: stdout: "externalname-service-lqzzq"
Sep 13 11:53:45.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-2516 exec execpod228mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.234.228 80'
Sep 13 11:53:46.190: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.234.228 80\nConnection to 10.43.234.228 80 port [tcp/http] succeeded!\n"
Sep 13 11:53:46.190: INFO: stdout: "externalname-service-p6ng8"
Sep 13 11:53:46.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-2516 exec execpod228mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.6 32274'
Sep 13 11:53:46.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.6 32274\nConnection to 192.168.1.6 32274 port [tcp/*] succeeded!\n"
Sep 13 11:53:46.429: INFO: stdout: "externalname-service-lqzzq"
Sep 13 11:53:46.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-2516 exec execpod228mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 32274'
Sep 13 11:53:46.685: INFO: stderr: "+ nc -v -t -w 2 192.168.1.5 32274\n+ echo hostName\nConnection to 192.168.1.5 32274 port [tcp/*] succeeded!\n"
Sep 13 11:53:46.685: INFO: stdout: "externalname-service-lqzzq"
Sep 13 11:53:46.685: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:53:46.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2516" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.503 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":206,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:53:46.715: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-645
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-645
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-645
Sep 13 11:53:46.761: INFO: Found 0 stateful pods, waiting for 1
Sep 13 11:53:56.777: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 13 11:53:56.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:53:57.058: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:53:57.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:53:57.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:53:57.065: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 13 11:54:07.085: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:54:07.085: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:54:07.115: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999263s
Sep 13 11:54:08.136: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992827719s
Sep 13 11:54:09.156: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.971124142s
Sep 13 11:54:10.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.951579133s
Sep 13 11:54:11.182: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.935732391s
Sep 13 11:54:12.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.925059181s
Sep 13 11:54:13.226: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.897726614s
Sep 13 11:54:14.234: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.881208016s
Sep 13 11:54:15.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.873757016s
Sep 13 11:54:16.253: INFO: Verifying statefulset ss doesn't scale past 1 for another 863.654587ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-645
Sep 13 11:54:17.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:54:17.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 11:54:17.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:54:17.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:54:17.560: INFO: Found 1 stateful pods, waiting for 3
Sep 13 11:54:27.584: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 11:54:27.584: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 11:54:27.584: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 13 11:54:27.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:54:27.841: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:54:27.841: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:54:27.841: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:54:27.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:54:28.083: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:54:28.084: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:54:28.084: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:54:28.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 11:54:28.323: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 11:54:28.323: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 11:54:28.323: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 11:54:28.323: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:54:28.331: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 13 11:54:38.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:54:38.358: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:54:38.358: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 11:54:38.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999976s
Sep 13 11:54:39.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995072921s
Sep 13 11:54:40.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978398224s
Sep 13 11:54:41.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96220693s
Sep 13 11:54:42.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.948444911s
Sep 13 11:54:43.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.933054667s
Sep 13 11:54:44.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.917411767s
Sep 13 11:54:45.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906217715s
Sep 13 11:54:46.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.89072116s
Sep 13 11:54:47.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 875.276178ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-645
Sep 13 11:54:48.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:54:48.806: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 11:54:48.806: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:54:48.806: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:54:48.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:54:49.046: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 11:54:49.046: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:54:49.046: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:54:49.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-645 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 11:54:49.263: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 11:54:49.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 11:54:49.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 13 11:54:49.263: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 11:54:59.321: INFO: Deleting all statefulset in ns statefulset-645
Sep 13 11:54:59.326: INFO: Scaling statefulset ss to 0
Sep 13 11:54:59.350: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 11:54:59.356: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:54:59.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-645" for this suite.

• [SLOW TEST:72.689 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":207,"skipped":3930,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:54:59.404: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-8a594fe8-15d2-4745-bdf3-b53f434fd82b
STEP: Creating a pod to test consume secrets
Sep 13 11:54:59.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051" in namespace "projected-2699" to be "Succeeded or Failed"
Sep 13 11:54:59.455: INFO: Pod "pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051": Phase="Pending", Reason="", readiness=false. Elapsed: 9.870625ms
Sep 13 11:55:01.467: INFO: Pod "pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022125438s
Sep 13 11:55:03.490: INFO: Pod "pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045018231s
STEP: Saw pod success
Sep 13 11:55:03.490: INFO: Pod "pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051" satisfied condition "Succeeded or Failed"
Sep 13 11:55:03.498: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 11:55:03.543: INFO: Waiting for pod pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051 to disappear
Sep 13 11:55:03.551: INFO: Pod pod-projected-secrets-d75ac240-1923-46fc-a8f5-0201fcff8051 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:03.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2699" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":3930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:03.573: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 13 11:55:03.606: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 11:55:03.614: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 11:55:03.617: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom before test
Sep 13 11:55:03.623: INFO: civo-csi-controller-0 from kube-system started at 2022-09-09 17:03:22 +0000 UTC (4 container statuses recorded)
Sep 13 11:55:03.623: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:55:03.623: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 13 11:55:03.623: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 13 11:55:03.623: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 13 11:55:03.623: INFO: civo-csi-node-62s5w from kube-system started at 2022-09-09 16:30:36 +0000 UTC (2 container statuses recorded)
Sep 13 11:55:03.623: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:55:03.623: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 11:55:03.623: INFO: civo-ccm-869574f9b7-sjssw from kube-system started at 2022-09-09 16:30:24 +0000 UTC (1 container statuses recorded)
Sep 13 11:55:03.623: INFO: 	Container civo-ccm ready: true, restart count 1
Sep 13 11:55:03.623: INFO: coredns-d76bd69b-h4czc from kube-system started at 2022-09-09 16:31:40 +0000 UTC (1 container statuses recorded)
Sep 13 11:55:03.623: INFO: 	Container coredns ready: true, restart count 0
Sep 13 11:55:03.623: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl before test
Sep 13 11:55:03.629: INFO: sonobuoy-e2e-job-7e148db81c5d4b8a from sonobuoy started at 2022-09-13 11:11:39 +0000 UTC (2 container statuses recorded)
Sep 13 11:55:03.629: INFO: 	Container e2e ready: true, restart count 0
Sep 13 11:55:03.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 11:55:03.630: INFO: civo-csi-node-g5k77 from kube-system started at 2022-09-13 11:44:36 +0000 UTC (2 container statuses recorded)
Sep 13 11:55:03.630: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 11:55:03.630: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 11:55:03.630: INFO: sonobuoy from sonobuoy started at 2022-09-13 11:11:38 +0000 UTC (1 container statuses recorded)
Sep 13 11:55:03.630: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17146972d776d724], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:04.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1692" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":209,"skipped":3965,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:04.682: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
Sep 13 11:55:04.737: INFO: created test-podtemplate-1
Sep 13 11:55:04.741: INFO: created test-podtemplate-2
Sep 13 11:55:04.745: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep 13 11:55:04.748: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep 13 11:55:04.762: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:04.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3676" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":210,"skipped":3967,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:04.782: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 11:55:04.837: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8da11b5e-0a28-48a5-9971-a3e7b8861502" in namespace "security-context-test-1918" to be "Succeeded or Failed"
Sep 13 11:55:04.845: INFO: Pod "busybox-user-65534-8da11b5e-0a28-48a5-9971-a3e7b8861502": Phase="Pending", Reason="", readiness=false. Elapsed: 7.651656ms
Sep 13 11:55:06.860: INFO: Pod "busybox-user-65534-8da11b5e-0a28-48a5-9971-a3e7b8861502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02286685s
Sep 13 11:55:08.870: INFO: Pod "busybox-user-65534-8da11b5e-0a28-48a5-9971-a3e7b8861502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032844891s
Sep 13 11:55:08.870: INFO: Pod "busybox-user-65534-8da11b5e-0a28-48a5-9971-a3e7b8861502" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1918" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3983,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:08.880: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 11:55:09.395: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 11:55:12.436: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:12.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1961" for this suite.
STEP: Destroying namespace "webhook-1961-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":212,"skipped":4001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:12.590: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 13 11:55:12.632: INFO: Waiting up to 5m0s for pod "pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3" in namespace "emptydir-9510" to be "Succeeded or Failed"
Sep 13 11:55:12.639: INFO: Pod "pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.74387ms
Sep 13 11:55:14.650: INFO: Pod "pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3": Phase="Running", Reason="", readiness=false. Elapsed: 2.017645285s
Sep 13 11:55:16.664: INFO: Pod "pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031958864s
STEP: Saw pod success
Sep 13 11:55:16.664: INFO: Pod "pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3" satisfied condition "Succeeded or Failed"
Sep 13 11:55:16.669: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3 container test-container: <nil>
STEP: delete the pod
Sep 13 11:55:16.703: INFO: Waiting for pod pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3 to disappear
Sep 13 11:55:16.711: INFO: Pod pod-365fc42b-b7d2-4cd5-b62c-1129736e62d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:16.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9510" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":4091,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:16.730: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 11:55:16.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5018" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":214,"skipped":4105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 11:55:16.789: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:16.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-130" for this suite.

• [SLOW TEST:300.109 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":215,"skipped":4155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:16.900: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 13 12:00:16.962: INFO: Waiting up to 5m0s for pod "pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3" in namespace "emptydir-6074" to be "Succeeded or Failed"
Sep 13 12:00:16.966: INFO: Pod "pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.783907ms
Sep 13 12:00:18.986: INFO: Pod "pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024575241s
Sep 13 12:00:20.999: INFO: Pod "pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037109455s
STEP: Saw pod success
Sep 13 12:00:21.000: INFO: Pod "pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3" satisfied condition "Succeeded or Failed"
Sep 13 12:00:21.006: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3 container test-container: <nil>
STEP: delete the pod
Sep 13 12:00:21.064: INFO: Waiting for pod pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3 to disappear
Sep 13 12:00:21.069: INFO: Pod pod-ab9d3824-1d8a-4a44-a16c-2f50fb6fbcd3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:21.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6074" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":4181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:21.083: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-6647
STEP: creating service affinity-nodeport-transition in namespace services-6647
STEP: creating replication controller affinity-nodeport-transition in namespace services-6647
I0913 12:00:21.132565      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6647, replica count: 3
I0913 12:00:24.184243      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 12:00:24.213: INFO: Creating new exec pod
Sep 13 12:00:27.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6647 exec execpod-affinitykgmjt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Sep 13 12:00:27.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 13 12:00:27.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:00:27.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6647 exec execpod-affinitykgmjt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.104.168 80'
Sep 13 12:00:27.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.104.168 80\nConnection to 10.43.104.168 80 port [tcp/http] succeeded!\n"
Sep 13 12:00:27.771: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:00:27.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6647 exec execpod-affinitykgmjt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.6 32391'
Sep 13 12:00:28.021: INFO: stderr: "+ + nc -v -t -w 2 192.168.1.6 32391\necho hostName\nConnection to 192.168.1.6 32391 port [tcp/*] succeeded!\n"
Sep 13 12:00:28.021: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:00:28.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6647 exec execpod-affinitykgmjt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 32391'
Sep 13 12:00:28.249: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.5 32391\nConnection to 192.168.1.5 32391 port [tcp/*] succeeded!\n"
Sep 13 12:00:28.249: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:00:28.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6647 exec execpod-affinitykgmjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.6:32391/ ; done'
Sep 13 12:00:28.641: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n"
Sep 13 12:00:28.641: INFO: stdout: "\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-98vw8\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-b88jw\naffinity-nodeport-transition-98vw8\naffinity-nodeport-transition-98vw8\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-98vw8\naffinity-nodeport-transition-b88jw\naffinity-nodeport-transition-b88jw\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-98vw8\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-98vw8"
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-98vw8
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-b88jw
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-98vw8
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-98vw8
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-98vw8
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-b88jw
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-b88jw
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-98vw8
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:28.641: INFO: Received response from host: affinity-nodeport-transition-98vw8
Sep 13 12:00:28.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-6647 exec execpod-affinitykgmjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.6:32391/ ; done'
Sep 13 12:00:29.114: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.6:32391/\n"
Sep 13 12:00:29.114: INFO: stdout: "\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn\naffinity-nodeport-transition-j64mn"
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Received response from host: affinity-nodeport-transition-j64mn
Sep 13 12:00:29.114: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6647, will wait for the garbage collector to delete the pods
Sep 13 12:00:29.206: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.103499ms
Sep 13 12:00:29.306: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.158615ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:31.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6647" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.481 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":217,"skipped":4206,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:31.564: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 12:00:31.621: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833" in namespace "downward-api-46" to be "Succeeded or Failed"
Sep 13 12:00:31.627: INFO: Pod "downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833": Phase="Pending", Reason="", readiness=false. Elapsed: 5.553139ms
Sep 13 12:00:33.648: INFO: Pod "downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026126203s
Sep 13 12:00:35.664: INFO: Pod "downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042323059s
STEP: Saw pod success
Sep 13 12:00:35.664: INFO: Pod "downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833" satisfied condition "Succeeded or Failed"
Sep 13 12:00:35.671: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833 container client-container: <nil>
STEP: delete the pod
Sep 13 12:00:35.703: INFO: Waiting for pod downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833 to disappear
Sep 13 12:00:35.709: INFO: Pod downwardapi-volume-a9fb99bd-a5c3-48bb-b6d4-86f51b080833 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:35.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-46" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4209,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:35.729: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:41.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8725" for this suite.
STEP: Destroying namespace "nsdeletetest-597" for this suite.
Sep 13 12:00:41.969: INFO: Namespace nsdeletetest-597 was already deleted
STEP: Destroying namespace "nsdeletetest-35" for this suite.

• [SLOW TEST:6.260 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":219,"skipped":4218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:41.991: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:00:42.124: INFO: Creating pod...
Sep 13 12:00:42.198: INFO: Pod Quantity: 1 Status: Pending
Sep 13 12:00:43.218: INFO: Pod Quantity: 1 Status: Pending
Sep 13 12:00:44.215: INFO: Pod Status: Running
Sep 13 12:00:44.215: INFO: Creating service...
Sep 13 12:00:44.236: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/DELETE
Sep 13 12:00:44.248: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 13 12:00:44.248: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/GET
Sep 13 12:00:44.252: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 13 12:00:44.252: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/HEAD
Sep 13 12:00:44.260: INFO: http.Client request:HEAD | StatusCode:200
Sep 13 12:00:44.260: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/OPTIONS
Sep 13 12:00:44.265: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 13 12:00:44.265: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/PATCH
Sep 13 12:00:44.269: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 13 12:00:44.269: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/POST
Sep 13 12:00:44.277: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 13 12:00:44.277: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/pods/agnhost/proxy/some/path/with/PUT
Sep 13 12:00:44.283: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep 13 12:00:44.283: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/DELETE
Sep 13 12:00:44.292: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 13 12:00:44.292: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/GET
Sep 13 12:00:44.299: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 13 12:00:44.299: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/HEAD
Sep 13 12:00:44.307: INFO: http.Client request:HEAD | StatusCode:200
Sep 13 12:00:44.307: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/OPTIONS
Sep 13 12:00:44.314: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 13 12:00:44.314: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/PATCH
Sep 13 12:00:44.321: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 13 12:00:44.321: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/POST
Sep 13 12:00:44.327: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 13 12:00:44.327: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-6881/services/test-service/proxy/some/path/with/PUT
Sep 13 12:00:44.336: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:44.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6881" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":220,"skipped":4261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:44.379: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 13 12:00:44.424: INFO: Waiting up to 5m0s for pod "pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b" in namespace "emptydir-1708" to be "Succeeded or Failed"
Sep 13 12:00:44.434: INFO: Pod "pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.865624ms
Sep 13 12:00:46.449: INFO: Pod "pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025050398s
Sep 13 12:00:48.474: INFO: Pod "pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049569314s
STEP: Saw pod success
Sep 13 12:00:48.474: INFO: Pod "pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b" satisfied condition "Succeeded or Failed"
Sep 13 12:00:48.479: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b container test-container: <nil>
STEP: delete the pod
Sep 13 12:00:48.501: INFO: Waiting for pod pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b to disappear
Sep 13 12:00:48.507: INFO: Pod pod-8da9fe0b-dc97-466d-828f-50eb6d3b1f1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:48.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1708" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4284,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:48.523: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Sep 13 12:00:48.567: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 13 12:00:53.578: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:53.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9490" for this suite.

• [SLOW TEST:5.104 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":222,"skipped":4288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:53.627: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 12:00:53.712: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:00:53.712: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 12:00:54.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:00:54.730: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 12:00:55.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 12:00:55.735: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Sep 13 12:00:55.784: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"416317"},"items":null}

Sep 13 12:00:55.789: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"416318"},"items":[{"metadata":{"name":"daemon-set-xq6pz","generateName":"daemon-set-","namespace":"daemonsets-1176","uid":"187652c8-f2fb-4441-8d76-01f9a1ea6fb5","resourceVersion":"416316","creationTimestamp":"2022-09-13T12:00:54Z","deletionTimestamp":"2022-09-13T12:01:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f12fd8a2-415f-48ef-a2a7-553757070224","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2022-09-13T12:00:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f12fd8a2-415f-48ef-a2a7-553757070224\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2022-09-13T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6vk7r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6vk7r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k3s-conformance-test-12310-81a66c-node-pool-5423-44vom","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k3s-conformance-test-12310-81a66c-node-pool-5423-44vom"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:54Z"}],"hostIP":"192.168.1.6","podIP":"10.42.0.13","podIPs":[{"ip":"10.42.0.13"}],"startTime":"2022-09-13T12:00:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-09-13T12:00:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://31b4d1eadce33c93bf0da4c083eccd4c8fc90166e020d10b021e6922fc6af93a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-sp9w5","generateName":"daemon-set-","namespace":"daemonsets-1176","uid":"0e7a4b82-38c8-41f4-84e6-1367cd8c2b1f","resourceVersion":"416318","creationTimestamp":"2022-09-13T12:00:54Z","deletionTimestamp":"2022-09-13T12:01:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f12fd8a2-415f-48ef-a2a7-553757070224","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2022-09-13T12:00:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f12fd8a2-415f-48ef-a2a7-553757070224\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2022-09-13T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7gvcj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7gvcj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-09-13T12:00:54Z"}],"hostIP":"192.168.1.5","podIP":"10.42.1.246","podIPs":[{"ip":"10.42.1.246"}],"startTime":"2022-09-13T12:00:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-09-13T12:00:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://69e3d412e0de36fa6dde931ac8023030c1ef54c1c461bab0a583a6f9ac3e40b8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:55.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1176" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":223,"skipped":4327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:55.811: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:55.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5034" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":224,"skipped":4368,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:00:55.859: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 12:00:55.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44023003-523e-4381-a811-bebac4668d79" in namespace "projected-1855" to be "Succeeded or Failed"
Sep 13 12:00:55.900: INFO: Pod "downwardapi-volume-44023003-523e-4381-a811-bebac4668d79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635185ms
Sep 13 12:00:57.923: INFO: Pod "downwardapi-volume-44023003-523e-4381-a811-bebac4668d79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026383235s
Sep 13 12:00:59.951: INFO: Pod "downwardapi-volume-44023003-523e-4381-a811-bebac4668d79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05446464s
STEP: Saw pod success
Sep 13 12:00:59.951: INFO: Pod "downwardapi-volume-44023003-523e-4381-a811-bebac4668d79" satisfied condition "Succeeded or Failed"
Sep 13 12:00:59.958: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-44023003-523e-4381-a811-bebac4668d79 container client-container: <nil>
STEP: delete the pod
Sep 13 12:00:59.989: INFO: Waiting for pod downwardapi-volume-44023003-523e-4381-a811-bebac4668d79 to disappear
Sep 13 12:00:59.995: INFO: Pod downwardapi-volume-44023003-523e-4381-a811-bebac4668d79 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:00:59.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1855" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:00.011: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Sep 13 12:01:00.073: INFO: Pod name sample-pod: Found 1 pods out of 3
Sep 13 12:01:05.077: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Sep 13 12:01:05.100: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7876" for this suite.

• [SLOW TEST:5.149 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":226,"skipped":4425,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:05.163: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0160c110-79a8-42df-a060-00850eb95038
STEP: Creating the pod
Sep 13 12:01:05.227: INFO: The status of Pod pod-projected-configmaps-ef27cd5e-b2a3-4c61-acc9-b9c8b856a9b3 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:01:07.246: INFO: The status of Pod pod-projected-configmaps-ef27cd5e-b2a3-4c61-acc9-b9c8b856a9b3 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-0160c110-79a8-42df-a060-00850eb95038
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:09.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-640" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4446,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:01:09.946: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:01:12.994: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:13.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3036" for this suite.
STEP: Destroying namespace "webhook-3036-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":228,"skipped":4457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:01:13.845: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:01:16.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
Sep 13 12:01:16.928: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 13 12:01:19.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=webhook-5579 attach --namespace=webhook-5579 to-be-attached-pod -i -c=container1'
Sep 13 12:01:19.248: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:19.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5579" for this suite.
STEP: Destroying namespace "webhook-5579-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.056 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":229,"skipped":4507,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:19.343: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-6110
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 12:01:19.368: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 13 12:01:19.393: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:01:21.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:23.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:25.404: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:27.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:29.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:31.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:33.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:35.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:37.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:39.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:01:41.407: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 13 12:01:41.419: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 13 12:01:43.469: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 13 12:01:43.469: INFO: Breadth first check of 10.42.0.15 on host 192.168.1.6...
Sep 13 12:01:43.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.2:9080/dial?request=hostname&protocol=http&host=10.42.0.15&port=8083&tries=1'] Namespace:pod-network-test-6110 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 12:01:43.475: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 12:01:43.476: INFO: ExecWithOptions: Clientset creation
Sep 13 12:01:43.476: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.1.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.0.15%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Sep 13 12:01:43.653: INFO: Waiting for responses: map[]
Sep 13 12:01:43.653: INFO: reached 10.42.0.15 after 0/1 tries
Sep 13 12:01:43.653: INFO: Breadth first check of 10.42.1.254 on host 192.168.1.5...
Sep 13 12:01:43.663: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.2:9080/dial?request=hostname&protocol=http&host=10.42.1.254&port=8083&tries=1'] Namespace:pod-network-test-6110 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 12:01:43.663: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 12:01:43.664: INFO: ExecWithOptions: Clientset creation
Sep 13 12:01:43.664: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.1.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.1.254%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Sep 13 12:01:43.814: INFO: Waiting for responses: map[]
Sep 13 12:01:43.814: INFO: reached 10.42.1.254 after 0/1 tries
Sep 13 12:01:43.814: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6110" for this suite.

• [SLOW TEST:24.497 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4517,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:43.841: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4574
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4574
STEP: creating replication controller externalsvc in namespace services-4574
I0913 12:01:43.949521      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4574, replica count: 2
I0913 12:01:47.000730      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 13 12:01:47.040: INFO: Creating new exec pod
Sep 13 12:01:49.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-4574 exec execpodrcbbx -- /bin/sh -x -c nslookup clusterip-service.services-4574.svc.cluster.local'
Sep 13 12:01:49.278: INFO: stderr: "+ nslookup clusterip-service.services-4574.svc.cluster.local\n"
Sep 13 12:01:49.278: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-4574.svc.cluster.local\tcanonical name = externalsvc.services-4574.svc.cluster.local.\nName:\texternalsvc.services-4574.svc.cluster.local\nAddress: 10.43.63.130\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4574, will wait for the garbage collector to delete the pods
Sep 13 12:01:49.353: INFO: Deleting ReplicationController externalsvc took: 15.543396ms
Sep 13 12:01:49.454: INFO: Terminating ReplicationController externalsvc pods took: 100.69175ms
Sep 13 12:01:51.778: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:51.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4574" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.967 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":231,"skipped":4522,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-c5ef35d0-3aaf-4021-a9e4-08c92e3d3384
STEP: Creating a pod to test consume secrets
Sep 13 12:01:51.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57" in namespace "projected-8536" to be "Succeeded or Failed"
Sep 13 12:01:51.863: INFO: Pod "pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57": Phase="Pending", Reason="", readiness=false. Elapsed: 14.957176ms
Sep 13 12:01:53.883: INFO: Pod "pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57": Phase="Running", Reason="", readiness=false. Elapsed: 2.034919406s
Sep 13 12:01:55.894: INFO: Pod "pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045861845s
STEP: Saw pod success
Sep 13 12:01:55.894: INFO: Pod "pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57" satisfied condition "Succeeded or Failed"
Sep 13 12:01:55.903: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 12:01:55.939: INFO: Waiting for pod pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57 to disappear
Sep 13 12:01:55.948: INFO: Pod pod-projected-secrets-4e974c45-ea17-4dd6-911d-e4a84a584d57 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:55.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8536" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4531,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:55.964: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:56.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9147" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":233,"skipped":4550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:56.053: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
Sep 13 12:01:56.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6426 cluster-info'
Sep 13 12:01:56.190: INFO: stderr: ""
Sep 13 12:01:56.190: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:01:56.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6426" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":234,"skipped":4581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:01:56.213: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-rdnc
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 12:01:56.269: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rdnc" in namespace "subpath-805" to be "Succeeded or Failed"
Sep 13 12:01:56.273: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.792312ms
Sep 13 12:01:58.294: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 2.024866307s
Sep 13 12:02:00.317: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 4.048052799s
Sep 13 12:02:02.335: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 6.065089693s
Sep 13 12:02:04.355: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 8.085950393s
Sep 13 12:02:06.368: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 10.098838582s
Sep 13 12:02:08.394: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 12.124957036s
Sep 13 12:02:10.416: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 14.146746038s
Sep 13 12:02:12.435: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 16.165986175s
Sep 13 12:02:14.451: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 18.181616483s
Sep 13 12:02:16.466: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=true. Elapsed: 20.196157398s
Sep 13 12:02:18.485: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Running", Reason="", readiness=false. Elapsed: 22.215121276s
Sep 13 12:02:20.507: INFO: Pod "pod-subpath-test-configmap-rdnc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.237297753s
STEP: Saw pod success
Sep 13 12:02:20.507: INFO: Pod "pod-subpath-test-configmap-rdnc" satisfied condition "Succeeded or Failed"
Sep 13 12:02:20.512: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-subpath-test-configmap-rdnc container test-container-subpath-configmap-rdnc: <nil>
STEP: delete the pod
Sep 13 12:02:20.539: INFO: Waiting for pod pod-subpath-test-configmap-rdnc to disappear
Sep 13 12:02:20.547: INFO: Pod pod-subpath-test-configmap-rdnc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rdnc
Sep 13 12:02:20.548: INFO: Deleting pod "pod-subpath-test-configmap-rdnc" in namespace "subpath-805"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:02:20.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-805" for this suite.

• [SLOW TEST:24.354 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":235,"skipped":4669,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:02:20.569: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:02:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8878" for this suite.

• [SLOW TEST:25.638 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:02:46.208: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:02:48.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1225" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:02:48.334: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 13 12:02:50.954: INFO: Successfully updated pod "adopt-release-ztlk2"
STEP: Checking that the Job readopts the Pod
Sep 13 12:02:50.954: INFO: Waiting up to 15m0s for pod "adopt-release-ztlk2" in namespace "job-5195" to be "adopted"
Sep 13 12:02:50.961: INFO: Pod "adopt-release-ztlk2": Phase="Running", Reason="", readiness=true. Elapsed: 7.18637ms
Sep 13 12:02:52.973: INFO: Pod "adopt-release-ztlk2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018562318s
Sep 13 12:02:52.973: INFO: Pod "adopt-release-ztlk2" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 13 12:02:53.484: INFO: Successfully updated pod "adopt-release-ztlk2"
STEP: Checking that the Job releases the Pod
Sep 13 12:02:53.484: INFO: Waiting up to 15m0s for pod "adopt-release-ztlk2" in namespace "job-5195" to be "released"
Sep 13 12:02:53.492: INFO: Pod "adopt-release-ztlk2": Phase="Running", Reason="", readiness=true. Elapsed: 7.671977ms
Sep 13 12:02:55.504: INFO: Pod "adopt-release-ztlk2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020073145s
Sep 13 12:02:55.505: INFO: Pod "adopt-release-ztlk2" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:02:55.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5195" for this suite.

• [SLOW TEST:7.192 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":238,"skipped":4770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:02:55.527: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Sep 13 12:02:55.578: INFO: The status of Pod labelsupdate9e4d10c2-f2ab-45d5-ab60-4faeeca3eecd is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:02:57.599: INFO: The status of Pod labelsupdate9e4d10c2-f2ab-45d5-ab60-4faeeca3eecd is Running (Ready = true)
Sep 13 12:02:58.154: INFO: Successfully updated pod "labelsupdate9e4d10c2-f2ab-45d5-ab60-4faeeca3eecd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:02.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9364" for this suite.

• [SLOW TEST:6.715 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4815,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:02.243: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-bf762f97-06d4-4508-8c93-a1c291cf4c28
STEP: Creating a pod to test consume configMaps
Sep 13 12:03:02.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc" in namespace "projected-4937" to be "Succeeded or Failed"
Sep 13 12:03:02.502: INFO: Pod "pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 43.87367ms
Sep 13 12:03:04.521: INFO: Pod "pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063003957s
Sep 13 12:03:06.537: INFO: Pod "pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079254109s
STEP: Saw pod success
Sep 13 12:03:06.538: INFO: Pod "pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc" satisfied condition "Succeeded or Failed"
Sep 13 12:03:06.544: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc container agnhost-container: <nil>
STEP: delete the pod
Sep 13 12:03:06.584: INFO: Waiting for pod pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc to disappear
Sep 13 12:03:06.593: INFO: Pod pod-projected-configmaps-9ea89ec7-5adb-4085-96e1-53aa845f3dfc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:06.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4937" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4818,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:06.610: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:03:06.664: INFO: The status of Pod server-envvars-9767901f-3022-4f46-8ccc-1c9fa3b4e70c is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:03:08.682: INFO: The status of Pod server-envvars-9767901f-3022-4f46-8ccc-1c9fa3b4e70c is Running (Ready = true)
Sep 13 12:03:08.728: INFO: Waiting up to 5m0s for pod "client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877" in namespace "pods-7343" to be "Succeeded or Failed"
Sep 13 12:03:08.739: INFO: Pod "client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877": Phase="Pending", Reason="", readiness=false. Elapsed: 10.957956ms
Sep 13 12:03:10.765: INFO: Pod "client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037094336s
Sep 13 12:03:12.784: INFO: Pod "client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056646719s
STEP: Saw pod success
Sep 13 12:03:12.784: INFO: Pod "client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877" satisfied condition "Succeeded or Failed"
Sep 13 12:03:12.790: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877 container env3cont: <nil>
STEP: delete the pod
Sep 13 12:03:12.817: INFO: Waiting for pod client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877 to disappear
Sep 13 12:03:12.823: INFO: Pod client-envvars-91fae6c8-58a4-4acc-a4c9-4777b2e85877 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:12.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7343" for this suite.

• [SLOW TEST:6.224 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4828,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:12.835: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-4650e667-00b0-4e4c-b8f2-08a647f1a839
STEP: Creating a pod to test consume secrets
Sep 13 12:03:12.887: INFO: Waiting up to 5m0s for pod "pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0" in namespace "secrets-1722" to be "Succeeded or Failed"
Sep 13 12:03:12.893: INFO: Pod "pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.724723ms
Sep 13 12:03:14.913: INFO: Pod "pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025701s
Sep 13 12:03:16.933: INFO: Pod "pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046105373s
STEP: Saw pod success
Sep 13 12:03:16.934: INFO: Pod "pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0" satisfied condition "Succeeded or Failed"
Sep 13 12:03:16.942: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 12:03:16.977: INFO: Waiting for pod pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0 to disappear
Sep 13 12:03:16.984: INFO: Pod pod-secrets-0c9db79d-e3fd-4b41-ae52-cbf6a71666d0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:16.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1722" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4836,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:16.999: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:30.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9570" for this suite.

• [SLOW TEST:13.191 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":243,"skipped":4843,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:30.191: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:03:30.898: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:03:33.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:34.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5371" for this suite.
STEP: Destroying namespace "webhook-5371-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":244,"skipped":4856,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:34.138: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 12:03:34.175: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d" in namespace "downward-api-1211" to be "Succeeded or Failed"
Sep 13 12:03:34.182: INFO: Pod "downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.363446ms
Sep 13 12:03:36.193: INFO: Pod "downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018591257s
Sep 13 12:03:38.215: INFO: Pod "downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040294455s
STEP: Saw pod success
Sep 13 12:03:38.215: INFO: Pod "downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d" satisfied condition "Succeeded or Failed"
Sep 13 12:03:38.223: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d container client-container: <nil>
STEP: delete the pod
Sep 13 12:03:38.258: INFO: Waiting for pod downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d to disappear
Sep 13 12:03:38.264: INFO: Pod downwardapi-volume-13b68666-1d9b-468a-b19a-bc4561ca0a3d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:38.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1211" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:38.278: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-4a95272f-70d7-44da-b751-2d3f7ee33425
STEP: Creating a pod to test consume secrets
Sep 13 12:03:38.327: INFO: Waiting up to 5m0s for pod "pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398" in namespace "secrets-8430" to be "Succeeded or Failed"
Sep 13 12:03:38.333: INFO: Pod "pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551237ms
Sep 13 12:03:40.359: INFO: Pod "pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398": Phase="Running", Reason="", readiness=false. Elapsed: 2.031916554s
Sep 13 12:03:42.375: INFO: Pod "pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048715141s
STEP: Saw pod success
Sep 13 12:03:42.375: INFO: Pod "pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398" satisfied condition "Succeeded or Failed"
Sep 13 12:03:42.381: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 12:03:42.419: INFO: Waiting for pod pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398 to disappear
Sep 13 12:03:42.426: INFO: Pod pod-secrets-c4f2e642-976c-47af-8fdd-52bd6a082398 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:42.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8430" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":246,"skipped":4889,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:42.442: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:03:42.932: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:03:45.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:03:45.995: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:03:49.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7741" for this suite.
STEP: Destroying namespace "webhook-7741-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.832 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":247,"skipped":4889,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:03:49.275: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Sep 13 12:03:55.363: INFO: 80 pods remaining
Sep 13 12:03:55.363: INFO: 80 pods has nil DeletionTimestamp
Sep 13 12:03:55.363: INFO: 
Sep 13 12:03:56.364: INFO: 71 pods remaining
Sep 13 12:03:56.364: INFO: 71 pods has nil DeletionTimestamp
Sep 13 12:03:56.364: INFO: 
Sep 13 12:03:57.381: INFO: 60 pods remaining
Sep 13 12:03:57.381: INFO: 59 pods has nil DeletionTimestamp
Sep 13 12:03:57.381: INFO: 
Sep 13 12:03:58.356: INFO: 40 pods remaining
Sep 13 12:03:58.356: INFO: 40 pods has nil DeletionTimestamp
Sep 13 12:03:58.356: INFO: 
Sep 13 12:03:59.379: INFO: 32 pods remaining
Sep 13 12:03:59.379: INFO: 31 pods has nil DeletionTimestamp
Sep 13 12:03:59.379: INFO: 
Sep 13 12:04:00.360: INFO: 18 pods remaining
Sep 13 12:04:00.360: INFO: 18 pods has nil DeletionTimestamp
Sep 13 12:04:00.360: INFO: 
STEP: Gathering metrics
Sep 13 12:04:01.366: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:04:01.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0913 12:04:01.366720      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-2635" for this suite.

• [SLOW TEST:12.108 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":248,"skipped":4897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:04:01.384: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-d315065f-a3ae-4684-bd41-dae4546e4cf3
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:04:01.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-247" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":249,"skipped":4927,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:04:01.434: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 12:04:01.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3" in namespace "projected-851" to be "Succeeded or Failed"
Sep 13 12:04:01.485: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.23627ms
Sep 13 12:04:03.496: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017922931s
Sep 13 12:04:05.512: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033663965s
Sep 13 12:04:07.523: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044372991s
Sep 13 12:04:09.542: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064166009s
Sep 13 12:04:11.552: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.074074792s
Sep 13 12:04:13.565: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.087065449s
Sep 13 12:04:15.578: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.099951694s
Sep 13 12:04:17.595: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.116524064s
STEP: Saw pod success
Sep 13 12:04:17.595: INFO: Pod "downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3" satisfied condition "Succeeded or Failed"
Sep 13 12:04:17.600: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3 container client-container: <nil>
STEP: delete the pod
Sep 13 12:04:17.632: INFO: Waiting for pod downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3 to disappear
Sep 13 12:04:17.640: INFO: Pod downwardapi-volume-c9fec78b-61df-4ddb-a384-e58eaa4f93a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:04:17.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-851" for this suite.

• [SLOW TEST:16.223 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:04:17.657: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:04:45.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5436" for this suite.

• [SLOW TEST:28.119 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":251,"skipped":5008,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:04:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Sep 13 12:04:45.910: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.910: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.924: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.924: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.940: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.940: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.966: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:45.966: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 13 12:04:46.788: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 13 12:04:46.789: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 13 12:04:47.564: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Sep 13 12:04:47.582: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Sep 13 12:04:47.588: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 0
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.589: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.595: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.595: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.607: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.607: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:47.634: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:47.634: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:47.642: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:47.642: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:48.795: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:48.795: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:48.821: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
STEP: listing Deployments
Sep 13 12:04:48.827: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Sep 13 12:04:48.851: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Sep 13 12:04:48.865: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:48.908: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:49.013: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:49.077: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:49.112: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:50.583: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:50.818: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:50.858: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:50.873: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 13 12:04:52.605: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Sep 13 12:04:52.654: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 1
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 3
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:52.655: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 2
Sep 13 12:04:52.656: INFO: observed Deployment test-deployment in namespace deployment-2358 with ReadyReplicas 3
STEP: deleting the Deployment
Sep 13 12:04:52.670: INFO: observed event type MODIFIED
Sep 13 12:04:52.671: INFO: observed event type MODIFIED
Sep 13 12:04:52.671: INFO: observed event type MODIFIED
Sep 13 12:04:52.674: INFO: observed event type MODIFIED
Sep 13 12:04:52.674: INFO: observed event type MODIFIED
Sep 13 12:04:52.674: INFO: observed event type MODIFIED
Sep 13 12:04:52.674: INFO: observed event type MODIFIED
Sep 13 12:04:52.674: INFO: observed event type MODIFIED
Sep 13 12:04:52.675: INFO: observed event type MODIFIED
Sep 13 12:04:52.675: INFO: observed event type MODIFIED
Sep 13 12:04:52.675: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 13 12:04:52.689: INFO: Log out all the ReplicaSets if there is no deployment created
Sep 13 12:04:52.695: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-2358  e11fe5f5-0d69-4d93-ba74-45892e28d1f8 418788 3 2022-09-13 12:04:46 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment c176cd00-b6b7-48ed-a3f3-c4bf21c62847 0xc005035777 0xc005035778}] []  [{k3s Update apps/v1 2022-09-13 12:04:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c176cd00-b6b7-48ed-a3f3-c4bf21c62847\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 12:04:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005035810 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep 13 12:04:52.700: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-2358  ca558c75-930b-41eb-974b-6a2889ee6684 418863 2 2022-09-13 12:04:49 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment c176cd00-b6b7-48ed-a3f3-c4bf21c62847 0xc005035877 0xc005035878}] []  [{k3s Update apps/v1 2022-09-13 12:04:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c176cd00-b6b7-48ed-a3f3-c4bf21c62847\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 12:04:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005035910 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep 13 12:04:52.707: INFO: pod: "test-deployment-854fdc678-nx8pr":
&Pod{ObjectMeta:{test-deployment-854fdc678-nx8pr test-deployment-854fdc678- deployment-2358  e70f3c3b-87f6-4dbe-865c-39b60380fbc5 418824 0 2022-09-13 12:04:49 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 ca558c75-930b-41eb-974b-6a2889ee6684 0xc005035f57 0xc005035f58}] []  [{k3s Update v1 2022-09-13 12:04:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca558c75-930b-41eb-974b-6a2889ee6684\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 12:04:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvxxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvxxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:10.42.1.65,StartTime:2022-09-13 12:04:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 12:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://28fc37b5735f1e0484f71d9ab3c6ffb5a55c8cf9f6f39d2ca2442aa13aba5f9c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 13 12:04:52.708: INFO: pod: "test-deployment-854fdc678-hffvr":
&Pod{ObjectMeta:{test-deployment-854fdc678-hffvr test-deployment-854fdc678- deployment-2358  9746981b-60f1-4ce2-9049-4d7fd30dea18 418862 0 2022-09-13 12:04:51 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 ca558c75-930b-41eb-974b-6a2889ee6684 0xc0045d2c67 0xc0045d2c68}] []  [{k3s Update v1 2022-09-13 12:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca558c75-930b-41eb-974b-6a2889ee6684\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 12:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4mvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4mvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.58,StartTime:2022-09-13 12:04:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 12:04:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://229bc8b6868048605de9da6f400885bf18659b48f3e0ed46f5966f383d04c28f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 13 12:04:52.708: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-2358  4e2265fb-ad79-4b88-b827-9f84f86915bb 418870 4 2022-09-13 12:04:48 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment c176cd00-b6b7-48ed-a3f3-c4bf21c62847 0xc005035977 0xc005035978}] []  [{k3s Update apps/v1 2022-09-13 12:04:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c176cd00-b6b7-48ed-a3f3-c4bf21c62847\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2022-09-13 12:04:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005035a10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep 13 12:04:52.713: INFO: pod: "test-deployment-5ddd8b47d8-ngw6t":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-ngw6t test-deployment-5ddd8b47d8- deployment-2358  fd83aec5-8a39-4402-a384-a0d991cde217 418866 0 2022-09-13 12:04:49 +0000 UTC 2022-09-13 12:04:54 +0000 UTC 0xc00570aa98 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 4e2265fb-ad79-4b88-b827-9f84f86915bb 0xc00570aac7 0xc00570aac8}] []  [{k3s Update v1 2022-09-13 12:04:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e2265fb-ad79-4b88-b827-9f84f86915bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2022-09-13 12:04:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lv9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lv9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k3s-conformance-test-12310-81a66c-node-pool-5423-44vom,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-13 12:04:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:10.42.0.57,StartTime:2022-09-13 12:04:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-13 12:04:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/rancher/mirrored-pause:3.6,ImageID:docker.io/rancher/mirrored-pause@sha256:74c4244427b7312c5b901fe0f67cbc53683d06f4f24c6faee65d4182bf0fa893,ContainerID:containerd://add60dd9b85ecad4977501f8bfcef2b9341f12da8f792dfa2c0bead22ea01efb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:04:52.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2358" for this suite.

• [SLOW TEST:6.950 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":252,"skipped":5013,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:04:52.727: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Sep 13 12:04:52.768: INFO: Waiting up to 5m0s for pod "downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2" in namespace "downward-api-5064" to be "Succeeded or Failed"
Sep 13 12:04:52.776: INFO: Pod "downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.263386ms
Sep 13 12:04:54.794: INFO: Pod "downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025695492s
Sep 13 12:04:56.811: INFO: Pod "downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042150638s
STEP: Saw pod success
Sep 13 12:04:56.811: INFO: Pod "downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2" satisfied condition "Succeeded or Failed"
Sep 13 12:04:56.819: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2 container dapi-container: <nil>
STEP: delete the pod
Sep 13 12:04:56.865: INFO: Waiting for pod downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2 to disappear
Sep 13 12:04:56.874: INFO: Pod downward-api-5de1ec25-6a3a-4ba4-b1e8-52c2a12742e2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:04:56.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5064" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":253,"skipped":5032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:04:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Sep 13 12:04:56.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6595 create -f -'
Sep 13 12:04:58.263: INFO: stderr: ""
Sep 13 12:04:58.263: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 13 12:04:59.279: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:04:59.279: INFO: Found 0 / 1
Sep 13 12:05:00.277: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:05:00.277: INFO: Found 1 / 1
Sep 13 12:05:00.277: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 13 12:05:00.285: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:05:00.286: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 13 12:05:00.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-6595 patch pod agnhost-primary-khw2n -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 13 12:05:00.410: INFO: stderr: ""
Sep 13 12:05:00.410: INFO: stdout: "pod/agnhost-primary-khw2n patched\n"
STEP: checking annotations
Sep 13 12:05:00.418: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:05:00.418: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:05:00.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6595" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":254,"skipped":5060,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:05:00.435: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-700ddf60-4104-4112-83c5-e97b39788377 in namespace container-probe-9217
Sep 13 12:05:02.500: INFO: Started pod liveness-700ddf60-4104-4112-83c5-e97b39788377 in namespace container-probe-9217
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 12:05:02.508: INFO: Initial restart count of pod liveness-700ddf60-4104-4112-83c5-e97b39788377 is 0
Sep 13 12:05:22.712: INFO: Restart count of pod container-probe-9217/liveness-700ddf60-4104-4112-83c5-e97b39788377 is now 1 (20.204006509s elapsed)
Sep 13 12:05:42.897: INFO: Restart count of pod container-probe-9217/liveness-700ddf60-4104-4112-83c5-e97b39788377 is now 2 (40.38879232s elapsed)
Sep 13 12:06:03.059: INFO: Restart count of pod container-probe-9217/liveness-700ddf60-4104-4112-83c5-e97b39788377 is now 3 (1m0.550263014s elapsed)
Sep 13 12:06:21.232: INFO: Restart count of pod container-probe-9217/liveness-700ddf60-4104-4112-83c5-e97b39788377 is now 4 (1m18.723400639s elapsed)
Sep 13 12:07:33.874: INFO: Restart count of pod container-probe-9217/liveness-700ddf60-4104-4112-83c5-e97b39788377 is now 5 (2m31.365528356s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:07:33.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9217" for this suite.

• [SLOW TEST:153.474 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":5061,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:07:33.910: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 13 12:07:38.013: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:07:38.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3257" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":5076,"failed":0}

------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:07:38.051: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 13 12:07:38.086: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 13 12:08:38.127: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:08:38.134: INFO: Starting informer...
STEP: Starting pod...
Sep 13 12:08:38.364: INFO: Pod is running on k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 13 12:08:38.391: INFO: Pod wasn't evicted. Proceeding
Sep 13 12:08:38.391: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 13 12:09:53.415: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:09:53.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5393" for this suite.

• [SLOW TEST:135.409 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":257,"skipped":5076,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:09:53.461: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:09:53.509: INFO: Endpoints addresses: [74.220.26.112] , ports: [6443]
Sep 13 12:09:53.509: INFO: EndpointSlices addresses: [74.220.26.112] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:09:53.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6782" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":258,"skipped":5087,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:09:53.518: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-afb1f9a1-16f6-49e7-971d-dc137ee5ae16
STEP: Creating a pod to test consume configMaps
Sep 13 12:09:53.555: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc" in namespace "projected-8282" to be "Succeeded or Failed"
Sep 13 12:09:53.560: INFO: Pod "pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968552ms
Sep 13 12:09:55.581: INFO: Pod "pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02597425s
Sep 13 12:09:57.602: INFO: Pod "pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047174129s
STEP: Saw pod success
Sep 13 12:09:57.602: INFO: Pod "pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc" satisfied condition "Succeeded or Failed"
Sep 13 12:09:57.610: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc container agnhost-container: <nil>
STEP: delete the pod
Sep 13 12:09:57.654: INFO: Waiting for pod pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc to disappear
Sep 13 12:09:57.662: INFO: Pod pod-projected-configmaps-105872fc-371f-4c11-b138-a580dc1d9dfc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:09:57.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8282" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":5102,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:09:57.680: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-3487
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3487 to expose endpoints map[]
Sep 13 12:09:57.735: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep 13 12:09:58.748: INFO: successfully validated that service endpoint-test2 in namespace services-3487 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3487
Sep 13 12:09:58.762: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:10:00.775: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3487 to expose endpoints map[pod1:[80]]
Sep 13 12:10:00.794: INFO: successfully validated that service endpoint-test2 in namespace services-3487 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Sep 13 12:10:00.794: INFO: Creating new exec pod
Sep 13 12:10:03.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3487 exec execpodgj79n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep 13 12:10:04.098: INFO: stderr: "+ + nc -v -t -w 2 endpoint-test2echo 80\n hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 13 12:10:04.098: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:10:04.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3487 exec execpodgj79n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.243.130 80'
Sep 13 12:10:04.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.243.130 80\nConnection to 10.43.243.130 80 port [tcp/http] succeeded!\n"
Sep 13 12:10:04.360: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3487
Sep 13 12:10:04.382: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:10:06.394: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3487 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 13 12:10:06.427: INFO: successfully validated that service endpoint-test2 in namespace services-3487 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Sep 13 12:10:07.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3487 exec execpodgj79n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep 13 12:10:07.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 13 12:10:07.658: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:10:07.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3487 exec execpodgj79n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.243.130 80'
Sep 13 12:10:07.870: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.243.130 80\nConnection to 10.43.243.130 80 port [tcp/http] succeeded!\n"
Sep 13 12:10:07.870: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3487
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3487 to expose endpoints map[pod2:[80]]
Sep 13 12:10:08.946: INFO: successfully validated that service endpoint-test2 in namespace services-3487 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Sep 13 12:10:09.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3487 exec execpodgj79n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep 13 12:10:10.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 13 12:10:10.186: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:10:10.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3487 exec execpodgj79n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.243.130 80'
Sep 13 12:10:10.434: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.243.130 80\nConnection to 10.43.243.130 80 port [tcp/http] succeeded!\n"
Sep 13 12:10:10.434: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3487
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3487 to expose endpoints map[]
Sep 13 12:10:10.471: INFO: successfully validated that service endpoint-test2 in namespace services-3487 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:10:10.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3487" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.827 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":260,"skipped":5115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:10:10.507: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep 13 12:10:10.555: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:10:10.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7978" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":261,"skipped":5156,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:10:10.613: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:10:10.652: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Creating first CR 
Sep 13 12:10:13.269: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-13T12:10:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-13T12:10:13Z]] name:name1 resourceVersion:419518 uid:9d38f97f-a912-4b7c-ac1a-02e5635e10ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 13 12:10:23.297: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-13T12:10:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-13T12:10:24Z]] name:name2 resourceVersion:419556 uid:44cf2e5f-dd95-4b1d-92cd-c43924ea7c95] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 13 12:10:33.326: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-13T12:10:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-13T12:10:34Z]] name:name1 resourceVersion:419566 uid:9d38f97f-a912-4b7c-ac1a-02e5635e10ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 13 12:10:43.353: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-13T12:10:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-13T12:10:44Z]] name:name2 resourceVersion:419576 uid:44cf2e5f-dd95-4b1d-92cd-c43924ea7c95] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 13 12:10:53.375: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-13T12:10:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-13T12:10:34Z]] name:name1 resourceVersion:419587 uid:9d38f97f-a912-4b7c-ac1a-02e5635e10ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 13 12:11:03.404: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-13T12:10:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-13T12:10:44Z]] name:name2 resourceVersion:419598 uid:44cf2e5f-dd95-4b1d-92cd-c43924ea7c95] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:11:13.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-473" for this suite.

• [SLOW TEST:63.370 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":262,"skipped":5165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:11:13.983: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:11:14.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7812" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":263,"skipped":5195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:11:14.098: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:11:14.893: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:11:17.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:11:17.956: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8811-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:11:21.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2709" for this suite.
STEP: Destroying namespace "webhook-2709-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.154 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":264,"skipped":5218,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:11:21.253: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-ac462db8-2fe8-482b-abf5-67a35bef2fc4
STEP: Creating a pod to test consume configMaps
Sep 13 12:11:21.301: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f" in namespace "projected-1344" to be "Succeeded or Failed"
Sep 13 12:11:21.305: INFO: Pod "pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302426ms
Sep 13 12:11:23.327: INFO: Pod "pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025155802s
Sep 13 12:11:25.343: INFO: Pod "pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041265345s
STEP: Saw pod success
Sep 13 12:11:25.343: INFO: Pod "pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f" satisfied condition "Succeeded or Failed"
Sep 13 12:11:25.351: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 12:11:25.382: INFO: Waiting for pod pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f to disappear
Sep 13 12:11:25.389: INFO: Pod pod-projected-configmaps-50a7e8b8-2b5b-44d7-bd87-80cfc7f9ed2f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:11:25.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1344" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":5226,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:11:25.399: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-7653
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7653 to expose endpoints map[]
Sep 13 12:11:25.451: INFO: successfully validated that service multi-endpoint-test in namespace services-7653 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7653
Sep 13 12:11:25.464: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:11:27.479: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7653 to expose endpoints map[pod1:[100]]
Sep 13 12:11:27.502: INFO: successfully validated that service multi-endpoint-test in namespace services-7653 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7653
Sep 13 12:11:27.515: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:11:29.529: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7653 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 13 12:11:29.566: INFO: successfully validated that service multi-endpoint-test in namespace services-7653 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Sep 13 12:11:29.566: INFO: Creating new exec pod
Sep 13 12:11:32.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-7653 exec execpodwbglt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Sep 13 12:11:32.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep 13 12:11:32.864: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:11:32.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-7653 exec execpodwbglt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.160.75 80'
Sep 13 12:11:33.120: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.160.75 80\nConnection to 10.43.160.75 80 port [tcp/http] succeeded!\n"
Sep 13 12:11:33.120: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:11:33.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-7653 exec execpodwbglt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Sep 13 12:11:33.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep 13 12:11:33.369: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 13 12:11:33.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-7653 exec execpodwbglt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.160.75 81'
Sep 13 12:11:33.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.160.75 81\nConnection to 10.43.160.75 81 port [tcp/*] succeeded!\n"
Sep 13 12:11:33.618: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7653
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7653 to expose endpoints map[pod2:[101]]
Sep 13 12:11:33.672: INFO: successfully validated that service multi-endpoint-test in namespace services-7653 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7653
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7653 to expose endpoints map[]
Sep 13 12:11:33.693: INFO: successfully validated that service multi-endpoint-test in namespace services-7653 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:11:33.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7653" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.312 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":266,"skipped":5228,"failed":0}
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:11:33.712: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:12:59.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9875" for this suite.

• [SLOW TEST:86.096 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":267,"skipped":5228,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:12:59.808: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:13:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6136" for this suite.

• [SLOW TEST:11.152 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":268,"skipped":5235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:13:10.960: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:13:11.010: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cabb0e93-56fd-450a-879d-6327a590dee8" in namespace "security-context-test-6882" to be "Succeeded or Failed"
Sep 13 12:13:11.015: INFO: Pod "alpine-nnp-false-cabb0e93-56fd-450a-879d-6327a590dee8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45208ms
Sep 13 12:13:13.041: INFO: Pod "alpine-nnp-false-cabb0e93-56fd-450a-879d-6327a590dee8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030652357s
Sep 13 12:13:15.062: INFO: Pod "alpine-nnp-false-cabb0e93-56fd-450a-879d-6327a590dee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052069145s
Sep 13 12:13:15.062: INFO: Pod "alpine-nnp-false-cabb0e93-56fd-450a-879d-6327a590dee8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:13:15.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6882" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":5263,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:13:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:13:17.168: INFO: Deleting pod "var-expansion-85ae568c-b710-4466-a0ee-4fb49daad95a" in namespace "var-expansion-5879"
Sep 13 12:13:17.182: INFO: Wait up to 5m0s for pod "var-expansion-85ae568c-b710-4466-a0ee-4fb49daad95a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:13:19.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5879" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":270,"skipped":5273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:13:19.220: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 12:13:19.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:13:19.277: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 12:13:20.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:13:20.288: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 12:13:21.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 12:13:21.292: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 13 12:13:21.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 12:13:21.329: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 12:13:22.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 12:13:22.346: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 12:13:23.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 12:13:23.354: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-561, will wait for the garbage collector to delete the pods
Sep 13 12:13:23.425: INFO: Deleting DaemonSet.extensions daemon-set took: 8.989955ms
Sep 13 12:13:23.526: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.997336ms
Sep 13 12:13:25.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:13:25.545: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 13 12:13:25.553: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"420133"},"items":null}

Sep 13 12:13:25.557: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"420133"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:13:25.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-561" for this suite.

• [SLOW TEST:6.370 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":271,"skipped":5299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:13:25.590: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 13 12:13:25.613: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 12:13:28.044: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:13:37.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2690" for this suite.

• [SLOW TEST:11.974 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":272,"skipped":5323,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:13:37.567: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:13:37.639: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:13:39.661: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:41.649: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:43.659: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:45.658: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:47.651: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:49.652: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:51.655: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:53.654: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:55.660: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:57.654: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = false)
Sep 13 12:13:59.659: INFO: The status of Pod test-webserver-26b797cf-642f-43eb-ab4a-91b9198594cb is Running (Ready = true)
Sep 13 12:13:59.666: INFO: Container started at 2022-09-13 12:13:38 +0000 UTC, pod became ready at 2022-09-13 12:13:57 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:13:59.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3" for this suite.

• [SLOW TEST:22.120 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":5333,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:13:59.687: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Sep 13 12:13:59.726: INFO: namespace kubectl-3132
Sep 13 12:13:59.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3132 create -f -'
Sep 13 12:14:00.310: INFO: stderr: ""
Sep 13 12:14:00.310: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 13 12:14:01.326: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:14:01.326: INFO: Found 0 / 1
Sep 13 12:14:02.328: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:14:02.328: INFO: Found 1 / 1
Sep 13 12:14:02.329: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 13 12:14:02.333: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:14:02.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 13 12:14:02.333: INFO: wait on agnhost-primary startup in kubectl-3132 
Sep 13 12:14:02.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3132 logs agnhost-primary-jgvv9 agnhost-primary'
Sep 13 12:14:02.422: INFO: stderr: ""
Sep 13 12:14:02.422: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 13 12:14:02.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3132 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep 13 12:14:02.558: INFO: stderr: ""
Sep 13 12:14:02.558: INFO: stdout: "service/rm2 exposed\n"
Sep 13 12:14:02.563: INFO: Service rm2 in namespace kubectl-3132 found.
STEP: exposing service
Sep 13 12:14:04.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-3132 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep 13 12:14:04.724: INFO: stderr: ""
Sep 13 12:14:04.724: INFO: stdout: "service/rm3 exposed\n"
Sep 13 12:14:04.730: INFO: Service rm3 in namespace kubectl-3132 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:14:06.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3132" for this suite.

• [SLOW TEST:7.082 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":274,"skipped":5343,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:14:06.770: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-7cdd4efa-9d8d-4ece-b63a-05f65bb2b628
STEP: Creating a pod to test consume secrets
Sep 13 12:14:06.819: INFO: Waiting up to 5m0s for pod "pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6" in namespace "secrets-8324" to be "Succeeded or Failed"
Sep 13 12:14:06.824: INFO: Pod "pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845585ms
Sep 13 12:14:08.832: INFO: Pod "pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012901949s
Sep 13 12:14:10.841: INFO: Pod "pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022090359s
STEP: Saw pod success
Sep 13 12:14:10.841: INFO: Pod "pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6" satisfied condition "Succeeded or Failed"
Sep 13 12:14:10.848: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 12:14:10.878: INFO: Waiting for pod pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6 to disappear
Sep 13 12:14:10.887: INFO: Pod pod-secrets-4d4ce7c5-8772-4932-8d45-402b59bcaee6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:14:10.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8324" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5351,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:14:10.901: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:14:28.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5639" for this suite.

• [SLOW TEST:17.166 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":276,"skipped":5351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:14:28.068: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Sep 13 12:14:28.167: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:14:30.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5853" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":277,"skipped":5380,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:14:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 13 12:14:30.263: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 13 12:15:30.308: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Sep 13 12:15:30.354: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep 13 12:15:30.368: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep 13 12:15:30.389: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep 13 12:15:30.398: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:15:38.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3391" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.383 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":278,"skipped":5381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:15:38.598: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-hgt2
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 12:15:38.658: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hgt2" in namespace "subpath-8854" to be "Succeeded or Failed"
Sep 13 12:15:38.664: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.33706ms
Sep 13 12:15:40.677: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018390491s
Sep 13 12:15:42.698: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 4.039432465s
Sep 13 12:15:44.707: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 6.04898503s
Sep 13 12:15:46.720: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 8.061211229s
Sep 13 12:15:48.735: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 10.0763901s
Sep 13 12:15:50.757: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 12.098334779s
Sep 13 12:15:52.779: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 14.120945413s
Sep 13 12:15:54.800: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 16.141543518s
Sep 13 12:15:56.813: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 18.155044593s
Sep 13 12:15:58.825: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=true. Elapsed: 20.166731482s
Sep 13 12:16:00.834: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Running", Reason="", readiness=false. Elapsed: 22.175915151s
Sep 13 12:16:02.852: INFO: Pod "pod-subpath-test-projected-hgt2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.193545826s
STEP: Saw pod success
Sep 13 12:16:02.852: INFO: Pod "pod-subpath-test-projected-hgt2" satisfied condition "Succeeded or Failed"
Sep 13 12:16:02.858: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-subpath-test-projected-hgt2 container test-container-subpath-projected-hgt2: <nil>
STEP: delete the pod
Sep 13 12:16:02.899: INFO: Waiting for pod pod-subpath-test-projected-hgt2 to disappear
Sep 13 12:16:02.908: INFO: Pod pod-subpath-test-projected-hgt2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hgt2
Sep 13 12:16:02.908: INFO: Deleting pod "pod-subpath-test-projected-hgt2" in namespace "subpath-8854"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:16:02.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8854" for this suite.

• [SLOW TEST:24.329 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":279,"skipped":5403,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:16:02.927: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:16:03.704: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:16:06.754: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:16:06.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2395" for this suite.
STEP: Destroying namespace "webhook-2395-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":280,"skipped":5406,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:16:06.932: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-9e511e44-43aa-4763-974c-9e60cd635da1 in namespace container-probe-9077
Sep 13 12:16:09.005: INFO: Started pod liveness-9e511e44-43aa-4763-974c-9e60cd635da1 in namespace container-probe-9077
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 12:16:09.012: INFO: Initial restart count of pod liveness-9e511e44-43aa-4763-974c-9e60cd635da1 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:20:10.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9077" for this suite.

• [SLOW TEST:244.077 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:20:11.011: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:20:13.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-581" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":282,"skipped":5456,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:20:13.926: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Sep 13 12:20:13.991: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 13 12:20:18.999: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Sep 13 12:20:19.014: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Sep 13 12:20:19.026: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Sep 13 12:20:19.028: INFO: Observed &ReplicaSet event: ADDED
Sep 13 12:20:19.029: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.029: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.029: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.029: INFO: Found replicaset test-rs in namespace replicaset-5345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 13 12:20:19.030: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Sep 13 12:20:19.030: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 13 12:20:19.042: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Sep 13 12:20:19.047: INFO: Observed &ReplicaSet event: ADDED
Sep 13 12:20:19.047: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.047: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.048: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.048: INFO: Observed replicaset test-rs in namespace replicaset-5345 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 13 12:20:19.048: INFO: Observed &ReplicaSet event: MODIFIED
Sep 13 12:20:19.048: INFO: Found replicaset test-rs in namespace replicaset-5345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep 13 12:20:19.048: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:20:19.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5345" for this suite.

• [SLOW TEST:5.144 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":283,"skipped":5476,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:20:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:22:01.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2439" for this suite.

• [SLOW TEST:102.114 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":284,"skipped":5480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:22:01.187: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-ce6e241a-c93d-47d4-abc6-80d9bb2c059f in namespace container-probe-9801
Sep 13 12:22:03.246: INFO: Started pod test-webserver-ce6e241a-c93d-47d4-abc6-80d9bb2c059f in namespace container-probe-9801
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 12:22:03.250: INFO: Initial restart count of pod test-webserver-ce6e241a-c93d-47d4-abc6-80d9bb2c059f is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:26:05.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9801" for this suite.

• [SLOW TEST:244.096 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5533,"failed":0}
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:26:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:26:05.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2159" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":286,"skipped":5537,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:26:05.344: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-deb47798-1b31-4028-b3ea-153ed9c9c458
STEP: Creating secret with name secret-projected-all-test-volume-72dce5ac-69e7-4f59-882a-55fe5367b3a3
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 13 12:26:05.386: INFO: Waiting up to 5m0s for pod "projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1" in namespace "projected-7885" to be "Succeeded or Failed"
Sep 13 12:26:05.393: INFO: Pod "projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4263ms
Sep 13 12:26:07.402: INFO: Pod "projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015130301s
Sep 13 12:26:09.424: INFO: Pod "projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037573865s
STEP: Saw pod success
Sep 13 12:26:09.424: INFO: Pod "projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1" satisfied condition "Succeeded or Failed"
Sep 13 12:26:09.431: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 13 12:26:09.491: INFO: Waiting for pod projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1 to disappear
Sep 13 12:26:09.498: INFO: Pod projected-volume-30338de0-e1ca-469e-be80-86a5a17679d1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:26:09.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7885" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5549,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:26:09.509: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
Sep 13 12:26:09.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2012 create -f -'
Sep 13 12:26:10.133: INFO: stderr: ""
Sep 13 12:26:10.133: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep 13 12:26:10.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2012 diff -f -'
Sep 13 12:26:10.432: INFO: rc: 1
Sep 13 12:26:10.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2012 delete -f -'
Sep 13 12:26:10.522: INFO: stderr: ""
Sep 13 12:26:10.522: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:26:10.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2012" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":288,"skipped":5550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:26:10.538: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0913 12:26:50.668255      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 13 12:26:50.668: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 13 12:26:50.668: INFO: Deleting pod "simpletest.rc-dt678" in namespace "gc-4962"
Sep 13 12:26:50.686: INFO: Deleting pod "simpletest.rc-bp5lt" in namespace "gc-4962"
Sep 13 12:26:50.711: INFO: Deleting pod "simpletest.rc-qmzl4" in namespace "gc-4962"
Sep 13 12:26:50.734: INFO: Deleting pod "simpletest.rc-t7lkx" in namespace "gc-4962"
Sep 13 12:26:50.745: INFO: Deleting pod "simpletest.rc-ffs5r" in namespace "gc-4962"
Sep 13 12:26:50.764: INFO: Deleting pod "simpletest.rc-mmr6v" in namespace "gc-4962"
Sep 13 12:26:50.776: INFO: Deleting pod "simpletest.rc-wkwdq" in namespace "gc-4962"
Sep 13 12:26:50.788: INFO: Deleting pod "simpletest.rc-f9427" in namespace "gc-4962"
Sep 13 12:26:50.840: INFO: Deleting pod "simpletest.rc-dwf7q" in namespace "gc-4962"
Sep 13 12:26:50.891: INFO: Deleting pod "simpletest.rc-qjrln" in namespace "gc-4962"
Sep 13 12:26:50.912: INFO: Deleting pod "simpletest.rc-75fd7" in namespace "gc-4962"
Sep 13 12:26:50.942: INFO: Deleting pod "simpletest.rc-z7kb9" in namespace "gc-4962"
Sep 13 12:26:50.976: INFO: Deleting pod "simpletest.rc-p7lk4" in namespace "gc-4962"
Sep 13 12:26:50.989: INFO: Deleting pod "simpletest.rc-5jb2v" in namespace "gc-4962"
Sep 13 12:26:51.026: INFO: Deleting pod "simpletest.rc-727gq" in namespace "gc-4962"
Sep 13 12:26:51.037: INFO: Deleting pod "simpletest.rc-x67t2" in namespace "gc-4962"
Sep 13 12:26:51.063: INFO: Deleting pod "simpletest.rc-kcnrt" in namespace "gc-4962"
Sep 13 12:26:51.092: INFO: Deleting pod "simpletest.rc-cprqj" in namespace "gc-4962"
Sep 13 12:26:51.105: INFO: Deleting pod "simpletest.rc-qcc2t" in namespace "gc-4962"
Sep 13 12:26:51.158: INFO: Deleting pod "simpletest.rc-w98cz" in namespace "gc-4962"
Sep 13 12:26:51.184: INFO: Deleting pod "simpletest.rc-brsxb" in namespace "gc-4962"
Sep 13 12:26:51.222: INFO: Deleting pod "simpletest.rc-824rj" in namespace "gc-4962"
Sep 13 12:26:51.247: INFO: Deleting pod "simpletest.rc-hz96d" in namespace "gc-4962"
Sep 13 12:26:51.260: INFO: Deleting pod "simpletest.rc-wvh5c" in namespace "gc-4962"
Sep 13 12:26:51.281: INFO: Deleting pod "simpletest.rc-7z9pw" in namespace "gc-4962"
Sep 13 12:26:51.296: INFO: Deleting pod "simpletest.rc-s4g8v" in namespace "gc-4962"
Sep 13 12:26:51.316: INFO: Deleting pod "simpletest.rc-tvbvr" in namespace "gc-4962"
Sep 13 12:26:51.326: INFO: Deleting pod "simpletest.rc-5mb64" in namespace "gc-4962"
Sep 13 12:26:51.346: INFO: Deleting pod "simpletest.rc-j84t2" in namespace "gc-4962"
Sep 13 12:26:51.353: INFO: Deleting pod "simpletest.rc-75sv5" in namespace "gc-4962"
Sep 13 12:26:51.372: INFO: Deleting pod "simpletest.rc-h8jl4" in namespace "gc-4962"
Sep 13 12:26:51.386: INFO: Deleting pod "simpletest.rc-ctbc9" in namespace "gc-4962"
Sep 13 12:26:51.412: INFO: Deleting pod "simpletest.rc-drb7x" in namespace "gc-4962"
Sep 13 12:26:51.426: INFO: Deleting pod "simpletest.rc-b8c5h" in namespace "gc-4962"
Sep 13 12:26:51.442: INFO: Deleting pod "simpletest.rc-cj7k7" in namespace "gc-4962"
Sep 13 12:26:51.456: INFO: Deleting pod "simpletest.rc-k7mbg" in namespace "gc-4962"
Sep 13 12:26:51.467: INFO: Deleting pod "simpletest.rc-tg2xb" in namespace "gc-4962"
Sep 13 12:26:51.483: INFO: Deleting pod "simpletest.rc-62zkk" in namespace "gc-4962"
Sep 13 12:26:51.493: INFO: Deleting pod "simpletest.rc-nn9rc" in namespace "gc-4962"
Sep 13 12:26:51.513: INFO: Deleting pod "simpletest.rc-zt9hv" in namespace "gc-4962"
Sep 13 12:26:51.533: INFO: Deleting pod "simpletest.rc-rwjw7" in namespace "gc-4962"
Sep 13 12:26:51.556: INFO: Deleting pod "simpletest.rc-qrqnm" in namespace "gc-4962"
Sep 13 12:26:51.573: INFO: Deleting pod "simpletest.rc-n5bwp" in namespace "gc-4962"
Sep 13 12:26:51.594: INFO: Deleting pod "simpletest.rc-p9qsh" in namespace "gc-4962"
Sep 13 12:26:51.625: INFO: Deleting pod "simpletest.rc-fkq2m" in namespace "gc-4962"
Sep 13 12:26:51.642: INFO: Deleting pod "simpletest.rc-nrfvs" in namespace "gc-4962"
Sep 13 12:26:51.658: INFO: Deleting pod "simpletest.rc-bn27l" in namespace "gc-4962"
Sep 13 12:26:51.669: INFO: Deleting pod "simpletest.rc-7zk4f" in namespace "gc-4962"
Sep 13 12:26:51.681: INFO: Deleting pod "simpletest.rc-g6pzm" in namespace "gc-4962"
Sep 13 12:26:51.692: INFO: Deleting pod "simpletest.rc-kcvr6" in namespace "gc-4962"
Sep 13 12:26:51.702: INFO: Deleting pod "simpletest.rc-xp9vq" in namespace "gc-4962"
Sep 13 12:26:51.715: INFO: Deleting pod "simpletest.rc-mq9tn" in namespace "gc-4962"
Sep 13 12:26:51.727: INFO: Deleting pod "simpletest.rc-qngvv" in namespace "gc-4962"
Sep 13 12:26:51.738: INFO: Deleting pod "simpletest.rc-tlr2r" in namespace "gc-4962"
Sep 13 12:26:51.749: INFO: Deleting pod "simpletest.rc-qkjmw" in namespace "gc-4962"
Sep 13 12:26:51.761: INFO: Deleting pod "simpletest.rc-9tnhn" in namespace "gc-4962"
Sep 13 12:26:51.771: INFO: Deleting pod "simpletest.rc-bbvql" in namespace "gc-4962"
Sep 13 12:26:51.786: INFO: Deleting pod "simpletest.rc-ch6z5" in namespace "gc-4962"
Sep 13 12:26:51.805: INFO: Deleting pod "simpletest.rc-rwnsq" in namespace "gc-4962"
Sep 13 12:26:51.817: INFO: Deleting pod "simpletest.rc-hzr4f" in namespace "gc-4962"
Sep 13 12:26:51.831: INFO: Deleting pod "simpletest.rc-5rwrq" in namespace "gc-4962"
Sep 13 12:26:51.845: INFO: Deleting pod "simpletest.rc-rc9xp" in namespace "gc-4962"
Sep 13 12:26:51.889: INFO: Deleting pod "simpletest.rc-xd8c7" in namespace "gc-4962"
Sep 13 12:26:51.906: INFO: Deleting pod "simpletest.rc-nzlqc" in namespace "gc-4962"
Sep 13 12:26:51.917: INFO: Deleting pod "simpletest.rc-9m8d9" in namespace "gc-4962"
Sep 13 12:26:51.930: INFO: Deleting pod "simpletest.rc-rc5v7" in namespace "gc-4962"
Sep 13 12:26:51.939: INFO: Deleting pod "simpletest.rc-6tlz9" in namespace "gc-4962"
Sep 13 12:26:51.948: INFO: Deleting pod "simpletest.rc-cdsj5" in namespace "gc-4962"
Sep 13 12:26:51.957: INFO: Deleting pod "simpletest.rc-f7w2x" in namespace "gc-4962"
Sep 13 12:26:51.968: INFO: Deleting pod "simpletest.rc-vntw4" in namespace "gc-4962"
Sep 13 12:26:51.978: INFO: Deleting pod "simpletest.rc-fmg7f" in namespace "gc-4962"
Sep 13 12:26:52.000: INFO: Deleting pod "simpletest.rc-d2l8d" in namespace "gc-4962"
Sep 13 12:26:52.022: INFO: Deleting pod "simpletest.rc-847pt" in namespace "gc-4962"
Sep 13 12:26:52.032: INFO: Deleting pod "simpletest.rc-t6jcf" in namespace "gc-4962"
Sep 13 12:26:52.046: INFO: Deleting pod "simpletest.rc-cpjbs" in namespace "gc-4962"
Sep 13 12:26:52.053: INFO: Deleting pod "simpletest.rc-5dqxm" in namespace "gc-4962"
Sep 13 12:26:52.063: INFO: Deleting pod "simpletest.rc-dmgnj" in namespace "gc-4962"
Sep 13 12:26:52.120: INFO: Deleting pod "simpletest.rc-wkzvj" in namespace "gc-4962"
Sep 13 12:26:52.161: INFO: Deleting pod "simpletest.rc-456hw" in namespace "gc-4962"
Sep 13 12:26:52.215: INFO: Deleting pod "simpletest.rc-6fdlc" in namespace "gc-4962"
Sep 13 12:26:52.252: INFO: Deleting pod "simpletest.rc-dhxvt" in namespace "gc-4962"
Sep 13 12:26:52.321: INFO: Deleting pod "simpletest.rc-l67db" in namespace "gc-4962"
Sep 13 12:26:52.357: INFO: Deleting pod "simpletest.rc-rspjr" in namespace "gc-4962"
Sep 13 12:26:52.406: INFO: Deleting pod "simpletest.rc-v6pzw" in namespace "gc-4962"
Sep 13 12:26:52.452: INFO: Deleting pod "simpletest.rc-ftxdt" in namespace "gc-4962"
Sep 13 12:26:52.526: INFO: Deleting pod "simpletest.rc-ghfpm" in namespace "gc-4962"
Sep 13 12:26:52.553: INFO: Deleting pod "simpletest.rc-ffkdm" in namespace "gc-4962"
Sep 13 12:26:52.615: INFO: Deleting pod "simpletest.rc-b4t5q" in namespace "gc-4962"
Sep 13 12:26:52.653: INFO: Deleting pod "simpletest.rc-q4vht" in namespace "gc-4962"
Sep 13 12:26:52.705: INFO: Deleting pod "simpletest.rc-p899g" in namespace "gc-4962"
Sep 13 12:26:52.763: INFO: Deleting pod "simpletest.rc-mqvhw" in namespace "gc-4962"
Sep 13 12:26:52.814: INFO: Deleting pod "simpletest.rc-72s6d" in namespace "gc-4962"
Sep 13 12:26:52.864: INFO: Deleting pod "simpletest.rc-9nh87" in namespace "gc-4962"
Sep 13 12:26:52.917: INFO: Deleting pod "simpletest.rc-kp299" in namespace "gc-4962"
Sep 13 12:26:52.963: INFO: Deleting pod "simpletest.rc-vwplz" in namespace "gc-4962"
Sep 13 12:26:53.025: INFO: Deleting pod "simpletest.rc-slntm" in namespace "gc-4962"
Sep 13 12:26:53.056: INFO: Deleting pod "simpletest.rc-gvj6k" in namespace "gc-4962"
Sep 13 12:26:53.112: INFO: Deleting pod "simpletest.rc-khv82" in namespace "gc-4962"
Sep 13 12:26:53.166: INFO: Deleting pod "simpletest.rc-n85gz" in namespace "gc-4962"
Sep 13 12:26:53.214: INFO: Deleting pod "simpletest.rc-xkkd4" in namespace "gc-4962"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:26:53.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4962" for this suite.

• [SLOW TEST:42.818 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":289,"skipped":5639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:26:53.356: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:26:53.414: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 13 12:26:58.433: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Sep 13 12:27:04.482: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Sep 13 12:27:04.500: INFO: observed ReplicaSet test-rs in namespace replicaset-8856 with ReadyReplicas 1, AvailableReplicas 1
Sep 13 12:27:04.520: INFO: observed ReplicaSet test-rs in namespace replicaset-8856 with ReadyReplicas 1, AvailableReplicas 1
Sep 13 12:27:04.537: INFO: observed ReplicaSet test-rs in namespace replicaset-8856 with ReadyReplicas 1, AvailableReplicas 1
Sep 13 12:27:04.550: INFO: observed ReplicaSet test-rs in namespace replicaset-8856 with ReadyReplicas 1, AvailableReplicas 1
Sep 13 12:27:05.862: INFO: observed ReplicaSet test-rs in namespace replicaset-8856 with ReadyReplicas 2, AvailableReplicas 2
Sep 13 12:27:06.295: INFO: observed Replicaset test-rs in namespace replicaset-8856 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:06.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8856" for this suite.

• [SLOW TEST:12.965 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":290,"skipped":5680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:06.321: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 13 12:27:06.370: INFO: Waiting up to 5m0s for pod "pod-3bcc2631-d478-40f6-aa47-ca88c71008dc" in namespace "emptydir-1189" to be "Succeeded or Failed"
Sep 13 12:27:06.374: INFO: Pod "pod-3bcc2631-d478-40f6-aa47-ca88c71008dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.524211ms
Sep 13 12:27:08.390: INFO: Pod "pod-3bcc2631-d478-40f6-aa47-ca88c71008dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019843086s
Sep 13 12:27:10.411: INFO: Pod "pod-3bcc2631-d478-40f6-aa47-ca88c71008dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040719479s
STEP: Saw pod success
Sep 13 12:27:10.411: INFO: Pod "pod-3bcc2631-d478-40f6-aa47-ca88c71008dc" satisfied condition "Succeeded or Failed"
Sep 13 12:27:10.418: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom pod pod-3bcc2631-d478-40f6-aa47-ca88c71008dc container test-container: <nil>
STEP: delete the pod
Sep 13 12:27:10.458: INFO: Waiting for pod pod-3bcc2631-d478-40f6-aa47-ca88c71008dc to disappear
Sep 13 12:27:10.465: INFO: Pod pod-3bcc2631-d478-40f6-aa47-ca88c71008dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:10.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1189" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5708,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:10.488: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Sep 13 12:27:10.540: INFO: Waiting up to 5m0s for pod "downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad" in namespace "downward-api-2089" to be "Succeeded or Failed"
Sep 13 12:27:10.548: INFO: Pod "downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.576734ms
Sep 13 12:27:12.569: INFO: Pod "downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029477661s
Sep 13 12:27:14.594: INFO: Pod "downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053817241s
STEP: Saw pod success
Sep 13 12:27:14.594: INFO: Pod "downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad" satisfied condition "Succeeded or Failed"
Sep 13 12:27:14.602: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom pod downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad container dapi-container: <nil>
STEP: delete the pod
Sep 13 12:27:14.633: INFO: Waiting for pod downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad to disappear
Sep 13 12:27:14.637: INFO: Pod downward-api-20a56c00-7ac1-4fd0-9854-39ac6d66f7ad no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:14.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2089" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5708,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:14.653: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Sep 13 12:27:14.715: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:27:16.732: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Sep 13 12:27:16.756: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:27:18.775: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Sep 13 12:27:18.803: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 12:27:18.808: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 12:27:20.809: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 12:27:20.819: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 12:27:22.809: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 12:27:22.821: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:22.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8817" for this suite.

• [SLOW TEST:8.198 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5713,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:22.852: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-46
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 12:27:22.877: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 13 12:27:22.919: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:27:24.939: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:26.940: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:28.928: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:30.928: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:32.941: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:34.931: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:36.939: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:38.937: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:40.932: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:42.937: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 13 12:27:44.952: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 13 12:27:44.967: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 13 12:27:47.027: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 13 12:27:47.027: INFO: Breadth first check of 10.42.1.154 on host 192.168.1.5...
Sep 13 12:27:47.035: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.155:9080/dial?request=hostname&protocol=udp&host=10.42.1.154&port=8081&tries=1'] Namespace:pod-network-test-46 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 12:27:47.035: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 12:27:47.037: INFO: ExecWithOptions: Clientset creation
Sep 13 12:27:47.037: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-46/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.1.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.1.154%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Sep 13 12:27:47.204: INFO: Waiting for responses: map[]
Sep 13 12:27:47.204: INFO: reached 10.42.1.154 after 0/1 tries
Sep 13 12:27:47.204: INFO: Breadth first check of 10.42.0.119 on host 192.168.1.6...
Sep 13 12:27:47.216: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.155:9080/dial?request=hostname&protocol=udp&host=10.42.0.119&port=8081&tries=1'] Namespace:pod-network-test-46 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 13 12:27:47.216: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
Sep 13 12:27:47.217: INFO: ExecWithOptions: Clientset creation
Sep 13 12:27:47.217: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-46/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.1.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.0.119%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Sep 13 12:27:47.349: INFO: Waiting for responses: map[]
Sep 13 12:27:47.349: INFO: reached 10.42.0.119 after 0/1 tries
Sep 13 12:27:47.349: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:47.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-46" for this suite.

• [SLOW TEST:24.517 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5721,"failed":0}
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 12:27:47.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:27:47.457: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl is running 0 daemon pod, expected 1
Sep 13 12:27:48.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 13 12:27:48.480: INFO: Node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom is running 0 daemon pod, expected 1
Sep 13 12:27:49.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 13 12:27:49.475: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Sep 13 12:27:49.496: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Sep 13 12:27:49.507: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Sep 13 12:27:49.511: INFO: Observed &DaemonSet event: ADDED
Sep 13 12:27:49.511: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.512: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.512: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.512: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.513: INFO: Found daemon set daemon-set in namespace daemonsets-5984 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 13 12:27:49.513: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Sep 13 12:27:49.522: INFO: Observed &DaemonSet event: ADDED
Sep 13 12:27:49.523: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.523: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.523: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.523: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.524: INFO: Observed daemon set daemon-set in namespace daemonsets-5984 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 13 12:27:49.524: INFO: Observed &DaemonSet event: MODIFIED
Sep 13 12:27:49.524: INFO: Found daemon set daemon-set in namespace daemonsets-5984 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep 13 12:27:49.524: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5984, will wait for the garbage collector to delete the pods
Sep 13 12:27:49.596: INFO: Deleting DaemonSet.extensions daemon-set took: 12.008038ms
Sep 13 12:27:49.697: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.053624ms
Sep 13 12:27:52.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 13 12:27:52.212: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 13 12:27:52.224: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"423533"},"items":null}

Sep 13 12:27:52.232: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"423533"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5984" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":295,"skipped":5721,"failed":0}
SSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:52.268: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
Sep 13 12:27:52.305: INFO: Waiting up to 5m0s for pod "client-containers-10c20592-a82c-48a0-9212-088df986ef8d" in namespace "containers-1848" to be "Succeeded or Failed"
Sep 13 12:27:52.312: INFO: Pod "client-containers-10c20592-a82c-48a0-9212-088df986ef8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.679666ms
Sep 13 12:27:54.332: INFO: Pod "client-containers-10c20592-a82c-48a0-9212-088df986ef8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026835368s
Sep 13 12:27:56.346: INFO: Pod "client-containers-10c20592-a82c-48a0-9212-088df986ef8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041164702s
STEP: Saw pod success
Sep 13 12:27:56.346: INFO: Pod "client-containers-10c20592-a82c-48a0-9212-088df986ef8d" satisfied condition "Succeeded or Failed"
Sep 13 12:27:56.354: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod client-containers-10c20592-a82c-48a0-9212-088df986ef8d container agnhost-container: <nil>
STEP: delete the pod
Sep 13 12:27:56.400: INFO: Waiting for pod client-containers-10c20592-a82c-48a0-9212-088df986ef8d to disappear
Sep 13 12:27:56.409: INFO: Pod client-containers-10c20592-a82c-48a0-9212-088df986ef8d no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:56.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1848" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:56.433: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:27:56.912: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 13 12:27:56.914: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 13 12:27:56.914: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep 13 12:27:56.914: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 13 12:27:56.914: INFO: Checking APIGroup: apps
Sep 13 12:27:56.916: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 13 12:27:56.916: INFO: Versions found [{apps/v1 v1}]
Sep 13 12:27:56.916: INFO: apps/v1 matches apps/v1
Sep 13 12:27:56.916: INFO: Checking APIGroup: events.k8s.io
Sep 13 12:27:56.919: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 13 12:27:56.919: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep 13 12:27:56.919: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 13 12:27:56.919: INFO: Checking APIGroup: authentication.k8s.io
Sep 13 12:27:56.921: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 13 12:27:56.921: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep 13 12:27:56.921: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 13 12:27:56.921: INFO: Checking APIGroup: authorization.k8s.io
Sep 13 12:27:56.923: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 13 12:27:56.923: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep 13 12:27:56.923: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 13 12:27:56.923: INFO: Checking APIGroup: autoscaling
Sep 13 12:27:56.926: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep 13 12:27:56.926: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep 13 12:27:56.926: INFO: autoscaling/v2 matches autoscaling/v2
Sep 13 12:27:56.926: INFO: Checking APIGroup: batch
Sep 13 12:27:56.929: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 13 12:27:56.929: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep 13 12:27:56.929: INFO: batch/v1 matches batch/v1
Sep 13 12:27:56.929: INFO: Checking APIGroup: certificates.k8s.io
Sep 13 12:27:56.931: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 13 12:27:56.931: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep 13 12:27:56.931: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 13 12:27:56.931: INFO: Checking APIGroup: networking.k8s.io
Sep 13 12:27:56.933: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 13 12:27:56.933: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep 13 12:27:56.933: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 13 12:27:56.933: INFO: Checking APIGroup: policy
Sep 13 12:27:56.935: INFO: PreferredVersion.GroupVersion: policy/v1
Sep 13 12:27:56.935: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Sep 13 12:27:56.935: INFO: policy/v1 matches policy/v1
Sep 13 12:27:56.935: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 13 12:27:56.937: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 13 12:27:56.937: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep 13 12:27:56.937: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 13 12:27:56.937: INFO: Checking APIGroup: storage.k8s.io
Sep 13 12:27:56.939: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 13 12:27:56.939: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 13 12:27:56.939: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 13 12:27:56.939: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 13 12:27:56.941: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 13 12:27:56.941: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep 13 12:27:56.941: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 13 12:27:56.941: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 13 12:27:56.943: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 13 12:27:56.943: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep 13 12:27:56.943: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 13 12:27:56.943: INFO: Checking APIGroup: scheduling.k8s.io
Sep 13 12:27:56.946: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 13 12:27:56.946: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep 13 12:27:56.946: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 13 12:27:56.946: INFO: Checking APIGroup: coordination.k8s.io
Sep 13 12:27:56.948: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 13 12:27:56.948: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep 13 12:27:56.948: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 13 12:27:56.948: INFO: Checking APIGroup: node.k8s.io
Sep 13 12:27:56.950: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep 13 12:27:56.950: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Sep 13 12:27:56.950: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep 13 12:27:56.950: INFO: Checking APIGroup: discovery.k8s.io
Sep 13 12:27:56.953: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep 13 12:27:56.953: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Sep 13 12:27:56.953: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep 13 12:27:56.953: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep 13 12:27:56.956: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Sep 13 12:27:56.956: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Sep 13 12:27:56.956: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Sep 13 12:27:56.956: INFO: Checking APIGroup: helm.cattle.io
Sep 13 12:27:56.958: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
Sep 13 12:27:56.958: INFO: Versions found [{helm.cattle.io/v1 v1}]
Sep 13 12:27:56.958: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
Sep 13 12:27:56.958: INFO: Checking APIGroup: k3s.cattle.io
Sep 13 12:27:56.961: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
Sep 13 12:27:56.961: INFO: Versions found [{k3s.cattle.io/v1 v1}]
Sep 13 12:27:56.961: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:27:56.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9719" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":297,"skipped":5756,"failed":0}

------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:27:56.984: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:28:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1371" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":298,"skipped":5756,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:28:01.158: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:28:01.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9564" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":299,"skipped":5766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:28:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:28:29.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9193" for this suite.

• [SLOW TEST:28.163 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":300,"skipped":5788,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:28:29.379: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 13 12:28:29.423: INFO: Waiting up to 5m0s for pod "pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f" in namespace "emptydir-3508" to be "Succeeded or Failed"
Sep 13 12:28:29.428: INFO: Pod "pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.543476ms
Sep 13 12:28:31.445: INFO: Pod "pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02135161s
Sep 13 12:28:33.456: INFO: Pod "pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03310654s
STEP: Saw pod success
Sep 13 12:28:33.456: INFO: Pod "pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f" satisfied condition "Succeeded or Failed"
Sep 13 12:28:33.461: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f container test-container: <nil>
STEP: delete the pod
Sep 13 12:28:33.497: INFO: Waiting for pod pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f to disappear
Sep 13 12:28:33.505: INFO: Pod pod-7e7f1c78-e84f-4bee-87f8-6297528e6f3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:28:33.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3508" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5799,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:28:33.520: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 13 12:28:33.546: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 12:28:33.555: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 12:28:33.559: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl before test
Sep 13 12:28:33.565: INFO: sonobuoy-e2e-job-7e148db81c5d4b8a from sonobuoy started at 2022-09-13 11:11:39 +0000 UTC (2 container statuses recorded)
Sep 13 12:28:33.565: INFO: 	Container e2e ready: true, restart count 0
Sep 13 12:28:33.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 12:28:33.565: INFO: civo-csi-node-ln7f9 from kube-system started at 2022-09-13 12:08:39 +0000 UTC (2 container statuses recorded)
Sep 13 12:28:33.565: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 12:28:33.565: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 12:28:33.565: INFO: sonobuoy from sonobuoy started at 2022-09-13 11:11:38 +0000 UTC (1 container statuses recorded)
Sep 13 12:28:33.565: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 13 12:28:33.565: INFO: 
Logging pods the apiserver thinks is on node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom before test
Sep 13 12:28:33.570: INFO: civo-csi-controller-0 from kube-system started at 2022-09-09 17:03:22 +0000 UTC (4 container statuses recorded)
Sep 13 12:28:33.570: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 12:28:33.570: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 13 12:28:33.570: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 13 12:28:33.570: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 13 12:28:33.570: INFO: civo-csi-node-62s5w from kube-system started at 2022-09-09 16:30:36 +0000 UTC (2 container statuses recorded)
Sep 13 12:28:33.570: INFO: 	Container civo-csi-plugin ready: true, restart count 0
Sep 13 12:28:33.570: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Sep 13 12:28:33.570: INFO: civo-ccm-869574f9b7-sjssw from kube-system started at 2022-09-09 16:30:24 +0000 UTC (1 container statuses recorded)
Sep 13 12:28:33.570: INFO: 	Container civo-ccm ready: true, restart count 1
Sep 13 12:28:33.571: INFO: coredns-d76bd69b-h4czc from kube-system started at 2022-09-09 16:31:40 +0000 UTC (1 container statuses recorded)
Sep 13 12:28:33.571: INFO: 	Container coredns ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9c1e2727-5805-480d-a05f-9a3fa1570d07 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.1.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-9c1e2727-5805-480d-a05f-9a3fa1570d07 off the node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9c1e2727-5805-480d-a05f-9a3fa1570d07
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:37.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6529" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.262 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":302,"skipped":5801,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:37.783: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 13 12:33:37.866: INFO: starting watch
STEP: patching
STEP: updating
Sep 13 12:33:37.878: INFO: waiting for watch events with expected annotations
Sep 13 12:33:37.878: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:37.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7278" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":303,"skipped":5803,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:37.927: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:33:37.996: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8ead465a-1454-4f24-9a25-8ee8290cef56", Controller:(*bool)(0xc005259b96), BlockOwnerDeletion:(*bool)(0xc005259b97)}}
Sep 13 12:33:38.006: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"98c7dcd0-5a97-42ce-bfa4-426661b82f5e", Controller:(*bool)(0xc002638042), BlockOwnerDeletion:(*bool)(0xc002638043)}}
Sep 13 12:33:38.010: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e560c943-ce34-458d-9493-e85bfc376dd0", Controller:(*bool)(0xc005259e36), BlockOwnerDeletion:(*bool)(0xc005259e37)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:43.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-504" for this suite.

• [SLOW TEST:5.109 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":304,"skipped":5810,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:43.038: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 13 12:33:43.722: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 13 12:33:46.757: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:33:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-23-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:50.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3440" for this suite.
STEP: Destroying namespace "webhook-3440-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.076 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":305,"skipped":5810,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:50.115: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:50.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9803" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":306,"skipped":5817,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:50.212: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:50.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-545" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":307,"skipped":5822,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-d07c40a3-97a5-4680-b2d5-404fd55847ea
STEP: Creating configMap with name cm-test-opt-upd-6a1ffb22-fe06-4d1c-a0c8-bd95509b7b43
STEP: Creating the pod
Sep 13 12:33:50.378: INFO: The status of Pod pod-projected-configmaps-4c656a40-9b39-410e-850c-26cc2b18ccb4 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:33:52.388: INFO: The status of Pod pod-projected-configmaps-4c656a40-9b39-410e-850c-26cc2b18ccb4 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-d07c40a3-97a5-4680-b2d5-404fd55847ea
STEP: Updating configmap cm-test-opt-upd-6a1ffb22-fe06-4d1c-a0c8-bd95509b7b43
STEP: Creating configMap with name cm-test-opt-create-cad02561-65e0-473f-9e62-fcc9f1cc562c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:33:54.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7107" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5832,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:33:54.576: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:33:54.612: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:34:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8585" for this suite.

• [SLOW TEST:6.416 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":309,"skipped":5840,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:34:00.992: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-b1cff8c0-6a79-4026-9ebe-2df629d37c54
STEP: Creating a pod to test consume secrets
Sep 13 12:34:01.046: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a" in namespace "projected-4559" to be "Succeeded or Failed"
Sep 13 12:34:01.049: INFO: Pod "pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162673ms
Sep 13 12:34:03.069: INFO: Pod "pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023033387s
Sep 13 12:34:05.080: INFO: Pod "pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033754954s
STEP: Saw pod success
Sep 13 12:34:05.080: INFO: Pod "pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a" satisfied condition "Succeeded or Failed"
Sep 13 12:34:05.085: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 12:34:05.108: INFO: Waiting for pod pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a to disappear
Sep 13 12:34:05.112: INFO: Pod pod-projected-secrets-d9f64bcd-48ba-49d0-b3ee-282efdc75f7a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:34:05.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4559" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:34:05.132: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:34:05.197: INFO: created pod
Sep 13 12:34:05.197: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-431" to be "Succeeded or Failed"
Sep 13 12:34:05.204: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.101929ms
Sep 13 12:34:07.225: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028591155s
Sep 13 12:34:09.240: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042930265s
STEP: Saw pod success
Sep 13 12:34:09.240: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep 13 12:34:39.241: INFO: polling logs
Sep 13 12:34:39.257: INFO: Pod logs: 
2022/09/13 12:34:05 OK: Got token
2022/09/13 12:34:05 validating with in-cluster discovery
2022/09/13 12:34:05 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/09/13 12:34:05 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-431:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1663073046, NotBefore:1663072446, IssuedAt:1663072446, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-431", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8dfd42ac-a976-4b74-9e9a-16bc089d127e"}}}
2022/09/13 12:34:06 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/09/13 12:34:06 OK: Validated signature on JWT
2022/09/13 12:34:06 OK: Got valid claims from token!
2022/09/13 12:34:06 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-431:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1663073046, NotBefore:1663072446, IssuedAt:1663072446, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-431", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8dfd42ac-a976-4b74-9e9a-16bc089d127e"}}}

Sep 13 12:34:39.257: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:34:39.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-431" for this suite.

• [SLOW TEST:34.168 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":311,"skipped":5876,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:34:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Sep 13 12:34:39.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-8193 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Sep 13 12:34:39.457: INFO: stderr: ""
Sep 13 12:34:39.457: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
Sep 13 12:34:39.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-8193 delete pods e2e-test-httpd-pod'
Sep 13 12:34:41.781: INFO: stderr: ""
Sep 13 12:34:41.781: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:34:41.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8193" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":312,"skipped":5893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:34:41.799: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:34:52.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9710" for this suite.

• [SLOW TEST:11.148 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":313,"skipped":5921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:34:52.948: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-9252
STEP: creating replication controller nodeport-test in namespace services-9252
I0913 12:34:53.022636      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9252, replica count: 2
Sep 13 12:34:56.074: INFO: Creating new exec pod
I0913 12:34:56.074369      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 12:34:59.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 13 12:34:59.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 13 12:34:59.378: INFO: stdout: ""
Sep 13 12:35:00.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 13 12:35:00.617: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 13 12:35:00.617: INFO: stdout: ""
Sep 13 12:35:01.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 13 12:35:01.632: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 13 12:35:01.632: INFO: stdout: ""
Sep 13 12:35:02.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 13 12:35:02.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 13 12:35:02.639: INFO: stdout: "nodeport-test-4czv8"
Sep 13 12:35:02.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.224.225 80'
Sep 13 12:35:02.907: INFO: stderr: "+ echo+ nc -v -t -w 2 10.43.224.225 80\n hostName\nConnection to 10.43.224.225 80 port [tcp/http] succeeded!\n"
Sep 13 12:35:02.907: INFO: stdout: "nodeport-test-hvt4v"
Sep 13 12:35:02.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.6 31604'
Sep 13 12:35:03.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.6 31604\nConnection to 192.168.1.6 31604 port [tcp/*] succeeded!\n"
Sep 13 12:35:03.171: INFO: stdout: ""
Sep 13 12:35:04.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.6 31604'
Sep 13 12:35:04.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.6 31604\nConnection to 192.168.1.6 31604 port [tcp/*] succeeded!\n"
Sep 13 12:35:04.441: INFO: stdout: "nodeport-test-hvt4v"
Sep 13 12:35:04.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-9252 exec execpodlrszv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.5 31604'
Sep 13 12:35:04.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.5 31604\nConnection to 192.168.1.5 31604 port [tcp/*] succeeded!\n"
Sep 13 12:35:04.662: INFO: stdout: "nodeport-test-4czv8"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:35:04.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9252" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.734 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":314,"skipped":5945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:35:04.684: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 13 12:35:04.733: INFO: Waiting up to 5m0s for pod "pod-45d63044-05f5-4153-a047-04c3c5d65512" in namespace "emptydir-3024" to be "Succeeded or Failed"
Sep 13 12:35:04.736: INFO: Pod "pod-45d63044-05f5-4153-a047-04c3c5d65512": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227574ms
Sep 13 12:35:06.753: INFO: Pod "pod-45d63044-05f5-4153-a047-04c3c5d65512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019848991s
Sep 13 12:35:08.765: INFO: Pod "pod-45d63044-05f5-4153-a047-04c3c5d65512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03215383s
STEP: Saw pod success
Sep 13 12:35:08.765: INFO: Pod "pod-45d63044-05f5-4153-a047-04c3c5d65512" satisfied condition "Succeeded or Failed"
Sep 13 12:35:08.773: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-45d63044-05f5-4153-a047-04c3c5d65512 container test-container: <nil>
STEP: delete the pod
Sep 13 12:35:08.812: INFO: Waiting for pod pod-45d63044-05f5-4153-a047-04c3c5d65512 to disappear
Sep 13 12:35:08.819: INFO: Pod pod-45d63044-05f5-4153-a047-04c3c5d65512 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:35:08.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3024" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:35:08.839: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 13 12:35:08.934: INFO: Waiting up to 5m0s for pod "pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3" in namespace "emptydir-9461" to be "Succeeded or Failed"
Sep 13 12:35:08.940: INFO: Pod "pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782843ms
Sep 13 12:35:10.947: INFO: Pod "pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3": Phase="Running", Reason="", readiness=false. Elapsed: 2.012834965s
Sep 13 12:35:12.958: INFO: Pod "pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023843955s
STEP: Saw pod success
Sep 13 12:35:12.958: INFO: Pod "pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3" satisfied condition "Succeeded or Failed"
Sep 13 12:35:12.963: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3 container test-container: <nil>
STEP: delete the pod
Sep 13 12:35:12.990: INFO: Waiting for pod pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3 to disappear
Sep 13 12:35:12.999: INFO: Pod pod-7cfeca3a-330f-460c-b2d7-a8a785113ea3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:35:12.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9461" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":6019,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:35:13.019: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Sep 13 12:35:13.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7522 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep 13 12:35:13.193: INFO: stderr: ""
Sep 13 12:35:13.193: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep 13 12:35:13.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7522 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Sep 13 12:35:14.210: INFO: stderr: ""
Sep 13 12:35:14.210: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Sep 13 12:35:14.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-7522 delete pods e2e-test-httpd-pod'
Sep 13 12:35:16.933: INFO: stderr: ""
Sep 13 12:35:16.933: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:35:16.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7522" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":317,"skipped":6022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:35:16.949: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:35:16.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-66" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":318,"skipped":6054,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:35:16.999: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5147
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Sep 13 12:35:17.044: INFO: Found 0 stateful pods, waiting for 3
Sep 13 12:35:27.056: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:35:27.056: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:35:27.056: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:35:27.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-5147 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 12:35:27.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 12:35:27.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 12:35:27.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Sep 13 12:35:37.376: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 13 12:35:47.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-5147 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 12:35:47.704: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 12:35:47.704: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 12:35:47.704: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Sep 13 12:35:57.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-5147 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 13 12:35:58.032: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 13 12:35:58.032: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 13 12:35:58.032: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 13 12:36:08.107: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 13 12:36:18.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=statefulset-5147 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 13 12:36:18.706: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 13 12:36:18.706: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 13 12:36:18.706: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 12:36:28.762: INFO: Deleting all statefulset in ns statefulset-5147
Sep 13 12:36:28.769: INFO: Scaling statefulset ss2 to 0
Sep 13 12:36:38.816: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 12:36:38.823: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:36:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5147" for this suite.

• [SLOW TEST:81.878 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":319,"skipped":6057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:36:38.879: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:36:38.920: INFO: The status of Pod busybox-readonly-fs5bbac73a-4840-407d-a9cc-1a5a59d49073 is Pending, waiting for it to be Running (with Ready = true)
Sep 13 12:36:40.928: INFO: The status of Pod busybox-readonly-fs5bbac73a-4840-407d-a9cc-1a5a59d49073 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:36:40.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8865" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":6101,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:36:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:36:41.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6025" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":321,"skipped":6122,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:36:41.073: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
Sep 13 12:36:41.118: INFO: Waiting up to 5m0s for pod "var-expansion-811ae30d-b036-4673-9f32-c165c673db02" in namespace "var-expansion-3378" to be "Succeeded or Failed"
Sep 13 12:36:41.131: INFO: Pod "var-expansion-811ae30d-b036-4673-9f32-c165c673db02": Phase="Pending", Reason="", readiness=false. Elapsed: 12.479659ms
Sep 13 12:36:43.159: INFO: Pod "var-expansion-811ae30d-b036-4673-9f32-c165c673db02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04047801s
Sep 13 12:36:45.183: INFO: Pod "var-expansion-811ae30d-b036-4673-9f32-c165c673db02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064399263s
STEP: Saw pod success
Sep 13 12:36:45.183: INFO: Pod "var-expansion-811ae30d-b036-4673-9f32-c165c673db02" satisfied condition "Succeeded or Failed"
Sep 13 12:36:45.190: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod var-expansion-811ae30d-b036-4673-9f32-c165c673db02 container dapi-container: <nil>
STEP: delete the pod
Sep 13 12:36:45.232: INFO: Waiting for pod var-expansion-811ae30d-b036-4673-9f32-c165c673db02 to disappear
Sep 13 12:36:45.243: INFO: Pod var-expansion-811ae30d-b036-4673-9f32-c165c673db02 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:36:45.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3378" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":322,"skipped":6124,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:36:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 13 12:36:45.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425233 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 12:36:45.310: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425233 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 13 12:36:45.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425234 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 12:36:45.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425234 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 13 12:36:45.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425235 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 12:36:45.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425235 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 13 12:36:45.347: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425236 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 12:36:45.347: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3274  381cc434-7afe-4202-b2fa-467887b49796 425236 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 13 12:36:45.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3274  05ad58d1-88d1-4c1e-b361-17da8c2b861a 425237 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 12:36:45.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3274  05ad58d1-88d1-4c1e-b361-17da8c2b861a 425237 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 13 12:36:55.390: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3274  05ad58d1-88d1-4c1e-b361-17da8c2b861a 425272 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 13 12:36:55.390: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3274  05ad58d1-88d1-4c1e-b361-17da8c2b861a 425272 0 2022-09-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-13 12:36:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:37:05.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3274" for this suite.

• [SLOW TEST:20.178 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":323,"skipped":6125,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:37:05.438: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Sep 13 12:37:07.554: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:37:09.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8531" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":324,"skipped":6129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:37:09.655: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Sep 13 12:37:11.736: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Sep 13 12:37:15.887: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:37:17.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1437" for this suite.

• [SLOW TEST:8.350 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":325,"skipped":6181,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:37:18.005: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-1ec9fa7a-11b7-4cc1-b037-fc11f31a914c
STEP: Creating a pod to test consume configMaps
Sep 13 12:37:18.133: INFO: Waiting up to 5m0s for pod "pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a" in namespace "configmap-5021" to be "Succeeded or Failed"
Sep 13 12:37:18.142: INFO: Pod "pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35111ms
Sep 13 12:37:20.157: INFO: Pod "pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023513866s
Sep 13 12:37:22.177: INFO: Pod "pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043359459s
STEP: Saw pod success
Sep 13 12:37:22.177: INFO: Pod "pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a" satisfied condition "Succeeded or Failed"
Sep 13 12:37:22.183: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a container agnhost-container: <nil>
STEP: delete the pod
Sep 13 12:37:22.219: INFO: Waiting for pod pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a to disappear
Sep 13 12:37:22.225: INFO: Pod pod-configmaps-e76631c4-aff9-4244-95e1-04818a2ba63a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:37:22.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5021" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6184,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:37:22.243: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:00.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7470" for this suite.

• [SLOW TEST:338.149 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":327,"skipped":6187,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:00.394: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
Sep 13 12:43:00.429: INFO: Waiting up to 5m0s for pod "test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0" in namespace "svcaccounts-5805" to be "Succeeded or Failed"
Sep 13 12:43:00.433: INFO: Pod "test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51004ms
Sep 13 12:43:02.454: INFO: Pod "test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02432927s
Sep 13 12:43:04.476: INFO: Pod "test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046632133s
STEP: Saw pod success
Sep 13 12:43:04.476: INFO: Pod "test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0" satisfied condition "Succeeded or Failed"
Sep 13 12:43:04.483: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 12:43:04.536: INFO: Waiting for pod test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0 to disappear
Sep 13 12:43:04.543: INFO: Pod test-pod-07ce2789-c52d-4949-88fc-3cb4d8a55bc0 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:04.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5805" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":328,"skipped":6200,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:04.560: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:43:04.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 create -f -'
Sep 13 12:43:04.926: INFO: stderr: ""
Sep 13 12:43:04.926: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 13 12:43:04.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 create -f -'
Sep 13 12:43:05.213: INFO: stderr: ""
Sep 13 12:43:05.213: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 13 12:43:06.227: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:43:06.227: INFO: Found 0 / 1
Sep 13 12:43:07.233: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:43:07.233: INFO: Found 1 / 1
Sep 13 12:43:07.233: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 13 12:43:07.241: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 13 12:43:07.241: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 13 12:43:07.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 describe pod agnhost-primary-gwtmj'
Sep 13 12:43:07.418: INFO: stderr: ""
Sep 13 12:43:07.418: INFO: stdout: "Name:         agnhost-primary-gwtmj\nNamespace:    kubectl-2143\nPriority:     0\nNode:         k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl/192.168.1.5\nStart Time:   Tue, 13 Sep 2022 12:43:04 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.42.1.187\nIPs:\n  IP:           10.42.1.187\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://be57e079b1b8a7e59cb18cf46c1a06e2a4bb2a49d94ba474c7a56dc94e855312\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 13 Sep 2022 12:43:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mvfhr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-mvfhr:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2143/agnhost-primary-gwtmj to k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Sep 13 12:43:07.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 describe rc agnhost-primary'
Sep 13 12:43:07.572: INFO: stderr: ""
Sep 13 12:43:07.572: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2143\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-gwtmj\n"
Sep 13 12:43:07.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 describe service agnhost-primary'
Sep 13 12:43:07.705: INFO: stderr: ""
Sep 13 12:43:07.705: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2143\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.93.20\nIPs:               10.43.93.20\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.1.187:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 13 12:43:07.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 describe node k3s-conformance-test-12310-81a66c-node-pool-5423-44vom'
Sep 13 12:43:07.893: INFO: stderr: ""
Sep 13 12:43:07.893: INFO: stdout: "Name:               k3s-conformance-test-12310-81a66c-node-pool-5423-44vom\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g4s.kube.large\n                    beta.kubernetes.io/os=linux\n                    k3s.io/external-ip=74.220.26.112\n                    kubernetes.civo.com/civo-node-pool=6dd7be7c-f019-4a48-b429-5037f8d3f584\n                    kubernetes.civo.com/civo-node-size=g4s.kube.large\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k3s-conformance-test-12310-81a66c-node-pool-5423-44vom\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=g4s.kube.large\n                    region=fra1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.civo.com\":\"ac3176fe-7f93-400a-938f-afac13d319f7\"}\n                    flannel.alpha.coreos.com/backend-data: null\n                    flannel.alpha.coreos.com/backend-type: host-gw\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.6\n                    k3s.io/node-args:\n                      [\"agent\",\"--server\",\"https://74.220.26.112:6443\",\"--token\",\"********\",\"--node-label\",\"kubernetes.civo.com/civo-node-pool=6dd7be7c-f019-4a4...\n                    k3s.io/node-config-hash: WLAJY5FBHP4K2QH2AW7ZR5Y7OJTPSLGUV4753QLUHRZXN7FG3W4Q====\n                    k3s.io/node-env: {\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/8c2b0191f6e36ec6f3cb68e2302fcc4be850c6db31ec5f8a74e4b3be403101d8\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 09 Sep 2022 16:30:20 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k3s-conformance-test-12310-81a66c-node-pool-5423-44vom\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 13 Sep 2022 12:43:01 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 13 Sep 2022 12:40:58 +0000   Tue, 13 Sep 2022 00:30:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 13 Sep 2022 12:40:58 +0000   Tue, 13 Sep 2022 00:30:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 13 Sep 2022 12:40:58 +0000   Tue, 13 Sep 2022 00:30:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 13 Sep 2022 12:40:58 +0000   Tue, 13 Sep 2022 00:30:16 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.6\n  Hostname:    k3s-conformance-test-12310-81a66c-node-pool-5423-44vom\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      58016408Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7770488Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      56438361659\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7770488Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 c7aeabd08f7f26ad6dbdf5bd631b6920\n  System UUID:                1959220a-450f-5e45-ad35-cee984f7b228\n  Boot ID:                    76c4ba99-e539-4966-a919-ca1598290b22\n  Kernel Version:             5.15.64-0-virt\n  OS Image:                   Alpine Linux v3.16\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.11-k3s2\n  Kubelet Version:            v1.23.6+k3s1\n  Kube-Proxy Version:         v1.23.6+k3s1\nPodCIDR:                      10.42.0.0/24\nPodCIDRs:                     10.42.0.0/24\nProviderID:                   civo://ac3176fe-7f93-400a-938f-afac13d319f7\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                         ------------  ----------  ---------------  -------------  ---\n  kube-system                 civo-csi-controller-0        0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d19h\n  kube-system                 civo-csi-node-62s5w          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 civo-ccm-869574f9b7-sjssw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 coredns-d76bd69b-h4czc       100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     3d20h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests   Limits\n  --------               --------   ------\n  cpu                    100m (2%)  0 (0%)\n  memory                 70Mi (0%)  170Mi (2%)\n  ephemeral-storage      0 (0%)     0 (0%)\n  hugepages-1Gi          0 (0%)     0 (0%)\n  hugepages-2Mi          0 (0%)     0 (0%)\n  scheduling.k8s.io/foo  0          0\nEvents:                  <none>\n"
Sep 13 12:43:07.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-2143 describe namespace kubectl-2143'
Sep 13 12:43:08.032: INFO: stderr: ""
Sep 13 12:43:08.032: INFO: stdout: "Name:         kubectl-2143\nLabels:       e2e-framework=kubectl\n              e2e-run=40abff15-449d-475b-824e-8301d7601346\n              kubernetes.io/metadata.name=kubectl-2143\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:08.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2143" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":329,"skipped":6206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:08.046: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:12.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3125" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":330,"skipped":6251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Sep 13 12:43:12.264: INFO: Waiting up to 5m0s for pod "security-context-b09ce77c-3d58-4396-a340-75cc32092f8c" in namespace "security-context-2926" to be "Succeeded or Failed"
Sep 13 12:43:12.269: INFO: Pod "security-context-b09ce77c-3d58-4396-a340-75cc32092f8c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.122992ms
Sep 13 12:43:14.292: INFO: Pod "security-context-b09ce77c-3d58-4396-a340-75cc32092f8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028224327s
Sep 13 12:43:16.305: INFO: Pod "security-context-b09ce77c-3d58-4396-a340-75cc32092f8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041541641s
STEP: Saw pod success
Sep 13 12:43:16.305: INFO: Pod "security-context-b09ce77c-3d58-4396-a340-75cc32092f8c" satisfied condition "Succeeded or Failed"
Sep 13 12:43:16.310: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod security-context-b09ce77c-3d58-4396-a340-75cc32092f8c container test-container: <nil>
STEP: delete the pod
Sep 13 12:43:16.337: INFO: Waiting for pod security-context-b09ce77c-3d58-4396-a340-75cc32092f8c to disappear
Sep 13 12:43:16.347: INFO: Pod security-context-b09ce77c-3d58-4396-a340-75cc32092f8c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:16.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2926" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":331,"skipped":6332,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:16.364: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Sep 13 12:43:16.394: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
Sep 13 12:43:16.777: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 13 12:43:18.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.September, 13, 12, 43, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 12, 43, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.September, 13, 12, 43, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.September, 13, 12, 43, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 12:43:21.126: INFO: Waited 250.578492ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Sep 13 12:43:21.270: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:21.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1877" for this suite.

• [SLOW TEST:5.595 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":332,"skipped":6340,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:21.960: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:43:21.998: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-da0851ed-0608-4335-be87-cfb4b631e0a5" in namespace "security-context-test-1521" to be "Succeeded or Failed"
Sep 13 12:43:22.001: INFO: Pod "busybox-privileged-false-da0851ed-0608-4335-be87-cfb4b631e0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.232388ms
Sep 13 12:43:24.018: INFO: Pod "busybox-privileged-false-da0851ed-0608-4335-be87-cfb4b631e0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020696463s
Sep 13 12:43:26.027: INFO: Pod "busybox-privileged-false-da0851ed-0608-4335-be87-cfb4b631e0a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028798693s
Sep 13 12:43:26.027: INFO: Pod "busybox-privileged-false-da0851ed-0608-4335-be87-cfb4b631e0a5" satisfied condition "Succeeded or Failed"
Sep 13 12:43:26.038: INFO: Got logs for pod "busybox-privileged-false-da0851ed-0608-4335-be87-cfb4b631e0a5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:26.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1521" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6350,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:26.054: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5300
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Sep 13 12:43:26.104: INFO: Found 0 stateful pods, waiting for 1
Sep 13 12:43:36.116: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Sep 13 12:43:36.163: INFO: Found 1 stateful pods, waiting for 2
Sep 13 12:43:46.176: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:43:46.176: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 12:43:46.219: INFO: Deleting all statefulset in ns statefulset-5300
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:46.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5300" for this suite.

• [SLOW TEST:20.203 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":334,"skipped":6358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:46.265: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
Sep 13 12:43:46.301: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Sep 13 12:43:48.323: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Sep 13 12:43:50.364: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:52.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9946" for this suite.

• [SLOW TEST:6.132 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":335,"skipped":6439,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:52.398: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 13 12:43:52.441: INFO: Waiting up to 5m0s for pod "pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3" in namespace "emptydir-226" to be "Succeeded or Failed"
Sep 13 12:43:52.444: INFO: Pod "pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.493596ms
Sep 13 12:43:54.466: INFO: Pod "pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025422653s
Sep 13 12:43:56.483: INFO: Pod "pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042058532s
STEP: Saw pod success
Sep 13 12:43:56.483: INFO: Pod "pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3" satisfied condition "Succeeded or Failed"
Sep 13 12:43:56.490: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3 container test-container: <nil>
STEP: delete the pod
Sep 13 12:43:56.524: INFO: Waiting for pod pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3 to disappear
Sep 13 12:43:56.533: INFO: Pod pod-bb02df74-06b2-4a43-a8ce-cbffa45c63c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:43:56.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-226" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6464,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:43:56.548: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3649
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3649
I0913 12:43:56.607231      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3649, replica count: 2
I0913 12:43:59.659798      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 12:43:59.659: INFO: Creating new exec pod
Sep 13 12:44:02.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3649 exec execpodw4mcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 13 12:44:03.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 13 12:44:03.044: INFO: stdout: "externalname-service-jqjd4"
Sep 13 12:44:03.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=services-3649 exec execpodw4mcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.58.131 80'
Sep 13 12:44:03.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.58.131 80\nConnection to 10.43.58.131 80 port [tcp/http] succeeded!\n"
Sep 13 12:44:03.282: INFO: stdout: "externalname-service-jqjd4"
Sep 13 12:44:03.282: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:03.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3649" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:6.776 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":337,"skipped":6484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:03.325: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
Sep 13 12:44:03.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-383953386 --namespace=kubectl-5067 api-versions'
Sep 13 12:44:03.447: INFO: stderr: ""
Sep 13 12:44:03.447: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd-publish-openapi-test-common-group.example.com/v6\ncrd-publish-openapi-test-multi-to-single-ver.example.com/v5\ncrd-publish-openapi-test-multi-ver.example.com/v2\ncrd-publish-openapi-test-multi-ver.example.com/v3\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:03.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5067" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":338,"skipped":6519,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:03.469: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Sep 13 12:44:03.516: INFO: Waiting up to 5m0s for pod "downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d" in namespace "downward-api-8727" to be "Succeeded or Failed"
Sep 13 12:44:03.523: INFO: Pod "downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.326765ms
Sep 13 12:44:05.547: INFO: Pod "downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030947032s
Sep 13 12:44:07.571: INFO: Pod "downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055072363s
STEP: Saw pod success
Sep 13 12:44:07.571: INFO: Pod "downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d" satisfied condition "Succeeded or Failed"
Sep 13 12:44:07.578: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d container dapi-container: <nil>
STEP: delete the pod
Sep 13 12:44:07.616: INFO: Waiting for pod downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d to disappear
Sep 13 12:44:07.622: INFO: Pod downward-api-4acc1685-4466-401f-94fc-3cb3f34ec66d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:07.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8727" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":339,"skipped":6531,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:07.637: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-6506f26f-5c26-472a-a6da-5ede83f570cd
STEP: Creating a pod to test consume configMaps
Sep 13 12:44:07.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3" in namespace "configmap-9599" to be "Succeeded or Failed"
Sep 13 12:44:07.706: INFO: Pod "pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033506ms
Sep 13 12:44:09.714: INFO: Pod "pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010794323s
Sep 13 12:44:11.730: INFO: Pod "pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02677098s
STEP: Saw pod success
Sep 13 12:44:11.730: INFO: Pod "pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3" satisfied condition "Succeeded or Failed"
Sep 13 12:44:11.735: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3 container agnhost-container: <nil>
STEP: delete the pod
Sep 13 12:44:11.768: INFO: Waiting for pod pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3 to disappear
Sep 13 12:44:11.777: INFO: Pod pod-configmaps-887628b6-84b8-4a3e-ae43-66d9b5f2edd3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:11.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9599" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6531,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:11.795: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9126 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9126;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9126 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9126;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9126.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9126.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9126.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9126.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9126.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9126.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9126.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9126.svc;check="$$(dig +notcp +noall +answer +search 118.214.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.214.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.214.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.214.118_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9126 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9126;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9126 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9126;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9126.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9126.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9126.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9126.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9126.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9126.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9126.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9126.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9126.svc;check="$$(dig +notcp +noall +answer +search 118.214.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.214.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.214.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.214.118_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 12:44:13.928: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.937: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.946: INFO: Unable to read wheezy_udp@dns-test-service.dns-9126 from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.956: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9126 from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.972: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.981: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:13.989: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.031: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.040: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.048: INFO: Unable to read jessie_udp@dns-test-service.dns-9126 from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-9126 from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.064: INFO: Unable to read jessie_udp@dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.071: INFO: Unable to read jessie_tcp@dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.079: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.086: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9126.svc from pod dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc: the server could not find the requested resource (get pods dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc)
Sep 13 12:44:14.117: INFO: Lookups using dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9126 wheezy_tcp@dns-test-service.dns-9126 wheezy_udp@dns-test-service.dns-9126.svc wheezy_tcp@dns-test-service.dns-9126.svc wheezy_udp@_http._tcp.dns-test-service.dns-9126.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9126.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9126 jessie_tcp@dns-test-service.dns-9126 jessie_udp@dns-test-service.dns-9126.svc jessie_tcp@dns-test-service.dns-9126.svc jessie_udp@_http._tcp.dns-test-service.dns-9126.svc jessie_tcp@_http._tcp.dns-test-service.dns-9126.svc]

Sep 13 12:44:19.293: INFO: DNS probes using dns-9126/dns-test-0b6f7987-b597-4f5d-8bf8-652aa54373cc succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:19.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9126" for this suite.

• [SLOW TEST:7.593 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":341,"skipped":6544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:19.389: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 13 12:44:19.425: INFO: Waiting up to 5m0s for pod "pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52" in namespace "emptydir-7017" to be "Succeeded or Failed"
Sep 13 12:44:19.430: INFO: Pod "pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.647425ms
Sep 13 12:44:21.444: INFO: Pod "pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018812609s
Sep 13 12:44:23.463: INFO: Pod "pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038102527s
STEP: Saw pod success
Sep 13 12:44:23.464: INFO: Pod "pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52" satisfied condition "Succeeded or Failed"
Sep 13 12:44:23.470: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52 container test-container: <nil>
STEP: delete the pod
Sep 13 12:44:23.510: INFO: Waiting for pod pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52 to disappear
Sep 13 12:44:23.516: INFO: Pod pod-5f4152a7-df4f-4845-b62b-b61e0b1b9f52 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:23.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7017" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":6577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Sep 13 12:44:23.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77" in namespace "projected-9611" to be "Succeeded or Failed"
Sep 13 12:44:23.614: INFO: Pod "downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003039ms
Sep 13 12:44:25.634: INFO: Pod "downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028092841s
Sep 13 12:44:27.656: INFO: Pod "downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049849963s
STEP: Saw pod success
Sep 13 12:44:27.656: INFO: Pod "downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77" satisfied condition "Succeeded or Failed"
Sep 13 12:44:27.662: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77 container client-container: <nil>
STEP: delete the pod
Sep 13 12:44:27.698: INFO: Waiting for pod downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77 to disappear
Sep 13 12:44:27.705: INFO: Pod downwardapi-volume-506cf64d-9b51-4753-9adf-d00accb28b77 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:27.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9611" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:27.722: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 13 12:44:27.773: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 13 12:44:27.783: INFO: starting watch
STEP: patching
STEP: updating
Sep 13 12:44:27.804: INFO: waiting for watch events with expected annotations
Sep 13 12:44:27.804: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:27.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6220" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":344,"skipped":6644,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:27.868: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-dd4bf660-7213-4aaf-973b-95645268121c
STEP: Creating a pod to test consume secrets
Sep 13 12:44:27.933: INFO: Waiting up to 5m0s for pod "pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890" in namespace "secrets-9115" to be "Succeeded or Failed"
Sep 13 12:44:27.938: INFO: Pod "pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863832ms
Sep 13 12:44:29.954: INFO: Pod "pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021097161s
Sep 13 12:44:31.963: INFO: Pod "pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030422195s
STEP: Saw pod success
Sep 13 12:44:31.963: INFO: Pod "pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890" satisfied condition "Succeeded or Failed"
Sep 13 12:44:31.971: INFO: Trying to get logs from node k3s-conformance-test-12310-81a66c-node-pool-5423-am6tl pod pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 12:44:32.002: INFO: Waiting for pod pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890 to disappear
Sep 13 12:44:32.008: INFO: Pod pod-secrets-27a3b00b-47e6-4d76-9e2b-407efb1fd890 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:44:32.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9115" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 13 12:44:32.023: INFO: >>> kubeConfig: /tmp/kubeconfig-383953386
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2687
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Sep 13 12:44:32.078: INFO: Found 0 stateful pods, waiting for 3
Sep 13 12:44:42.098: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:44:42.099: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:44:42.099: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Sep 13 12:44:42.145: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 13 12:44:52.229: INFO: Updating stateful set ss2
Sep 13 12:44:52.241: INFO: Waiting for Pod statefulset-2687/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Sep 13 12:45:02.333: INFO: Found 1 stateful pods, waiting for 3
Sep 13 12:45:12.355: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:45:12.356: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 12:45:12.356: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 13 12:45:12.402: INFO: Updating stateful set ss2
Sep 13 12:45:12.414: INFO: Waiting for Pod statefulset-2687/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Sep 13 12:45:22.477: INFO: Updating stateful set ss2
Sep 13 12:45:22.491: INFO: Waiting for StatefulSet statefulset-2687/ss2 to complete update
Sep 13 12:45:22.491: INFO: Waiting for Pod statefulset-2687/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Sep 13 12:45:32.523: INFO: Deleting all statefulset in ns statefulset-2687
Sep 13 12:45:32.528: INFO: Scaling statefulset ss2 to 0
Sep 13 12:45:42.585: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 12:45:42.592: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 13 12:45:42.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2687" for this suite.

• [SLOW TEST:70.633 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":346,"skipped":6692,"failed":0}
SSSSSSSep 13 12:45:42.656: INFO: Running AfterSuite actions on all nodes
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Sep 13 12:45:42.656: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Sep 13 12:45:42.656: INFO: Running AfterSuite actions on node 1
Sep 13 12:45:42.656: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6698,"failed":0}

Ran 346 of 7044 Specs in 5639.682 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6698 Skipped
PASS

Ginkgo ran 1 suite in 1h34m2.122747677s
Test Suite Passed
